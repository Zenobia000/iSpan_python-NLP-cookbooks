{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 下載訓練資料(這邊使用 wiki Data)\n",
    "## 主要以 pages-articles.xml.bz2 結尾之檔案類型\n",
    "* 維基資料集: https://zh.wikipedia.org/wiki/Wikipedia:%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%8B%E8%BD%BD\n",
    "* 這邊以　[zhwiki-latest-pages-meta-current.xml.bz2　為訓練集](https://dumps.wikimedia.org/zhwiki/latest/)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 將 Wiki 資料轉為 txt(請先下載 zhwiki-latest-pages-meta-current.xml.bz2 至同一個目錄下)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m      6\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(action \u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m, category \u001b[38;5;241m=\u001b[39m \u001b[38;5;167;01mUserWarning\u001b[39;00m, module \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgensim\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpora\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WikiCorpus\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# 將 wiki 資料集載下後進行 xml convert to txt \u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mWiki_to_txt\u001b[39;00m(\u001b[38;5;28mobject\u001b[39m):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "__author__ = \"ALEX-CHUN-YU (P76064538@mail.ncku.edu.tw)\"\n",
    "import logging\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings(action ='ignore', category = UserWarning, module = 'gensim')\n",
    "from gensim.corpora import WikiCorpus\n",
    "\n",
    "# 將 wiki 資料集載下後進行 xml convert to txt \n",
    "class Wiki_to_txt(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        # 用默認 Formatter 為日誌系統建立一個 StreamHandler ，設置基礎配置並加到 root logger 中\n",
    "        logging.basicConfig(format = '%(asctime)s : %(levelname)s : %(message)s', level = logging.INFO)\n",
    "\n",
    "    # 使用方法 https://radimrehurek.com/gensim/corpora/wikicorpus.html\n",
    "    def set_wiki_to_txt(self, wiki_data_path = None):\n",
    "        if wiki_data_path == None:\n",
    "            # 系統下參數\n",
    "            if len(sys.argv) != 2:\n",
    "                print(\"Please Usage: python3 \" + sys.argv[0] + \" wiki_data_path\")\n",
    "                exit()\n",
    "            else:\n",
    "                wiki_corpus = WikiCorpus(sys.argv[1], dictionary = {})\n",
    "        else:\n",
    "            wiki_corpus = WikiCorpus(wiki_data_path, dictionary = {})\n",
    "        # wiki.xml convert to wiki.txt\n",
    "        with open(\"wiki_text.txt\", 'w', encoding = 'utf-8') as output:\n",
    "            text_count = 0\n",
    "            for text in wiki_corpus.get_texts():\n",
    "                # save use string(gensim)\n",
    "                output.write(' '.join(text) + '\\n')\n",
    "                text_count += 1\n",
    "                if text_count % 10000 == 0:\n",
    "                    logging.info(\"目前已處理 %d 篇文章\" % text_count)\n",
    "            print(\"轉檔完畢!\")\n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    wiki_to_txt = Wiki_to_txt()\n",
    "    # 將 wiki xml 轉換成 wiki txt\n",
    "    wiki_to_txt.set_wiki_to_txt(\"zhwiki-latest-pages-meta-current.xml.bz2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 簡體轉繁體 -> 進行斷詞 -> 過濾[停用詞](https://github.com/Alex-CHUN-YU/Word2vec/blob/master/stopwords.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'base' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n base ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "__author__ = \"ALEX-CHUN-YU (P76064538@mail.ncku.edu.tw)\"\n",
    "import jieba\n",
    "import logging\n",
    "from hanziconv import HanziConv\n",
    "\n",
    "# 進行斷詞並過濾 stopword\n",
    "class Segmentation(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        # 用默認 Formatter 為日誌系統建立一個 StreamHandler ，設置基礎配置並加到 root logger 中\n",
    "        logging.basicConfig(format = \"%(asctime)s : %(levelname)s : %(message)s\", level = logging.INFO)\n",
    "        self.stopwordset = set()\n",
    "        \n",
    "    # 讀取 stopword 辭典，並存到 stopwordset\n",
    "    def set_stopword(self):\n",
    "        with open(\"stopwords.txt\", \"r\", encoding = \"utf-8\") as stopwords:\n",
    "            for stopword in stopwords:\n",
    "                self.stopwordset.add(stopword.strip('\\n'))\n",
    "        #print(self.stopwordset)\n",
    "        print(\"StopWord Set 已儲存!\")\n",
    "\n",
    "    # 簡 to 繁\n",
    "    def simplified_to_traditional(self):\n",
    "        logging.info(\"等待中..(簡 to 繁)\")\n",
    "        traditional = open(\"traditional.txt\", \"w\", encoding = \"utf-8\")\n",
    "        with open(\"wiki_text.txt\", \"r\", encoding = \"utf-8\") as simplified:\n",
    "            for s in simplified:\n",
    "                traditional.write(HanziConv.toTraditional(s))\n",
    "        print(\"成功簡體轉繁體!\")\n",
    "        traditional.close()\n",
    "\n",
    "    # 斷詞(Segmentation)並過濾掉停用詞(Stop Word)\n",
    "    def segmentation(self):\n",
    "        logging.info(\"等待中..(jieba 斷詞，並過濾停用詞)\")\n",
    "        segmentation = open(\"segmentation.txt\", \"w\", encoding = \"utf-8\")\n",
    "        with open(\"traditional.txt\", \"r\", encoding = \"utf-8\") as Corpus:\n",
    "            for sentence in Corpus:\n",
    "                sentence = sentence.strip(\"\\n\")\n",
    "                pos = jieba.cut(sentence, cut_all = False)\n",
    "                for term in pos:\n",
    "                    if term not in self.stopwordset:\n",
    "                        segmentation.write(term + \" \")\n",
    "        print(\"jieba 斷詞完畢，並已完成過濾停用詞!\")\n",
    "        segmentation.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    segmentation = Segmentation()\n",
    "    # 讀取停用詞辭典\n",
    "    segmentation.set_stopword()\n",
    "    # data 進行簡體轉繁體\n",
    "    segmentation.simplified_to_traditional()\n",
    "    # 進行 jieba 斷詞同步過濾停用詞，並產生辭典\n",
    "    segmentation.segmentation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 透過 Gensim Word2Vec 來進行訓練(這邊使用 Skip-gram 模型，Dimension 設為 300 維度)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'base' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n base ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "__author__ = \"ALEX-CHUN-YU (P76064538@mail.ncku.edu.tw)\"\n",
    "import warnings\n",
    "warnings.filterwarnings(action = 'ignore', category = UserWarning, module = 'gensim')\n",
    "from gensim.models import word2vec\n",
    "\n",
    "# 主要透過 gensim 訓練成 model 並供使用\n",
    "class Train(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    # 可參考 https://radimrehurek.com/gensim/models/word2vec.html 更多運用\n",
    "    def train(self):\n",
    "        print(\"訓練中...(喝個咖啡吧^0^)\")\n",
    "        # Load file\n",
    "        sentence = word2vec.Text8Corpus(\"segmentation.txt\")\n",
    "        # Setting degree and Produce Model(Train)\n",
    "        model = word2vec.Word2Vec(sentence, size = 300, window = 10, min_count = 5, workers = 4, sg = 1)\n",
    "        # Save model \n",
    "        model.wv.save_word2vec_format(u\"wiki300.model.bin\", binary = True)\n",
    "        print(\"model 已儲存完畢\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    t = Train()\n",
    "    # 訓練(shallow semantic space)\n",
    "    t.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 將訓練好的 Model 進行測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'base' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n base ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "__author__ = \"ALEX-CHUN-YU (P76064538@mail.ncku.edu.tw)\"\n",
    "import warnings\n",
    "warnings.filterwarnings(action = 'ignore', category = UserWarning, module = 'gensim')\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "# 載入 model 並去運用\n",
    "def main():\n",
    "    # 可參考 https://radimrehurek.com/gensim/models/word2vec.html 更多運用\n",
    "    # How to use bin(model)?\n",
    "    word_vectors = KeyedVectors.load_word2vec_format(\"wiki300.model.bin\", binary = True)\n",
    "    print(\"'爸爸'前10名相似:\")    \n",
    "    res = word_vectors.wv.most_similar('爸爸', topn = 10)\n",
    "    for item in res:\n",
    "        print(item[0] + \",\" + str(item[1]))\n",
    "    print(\"\\n'爸爸','媽媽'之間相似度:\")\n",
    "    res = word_vectors.similarity('爸爸', '媽媽')\n",
    "    print(res)\n",
    "    print(\"\\n'爸爸'之於'老公',如'媽媽'之於'老婆':\")\n",
    "    res = word_vectors.most_similar(positive = ['爸爸', '老公'], negative = ['媽媽'], topn = 5)\n",
    "    for item in res:\n",
    "        print(item[0] + \",\" + str(item[1]))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
