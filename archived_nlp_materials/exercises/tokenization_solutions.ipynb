{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization Exercises - Solutions\n",
    "\n",
    "In the lecture we took a look at a simple tokenizer and sentence segmenter. In this exercise we will expand our understanding of the problem by asking a few important questions, and looking at the problem from a different perspectives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <font color='green'>Setup 1</font>: Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Task 1 solution</font>\n",
    "\n",
    "To tokenise the following text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"'Curiouser and curiouser!' cried Alice (she was so much surprised, that for the moment she quite\n",
    "forgot how to speak good English); 'now I'm opening out like the largest telescope that ever was! Good-bye,\n",
    "feet!' (for when she looked down at her feet, they seemed to be almost out of sight, they were getting so far\n",
    "off). 'Oh, my poor little feet, I wonder who will put on your shoes and stockings for you now, dears? I'm sure I\n",
    "shan't be able! I shall be a great deal too far off to trouble myself about you: you must manage the best\n",
    "way you can; —but I must be kind to them,' thought Alice, 'or perhaps they won't walk the way I want to go!\n",
    "Let me see: I'll give them a new pair of boots every Christmas.'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to expand the list of tokens a bit to account for:\n",
    "- additional characters like ! ( ) , ; — - (notice the difference in the last two dashes)\n",
    "- separation of `n't` from the rest of the word (question: why?)\n",
    "- `'ll` `'m` (question: why?)\n",
    "\n",
    "Should you need additional help on regular expressions, check https://regex101.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'\", 'Curiouser', 'and', 'curiouser', '!', \"'\", 'cried', 'Alice', '(', 'she', 'was', 'so', 'much', 'surprised', ',', 'that', 'for', 'the', 'moment', 'she', 'quite', 'forgot', 'how', 'to', 'speak', 'good', 'English', ')', ';', \"'\", 'now', 'I', \"'m\", 'opening', 'out', 'like', 'the', 'largest', 'telescope', 'that', 'ever', 'was', '!', 'Good', '-', 'bye', ',', 'feet', '!', \"'\", '(', 'for', 'when', 'she', 'looked', 'down', 'at', 'her', 'feet', ',', 'they', 'seemed', 'to', 'be', 'almost', 'out', 'of', 'sight', ',', 'they', 'were', 'getting', 'so', 'far', 'off', ')', '.', \"'\", 'Oh', ',', 'my', 'poor', 'little', 'feet', ',', 'I', 'wonder', 'who', 'will', 'put', 'on', 'your', 'shoes', 'and', 'stockings', 'for', 'you', 'now', ',', 'dears', '?', 'I', \"'m\", 'sure', 'I', 'sha', \"n't\", 'be', 'able', '!', 'I', 'shall', 'be', 'a', 'great', 'deal', 'too', 'far', 'off', 'to', 'trouble', 'myself', 'about', 'you', ':', 'you', 'must', 'manage', 'the', 'best', 'way', 'you', 'can', ';', '—', 'but', 'I', 'must', 'be', 'kind', 'to', 'them', ',', \"'\", 'thought', 'Alice', ',', \"'\", 'or', 'perhaps', 'they', 'wo', \"n't\", 'walk', 'the', 'way', 'I', 'want', 'to', 'go', '!', 'Let', 'me', 'see', ':', 'I', \"'ll\", 'give', 'them', 'a', 'new', 'pair', 'of', 'boots', 'every', 'Christmas', '.', \"'\"]\n"
     ]
    }
   ],
   "source": [
    "token = re.compile(\"[\\w]+(?=n't)|n't|\\'m|\\'ll|[\\w]+|[.?!;,\\-\\(\\)—\\:']\")\n",
    "tokens = token.findall(text)\n",
    "print(tokens[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Task 2 solution</font>\n",
    "\n",
    "The following token from [UCLMR tweet](https://twitter.com/IAugenstein/status/766628888843812864):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = \"#emnlp2016 paper on numerical grounding for error correction http://arxiv.org/abs/1608.04147  @geospith @riedelcastro #NLProc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "can be tokenised in various ways, but the main objective here is to 'catch' hashtags, mentions and URLs. Hashtags and mentions can be extracted easily by just simply adding `#` and `@` to the regular expression part which catches alphanumeric sequences.\n",
    "\n",
    "However, catching URLs is a bit more tricky, and the minimum working example for this case would be the coded example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#emnlp2016', 'paper', 'on', 'numerical', 'grounding', 'for', 'error', 'correction', 'http://arxiv.org/abs/1608.04147', '@geospith', '@riedelcastro', '#NLProc']\n"
     ]
    }
   ],
   "source": [
    "# hashtags and user mentions should be included, as well as the hyperlinks - there are more elaborate URL regular expressions, but this one will do for now\n",
    "token = re.compile('http://[a-zA-Z0-9./]+|[@#\\w]+')\n",
    "tokens = token.findall(tweet)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, bear in mind that this is far from a correct solution for capturing URLs, and that many valid URLs would not be correctly caught with this expression (think https, querystrings, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Task 3 solution</font>\n",
    "\n",
    "We first make sure to account for:\n",
    "- URLs\n",
    "- abbreviations (U.S.A., M.Sc., etc.)\n",
    "- elipsis (...)\n",
    "- question and exclamation mark, and their composition as a single token\n",
    "\n",
    "We modify the tokeniser accordingly and check to verify we're happy with the tokenisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Llanfairpwllgwyngyllgogerychwyrndrobwllllantysiliogogogoch', 'is', 'the', 'longest', 'official', 'one', 'word', 'placename', 'in', 'U.K.', \"Isn't\", 'that', 'weird', '?', 'I', 'mean', 'someone', 'took', 'the', 'effort', 'to', 'really', 'make', 'this', 'name', 'as', 'complicated', 'as', 'possible', 'huh', '?!', 'Of', 'course', 'U.S.A.', 'also', 'has', 'its', 'own', 'record', 'in', 'the', 'longest', 'name', 'albeit', 'a', 'bit', 'shorter', '...', 'This', 'record', 'belongs', 'to', 'the', 'place', 'called', 'Chargoggagoggmanchauggagoggchaubunagungamaugg', '.', \"There's\", 'so', 'many', 'wonderful', 'little', 'details', 'one', 'can', 'find', 'out', 'while', 'browsing', 'http://www.wikipedia.org', 'during', 'their', 'Ph.D.', 'or', 'an', 'M.Sc.']\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Llanfairpwllgwyngyllgogerychwyrndrobwllllantysiliogogogoch is the longest official one-word placename in U.K. Isn't that weird? I mean, someone took the effort to really make this name as complicated as possible, huh?! Of course, U.S.A. also has its own record in the longest name, albeit a bit shorter... This record belongs to the place called Chargoggagoggmanchauggagoggchaubunagungamaugg. There's so many wonderful little details one can find out while browsing http://www.wikipedia.org during their Ph.D. or an M.Sc.\"\"\"\n",
    "\n",
    "token = re.compile('http://[a-zA-Z0-9./]+|(?:[A-Za-z]{1,2}\\.)+|[\\w\\']+|\\.\\.\\.|\\?\\!|[.?!]')\n",
    "tokens = token.findall(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to be fine. However, we won't be able to segment sentences here properly because one of the cases `U.K.` in front of `Isn't` cannot be catched with the existing method. Minima working solution to cover that case would be a solutio where we would check pairs of tokens, and check whether a token that ends on `.` is followed by a token starting with a capital letter. In addition, we expand the list of splitting symbols to the symbols we find in our sentence ends. Notice that this is a gross oversimplification which would fail miserably in specific cases (question: can you think of cases where this might happen?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Llanfairpwllgwyngyllgogerychwyrndrobwllllantysiliogogogoch', 'is', 'the', 'longest', 'official', 'one', 'word', 'placename', 'in', 'U.K.']\n",
      "[\"Isn't\", 'that', 'weird', '?']\n",
      "['I', 'mean', 'someone', 'took', 'the', 'effort', 'to', 'really', 'make', 'this', 'name', 'as', 'complicated', 'as', 'possible', 'huh', '?!']\n",
      "['Of', 'course', 'U.S.A.', 'also', 'has', 'its', 'own', 'record', 'in', 'the', 'longest', 'name', 'albeit', 'a', 'bit', 'shorter', '...']\n",
      "['This', 'record', 'belongs', 'to', 'the', 'place', 'called', 'Chargoggagoggmanchauggagoggchaubunagungamaugg', '.']\n",
      "[\"There's\", 'so', 'many', 'wonderful', 'little', 'details', 'one', 'can', 'find', 'out', 'while', 'browsing', 'http://www.wikipedia.org', 'during', 'their', 'Ph.D.', 'or', 'an', 'M.Sc.']\n"
     ]
    }
   ],
   "source": [
    "def bigrams(tokens):\n",
    "    \"\"\"Helper function to extract bigrams of tokens\"\"\"\n",
    "    tokens.append(' ')\n",
    "    return list(zip(tokens, tokens[1:]))\n",
    "\n",
    "def new_sentence_segment(match_regex, tokens):\n",
    "    current = []\n",
    "    sentences = [current]\n",
    "    \n",
    "    for tok, tok2 in bigrams(tokens):\n",
    "        current.append(tok)\n",
    "        # we additionally check for . at the end of the first and\n",
    "        # upper case letter in the beginning of the following token\n",
    "        if match_regex.match(tok) or (tok[-1]=='.' and tok2[0].isupper()):\n",
    "            current = []\n",
    "            sentences.append(current)\n",
    "    if not sentences[-1]:\n",
    "        sentences.pop(-1)\n",
    "    return sentences\n",
    "\n",
    "sentences = new_sentence_segment(re.compile('\\?|\\.\\.\\.|\\.'), tokens)\n",
    "for sentence in sentences:\n",
    "    print(sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
