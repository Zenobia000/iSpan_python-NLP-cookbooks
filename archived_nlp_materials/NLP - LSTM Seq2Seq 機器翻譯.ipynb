{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 專案的根目錄路徑\n",
    "ROOT_DIR = os.getcwd()\n",
    "\n",
    "# 置放訓練資料的目錄\n",
    "DATA_PATH = os.path.join(ROOT_DIR, \"dataSet\\machineTranslation\")\n",
    "\n",
    "# 訓練資料檔\n",
    "DATA_FILE = os.path.join(DATA_PATH, \"cmn-tw.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64 # 訓練時的批次數量\n",
    "epochs = 100 # 訓練循環數\n",
    "latent_dim = 256 # 編碼後的潛在空間的維度(dimensions of latent space)\n",
    "num_samples = 10000 # 用來訓練的樣本數\n",
    "\n",
    "# 資料向量化\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set() # 英文字符集\n",
    "target_characters = set() # 中文字符集\n",
    "lines = open(DATA_FILE, mode=\"r\", encoding=\"utf-8\").read().split('\\n')\n",
    "\n",
    "# 逐行的讀取與處理\n",
    "for line in lines[: min(num_samples, len(lines)-1)]:\n",
    "    input_text, target_text = line.split('\\t')\n",
    "    \n",
    "    # 我們使用“tab”作為“開始序列[SOS]”字符或目標，“\\n”作為“結束序列[EOS]”字符。 <-- **重要\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    \n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    \n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 10000\n",
      "Number of unique input tokens: 73\n",
      "Number of unique output tokens: 2165\n",
      "Max sequence length for inputs: 33\n",
      "Max sequence length for outputs: 22\n"
     ]
    }
   ],
   "source": [
    "input_characters = sorted(list(input_characters)) # 全部輸入的字符集\n",
    "target_characters = sorted(list(target_characters)) # 全部目標字符集\n",
    "\n",
    "num_encoder_tokens = len(input_characters) # 所有輸入字符的數量\n",
    "num_decoder_tokens = len(target_characters) # 所有輸出目標字符的數量\n",
    "\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts]) # 最長的輸入句子長度\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts]) # 最長的目標句子長度\n",
    "\n",
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 輸入字符的索引字典\n",
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "\n",
    "# 輸目標字符的索引字典\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "# 包含英文句子的one-hot向量化的三維形狀數組（num_pairs，max_english_sentence_length，num_english_characters）\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "# 包含中文句子的one-hot向量化的三維形狀數組（num_pairs，max_chinese_sentence_length，num_chinese_characters）\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "# decoder_target_data與decoder_input_data相同，但是偏移了一個時間步長。 \n",
    "# decoder_target_data [:, t，：]將與decoder_input_data [：，t + 1，：]相同\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "# 把資料轉換成要用來訓練用的張量資料結構 <-- 重要\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "        \n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_input (InputLayer)     [(None, None, 73)]   0           []                               \n",
      "                                                                                                  \n",
      " decoder_input (InputLayer)     [(None, None, 2165)  0           []                               \n",
      "                                ]                                                                 \n",
      "                                                                                                  \n",
      " encoder_lstm (LSTM)            [(None, 256),        337920      ['encoder_input[0][0]']          \n",
      "                                 (None, 256),                                                     \n",
      "                                 (None, 256)]                                                     \n",
      "                                                                                                  \n",
      " decoder_lstm (LSTM)            [(None, None, 256),  2480128     ['decoder_input[0][0]',          \n",
      "                                 (None, 256),                     'encoder_lstm[0][1]',           \n",
      "                                 (None, 256)]                     'encoder_lstm[0][2]']           \n",
      "                                                                                                  \n",
      " decoder_output (Dense)         (None, None, 2165)   556405      ['decoder_lstm[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,374,453\n",
      "Trainable params: 3,374,453\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# ===== 編碼 (encoder) ====\n",
    "\n",
    "# 定義輸入的序列\n",
    "# 注意：因為輸入序列長度(timesteps)可變的情況，使用input_shape =（None，num_features）\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens), name='encoder_input') \n",
    "encoder = LSTM(latent_dim, return_state=True, name='encoder_lstm') # 需要取得LSTM的內部state, 因此設定\"return_state=True\"\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "\n",
    "# 我們拋棄掉`encoder_outputs`因為我們只需要LSTM cell的內部state參數\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# ==== 解碼 (decoder) ====\n",
    "\n",
    "# 設定解碼器(decoder)\n",
    "# 注意：因為輸出序列的長度(timesteps)是變動的，使用input_shape =（None，num_features）\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens), name='decoder_input')\n",
    "\n",
    "# 我們設定我們的解碼器回傳整個輸出的序列同時也回傳內部的states參數\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, name='decoder_lstm')\n",
    "\n",
    "# 在訓練時我們不會使用這些回傳的states, 但是在預測時我們會用到這些states參數\n",
    "# **解碼器的初始狀態是使用編碼器的最後的狀態(states)**\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states) #我們使用`encoder_states`來做為初始值(initial state) <-- 重要\n",
    "\n",
    "# 接密集層(dense)來進行softmax運算每一個字符可能的機率\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax', name='decoder_output')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# 定義一個模型接收encoder_input_data` & `decoder_input_data`做為輸入而輸出`decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# 打印出模型結構\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import plot_model\n",
    "from IPython.display import Image\n",
    "\n",
    "# 產生網絡拓撲圖\n",
    "plot_model(model, to_file=\"model.png\", show_shapes=True, show_layer_names=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "125/125 [==============================] - 65s 496ms/step - loss: 2.1121 - val_loss: 2.6126\n",
      "Epoch 2/100\n",
      "125/125 [==============================] - 54s 429ms/step - loss: 2.0373 - val_loss: 2.5841\n",
      "Epoch 3/100\n",
      "125/125 [==============================] - 54s 432ms/step - loss: 2.0188 - val_loss: 2.5773\n",
      "Epoch 4/100\n",
      "125/125 [==============================] - 54s 434ms/step - loss: 2.0090 - val_loss: 2.5712\n",
      "Epoch 5/100\n",
      "125/125 [==============================] - 53s 422ms/step - loss: 1.9992 - val_loss: 2.5793\n",
      "Epoch 6/100\n",
      "125/125 [==============================] - 54s 434ms/step - loss: 1.9896 - val_loss: 2.5534\n",
      "Epoch 7/100\n",
      "125/125 [==============================] - 54s 433ms/step - loss: 1.9754 - val_loss: 2.5335\n",
      "Epoch 8/100\n",
      "125/125 [==============================] - 59s 473ms/step - loss: 1.9536 - val_loss: 2.5108\n",
      "Epoch 9/100\n",
      "125/125 [==============================] - 57s 454ms/step - loss: 1.9307 - val_loss: 2.5039\n",
      "Epoch 10/100\n",
      "125/125 [==============================] - 54s 431ms/step - loss: 1.9167 - val_loss: 2.4921\n",
      "Epoch 11/100\n",
      "125/125 [==============================] - 53s 428ms/step - loss: 1.9106 - val_loss: 2.4724\n",
      "Epoch 12/100\n",
      "125/125 [==============================] - 53s 427ms/step - loss: 1.8934 - val_loss: 2.4638\n",
      "Epoch 13/100\n",
      "125/125 [==============================] - 54s 430ms/step - loss: 1.8729 - val_loss: 2.4533\n",
      "Epoch 14/100\n",
      "125/125 [==============================] - 54s 434ms/step - loss: 1.8546 - val_loss: 2.4311\n",
      "Epoch 15/100\n",
      "125/125 [==============================] - 54s 433ms/step - loss: 1.8356 - val_loss: 2.4235\n",
      "Epoch 16/100\n",
      "125/125 [==============================] - 54s 433ms/step - loss: 1.8164 - val_loss: 2.4024\n",
      "Epoch 17/100\n",
      "125/125 [==============================] - 53s 426ms/step - loss: 1.7997 - val_loss: 2.3818\n",
      "Epoch 18/100\n",
      "125/125 [==============================] - 53s 424ms/step - loss: 1.7814 - val_loss: 2.3721\n",
      "Epoch 19/100\n",
      "125/125 [==============================] - 57s 456ms/step - loss: 1.7654 - val_loss: 2.4049\n",
      "Epoch 20/100\n",
      "125/125 [==============================] - 59s 475ms/step - loss: 1.7489 - val_loss: 2.3581\n",
      "Epoch 21/100\n",
      "125/125 [==============================] - 53s 424ms/step - loss: 1.7318 - val_loss: 2.3333\n",
      "Epoch 22/100\n",
      "125/125 [==============================] - 52s 419ms/step - loss: 1.7143 - val_loss: 2.3414\n",
      "Epoch 23/100\n",
      "125/125 [==============================] - 53s 423ms/step - loss: 1.6995 - val_loss: 2.3121\n",
      "Epoch 24/100\n",
      "125/125 [==============================] - 52s 418ms/step - loss: 1.6844 - val_loss: 2.2961\n",
      "Epoch 25/100\n",
      "125/125 [==============================] - 53s 424ms/step - loss: 1.6684 - val_loss: 2.2790\n",
      "Epoch 26/100\n",
      "125/125 [==============================] - 53s 424ms/step - loss: 1.6523 - val_loss: 2.2752\n",
      "Epoch 27/100\n",
      "125/125 [==============================] - 52s 419ms/step - loss: 1.6381 - val_loss: 2.2666\n",
      "Epoch 28/100\n",
      "125/125 [==============================] - 52s 419ms/step - loss: 1.6229 - val_loss: 2.2603\n",
      "Epoch 29/100\n",
      "125/125 [==============================] - 52s 419ms/step - loss: 1.6070 - val_loss: 2.2449\n",
      "Epoch 30/100\n",
      "125/125 [==============================] - 53s 422ms/step - loss: 1.5923 - val_loss: 2.2332\n",
      "Epoch 31/100\n",
      "125/125 [==============================] - 52s 415ms/step - loss: 1.5873 - val_loss: 2.2470\n",
      "Epoch 32/100\n",
      "125/125 [==============================] - 52s 420ms/step - loss: 1.5710 - val_loss: 2.2137\n",
      "Epoch 33/100\n",
      "125/125 [==============================] - 52s 419ms/step - loss: 1.5535 - val_loss: 2.2193\n",
      "Epoch 34/100\n",
      "125/125 [==============================] - 53s 421ms/step - loss: 1.5385 - val_loss: 2.2029\n",
      "Epoch 35/100\n",
      "125/125 [==============================] - 52s 419ms/step - loss: 1.5237 - val_loss: 2.1944\n",
      "Epoch 36/100\n",
      "125/125 [==============================] - 51s 406ms/step - loss: 1.5103 - val_loss: 2.1860\n",
      "Epoch 37/100\n",
      "125/125 [==============================] - 52s 417ms/step - loss: 1.4970 - val_loss: 2.1751\n",
      "Epoch 38/100\n",
      "125/125 [==============================] - 54s 430ms/step - loss: 1.4823 - val_loss: 2.1774\n",
      "Epoch 39/100\n",
      "125/125 [==============================] - 56s 444ms/step - loss: 1.4702 - val_loss: 2.1623\n",
      "Epoch 40/100\n",
      "125/125 [==============================] - 54s 436ms/step - loss: 1.4563 - val_loss: 2.1515\n",
      "Epoch 41/100\n",
      "125/125 [==============================] - 53s 422ms/step - loss: 1.4422 - val_loss: 2.1574\n",
      "Epoch 42/100\n",
      "125/125 [==============================] - 52s 418ms/step - loss: 1.4294 - val_loss: 2.1429\n",
      "Epoch 43/100\n",
      "125/125 [==============================] - 52s 419ms/step - loss: 1.4164 - val_loss: 2.1391\n",
      "Epoch 44/100\n",
      "125/125 [==============================] - 53s 422ms/step - loss: 1.4037 - val_loss: 2.1344\n",
      "Epoch 45/100\n",
      "125/125 [==============================] - 55s 444ms/step - loss: 1.3895 - val_loss: 2.1395\n",
      "Epoch 46/100\n",
      "125/125 [==============================] - 53s 425ms/step - loss: 1.3786 - val_loss: 2.1277\n",
      "Epoch 47/100\n",
      "125/125 [==============================] - 53s 421ms/step - loss: 1.3646 - val_loss: 2.1235\n",
      "Epoch 48/100\n",
      "125/125 [==============================] - 52s 419ms/step - loss: 1.3526 - val_loss: 2.1189\n",
      "Epoch 49/100\n",
      "125/125 [==============================] - 53s 422ms/step - loss: 1.3403 - val_loss: 2.1128\n",
      "Epoch 50/100\n",
      "125/125 [==============================] - 52s 415ms/step - loss: 1.3265 - val_loss: 2.1124\n",
      "Epoch 51/100\n",
      "125/125 [==============================] - 53s 423ms/step - loss: 1.3135 - val_loss: 2.1064\n",
      "Epoch 52/100\n",
      "125/125 [==============================] - 52s 420ms/step - loss: 1.3011 - val_loss: 2.1058\n",
      "Epoch 53/100\n",
      "125/125 [==============================] - 53s 421ms/step - loss: 1.2884 - val_loss: 2.0961\n",
      "Epoch 54/100\n",
      "125/125 [==============================] - 52s 420ms/step - loss: 1.2767 - val_loss: 2.0970\n",
      "Epoch 55/100\n",
      "125/125 [==============================] - 53s 421ms/step - loss: 1.2627 - val_loss: 2.1002\n",
      "Epoch 56/100\n",
      "125/125 [==============================] - 52s 419ms/step - loss: 1.2503 - val_loss: 2.0976\n",
      "Epoch 57/100\n",
      "125/125 [==============================] - 53s 423ms/step - loss: 1.2380 - val_loss: 2.0883\n",
      "Epoch 58/100\n",
      "125/125 [==============================] - 53s 421ms/step - loss: 1.2258 - val_loss: 2.1043\n",
      "Epoch 59/100\n",
      "125/125 [==============================] - 52s 413ms/step - loss: 1.2141 - val_loss: 2.0882\n",
      "Epoch 60/100\n",
      "125/125 [==============================] - 52s 419ms/step - loss: 1.2003 - val_loss: 2.0843\n",
      "Epoch 61/100\n",
      "125/125 [==============================] - 55s 440ms/step - loss: 1.1869 - val_loss: 2.0871\n",
      "Epoch 62/100\n",
      "125/125 [==============================] - 53s 423ms/step - loss: 1.1770 - val_loss: 2.0801\n",
      "Epoch 63/100\n",
      "125/125 [==============================] - 52s 417ms/step - loss: 1.1637 - val_loss: 2.0784\n",
      "Epoch 64/100\n",
      "125/125 [==============================] - 52s 420ms/step - loss: 1.1516 - val_loss: 2.0775\n",
      "Epoch 65/100\n",
      "125/125 [==============================] - 52s 421ms/step - loss: 1.1390 - val_loss: 2.0757\n",
      "Epoch 66/100\n",
      "125/125 [==============================] - 53s 427ms/step - loss: 1.1271 - val_loss: 2.0816\n",
      "Epoch 67/100\n",
      "125/125 [==============================] - 52s 417ms/step - loss: 1.1147 - val_loss: 2.0824\n",
      "Epoch 68/100\n",
      "125/125 [==============================] - 53s 425ms/step - loss: 1.1033 - val_loss: 2.0826\n",
      "Epoch 69/100\n",
      "125/125 [==============================] - 56s 449ms/step - loss: 1.0915 - val_loss: 2.0751\n",
      "Epoch 70/100\n",
      "125/125 [==============================] - 53s 428ms/step - loss: 1.0787 - val_loss: 2.0898\n",
      "Epoch 71/100\n",
      "125/125 [==============================] - 51s 406ms/step - loss: 1.0681 - val_loss: 2.0932\n",
      "Epoch 72/100\n",
      "125/125 [==============================] - 55s 437ms/step - loss: 1.0561 - val_loss: 2.0850\n",
      "Epoch 73/100\n",
      "125/125 [==============================] - 55s 439ms/step - loss: 1.0448 - val_loss: 2.0827\n",
      "Epoch 74/100\n",
      "125/125 [==============================] - 55s 442ms/step - loss: 1.0335 - val_loss: 2.0890\n",
      "Epoch 75/100\n",
      "125/125 [==============================] - 53s 422ms/step - loss: 1.0222 - val_loss: 2.0865\n",
      "Epoch 76/100\n",
      "125/125 [==============================] - 52s 419ms/step - loss: 1.0110 - val_loss: 2.0902\n",
      "Epoch 77/100\n",
      "125/125 [==============================] - 52s 415ms/step - loss: 0.9990 - val_loss: 2.0946\n",
      "Epoch 78/100\n",
      "125/125 [==============================] - 53s 423ms/step - loss: 0.9877 - val_loss: 2.1139\n",
      "Epoch 79/100\n",
      "125/125 [==============================] - 53s 425ms/step - loss: 0.9783 - val_loss: 2.0927\n",
      "Epoch 80/100\n",
      "125/125 [==============================] - 53s 426ms/step - loss: 0.9671 - val_loss: 2.1048\n",
      "Epoch 81/100\n",
      "125/125 [==============================] - 53s 424ms/step - loss: 0.9558 - val_loss: 2.0951\n",
      "Epoch 82/100\n",
      "125/125 [==============================] - 53s 427ms/step - loss: 0.9453 - val_loss: 2.1030\n",
      "Epoch 83/100\n",
      "125/125 [==============================] - 56s 452ms/step - loss: 0.9340 - val_loss: 2.1081\n",
      "Epoch 84/100\n",
      "125/125 [==============================] - 56s 452ms/step - loss: 0.9256 - val_loss: 2.1075\n",
      "Epoch 85/100\n",
      "125/125 [==============================] - 52s 418ms/step - loss: 0.9134 - val_loss: 2.1142\n",
      "Epoch 86/100\n",
      "125/125 [==============================] - 53s 424ms/step - loss: 0.9036 - val_loss: 2.1124\n",
      "Epoch 87/100\n",
      "125/125 [==============================] - 56s 446ms/step - loss: 0.8939 - val_loss: 2.1248\n",
      "Epoch 88/100\n",
      "125/125 [==============================] - 53s 425ms/step - loss: 0.8839 - val_loss: 2.1217\n",
      "Epoch 89/100\n",
      "125/125 [==============================] - 52s 420ms/step - loss: 0.8742 - val_loss: 2.1291\n",
      "Epoch 90/100\n",
      "125/125 [==============================] - 52s 419ms/step - loss: 0.8650 - val_loss: 2.1278\n",
      "Epoch 91/100\n",
      "125/125 [==============================] - 52s 419ms/step - loss: 0.8537 - val_loss: 2.1405\n",
      "Epoch 92/100\n",
      "125/125 [==============================] - 53s 425ms/step - loss: 0.8453 - val_loss: 2.1391\n",
      "Epoch 93/100\n",
      "125/125 [==============================] - 53s 423ms/step - loss: 0.8348 - val_loss: 2.1457\n",
      "Epoch 94/100\n",
      "125/125 [==============================] - 52s 416ms/step - loss: 0.8273 - val_loss: 2.1504\n",
      "Epoch 95/100\n",
      "125/125 [==============================] - 53s 421ms/step - loss: 0.8161 - val_loss: 2.1547\n",
      "Epoch 96/100\n",
      "125/125 [==============================] - 53s 423ms/step - loss: 0.8095 - val_loss: 2.1712\n",
      "Epoch 97/100\n",
      "125/125 [==============================] - 53s 425ms/step - loss: 0.7982 - val_loss: 2.1659\n",
      "Epoch 98/100\n",
      "125/125 [==============================] - 53s 422ms/step - loss: 0.7912 - val_loss: 2.1611\n",
      "Epoch 99/100\n",
      "125/125 [==============================] - 52s 416ms/step - loss: 0.7824 - val_loss: 2.1722\n",
      "Epoch 100/100\n",
      "125/125 [==============================] - 57s 460ms/step - loss: 0.7728 - val_loss: 2.1792\n"
     ]
    }
   ],
   "source": [
    "# 設定模型超參數\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "\n",
    "# 開始訓練\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)\n",
    "\n",
    "# 儲存模型\n",
    "model.save('s2s.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 393ms/step\n",
      "1/1 [==============================] - 0s 458ms/step\n",
      "[[[1.0229140e-12 1.0523190e-09 5.3797886e-09 ... 6.5692788e-05\n",
      "   1.6322971e-08 4.2083786e-10]]]\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "[[[8.7930586e-11 3.0096783e-06 2.3948428e-06 ... 2.9511859e-05\n",
      "   3.5051890e-08 2.8362413e-07]]]\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "[[[8.6536041e-08 3.0687610e-02 2.0449285e-03 ... 6.5487623e-04\n",
      "   3.4284210e-07 4.9827797e-03]]]\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "[[[1.3444798e-06 8.2308032e-02 1.7654890e-03 ... 1.7728317e-03\n",
      "   1.1589041e-05 1.1825795e-02]]]\n",
      "-\n",
      "Input sentence: Try some.\n",
      "Decoded sentence: 試試。\n",
      "\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "[[[4.4501334e-12 2.5183104e-09 1.7989316e-09 ... 2.4767473e-06\n",
      "   2.7892082e-08 2.2627713e-08]]]\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "[[[2.4794031e-09 1.5398520e-05 1.4877058e-06 ... 1.2535729e-05\n",
      "   1.1376044e-06 5.4933422e-05]]]\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "[[[1.00998172e-07 1.08887125e-02 3.56050208e-04 ... 1.33241294e-04\n",
      "   6.77114031e-06 6.60532415e-02]]]\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "[[[3.6038432e-07 8.6115859e-02 1.7492886e-03 ... 4.8162680e-04\n",
      "   3.2270048e-06 2.1963628e-01]]]\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "[[[1.2479595e-06 8.0969840e-02 1.3802979e-03 ... 1.4461867e-03\n",
      "   1.6101023e-05 1.8221855e-02]]]\n",
      "-\n",
      "Input sentence: Who died?\n",
      "Decoded sentence: 誰了了？\n",
      "\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "[[[6.4237349e-10 6.6793788e-08 5.4983985e-07 ... 3.2801177e-05\n",
      "   1.5079340e-07 3.4724678e-08]]]\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "[[[1.6694204e-09 2.2314650e-06 1.7564691e-05 ... 4.1836817e-04\n",
      "   3.6891824e-06 1.6010954e-06]]]\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "[[[1.0792242e-08 1.4785303e-04 1.5049335e-04 ... 2.0519843e-04\n",
      "   2.8675838e-06 8.9295070e-05]]]\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "[[[2.2643540e-07 4.2615280e-02 2.7376385e-03 ... 4.5717033e-04\n",
      "   2.1673245e-06 1.2425999e-02]]]\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "[[[1.2531932e-06 7.7510260e-02 1.6260479e-03 ... 1.3569565e-03\n",
      "   1.4957115e-05 1.2390304e-02]]]\n",
      "-\n",
      "Input sentence: Birds fly.\n",
      "Decoded sentence: 鳥類佑。\n",
      "\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "[[[6.3489235e-11 2.3540055e-08 8.3579089e-08 ... 1.4415049e-04\n",
      "   8.0364691e-07 7.4598372e-09]]]\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "[[[1.71244952e-09 1.22012032e-04 1.09795965e-05 ... 9.33445481e-05\n",
      "   3.95755222e-07 1.14994973e-05]]]\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "[[[8.8686782e-09 5.7606385e-03 2.2097761e-04 ... 6.0539064e-04\n",
      "   6.4714135e-07 7.3471112e-04]]]\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "[[[6.1894343e-07 4.4266921e-02 8.8353461e-04 ... 1.7800023e-03\n",
      "   1.4698407e-05 6.1704931e-03]]]\n",
      "-\n",
      "Input sentence: Call home!\n",
      "Decoded sentence: 下了。\n",
      "\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "[[[2.0246973e-12 5.3737087e-10 3.7928829e-09 ... 1.9683025e-06\n",
      "   1.4063437e-07 1.0959411e-10]]]\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "[[[2.1709882e-15 3.3719017e-11 4.0228498e-11 ... 2.0931228e-07\n",
      "   2.2689525e-08 3.9348338e-12]]]\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "[[[3.5780504e-10 1.4103376e-04 1.5777257e-05 ... 1.1477165e-04\n",
      "   1.2321552e-06 1.5232225e-05]]]\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "[[[4.3044842e-08 3.4534082e-02 9.3342643e-04 ... 3.4246745e-04\n",
      "   4.5671464e-07 5.4667541e-03]]]\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "[[[1.0582173e-06 8.0584712e-02 1.3919632e-03 ... 1.4777633e-03\n",
      "   1.4048607e-05 1.1496508e-02]]]\n",
      "-\n",
      "Input sentence: Catch him.\n",
      "Decoded sentence: 抓住他。\n",
      "\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "[[[4.1662696e-10 1.6797674e-07 1.1746328e-06 ... 6.5751665e-04\n",
      "   2.8561601e-07 3.8841813e-07]]]\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "[[[3.8650975e-11 1.3112045e-06 1.5519703e-06 ... 4.4262529e-04\n",
      "   3.0321463e-08 9.4338816e-07]]]\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "[[[1.3979587e-08 4.4830795e-03 3.7718043e-04 ... 1.0898103e-03\n",
      "   1.3683192e-06 1.1905108e-03]]]\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "[[[7.5127069e-08 6.0376149e-02 1.3368169e-03 ... 2.9430568e-04\n",
      "   3.2809461e-07 1.0553458e-02]]]\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "[[[1.1708609e-06 8.5998483e-02 1.4688877e-03 ... 1.5177368e-03\n",
      "   1.3318256e-05 1.2556738e-02]]]\n",
      "-\n",
      "Input sentence: Come home.\n",
      "Decoded sentence: 我試來。\n",
      "\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "[[[3.4190413e-15 7.2696948e-12 6.5569869e-11 ... 1.5464024e-05\n",
      "   2.3950653e-08 6.7398032e-12]]]\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "[[[7.5192761e-14 2.0678211e-08 1.2933778e-08 ... 6.0279162e-05\n",
      "   3.1388421e-08 6.8464749e-09]]]\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "[[[2.4370663e-11 2.2285665e-05 4.8113839e-06 ... 1.7621767e-03\n",
      "   1.2750108e-07 1.1858968e-05]]]\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "[[[1.43688945e-08 1.88491754e-02 5.69195836e-04 ... 7.13582966e-04\n",
      "   3.88078718e-07 4.48801508e-03]]]\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "[[[8.0306978e-07 6.4940281e-02 1.1862328e-03 ... 1.7617197e-03\n",
      "   1.2954562e-05 1.0115745e-02]]]\n",
      "-\n",
      "Input sentence: Do it now.\n",
      "Decoded sentence: 現在做。\n",
      "\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "[[[1.22893884e-09 3.64769733e-07 1.34100776e-06 ... 1.09515975e-04\n",
      "   4.54655037e-06 2.58352799e-07]]]\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "[[[9.2345220e-10 1.9185156e-05 3.4764337e-06 ... 1.7483692e-04\n",
      "   3.7601956e-07 3.4891880e-06]]]\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "[[[7.4164589e-08 2.1215694e-02 6.0022878e-04 ... 4.6848689e-04\n",
      "   7.1299218e-07 4.5513981e-03]]]\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "[[[1.09224970e-06 7.11792260e-02 1.24855654e-03 ... 1.49651710e-03\n",
      "   1.34106876e-05 1.02643585e-02]]]\n",
      "-\n",
      "Input sentence: Dogs bark.\n",
      "Decoded sentence: 等會。\n",
      "\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "[[[3.1254762e-15 9.6402790e-12 3.1938945e-11 ... 3.7042304e-05\n",
      "   5.1652176e-09 1.9551312e-11]]]\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "[[[1.4921103e-13 1.8170729e-08 1.1578251e-08 ... 2.9428882e-04\n",
      "   1.7660712e-09 2.8167435e-08]]]\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "[[[4.0254700e-10 1.3842770e-04 1.6336229e-05 ... 5.9111370e-04\n",
      "   5.5320985e-08 6.6272558e-05]]]\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "[[[4.1552074e-08 4.0994532e-02 1.0715987e-03 ... 3.1637755e-04\n",
      "   2.4258486e-07 8.6708944e-03]]]\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "[[[1.0297509e-06 8.0119923e-02 1.3992230e-03 ... 1.5158787e-03\n",
      "   1.2754938e-05 1.2040318e-02]]]\n",
      "-\n",
      "Input sentence: Don't cry.\n",
      "Decoded sentence: 不要好。\n",
      "\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "[[[5.0084764e-10 2.5534527e-07 1.8285496e-07 ... 2.1761448e-04\n",
      "   1.0594396e-05 7.0863550e-08]]]\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "[[[3.5634015e-10 1.6545311e-05 3.0800641e-06 ... 8.2037535e-05\n",
      "   4.5732040e-06 2.2149693e-06]]]\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "[[[8.4772340e-09 3.6644577e-03 1.6100502e-04 ... 4.2336408e-04\n",
      "   1.4784673e-06 5.2804407e-04]]]\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "[[[6.6900458e-07 4.6160571e-02 9.2810328e-04 ... 1.5153132e-03\n",
      "   1.7740809e-05 6.3750357e-03]]]\n",
      "-\n",
      "Input sentence: Excuse me.\n",
      "Decoded sentence: 祝我。\n",
      "\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "[[[1.4945462e-08 1.3160690e-06 7.3791866e-06 ... 8.4999760e-05\n",
      "   4.5971231e-07 1.0373221e-06]]]\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "[[[5.2545510e-09 1.8380407e-06 4.2589904e-06 ... 1.0269582e-04\n",
      "   1.0279751e-08 9.9920339e-07]]]\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "[[[6.2325132e-12 2.2597796e-07 2.3908856e-07 ... 9.8875251e-05\n",
      "   4.3276316e-08 9.0052559e-08]]]\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "[[[2.1479261e-11 1.3789285e-05 2.6718699e-06 ... 4.3914487e-04\n",
      "   4.5193083e-08 2.9517196e-06]]]\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "[[[1.0263848e-10 1.0871712e-04 1.0082686e-05 ... 2.6934035e-04\n",
      "   1.2967863e-07 1.3253439e-05]]]\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "[[[2.8105180e-08 2.1473724e-02 6.8272965e-04 ... 5.1673723e-04\n",
      "   3.4752847e-07 3.9183013e-03]]]\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "[[[1.0231386e-06 7.9683222e-02 1.3753839e-03 ... 1.5265051e-03\n",
      "   1.3612787e-05 1.1566667e-02]]]\n",
      "-\n",
      "Input sentence: Feel this.\n",
      "Decoded sentence: 來感信了我。\n",
      "\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "[[[9.6498276e-13 1.9079312e-10 1.5860218e-08 ... 1.5361782e-05\n",
      "   1.2118959e-08 1.8043544e-10]]]\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "[[[1.3665992e-11 5.9207995e-08 5.3308895e-07 ... 1.7158917e-04\n",
      "   1.6234497e-07 2.5891328e-08]]]\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "[[[1.5916850e-14 5.4315485e-09 1.1354919e-08 ... 8.9834582e-05\n",
      "   2.3105553e-09 1.3983468e-09]]]\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "[[[2.1422192e-10 2.1720631e-04 2.5483350e-05 ... 7.6987519e-04\n",
      "   8.1405489e-08 5.1127081e-05]]]\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "[[[3.5501994e-08 4.2238671e-02 9.5623673e-04 ... 2.6938992e-04\n",
      "   2.2897267e-07 7.3399805e-03]]]\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "[[[1.03950447e-06 8.18121955e-02 1.40972016e-03 ... 1.52687810e-03\n",
      "   1.28099055e-05 1.20568313e-02]]]\n",
      "-\n",
      "Input sentence: Follow me.\n",
      "Decoded sentence: 請跟我來。\n",
      "\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "[[[1.7837837e-11 1.4581065e-09 8.6503476e-08 ... 1.2731407e-04\n",
      "   3.3796147e-07 1.3430043e-09]]]\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "[[[2.4583607e-13 3.3573799e-10 5.1114144e-09 ... 1.2950708e-04\n",
      "   6.1125029e-08 1.9194099e-10]]]\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "[[[3.9517440e-13 3.1074876e-08 5.7295161e-08 ... 4.7527722e-04\n",
      "   5.9621186e-08 8.8370529e-09]]]\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "[[[5.5700982e-12 3.1758150e-06 8.5684832e-07 ... 3.1137982e-04\n",
      "   1.2456081e-07 4.0593400e-07]]]\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "[[[6.4727153e-09 4.7397828e-03 2.4460271e-04 ... 7.9403340e-04\n",
      "   2.3466801e-07 9.3903253e-04]]]\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "[[[8.0862333e-07 6.5198079e-02 1.2141346e-03 ... 1.6684628e-03\n",
      "   1.3066459e-05 9.7549194e-03]]]\n",
      "-\n",
      "Input sentence: Follow us.\n",
      "Decoded sentence: 請跟著我。\n",
      "\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "[[[3.5681276e-12 7.4322326e-09 1.6315933e-08 ... 2.4750601e-05\n",
      "   4.0141501e-08 6.7836656e-09]]]\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "[[[2.2284356e-10 2.8852590e-05 1.0900527e-05 ... 5.8035348e-05\n",
      "   4.1915396e-08 5.8438973e-06]]]\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "[[[3.7830445e-09 3.2174301e-03 2.4419549e-04 ... 5.5855972e-04\n",
      "   8.6248960e-08 7.4796320e-04]]]\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "[[[4.2621264e-07 3.6172703e-02 8.5507316e-04 ... 1.8590080e-03\n",
      "   8.1189810e-06 5.9877806e-03]]]\n",
      "-\n",
      "Input sentence: Good luck.\n",
      "Decoded sentence: 祝你。\n",
      "\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "[[[4.9185471e-13 1.0351663e-10 1.0077331e-09 ... 1.3343646e-06\n",
      "   4.7244324e-09 6.2705591e-11]]]\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "[[[4.1949403e-17 5.1964580e-13 2.0544866e-12 ... 5.6864106e-08\n",
      "   5.0777366e-10 1.9268989e-13]]]\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "[[[4.8733816e-11 1.5288681e-05 3.8420617e-06 ... 4.5233111e-05\n",
      "   2.9360376e-07 2.8343422e-06]]]\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "[[[2.0256139e-09 1.6498987e-03 1.2605166e-04 ... 4.4276513e-04\n",
      "   1.8584552e-07 5.1865686e-04]]]\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "[[[7.2458832e-08 6.3955128e-02 1.3641417e-03 ... 2.7863478e-04\n",
      "   3.2912783e-07 1.2256815e-02]]]\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "[[[1.0993193e-06 8.5679002e-02 1.4499167e-03 ... 1.4591496e-03\n",
      "   1.2955082e-05 1.2657555e-02]]]\n",
      "-\n",
      "Input sentence: Grab that.\n",
      "Decoded sentence: 抓住這個。\n",
      "\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "[[[4.9178017e-13 1.0703111e-10 1.0639573e-09 ... 1.4756569e-06\n",
      "   4.6121795e-09 6.2035661e-11]]]\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "[[[3.1817413e-17 3.9990353e-13 1.7233730e-12 ... 6.0194907e-08\n",
      "   4.2056453e-10 1.5091791e-13]]]\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "[[[3.3583771e-11 1.0087786e-05 2.8782283e-06 ... 3.7407539e-05\n",
      "   2.6356636e-07 1.9479821e-06]]]\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "[[[1.6550689e-09 1.3233005e-03 1.0894182e-04 ... 4.1966082e-04\n",
      "   1.7384073e-07 4.2950787e-04]]]\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "[[[7.12606649e-08 6.25301227e-02 1.36304437e-03 ... 2.88795272e-04\n",
      "   3.27580921e-07 1.20159425e-02]]]\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "[[[1.0952427e-06 8.5319608e-02 1.4477177e-03 ... 1.4612111e-03\n",
      "   1.2954394e-05 1.2619857e-02]]]\n",
      "-\n",
      "Input sentence: Grab this.\n",
      "Decoded sentence: 抓住這個。\n",
      "\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "[[[8.6285770e-09 1.6690125e-06 4.1703652e-06 ... 7.1597751e-04\n",
      "   5.6669110e-06 1.4059212e-06]]]\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "[[[1.7472416e-12 1.3518774e-08 1.5947492e-08 ... 2.7512418e-04\n",
      "   3.6104097e-07 2.6116320e-09]]]\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "[[[4.8510584e-12 2.0956323e-07 8.7358956e-08 ... 1.5141232e-05\n",
      "   1.1588731e-07 4.7997482e-08]]]\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "[[[3.45456215e-08 1.38276825e-02 6.26678520e-04 ... 4.51159285e-04\n",
      "   1.70023372e-06 2.70824670e-03]]]\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "[[[8.4881498e-07 5.5059191e-02 1.1427170e-03 ... 1.5238661e-03\n",
      "   1.5976910e-05 8.3881244e-03]]]\n",
      "-\n",
      "Input sentence: Hands off.\n",
      "Decoded sentence: 等一下。\n",
      "\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "[[[2.2165578e-16 1.7614633e-12 7.3397348e-13 ... 1.2437567e-07\n",
      "   3.6311915e-11 1.7341544e-12]]]\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "[[[7.6243884e-15 4.6014029e-10 1.0997544e-10 ... 2.3120060e-06\n",
      "   6.8535823e-11 4.2286744e-10]]]\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "[[[8.4419763e-11 2.7029440e-05 2.4921296e-06 ... 2.8289368e-04\n",
      "   1.4898957e-07 2.3876704e-05]]]\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "[[[9.2445475e-11 6.5404485e-05 5.8519354e-06 ... 9.5932752e-05\n",
      "   2.2687031e-08 2.1238913e-05]]]\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "[[[2.0396770e-08 1.2766057e-02 7.5183273e-04 ... 7.0956530e-04\n",
      "   1.4241367e-07 3.4460186e-03]]]\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "[[[8.3811369e-07 6.4026400e-02 1.2538140e-03 ... 1.6095360e-03\n",
      "   1.1376534e-05 9.8668272e-03]]]\n",
      "-\n",
      "Input sentence: He's a DJ.\n",
      "Decoded sentence: 他是一個。\n",
      "\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "[[[9.5724126e-17 2.9966502e-13 1.1786015e-13 ... 3.2111680e-09\n",
      "   2.1014553e-11 3.3764297e-13]]]\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "[[[2.0908031e-13 6.3541439e-09 1.7164692e-09 ... 8.4237008e-06\n",
      "   3.1623840e-10 6.3860259e-09]]]\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "[[[2.5442810e-09 6.2345242e-04 2.3072502e-05 ... 5.1076233e-05\n",
      "   2.9783607e-07 1.3251402e-04]]]\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "[[[7.1693378e-08 6.0603268e-02 1.0633030e-03 ... 1.8022985e-04\n",
      "   2.8348359e-07 1.0942769e-02]]]\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "[[[1.12772909e-06 8.54598209e-02 1.42031454e-03 ... 1.40754238e-03\n",
      "   1.28841384e-05 1.25330435e-02]]]\n",
      "-\n",
      "Input sentence: He's lazy.\n",
      "Decoded sentence: 他很窮。\n",
      "\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "[[[5.1167978e-13 3.0204808e-09 7.2148647e-09 ... 6.0486008e-04\n",
      "   4.4263899e-08 2.5360589e-09]]]\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "[[[1.4425366e-10 3.5021603e-05 5.3720482e-06 ... 5.0882931e-04\n",
      "   2.0839297e-08 1.0579769e-05]]]\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "[[[3.5016342e-08 2.0679083e-02 7.4011070e-04 ... 4.4714977e-04\n",
      "   4.0874451e-07 4.3536611e-03]]]\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "[[[9.7581744e-07 7.3767573e-02 1.3343818e-03 ... 1.6052549e-03\n",
      "   1.2556115e-05 1.1112793e-02]]]\n",
      "-\n",
      "Input sentence: Hold fire.\n",
      "Decoded sentence: 他的。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 定義要進行取樣的模型\n",
    "\n",
    "# 定義編碼器(encoder)的模型\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# 定義解碼器LSTM cell的初始權重輸入\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# # 解碼器(decoder)定義初始狀態(initial decoder state)\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs) #我們使用`decoder_states_inputs`來做為初始值(initial state)\n",
    "\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# 定義解碼器(decoder)的模型\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "\n",
    "# 反向查找字符索引來將序列解碼為可讀的內容。\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())\n",
    "\n",
    "# 對序列進行解碼\n",
    "def decode_sequence(input_seq):\n",
    "    # 將輸入編碼成為state向量\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    \n",
    "    # 產生長度為1的空白目標序列\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    \n",
    "    # 發佈特定的目標序列起始字符\"[SOS]\",在這個範例中是使用 \"\\t\"字符\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # 對批次的序列進行抽樣迴圈\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # 對符標抽樣\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # 停止迴圈的條件: 到達最大的長度或是找到\"停止[EOS]\"字符,在這個範例中是使用 \"\\n\"字符\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 更新目標序列(of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # 更新 states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "for seq_index in range(100,120):\n",
    "    # 從訓練集中取出一個序列並試著解碼\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
