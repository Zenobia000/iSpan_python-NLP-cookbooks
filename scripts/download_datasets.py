#!/usr/bin/env python3
"""
iSpan Python NLP Cookbooks v2 - çµ±ä¸€æ•¸æ“šä¸‹è¼‰è…³æœ¬
Unified Dataset Download Script

åŸ·è¡Œæ–¹å¼:
    python scripts/download_datasets.py --all
    python scripts/download_datasets.py --keras
    python scripts/download_datasets.py --huggingface
    python scripts/download_datasets.py --kaggle
    python scripts/download_datasets.py --chinese

ä½œè€…: iSpan NLP Team
ç‰ˆæœ¬: v1.0
æ—¥æœŸ: 2025-10-17
"""

import os
import sys
import argparse
from pathlib import Path
import requests
import zipfile
import shutil

# è¨­å®šé …ç›®æ ¹ç›®éŒ„
PROJECT_ROOT = Path(__file__).parent.parent
DATASETS_DIR = PROJECT_ROOT / "datasets"

def setup_directories():
    """å‰µå»ºå¿…è¦çš„ç›®éŒ„çµæ§‹"""
    print("ğŸ“ å‰µå»ºæ•¸æ“šé›†ç›®éŒ„çµæ§‹...")

    dirs_to_create = [
        DATASETS_DIR,
        DATASETS_DIR / "keras_datasets",
        DATASETS_DIR / "huggingface_cache",
        DATASETS_DIR / "glove",
        DATASETS_DIR / "sms_spam",
        DATASETS_DIR / "twitter_sentiment",
        DATASETS_DIR / "food_delivery",
        DATASETS_DIR / "movielens",
    ]

    for dir_path in dirs_to_create:
        dir_path.mkdir(parents=True, exist_ok=True)

    print("âœ… ç›®éŒ„çµæ§‹å‰µå»ºå®Œæˆ")


def download_keras_datasets():
    """ä¸‹è¼‰ Keras å…§å»ºæ•¸æ“šé›† (IMDB, Reuters)"""
    print("\n" + "=" * 60)
    print("ğŸ“¥ ä¸‹è¼‰ Keras å…§å»ºæ•¸æ“šé›†...")
    print("=" * 60)

    try:
        # è¨­å®š Keras æ•¸æ“šé›†ä¸‹è¼‰è·¯å¾‘åˆ°å°ˆæ¡ˆ datasets ç›®éŒ„
        keras_datasets_dir = DATASETS_DIR / "keras_datasets"
        keras_datasets_dir.mkdir(parents=True, exist_ok=True)
        os.environ['KERAS_HOME'] = str(keras_datasets_dir.parent)

        from tensorflow import keras

        # IMDB æ•¸æ“šé›†
        print("ğŸ“¥ ä¸‹è¼‰ IMDB é›»å½±è©•è«–æ•¸æ“šé›†...")
        (x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(num_words=10000)
        print(f"âœ… IMDB æ•¸æ“šé›†ä¸‹è¼‰å®Œæˆ: {len(x_train)} è¨“ç·´æ¨£æœ¬, {len(x_test)} æ¸¬è©¦æ¨£æœ¬")

        # Reuters æ•¸æ“šé›†
        print("ğŸ“¥ ä¸‹è¼‰ Reuters æ–°èæ•¸æ“šé›†...")
        (x_train, y_train), (x_test, y_test) = keras.datasets.reuters.load_data(num_words=10000)
        print(f"âœ… Reuters æ•¸æ“šé›†ä¸‹è¼‰å®Œæˆ: {len(x_train)} è¨“ç·´æ¨£æœ¬, {len(set(y_train))} é¡åˆ¥")

        print(f"â„¹ï¸  Keras æ•¸æ“šé›†å¿«å–æ–¼: {keras_datasets_dir}")
        print("âœ… æ‰€æœ‰ Keras æ•¸æ“šé›†ä¸‹è¼‰å®Œæˆ")

    except ImportError:
        print("âŒ éŒ¯èª¤: è«‹å…ˆå®‰è£ TensorFlow: pip install tensorflow")
        return False
    except Exception as e:
        print(f"âŒ Keras æ•¸æ“šé›†ä¸‹è¼‰å¤±æ•—: {e}")
        return False

    return True


def download_huggingface_datasets():
    """ä¸‹è¼‰ Hugging Face å¸¸ç”¨æ•¸æ“šé›†"""
    print("\n" + "=" * 60)
    print("ğŸ“¥ ä¸‹è¼‰ Hugging Face æ•¸æ“šé›†...")
    print("=" * 60)

    try:
        # è¨­å®š Hugging Face æ•¸æ“šé›†ä¸‹è¼‰è·¯å¾‘åˆ°å°ˆæ¡ˆ datasets ç›®éŒ„
        hf_cache_dir = DATASETS_DIR / "huggingface_cache"
        hf_cache_dir.mkdir(parents=True, exist_ok=True)
        os.environ['HF_HOME'] = str(hf_cache_dir)
        os.environ['HF_DATASETS_CACHE'] = str(hf_cache_dir / "datasets")

        from datasets import load_dataset

        datasets_to_load = [
            ("imdb", None, "IMDB é›»å½±è©•è«–"),
            ("ag_news", None, "AG News æ–°èåˆ†é¡"),
            # ("conll2003", None, "CoNLL-2003 å‘½åå¯¦é«”è­˜åˆ¥"), # æ”¹ç‚ºæ‰‹å‹•ä¸‹è¼‰
            ("cnn_dailymail", "3.0.0", "CNN/DailyMail æ–‡æœ¬æ‘˜è¦"),
            ("squad_v2", None, "SQuAD 2.0 å•ç­”ç³»çµ±"),
        ]

        for name, config, description in datasets_to_load:
            try:
                print(f"ğŸ“¥ ä¸‹è¼‰ {description} ({name})...")

                if config:
                    dataset = load_dataset(name, config, split="train[:100]", cache_dir=str(hf_cache_dir / "datasets"))
                else:
                    dataset = load_dataset(name, split="train[:100]", cache_dir=str(hf_cache_dir / "datasets"))

                print(f"âœ… {name} å¿«å–å®Œæˆ (å‰ 100 ç­†)")

            except Exception as e:
                print(f"âš ï¸  {name} ä¸‹è¼‰å¤±æ•—: {e}")

        print(f"â„¹ï¸  Hugging Face æ•¸æ“šé›†å¿«å–æ–¼: {hf_cache_dir}")
        print("âœ… Hugging Face æ•¸æ“šé›†ä¸‹è¼‰å®Œæˆ")

    except ImportError:
        print("âŒ éŒ¯èª¤: è«‹å…ˆå®‰è£ datasets: pip install datasets")
        return False
    except Exception as e:
        print(f"âŒ Hugging Face æ•¸æ“šé›†ä¸‹è¼‰å¤±æ•—: {e}")
        return False

    return True

def download_conll2003():
    """æ‰‹å‹•ä¸‹è¼‰ CoNLL-2003 æ•¸æ“šé›†"""
    print("\n" + "=" * 60)
    print("ğŸ“¥ æ‰‹å‹•ä¸‹è¼‰ CoNLL-2003 æ•¸æ“šé›†...")
    print("=" * 60)

    url = "https://data.deepai.org/conll2003.zip"
    output_dir = DATASETS_DIR / "conll2003"
    zip_path = output_dir / "conll2003.zip"
    train_file = output_dir / "train.txt"

    try:
        output_dir.mkdir(parents=True, exist_ok=True)

        # æª¢æŸ¥æ˜¯å¦å·²ä¸‹è¼‰
        if train_file.exists():
            print(f"âœ… CoNLL-2003 æ•¸æ“šé›†å·²å­˜åœ¨ -> {output_dir}")
            return True

        print(f"ğŸ“¥ é–‹å§‹ä¸‹è¼‰: {url}")
        response = requests.get(url, timeout=60)
        response.raise_for_status()

        zip_path.write_bytes(response.content)

        print("ğŸ“¦ è§£å£“ç¸®æª”æ¡ˆ...")
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            zip_ref.extractall(output_dir)

        # åˆªé™¤ zip æª”æ¡ˆ
        zip_path.unlink()

        print(f"âœ… CoNLL-2003 æ•¸æ“šé›†ä¸‹è¼‰å®Œæˆ -> {output_dir}")

    except Exception as e:
        print(f"âŒ CoNLL-2003 ä¸‹è¼‰å¤±æ•—: {e}")
        return False

    return True

def download_glove():
    """ä¸‹è¼‰ GloVe è©å‘é‡"""
    print("\n" + "=" * 60)
    print("ğŸ“¥ ä¸‹è¼‰ GloVe è©å‘é‡ (è­¦å‘Š: æª”æ¡ˆè¼ƒå¤§ ~822MB)...")
    print("=" * 60)

    url = "http://nlp.stanford.edu/data/glove.6B.zip"
    output_dir = DATASETS_DIR / "glove"
    zip_path = output_dir / "glove.6B.zip"

    try:
        # ç¢ºèªæ˜¯å¦è¦ä¸‹è¼‰
        confirm = input("âš ï¸  GloVe 6B æª”æ¡ˆç´„ 822MBï¼Œæ˜¯å¦ç¹¼çºŒä¸‹è¼‰? (y/n): ")
        if confirm.lower() != 'y':
            print("â­ï¸  è·³é GloVe ä¸‹è¼‰")
            return True

        print(f"ğŸ“¥ é–‹å§‹ä¸‹è¼‰: {url}")
        print("â³ é€™å¯èƒ½éœ€è¦å¹¾åˆ†é˜ï¼Œè«‹è€å¿ƒç­‰å¾…...")

        # ä¸‹è¼‰æª”æ¡ˆ
        response = requests.get(url, stream=True, timeout=120)
        response.raise_for_status()

        total_size = int(response.headers.get('content-length', 0))

        with open(zip_path, 'wb') as f:
            downloaded = 0
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)
                    downloaded += len(chunk)
                    # ç°¡å–®é€²åº¦é¡¯ç¤º
                    if total_size > 0:
                        percent = (downloaded / total_size) * 100
                        print(f"\rä¸‹è¼‰é€²åº¦: {percent:.1f}%", end='')

        print("\nğŸ“¦ è§£å£“ç¸®æª”æ¡ˆ...")
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            zip_ref.extractall(output_dir)

        # åˆªé™¤ zip æª”æ¡ˆ
        zip_path.unlink()

        print(f"âœ… GloVe è©å‘é‡ä¸‹è¼‰å®Œæˆ -> {output_dir}")

    except Exception as e:
        print(f"âŒ GloVe ä¸‹è¼‰å¤±æ•—: {e}")
        return False

    return True

def download_kaggle_datasets():
    """ä¸‹è¼‰ Kaggle æ•¸æ“šé›† (éœ€è¦ Kaggle API)"""
    print("\n" + "=" * 60)
    print("ğŸ“¥ ä¸‹è¼‰ Kaggle æ•¸æ“šé›†...")
    print("=" * 60)

    try:
        import kaggle
    except ImportError:
        print("âŒ éŒ¯èª¤: è«‹å…ˆå®‰è£ Kaggle API: pip install kaggle")
        print("â„¹ï¸  ä¸¦è¨­å®š API Token: https://www.kaggle.com/docs/api")
        return False

    datasets_to_download = [
        {
            "name": "uciml/sms-spam-collection-dataset",
            "description": "SMS Spam åƒåœ¾ç°¡è¨Š",
            "output_dir": DATASETS_DIR / "sms_spam"
        },
        {
            "name": "kazanova/sentiment140",
            "description": "Twitter Sentiment æƒ…æ„Ÿåˆ†æ",
            "output_dir": DATASETS_DIR / "twitter_sentiment"
        },
        {
            "name": "ghoshsaptarshi/av-genpact-hack-dec2018",
            "description": "Food Delivery å¤–é€å¹³å°",
            "output_dir": DATASETS_DIR / "food_delivery"
        }
    ]

    for dataset in datasets_to_download:
        try:
            print(f"ğŸ“¥ ä¸‹è¼‰ {dataset['description']} ({dataset['name']})...")

            # ä½¿ç”¨ Kaggle API ä¸‹è¼‰
            kaggle.api.dataset_download_files(
                dataset['name'],
                path=dataset['output_dir'],
                unzip=True
            )

            print(f"âœ… {dataset['description']} ä¸‹è¼‰å®Œæˆ -> {dataset['output_dir']}")

        except Exception as e:
            print(f"âš ï¸  {dataset['description']} ä¸‹è¼‰å¤±æ•—: {e}")

    print("âœ… Kaggle æ•¸æ“šé›†ä¸‹è¼‰å®Œæˆ")
    return True

def download_movielens():
    """ä¸‹è¼‰ MovieLens æ¨è–¦ç³»çµ±æ•¸æ“šé›†"""
    print("\n" + "=" * 60)
    print("ğŸ“¥ ä¸‹è¼‰ MovieLens æ¨è–¦ç³»çµ±æ•¸æ“šé›†...")
    print("=" * 60)

    url = "https://files.grouplens.org/datasets/movielens/ml-latest-small.zip"
    output_dir = DATASETS_DIR / "movielens"
    zip_path = output_dir / "ml-latest-small.zip"

    try:
        print(f"ğŸ“¥ ä¸‹è¼‰ MovieLens å°å‹æ•¸æ“šé›†: {url}")
        response = requests.get(url, timeout=60)
        response.raise_for_status()

        zip_path.write_bytes(response.content)

        print("ğŸ“¦ è§£å£“ç¸®æª”æ¡ˆ...")
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            zip_ref.extractall(output_dir)

        # åˆªé™¤ zip æª”æ¡ˆ
        zip_path.unlink()

        print(f"âœ… MovieLens æ•¸æ“šé›†ä¸‹è¼‰å®Œæˆ -> {output_dir}")

    except Exception as e:
        print(f"âŒ MovieLens ä¸‹è¼‰å¤±æ•—: {e}")
        return False

    return True

def check_existing_datasets():
    """æª¢æŸ¥å·²å­˜åœ¨çš„æ•¸æ“šé›†"""
    print("\n" + "=" * 60)
    print("ğŸ” æª¢æŸ¥ç¾æœ‰æ•¸æ“šé›†...")
    print("=" * 60)

    existing_datasets = []

    # æª¢æŸ¥å°ˆæ¡ˆè‡ªå»ºæ•¸æ“š
    project_datasets = [
        ("æƒ…æ­Œæ­Œè©", DATASETS_DIR / "lyrics" / "æƒ…æ­Œæ­Œè©"),
        ("Google å•†å®¶è©•è«–", DATASETS_DIR / "google_reviews"),
    ]

    for name, path in project_datasets:
        if path.exists():
            if path.is_dir():
                file_count = len(list(path.glob("*.*")))
                existing_datasets.append(f"âœ… {name}: {file_count} å€‹æª”æ¡ˆ")
            else:
                existing_datasets.append(f"âœ… {name}")
        else:
            existing_datasets.append(f"âŒ {name}: æœªæ‰¾åˆ°")

    for info in existing_datasets:
        print(info)

    print("=" * 60)

def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(
        description="iSpan NLP èª²ç¨‹æ•¸æ“šé›†çµ±ä¸€ä¸‹è¼‰å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¯„ä¾‹:
    python scripts/download_datasets.py --all          # ä¸‹è¼‰æ‰€æœ‰æ•¸æ“šé›†
    python scripts/download_datasets.py --keras        # åªä¸‹è¼‰ Keras æ•¸æ“šé›†
    python scripts/download_datasets.py --huggingface  # åªä¸‹è¼‰ Hugging Face æ•¸æ“šé›†
    python scripts/download_datasets.py --conll2003    # æ‰‹å‹•ä¸‹è¼‰ CoNLL-2003
    python scripts/download_datasets.py --chinese      # åªä¸‹è¼‰ä¸­æ–‡èªæ–™
        """
    )

    parser.add_argument('--all', action='store_true', help='ä¸‹è¼‰æ‰€æœ‰æ•¸æ“šé›†')
    parser.add_argument('--keras', action='store_true', help='ä¸‹è¼‰ Keras å…§å»ºæ•¸æ“šé›†')
    parser.add_argument('--huggingface', action='store_true', help='ä¸‹è¼‰ Hugging Face æ•¸æ“šé›†')
    parser.add_argument('--conll2003', action='store_true', help='æ‰‹å‹•ä¸‹è¼‰ CoNLL-2003 æ•¸æ“šé›†')
    parser.add_argument('--glove', action='store_true', help='ä¸‹è¼‰ GloVe è©å‘é‡')
    parser.add_argument('--kaggle', action='store_true', help='ä¸‹è¼‰ Kaggle æ•¸æ“šé›†')
    parser.add_argument('--movielens', action='store_true', help='ä¸‹è¼‰ MovieLens æ•¸æ“šé›†')

    args = parser.parse_args()

    # å¦‚æœæ²’æœ‰æŒ‡å®šä»»ä½•é¸é …ï¼Œé¡¯ç¤ºå¹«åŠ©
    if not any(vars(args).values()):
        parser.print_help()
        return

    # é¡¯ç¤ºæ­¡è¿è¨Šæ¯
    print("=" * 60)
    print("ğŸš€ iSpan Python NLP Cookbooks v2")
    print("   çµ±ä¸€æ•¸æ“šé›†ä¸‹è¼‰å·¥å…·")
    print("=" * 60)
    print(f"ğŸ“‚ æ•¸æ“šé›†ç›®éŒ„: {DATASETS_DIR}")
    print("=" * 60)

    # å‰µå»ºç›®éŒ„
    setup_directories()

    # æª¢æŸ¥ç¾æœ‰æ•¸æ“šé›†
    check_existing_datasets()

    # æ ¹æ“šåƒæ•¸ä¸‹è¼‰æ•¸æ“šé›†
    success_count = 0
    total_count = 0

    if args.all or args.keras:
        total_count += 1
        if download_keras_datasets():
            success_count += 1

    if args.all or args.huggingface:
        total_count += 1
        if download_huggingface_datasets():
            success_count += 1

    if args.all or args.conll2003:
        total_count += 1
        if download_conll2003():
            success_count += 1

    if args.glove:
        total_count += 1
        if download_glove():
            success_count += 1

    if args.kaggle:
        total_count += 1
        if download_kaggle_datasets():
            success_count += 1

    if args.movielens:
        total_count += 1
        if download_movielens():
            success_count += 1

    # é¡¯ç¤ºç¸½çµ
    print("\n" + "=" * 60)
    print(f"âœ… ä¸‹è¼‰å®Œæˆ: {success_count}/{total_count} æˆåŠŸ")
    print("=" * 60)
    print("\nğŸ“ æ³¨æ„äº‹é …:")
    print("1. éƒ¨åˆ†å¤§å‹æ•¸æ“šé›† (å¦‚ CNN/DailyMail) æœªè‡ªå‹•ä¸‹è¼‰ï¼Œè«‹åœ¨ä½¿ç”¨æ™‚æ‰‹å‹•è¼‰å…¥")
    print("2. Kaggle æ•¸æ“šé›†éœ€è¦å…ˆè¨­å®š API Token")
    print("3. æ‰€æœ‰æ•¸æ“šé›†åƒ…ç”¨æ–¼æ•™å­¸èˆ‡å­¸è¡“ç ”ç©¶ç›®çš„")
    print("\nğŸ“š æ•¸æ“šé›†ä½¿ç”¨èªªæ˜: è«‹åƒè€ƒ docs/16_wbs_development_plan_template.md")
    print("=" * 60)


if __name__ == "__main__":
    main()
