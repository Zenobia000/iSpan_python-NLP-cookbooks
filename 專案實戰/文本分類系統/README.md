# æ–‡æœ¬åˆ†é¡ç³»çµ± - BERT å¾®èª¿å¯¦æˆ°

**å°ˆæ¡ˆé¡å‹**: æ·±åº¦å­¸ç¿’ - Transformer æ¨¡å‹å¾®èª¿
**é›£åº¦**: â­â­â­â­ é€²éš
**é è¨ˆæ™‚é–“**: 4-5 å°æ™‚
**æŠ€è¡“æ£§**: BERT, DistilBERT, Trainer API, Hugging Face

---

## ğŸ“‹ å°ˆæ¡ˆæ¦‚è¿°

æœ¬å°ˆæ¡ˆå±•ç¤ºå¦‚ä½•ä½¿ç”¨ **BERT æ¨¡å‹å¾®èª¿** æŠ€è¡“æ§‹å»ºç”Ÿç”¢ç´šæ–‡æœ¬åˆ†é¡ç³»çµ±,æ‡‰ç”¨æ–¼æ–°èæ–‡ç« è‡ªå‹•åˆ†é¡å ´æ™¯ã€‚

### æ ¸å¿ƒæŠ€è¡“

- **é·ç§»å­¸ç¿’**: ä½¿ç”¨é è¨“ç·´ BERT æ¨¡å‹
- **Fine-Tuning**: Trainer API å®Œæ•´è¨“ç·´æµç¨‹
- **å¤šé¡åˆ¥åˆ†é¡**: 4 é¡æ–°èåˆ†é¡ (World, Sports, Business, Sci/Tech)
- **æ¨¡å‹å„ªåŒ–**: é‡åŒ–ã€åŠ é€Ÿã€éƒ¨ç½²

### å•†æ¥­åƒ¹å€¼

- ğŸ“° **æ–°èåª’é«”**: è‡ªå‹•åˆ†é¡æ–‡ç« ,æå‡ç·¨è¼¯æ•ˆç‡
- ğŸ“§ **éƒµä»¶ç³»çµ±**: æ™ºèƒ½éƒµä»¶è·¯ç”±
- ğŸ« **å®¢æœå·¥å–®**: è‡ªå‹•å·¥å–®åˆ†æ´¾
- ğŸ“š **æ–‡æª”ç®¡ç†**: æ™ºèƒ½æ–‡æª”æ­¸æª”

---

## ğŸ¯ å­¸ç¿’ç›®æ¨™

- âœ… æŒæ¡ BERT æ¨¡å‹å¾®èª¿å®Œæ•´æµç¨‹
- âœ… ä½¿ç”¨ Hugging Face Trainer API
- âœ… å¯¦ä½œå¤šé¡åˆ¥æ–‡æœ¬åˆ†é¡
- âœ… è©•ä¼°æ¨¡å‹æ€§èƒ½ (æ··æ·†çŸ©é™£ã€F1 Score)
- âœ… å„ªåŒ–èˆ‡éƒ¨ç½²æ¨¡å‹

---

## ğŸ“Š æ•¸æ“šé›†èªªæ˜

### AG News Dataset

- **ä¾†æº**: Hugging Face Datasets
- **è¦æ¨¡**:
  - è¨“ç·´é›†: 120,000 samples
  - æ¸¬è©¦é›†: 7,600 samples
- **é¡åˆ¥**: 4 é¡ (World, Sports, Business, Sci/Tech)
- **èªè¨€**: è‹±æ–‡
- **å¹³è¡¡æ€§**: å®Œå…¨å¹³è¡¡ (æ¯é¡ 30,000 è¨“ç·´æ¨£æœ¬)

### æ•¸æ“šæ ¼å¼

```python
{
    'text': 'Wall St. Bears Claw Back Into the Black...',
    'label': 2  # 0:World, 1:Sports, 2:Business, 3:Sci/Tech
}
```

### è¼‰å…¥æ–¹å¼

```python
from datasets import load_dataset

dataset = load_dataset(\"ag_news\")
# è‡ªå‹•ä¸‹è¼‰ä¸¦å¿«å–åˆ° ~/.cache/huggingface/datasets/
```

---

## ğŸš€ å¿«é€Ÿé–‹å§‹

### ç’°å¢ƒéœ€æ±‚

```bash
# å¿…éœ€å¥—ä»¶
poetry add transformers datasets torch
poetry add evaluate accelerate

# å¯è¦–åŒ– (é¸ç”¨)
poetry add matplotlib seaborn scikit-learn

# API éƒ¨ç½² (é¸ç”¨)
poetry add fastapi uvicorn
```

### é‹è¡Œå°ˆæ¡ˆ

```bash
# å•Ÿå‹• Jupyter
poetry run jupyter notebook

# é–‹å•Ÿ notebook
å°ˆæ¡ˆ_æ–‡æœ¬åˆ†é¡_BERTå¾®èª¿å¯¦æˆ°.ipynb

# åŸ·è¡Œæ‰€æœ‰ cells (é è¨ˆ 30-60 åˆ†é˜,å–æ±ºæ–¼ç¡¬é«”)
```

### ç¡¬é«”éœ€æ±‚

| ç¡¬é«” | æœ€ä½ | æ¨è–¦ | èªªæ˜ |
|------|------|------|------|
| **RAM** | 8GB | 16GB+ | é¿å… OOM |
| **GPU** | ç„¡ (å¯ç”¨ CPU) | GTX 1060+ | åŠ é€Ÿ 10-20x |
| **ç£ç¢Ÿ** | 2GB | 5GB+ | æ¨¡å‹ + æ•¸æ“šé›† |
| **è¨“ç·´æ™‚é–“** | 2-3 hr (CPU) | 10-15 min (GPU) | DistilBERT |

---

## ğŸ¨ é æœŸçµæœ

### æ¨¡å‹æ€§èƒ½

```
æ¸¬è©¦é›†çµæœ:
================
Accuracy:  94.2%
F1-Score:  0.941
Precision: 0.943
Recall:    0.942

å„é¡åˆ¥è¡¨ç¾:
            precision    recall  f1-score   support
World          0.93      0.95      0.94      1900
Sports         0.98      0.98      0.98      1900
Business       0.91      0.89      0.90      1900
Sci/Tech       0.94      0.95      0.94      1900
```

### åˆ†é¡ç¯„ä¾‹

```
Input: "Apple announces new iPhone with advanced AI features."
Output: Sci/Tech (Confidence: 98.7%)

Input: "Manchester United defeats Barcelona 3-1 in semifinals."
Output: Sports (Confidence: 99.2%)

Input: "Stock market reaches all-time high amid economic recovery."
Output: Business (Confidence: 96.5%)
```

---

## ğŸ”§ æŠ€è¡“äº®é»

### 1. Trainer API å„ªå‹¢

- âœ… è‡ªå‹•åŒ–è¨“ç·´å¾ªç’°
- âœ… å…§å»ºæ¢¯åº¦ç´¯ç©
- âœ… æ··åˆç²¾åº¦è¨“ç·´ (FP16)
- âœ… å¤š GPU æ”¯æŒ
- âœ… æª¢æŸ¥é»ç®¡ç†
- âœ… Early Stopping
- âœ… TensorBoard æ•´åˆ

### 2. æ¨¡å‹é¸æ“‡å°æ¯”

| æ¨¡å‹ | åƒæ•¸é‡ | è¨“ç·´æ™‚é–“ | æº–ç¢ºç‡ | æ¨è–¦ |
|------|--------|---------|-------|------|
| BERT-base | 110M | é•· | 95.2% | è¿½æ±‚æ¥µè‡´æº–ç¢ºç‡ |
| DistilBERT | 66M | ä¸­ | 94.2% | **å¹³è¡¡ä¹‹é¸** â­ |
| ALBERT-base | 12M | çŸ­ | 93.1% | è³‡æºå—é™ |
| RoBERTa-base | 125M | é•· | 95.8% | æœ€ä½³æ€§èƒ½ |

### 3. è¶…åƒæ•¸èª¿å„ª

```python
# é—œéµè¶…åƒæ•¸
learning_rate: 2e-5      # BERT æ¨è–¦ç¯„åœ: 2e-5 ~ 5e-5
batch_size: 16           # ä¾æ“š GPU è¨˜æ†¶é«”èª¿æ•´
epochs: 3                # é€šå¸¸ 2-5 epoch å³å¯
weight_decay: 0.01       # L2 æ­£å‰‡åŒ–
warmup_steps: 500        # å­¸ç¿’ç‡é ç†±
```

---

## ğŸ“ˆ æ€§èƒ½å°æ¯”

### DistilBERT vs Baseline

| æŒ‡æ¨™ | Baseline (Naive Bayes) | DistilBERT (å¾®èª¿å¾Œ) | æå‡ |
|------|---------------------|------------------|------|
| Accuracy | 88.3% | 94.2% | +5.9% |
| F1-Score | 0.875 | 0.941 | +7.5% |
| æ¨ç†é€Ÿåº¦ | 2ms | 15ms | -13ms |
| æ¨¡å‹å¤§å° | 1MB | 260MB | - |

**çµè«–**: æº–ç¢ºç‡å¤§å¹…æå‡,ä½†éœ€æ¬Šè¡¡é€Ÿåº¦èˆ‡å¤§å°

---

## ğŸ”§ å¸¸è¦‹å•é¡Œ

### Q1: CUDA Out of Memory

```python
# è§£æ±ºæ–¹æ¡ˆ 1: æ¸›å°‘ batch size
per_device_train_batch_size=8  # å¾ 16 é™åˆ° 8

# è§£æ±ºæ–¹æ¡ˆ 2: æ¢¯åº¦ç´¯ç©
gradient_accumulation_steps=4  # æ¨¡æ“¬å¤§ batch

# è§£æ±ºæ–¹æ¡ˆ 3: ä½¿ç”¨æ¢¯åº¦æª¢æŸ¥é»
model.gradient_checkpointing_enable()

# è§£æ±ºæ–¹æ¡ˆ 4: ä½¿ç”¨æ›´å°çš„æ¨¡å‹
model_name = "distilbert-base-uncased"  # è€Œé bert-large
```

### Q2: è¨“ç·´å¤ªæ…¢

```python
# ä½¿ç”¨ Google Colab (å…è²» GPU)
# æˆ– Kaggle Notebooks

# å•Ÿç”¨æ··åˆç²¾åº¦è¨“ç·´
training_args = TrainingArguments(
    fp16=True  # éœ€è¦ GPU
)

# æ¸›å°‘æ•¸æ“šé‡ (é–‹ç™¼éšæ®µ)
train_dataset = dataset['train'].select(range(10000))  # åªç”¨ 10k
```

### Q3: éæ“¬åˆå•é¡Œ

```python
# Early Stopping
from transformers import EarlyStoppingCallback

trainer = Trainer(
    ...,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]
)

# å¢åŠ  Dropout
config.hidden_dropout_prob = 0.2
config.attention_probs_dropout_prob = 0.2

# æ•¸æ“šå¢å¼· (å›è­¯ã€åŒç¾©è©æ›¿æ›)
```

---

## ğŸ“ˆ é€²éšå„ªåŒ–

### 1. è¶…åƒæ•¸æœç´¢

```python
def model_init():
    return AutoModelForSequenceClassification.from_pretrained(
        \"distilbert-base-uncased\",
        num_labels=4
    )

def hp_space(trial):
    return {
        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 5e-5, log=True),
        \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 2, 5),
        \"per_device_train_batch_size\": trial.suggest_categorical(
            \"per_device_train_batch_size\", [8, 16, 32]
        )
    }

best_run = trainer.hyperparameter_search(
    direction=\"maximize\",
    backend=\"optuna\",
    hp_space=hp_space,
    n_trials=10
)
```

### 2. é›†æˆå­¸ç¿’

```python
# è¨“ç·´å¤šå€‹æ¨¡å‹ä¸¦æŠ•ç¥¨
models = [
    \"distilbert-base-uncased\",
    \"roberta-base\",
    \"albert-base-v2\"
]

predictions_list = []
for model_name in models:
    # Train and predict
    predictions = train_and_predict(model_name)
    predictions_list.append(predictions)

# Majority voting
final_predictions = mode(predictions_list, axis=0)
```

---

## ğŸ† ä½œå“é›†å±•ç¤º

### æŠ€è¡“äº®é»

1. **å®Œæ•´å¾®èª¿æµç¨‹**: \"å¾é›¶é–‹å§‹å¾®èª¿ BERT æ¨¡å‹,é”åˆ° 94% æº–ç¢ºç‡\"
2. **ç”Ÿç”¢éƒ¨ç½²**: \"FastAPI æœå‹™åŒ–éƒ¨ç½²,æ”¯æ´å¯¦æ™‚åˆ†é¡\"
3. **æ€§èƒ½å„ªåŒ–**: \"æ¨¡å‹é‡åŒ–æ¸›å°‘ 75% å¤§å°,ä¿æŒ 93%+ æº–ç¢ºç‡\"
4. **éŒ¯èª¤åˆ†æ**: \"æ·±å…¥åˆ†æ misclassifications,æå‡ºæ”¹é€²æ–¹å‘\"

### Demo å»ºè­°

- å±•ç¤ºè¨“ç·´éç¨‹ (TensorBoard æ›²ç·š)
- å°æ¯”å¾®èª¿å‰å¾Œçš„æ€§èƒ½
- å¯¦æ™‚åˆ†é¡ Demo (API æˆ– Web ä»‹é¢)
- å±•ç¤ºæ··æ·†çŸ©é™£èˆ‡éŒ¯èª¤åˆ†æ

---

**å°ˆæ¡ˆç‰ˆæœ¬**: v1.0
**æ•¸æ“šé›†**: AG News (120k samples)
**æœ€ä½³æº–ç¢ºç‡**: 94.2%
**æœ€å¾Œæ›´æ–°**: 2025-10-17
