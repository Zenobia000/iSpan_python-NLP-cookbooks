{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 專案實戰: 文本分類系統 (BERT Fine-Tuning)\n",
    "\n",
    "**專案類型**: 深度學習 - BERT 模型微調\n",
    "**難度**: ⭐⭐⭐⭐ 進階\n",
    "**預計時間**: 4-5 小時\n",
    "**技術棧**: BERT, Transformers, Trainer API, Hugging Face Datasets\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 學習目標\n",
    "\n",
    "完成本專案後,您將能夠:\n",
    "\n",
    "1. ✅ 掌握 BERT 模型微調完整流程\n",
    "2. ✅ 使用 Trainer API 訓練模型\n",
    "3. ✅ 實作多類別文本分類\n",
    "4. ✅ 評估與優化模型性能\n",
    "5. ✅ 部署模型到生產環境\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 專案場景\n",
    "\n",
    "### 業務需求\n",
    "\n",
    "**場景**: 新聞媒體需要自動分類文章到正確類別\n",
    "\n",
    "**需求**:\n",
    "- 支援 4 類別分類 (World, Sports, Business, Sci/Tech)\n",
    "- 準確率 > 90%\n",
    "- 推理時間 < 100ms\n",
    "- 可處理 1000+ 字文章\n",
    "\n",
    "**數據集**: AG News (120,000 訓練樣本, 7,600 測試樣本)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: 環境準備與數據載入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install transformers datasets accelerate evaluate -q\n",
    "\n",
    "import transformers\n",
    "import datasets\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(f\"✅ Transformers: {transformers.__version__}\")\n",
    "print(f\"✅ Datasets: {datasets.__version__}\")\n",
    "print(f\"✅ PyTorch: {torch.__version__}\")\n",
    "print(f\"✅ CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 載入 AG News 數據集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load AG News dataset\n",
    "print(\"📥 Loading AG News dataset...\")\n",
    "\n",
    "dataset = load_dataset(\"ag_news\")\n",
    "\n",
    "print(f\"✅ Dataset loaded!\")\n",
    "print(f\"\\n📊 Dataset structure:\")\n",
    "print(dataset)\n",
    "\n",
    "# Label mapping\n",
    "label_names = ['World', 'Sports', 'Business', 'Sci/Tech']\n",
    "id2label = {i: label for i, label in enumerate(label_names)}\n",
    "label2id = {label: i for i, label in enumerate(label_names)}\n",
    "\n",
    "print(f\"\\n🏷️ Label mapping:\")\n",
    "for idx, label in id2label.items():\n",
    "    print(f\"   {idx}: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 數據探索性分析 (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas for EDA\n",
    "train_df = pd.DataFrame(dataset['train'])\n",
    "test_df = pd.DataFrame(dataset['test'])\n",
    "\n",
    "# Add label names\n",
    "train_df['label_name'] = train_df['label'].map(id2label)\n",
    "test_df['label_name'] = test_df['label'].map(id2label)\n",
    "\n",
    "print(\"📋 Training Data Overview:\")\n",
    "print(train_df.head())\n",
    "\n",
    "print(f\"\\n📊 Label Distribution:\")\n",
    "print(train_df['label_name'].value_counts())\n",
    "\n",
    "# Text length statistics\n",
    "train_df['text_length'] = train_df['text'].str.len()\n",
    "print(f\"\\n📏 Text Length Statistics:\")\n",
    "print(train_df['text_length'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize dataset\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Label distribution\n",
    "label_counts = train_df['label_name'].value_counts()\n",
    "axes[0].bar(label_counts.index, label_counts.values, color='steelblue')\n",
    "axes[0].set_title('Label Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Category')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Text length distribution\n",
    "axes[1].hist(train_df['text_length'], bins=50, color='coral', edgecolor='black')\n",
    "axes[1].set_title('Text Length Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Text Length (characters)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].axvline(train_df['text_length'].mean(), color='red', linestyle='--', label='Mean')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: 模型與分詞器準備\n",
    "\n",
    "### 載入 BERT 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Model selection\n",
    "model_name = \"distilbert-base-uncased\"  # Lighter and faster than BERT\n",
    "\n",
    "print(f\"📦 Loading model and tokenizer: {model_name}\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load model for sequence classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=4,  # 4 categories\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "print(f\"✅ Model loaded successfully!\")\n",
    "print(f\"   Parameters: {model.num_parameters():,}\")\n",
    "print(f\"   Vocabulary size: {tokenizer.vocab_size:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    \"\"\"\n",
    "    Tokenize text for BERT model\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=128  # Limit sequence length\n",
    "    )\n",
    "\n",
    "# Apply tokenization to dataset\n",
    "print(\"🔄 Tokenizing dataset...\")\n",
    "\n",
    "tokenized_datasets = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=['text']  # Remove original text\n",
    ")\n",
    "\n",
    "print(\"✅ Tokenization completed!\")\n",
    "print(f\"\\nTokenized dataset:\")\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect tokenized example\n",
    "sample = tokenized_datasets['train'][0]\n",
    "\n",
    "print(\"🔍 Tokenized Example:\")\n",
    "print(f\"Input IDs (first 20): {sample['input_ids'][:20]}\")\n",
    "print(f\"Attention Mask (first 20): {sample['attention_mask'][:20]}\")\n",
    "print(f\"Label: {sample['label']} ({id2label[sample['label']]})\")\n",
    "\n",
    "# Decode back to text\n",
    "decoded = tokenizer.decode(sample['input_ids'], skip_special_tokens=True)\n",
    "print(f\"\\nDecoded text: {decoded[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: 模型訓練\n",
    "\n",
    "### 設定評估指標"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "# Load evaluation metrics\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute evaluation metrics\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels, average='weighted')\n",
    "    precision = precision_metric.compute(predictions=predictions, references=labels, average='weighted')\n",
    "    recall = recall_metric.compute(predictions=predictions, references=labels, average='weighted')\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy['accuracy'],\n",
    "        'f1': f1['f1'],\n",
    "        'precision': precision['precision'],\n",
    "        'recall': recall['recall']\n",
    "    }\n",
    "\n",
    "print(\"✅ Metrics computation function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 訓練參數配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ag_news_classifier\",\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    \n",
    "    # Evaluation and saving\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    \n",
    "    # Logging\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=500,\n",
    "    logging_first_step=True,\n",
    "    \n",
    "    # Performance\n",
    "    fp16=torch.cuda.is_available(),  # Mixed precision (if GPU available)\n",
    "    dataloader_num_workers=2,\n",
    "    \n",
    "    # Reproducibility\n",
    "    seed=42,\n",
    "    \n",
    "    # Report to\n",
    "    report_to=\"none\"  # Disable W&B/TensorBoard for this demo\n",
    ")\n",
    "\n",
    "print(\"✅ Training arguments configured\")\n",
    "print(f\"\\n📋 Key settings:\")\n",
    "print(f\"   Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"   Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"   Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"   Device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化 Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['test'],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "\n",
    "print(\"✅ Trainer initialized\")\n",
    "print(f\"   Training samples: {len(tokenized_datasets['train']):,}\")\n",
    "print(f\"   Evaluation samples: {len(tokenized_datasets['test']):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 開始訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"🚀 Starting training...\\n\")\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\n✅ Training completed!\")\n",
    "print(f\"\\n📊 Training metrics:\")\n",
    "print(f\"   Final loss: {train_result.training_loss:.4f}\")\n",
    "print(f\"   Training time: {train_result.metrics['train_runtime']:.2f}s\")\n",
    "print(f\"   Samples/second: {train_result.metrics['train_samples_per_second']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: 模型評估\n",
    "\n",
    "### 在測試集上評估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"📊 Evaluating model on test set...\\n\")\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"✅ Evaluation completed!\")\n",
    "print(f\"\\n📈 Test Set Performance:\")\n",
    "print(f\"   Accuracy: {eval_results['eval_accuracy']:.4f} ({eval_results['eval_accuracy']*100:.2f}%)\")\n",
    "print(f\"   F1-score: {eval_results['eval_f1']:.4f}\")\n",
    "print(f\"   Precision: {eval_results['eval_precision']:.4f}\")\n",
    "print(f\"   Recall: {eval_results['eval_recall']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 混淆矩陣分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Get predictions\n",
    "predictions = trainer.predict(tokenized_datasets['test'])\n",
    "y_pred = np.argmax(predictions.predictions, axis=1)\n",
    "y_true = predictions.label_ids\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='Blues',\n",
    "    xticklabels=label_names,\n",
    "    yticklabels=label_names,\n",
    "    cbar_kws={'label': 'Count'}\n",
    ")\n",
    "plt.title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(\"\\n📋 Detailed Classification Report:\\n\")\n",
    "print(classification_report(y_true, y_pred, target_names=label_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 錯誤分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find misclassified examples\n",
    "test_df_results = test_df.copy()\n",
    "test_df_results['predicted_label'] = y_pred\n",
    "test_df_results['predicted_label_name'] = test_df_results['predicted_label'].map(id2label)\n",
    "\n",
    "# Filter errors\n",
    "errors = test_df_results[test_df_results['label'] != test_df_results['predicted_label']]\n",
    "\n",
    "print(f\"🔍 Error Analysis\")\n",
    "print(f\"   Total errors: {len(errors)} / {len(test_df_results)}\")\n",
    "print(f\"   Error rate: {len(errors)/len(test_df_results)*100:.2f}%\")\n",
    "\n",
    "print(f\"\\n❌ Sample misclassifications:\\n\")\n",
    "for idx, row in errors.head(5).iterrows():\n",
    "    print(f\"Text: {row['text'][:100]}...\")\n",
    "    print(f\"True label: {row['label_name']}\")\n",
    "    print(f\"Predicted: {row['predicted_label_name']}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: 模型保存與載入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "model_save_path = \"./ag_news_bert_classifier\"\n",
    "\n",
    "print(f\"💾 Saving model to {model_save_path}...\")\n",
    "\n",
    "trainer.save_model(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "print(\"✅ Model and tokenizer saved!\")\n",
    "\n",
    "# Verify saved files\n",
    "import os\n",
    "saved_files = os.listdir(model_save_path)\n",
    "print(f\"\\n📁 Saved files:\")\n",
    "for file in saved_files:\n",
    "    print(f\"   - {file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved model\n",
    "print(\"📥 Loading saved model...\")\n",
    "\n",
    "loaded_model = AutoModelForSequenceClassification.from_pretrained(model_save_path)\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(model_save_path)\n",
    "\n",
    "print(\"✅ Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: 實際應用\n",
    "\n",
    "### 建立分類 Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Create classification pipeline\n",
    "classifier = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=loaded_model,\n",
    "    tokenizer=loaded_tokenizer,\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "print(\"✅ Classification pipeline created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with custom news articles\n",
    "test_articles = [\n",
    "    \"Apple announces new iPhone with advanced AI features at tech conference in California.\",\n",
    "    \"Stock market reaches all-time high as investors show confidence in economic recovery.\",\n",
    "    \"Manchester United defeats Real Madrid 3-1 in Champions League semifinal match.\",\n",
    "    \"Scientists discover new exoplanet that could potentially support life.\",\n",
    "    \"The Federal Reserve announced interest rate cuts to stimulate economic growth.\"\n",
    "]\n",
    "\n",
    "print(\"🧪 Testing classifier with custom articles:\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, article in enumerate(test_articles, 1):\n",
    "    result = classifier(article)[0]\n",
    "    \n",
    "    print(f\"\\nArticle {i}:\")\n",
    "    print(f\"Text: {article}\")\n",
    "    print(f\"Category: {result['label']}\")\n",
    "    print(f\"Confidence: {result['score']:.2%}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 批量分類應用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch classification\n",
    "batch_results = classifier(test_articles)\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame({\n",
    "    'article': [a[:80] + '...' for a in test_articles],\n",
    "    'category': [r['label'] for r in batch_results],\n",
    "    'confidence': [r['score'] for r in batch_results]\n",
    "})\n",
    "\n",
    "print(\"📊 Batch Classification Results:\\n\")\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: 模型優化\n",
    "\n",
    "### 信心度分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze prediction confidence\n",
    "test_predictions = classifier(test_df['text'].head(100).tolist())\n",
    "confidences = [p['score'] for p in test_predictions]\n",
    "\n",
    "# Plot confidence distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.hist(confidences, bins=30, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "plt.axvline(np.mean(confidences), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(confidences):.3f}')\n",
    "plt.title('Prediction Confidence Distribution', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Confidence Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"📊 Confidence Statistics:\")\n",
    "print(f\"   Mean: {np.mean(confidences):.4f}\")\n",
    "print(f\"   Median: {np.median(confidences):.4f}\")\n",
    "print(f\"   Min: {np.min(confidences):.4f}\")\n",
    "print(f\"   Max: {np.max(confidences):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型量化 (減少大小)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic quantization\n",
    "import torch.quantization\n",
    "\n",
    "# Quantize model\n",
    "print(\"⚡ Quantizing model...\")\n",
    "\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    loaded_model,\n",
    "    {torch.nn.Linear},\n",
    "    dtype=torch.qint8\n",
    ")\n",
    "\n",
    "print(\"✅ Quantization completed!\")\n",
    "\n",
    "# Compare model sizes\n",
    "def get_model_size(model):\n",
    "    \"\"\"Calculate model size in MB\"\"\"\n",
    "    torch.save(model.state_dict(), \"temp_model.p\")\n",
    "    size = os.path.getsize(\"temp_model.p\") / 1e6\n",
    "    os.remove(\"temp_model.p\")\n",
    "    return size\n",
    "\n",
    "original_size = get_model_size(loaded_model)\n",
    "quantized_size = get_model_size(quantized_model)\n",
    "\n",
    "print(f\"\\n📦 Model Size Comparison:\")\n",
    "print(f\"   Original (FP32): {original_size:.2f} MB\")\n",
    "print(f\"   Quantized (INT8): {quantized_size:.2f} MB\")\n",
    "print(f\"   Reduction: {(1 - quantized_size/original_size)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: 生產部署\n",
    "\n",
    "### 建立 FastAPI 服務"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile news_classifier_api.py\n",
    "# news_classifier_api.py\n",
    "\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel, Field\n",
    "from transformers import pipeline\n",
    "from typing import List\n",
    "import uvicorn\n",
    "\n",
    "# Initialize FastAPI\n",
    "app = FastAPI(\n",
    "    title=\"News Classification API\",\n",
    "    description=\"Classify news articles into categories\",\n",
    "    version=\"1.0.0\"\n",
    ")\n",
    "\n",
    "# Global classifier\n",
    "classifier = None\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "async def load_model():\n",
    "    \"\"\"Load model on startup\"\"\"\n",
    "    global classifier\n",
    "    print(\"Loading classifier...\")\n",
    "    classifier = pipeline(\n",
    "        \"text-classification\",\n",
    "        model=\"./ag_news_bert_classifier\",\n",
    "        device=-1  # CPU\n",
    "    )\n",
    "    print(\"✅ Classifier loaded\")\n",
    "\n",
    "# Request/Response models\n",
    "class ArticleInput(BaseModel):\n",
    "    text: str = Field(..., min_length=10, max_length=5000)\n",
    "\n",
    "class BatchArticleInput(BaseModel):\n",
    "    articles: List[str]\n",
    "\n",
    "class ClassificationResponse(BaseModel):\n",
    "    category: str\n",
    "    confidence: float\n",
    "\n",
    "# Endpoints\n",
    "@app.post(\"/classify\", response_model=ClassificationResponse)\n",
    "async def classify_article(input_data: ArticleInput):\n",
    "    \"\"\"Classify single article\"\"\"\n",
    "    try:\n",
    "        result = classifier(input_data.text)[0]\n",
    "        return ClassificationResponse(\n",
    "            category=result['label'],\n",
    "            confidence=round(result['score'], 4)\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.post(\"/classify_batch\")\n",
    "async def classify_batch(input_data: BatchArticleInput):\n",
    "    \"\"\"Classify multiple articles\"\"\"\n",
    "    try:\n",
    "        results = classifier(input_data.articles)\n",
    "        return {\n",
    "            \"results\": [\n",
    "                {\n",
    "                    \"category\": r['label'],\n",
    "                    \"confidence\": round(r['score'], 4)\n",
    "                }\n",
    "                for r in results\n",
    "            ]\n",
    "        }\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health_check():\n",
    "    return {\n",
    "        \"status\": \"healthy\",\n",
    "        \"model_loaded\": classifier is not None\n",
    "    }\n",
    "\n",
    "@app.get(\"/categories\")\n",
    "async def get_categories():\n",
    "    return {\n",
    "        \"categories\": [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 測試 API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile test_api.py\n",
    "# test_api.py\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "API_URL = \"http://localhost:8000\"\n",
    "\n",
    "# Test single classification\n",
    "response = requests.post(\n",
    "    f\"{API_URL}/classify\",\n",
    "    json={\n",
    "        \"text\": \"Apple releases new MacBook Pro with M3 chip and improved performance.\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Single Classification:\")\n",
    "print(json.dumps(response.json(), indent=2))\n",
    "\n",
    "# Test batch classification\n",
    "response = requests.post(\n",
    "    f\"{API_URL}/classify_batch\",\n",
    "    json={\n",
    "        \"articles\": [\n",
    "            \"The Lakers won the championship game 110-98.\",\n",
    "            \"Scientists discover new treatment for cancer.\",\n",
    "            \"Stock markets soar as economy shows signs of recovery.\"\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"\\nBatch Classification:\")\n",
    "print(json.dumps(response.json(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 9: 總結與擴展\n",
    "\n",
    "### ✅ 本專案完成內容\n",
    "\n",
    "1. **完整微調流程**\n",
    "   - 數據載入與預處理\n",
    "   - BERT 模型微調\n",
    "   - 評估與優化\n",
    "   - 模型保存與載入\n",
    "\n",
    "2. **性能分析**\n",
    "   - 多指標評估 (Accuracy, F1, Precision, Recall)\n",
    "   - 混淆矩陣分析\n",
    "   - 錯誤案例分析\n",
    "   - 信心度分布分析\n",
    "\n",
    "3. **生產部署**\n",
    "   - 模型量化優化\n",
    "   - FastAPI 服務\n",
    "   - 批量分類支持\n",
    "\n",
    "### 🚀 進階擴展方向\n",
    "\n",
    "#### 功能擴展\n",
    "- [ ] 增加更多類別\n",
    "- [ ] 多語言支持\n",
    "- [ ] 階層式分類 (粗分類→細分類)\n",
    "- [ ] 多標籤分類\n",
    "\n",
    "#### 性能優化\n",
    "- [ ] 使用更大模型 (BERT-large, RoBERTa)\n",
    "- [ ] 知識蒸餾 (Knowledge Distillation)\n",
    "- [ ] ONNX 轉換加速\n",
    "- [ ] 模型剪枝 (Pruning)\n",
    "\n",
    "#### 應用場景\n",
    "- [ ] 郵件自動路由系統\n",
    "- [ ] 客服工單分類\n",
    "- [ ] 社交媒體內容審核\n",
    "- [ ] 文檔管理系統\n",
    "\n",
    "### 📚 延伸閱讀\n",
    "\n",
    "- [BERT 論文](https://arxiv.org/abs/1810.04805)\n",
    "- [DistilBERT 論文](https://arxiv.org/abs/1910.01108)\n",
    "- [Hugging Face Fine-Tuning Guide](https://huggingface.co/docs/transformers/training)\n",
    "- [Model Optimization Techniques](https://huggingface.co/docs/transformers/perf_train_gpu_one)\n",
    "\n",
    "---\n",
    "\n",
    "**專案版本**: v1.0\n",
    "**建立日期**: 2025-10-17\n",
    "**作者**: iSpan NLP Team\n",
    "**授權**: MIT License"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
