{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# å°ˆæ¡ˆå¯¦æˆ°: æ™ºèƒ½å•ç­”ç³»çµ± (QA System with RAG)\n",
    "\n",
    "**å°ˆæ¡ˆé¡å‹**: æª¢ç´¢å¢å¼·ç”Ÿæˆ (Retrieval-Augmented Generation)\n",
    "**é›£åº¦**: â­â­â­â­â­ å°ˆå®¶ç´š\n",
    "**é è¨ˆæ™‚é–“**: 5-6 å°æ™‚\n",
    "**æŠ€è¡“æ£§**: Hugging Face, FAISS, LangChain, Sentence Transformers\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š å­¸ç¿’ç›®æ¨™\n",
    "\n",
    "å®Œæˆæœ¬å°ˆæ¡ˆå¾Œ,æ‚¨å°‡èƒ½å¤ :\n",
    "\n",
    "1. âœ… ç†è§£å•ç­”ç³»çµ±çš„æ ¸å¿ƒæ¶æ§‹\n",
    "2. âœ… æŒæ¡æª¢ç´¢å¢å¼·ç”Ÿæˆ (RAG) æŠ€è¡“\n",
    "3. âœ… ä½¿ç”¨ FAISS å»ºç«‹å‘é‡æ•¸æ“šåº«\n",
    "4. âœ… æ•´åˆ LangChain æ§‹å»ºå®Œæ•´ QA ç³»çµ±\n",
    "5. âœ… éƒ¨ç½²ç”Ÿç”¢ç´šå•ç­”åŠ©æ‰‹\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: RAG æ¶æ§‹æ¦‚è¦½\n",
    "\n",
    "### 1.1 ä»€éº¼æ˜¯ RAG?\n",
    "\n",
    "**RAG (Retrieval-Augmented Generation)** çµåˆ:\n",
    "- **æª¢ç´¢ (Retrieval)**: å¾çŸ¥è­˜åº«æ‰¾ç›¸é—œè³‡è¨Š\n",
    "- **ç”Ÿæˆ (Generation)**: åŸºæ–¼æª¢ç´¢çµæœç”Ÿæˆç­”æ¡ˆ\n",
    "\n",
    "### 1.2 RAG vs å‚³çµ± QA\n",
    "\n",
    "| æ–¹æ³• | çŸ¥è­˜ä¾†æº | æº–ç¢ºæ€§ | å¯æ“´å±•æ€§ | æˆæœ¬ |\n",
    "|------|---------|--------|----------|------|\n",
    "| **å‚³çµ± QA** | æ¨¡å‹åƒæ•¸ | ä¸­ | ä½ (éœ€é‡æ–°è¨“ç·´) | é«˜ (è¨“ç·´æˆæœ¬) |\n",
    "| **RAG** | å¤–éƒ¨çŸ¥è­˜åº« | é«˜ | é«˜ (å‹•æ…‹æ›´æ–°) | ä½ (åªéœ€æª¢ç´¢) |\n",
    "\n",
    "### 1.3 ç³»çµ±æ¶æ§‹\n",
    "\n",
    "```\n",
    "ç”¨æˆ¶å•é¡Œ\n",
    "    â†“\n",
    "å•é¡Œç·¨ç¢¼ (Sentence Transformer)\n",
    "    â†“\n",
    "å‘é‡æª¢ç´¢ (FAISS)\n",
    "    â”œâ”€â”€ æ‰¾å‡ºæœ€ç›¸é—œçš„ K å€‹æ–‡æª”\n",
    "    â””â”€â”€ åŸºæ–¼é¤˜å¼¦ç›¸ä¼¼åº¦æ’åº\n",
    "    â†“\n",
    "Context æ§‹å»º\n",
    "    â”œâ”€â”€ åˆä½µæª¢ç´¢åˆ°çš„æ–‡æª”\n",
    "    â””â”€â”€ æ·»åŠ åˆ° Prompt\n",
    "    â†“\n",
    "ç­”æ¡ˆç”Ÿæˆ (LLM)\n",
    "    â”œâ”€â”€ åŸºæ–¼ Context ç”Ÿæˆ\n",
    "    â””â”€â”€ å¼•ç”¨ä¾†æº\n",
    "    â†“\n",
    "å›ç­”è¼¸å‡º\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: ç’°å¢ƒæº–å‚™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£å¿…è¦å¥—ä»¶\n",
    "# !pip install transformers sentence-transformers faiss-cpu langchain -q\n",
    "\n",
    "import transformers\n",
    "import sentence_transformers\n",
    "import faiss\n",
    "import langchain\n",
    "\n",
    "print(f\"âœ… Transformers: {transformers.__version__}\")\n",
    "print(f\"âœ… Sentence-Transformers: {sentence_transformers.__version__}\")\n",
    "print(f\"âœ… FAISS: {faiss.__version__}\")\n",
    "print(f\"âœ… LangChain: {langchain.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: çŸ¥è­˜åº«æ§‹å»º\n",
    "\n",
    "### 3.1 æº–å‚™çŸ¥è­˜æ–‡æª”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç¤ºç¯„çŸ¥è­˜åº« (NLP ç›¸é—œçŸ¥è­˜)\n",
    "knowledge_base = [\n",
    "    \"Natural Language Processing (NLP) is a field of AI that focuses on the interaction between computers and human language.\",\n",
    "    \"Transformers are a type of neural network architecture introduced in the paper 'Attention is All You Need' in 2017.\",\n",
    "    \"BERT stands for Bidirectional Encoder Representations from Transformers, developed by Google in 2018.\",\n",
    "    \"GPT (Generative Pre-trained Transformer) is an autoregressive language model developed by OpenAI.\",\n",
    "    \"Word embeddings are dense vector representations of words that capture semantic relationships.\",\n",
    "    \"Tokenization is the process of splitting text into smaller units called tokens.\",\n",
    "    \"Named Entity Recognition (NER) identifies and classifies entities like person names, organizations, and locations.\",\n",
    "    \"Sentiment analysis determines the emotional tone of a text, such as positive, negative, or neutral.\",\n",
    "    \"The attention mechanism allows models to focus on relevant parts of the input when generating output.\",\n",
    "    \"Transfer learning in NLP involves pre-training a model on large datasets and fine-tuning it for specific tasks.\",\n",
    "    \"LSTM (Long Short-Term Memory) networks are a type of RNN designed to handle long-term dependencies.\",\n",
    "    \"Hugging Face is a popular platform for sharing and using pre-trained NLP models.\",\n",
    "    \"TF-IDF (Term Frequency-Inverse Document Frequency) is a statistical measure used to evaluate word importance.\",\n",
    "    \"Machine translation automatically converts text from one language to another.\",\n",
    "    \"Text summarization condenses long documents into shorter versions while preserving key information.\"\n",
    "]\n",
    "\n",
    "print(f\"âœ… çŸ¥è­˜åº«åŒ…å« {len(knowledge_base)} ç¯‡æ–‡æª”\")\n",
    "print(f\"\\nå‰ 3 ç¯‡é è¦½:\")\n",
    "for i, doc in enumerate(knowledge_base[:3], 1):\n",
    "    print(f\"{i}. {doc[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 æ–‡æª”å‘é‡åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# è¼‰å…¥ Sentence Transformer æ¨¡å‹\n",
    "embedding_model_name = \"all-MiniLM-L6-v2\"  # è¼•é‡é«˜æ•ˆ\n",
    "\n",
    "print(f\"è¼‰å…¥ Embedding æ¨¡å‹: {embedding_model_name}\")\n",
    "embedding_model = SentenceTransformer(embedding_model_name)\n",
    "print(\"âœ… æ¨¡å‹è¼‰å…¥å®Œæˆ\")\n",
    "\n",
    "# å‘é‡åŒ–çŸ¥è­˜åº«\n",
    "print(\"\\nå‘é‡åŒ–çŸ¥è­˜åº«...\")\n",
    "document_embeddings = embedding_model.encode(\n",
    "    knowledge_base,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True\n",
    ")\n",
    "\n",
    "print(f\"âœ… å‘é‡åŒ–å®Œæˆ\")\n",
    "print(f\"   æ–‡æª”æ•¸é‡: {document_embeddings.shape[0]}\")\n",
    "print(f\"   å‘é‡ç¶­åº¦: {document_embeddings.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 å»ºç«‹ FAISS ç´¢å¼•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "# å»ºç«‹ FAISS ç´¢å¼•\n",
    "dimension = document_embeddings.shape[1]  # å‘é‡ç¶­åº¦\n",
    "\n",
    "# ä½¿ç”¨ L2 è·é›¢ (ä¹Ÿå¯ç”¨ Inner Product)\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "# æ·»åŠ å‘é‡åˆ°ç´¢å¼•\n",
    "index.add(document_embeddings.astype('float32'))\n",
    "\n",
    "print(f\"âœ… FAISS ç´¢å¼•å»ºç«‹å®Œæˆ\")\n",
    "print(f\"   ç´¢å¼•ä¸­æ–‡æª”æ•¸: {index.ntotal}\")\n",
    "print(f\"   å‘é‡ç¶­åº¦: {dimension}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: æª¢ç´¢åŠŸèƒ½å¯¦ä½œ\n",
    "\n",
    "### 4.1 èªç¾©æª¢ç´¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query, top_k=3):\n",
    "    \"\"\"\n",
    "    èªç¾©æª¢ç´¢: æ‰¾å‡ºæœ€ç›¸é—œçš„æ–‡æª”\n",
    "\n",
    "    Args:\n",
    "        query: æŸ¥è©¢å•é¡Œ\n",
    "        top_k: è¿”å›å‰ K å€‹çµæœ\n",
    "\n",
    "    Returns:\n",
    "        List of (document, score) tuples\n",
    "    \"\"\"\n",
    "    # ç·¨ç¢¼æŸ¥è©¢\n",
    "    query_embedding = embedding_model.encode([query], convert_to_numpy=True)\n",
    "\n",
    "    # æª¢ç´¢\n",
    "    distances, indices = index.search(query_embedding.astype('float32'), top_k)\n",
    "\n",
    "    # çµ„ç¹”çµæœ\n",
    "    results = []\n",
    "    for idx, distance in zip(indices[0], distances[0]):\n",
    "        results.append({\n",
    "            'document': knowledge_base[idx],\n",
    "            'score': float(distance),\n",
    "            'index': int(idx)\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# æ¸¬è©¦æª¢ç´¢\n",
    "test_queries = [\n",
    "    \"What is BERT?\",\n",
    "    \"How does attention mechanism work?\",\n",
    "    \"Explain word embeddings\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ” èªç¾©æª¢ç´¢æ¸¬è©¦\\n\")\n",
    "for query in test_queries:\n",
    "    print(f\"æŸ¥è©¢: {query}\")\n",
    "    results = semantic_search(query, top_k=2)\n",
    "\n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"  {i}. (è·é›¢: {result['score']:.4f})\")\n",
    "        print(f\"     {result['document'][:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: RAG å•ç­”ç³»çµ±\n",
    "\n",
    "### 5.1 åŸºç¤ RAG å¯¦ä½œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# è¼‰å…¥å•ç­”æ¨¡å‹\n",
    "qa_pipeline = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=\"distilbert-base-cased-distilled-squad\"\n",
    ")\n",
    "\n",
    "def rag_qa(question, top_k=3):\n",
    "    \"\"\"\n",
    "    RAG å•ç­”ç³»çµ±\n",
    "\n",
    "    æµç¨‹:\n",
    "    1. æª¢ç´¢ç›¸é—œæ–‡æª”\n",
    "    2. åˆä½µç‚º Context\n",
    "    3. å¾ Context ä¸­æå–ç­”æ¡ˆ\n",
    "    \"\"\"\n",
    "    # Step 1: æª¢ç´¢ç›¸é—œæ–‡æª”\n",
    "    retrieved_docs = semantic_search(question, top_k=top_k)\n",
    "\n",
    "    # Step 2: æ§‹å»º Context\n",
    "    context = \" \".join([doc['document'] for doc in retrieved_docs])\n",
    "\n",
    "    # Step 3: å•ç­”\n",
    "    result = qa_pipeline(\n",
    "        question=question,\n",
    "        context=context\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'answer': result['answer'],\n",
    "        'confidence': result['score'],\n",
    "        'context': context,\n",
    "        'retrieved_docs': retrieved_docs\n",
    "    }\n",
    "\n",
    "\n",
    "# æ¸¬è©¦ RAG QA\n",
    "questions = [\n",
    "    \"What is BERT?\",\n",
    "    \"Who developed GPT?\",\n",
    "    \"What is the purpose of tokenization?\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ¤– RAG å•ç­”ç³»çµ±æ¸¬è©¦\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for q in questions:\n",
    "    result = rag_qa(q)\n",
    "\n",
    "    print(f\"\\nâ“ Question: {q}\")\n",
    "    print(f\"âœ… Answer: {result['answer']}\")\n",
    "    print(f\"   Confidence: {result['confidence']:.2%}\")\n",
    "    print(f\"   Retrieved docs: {len(result['retrieved_docs'])}\")\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 ä½¿ç”¨ LangChain ç°¡åŒ–å¯¦ä½œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS as LangChainFAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import pipeline as hf_pipeline\n",
    "\n",
    "# 1. å»ºç«‹ Embeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# 2. å»ºç«‹å‘é‡å­˜å„²\n",
    "vectorstore = LangChainFAISS.from_texts(\n",
    "    texts=knowledge_base,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "print(\"âœ… LangChain å‘é‡å­˜å„²å»ºç«‹å®Œæˆ\")\n",
    "\n",
    "# 3. å»ºç«‹ LLM\n",
    "llm_pipeline = hf_pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=\"google/flan-t5-small\",\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=llm_pipeline)\n",
    "\n",
    "# 4. å»ºç«‹ RetrievalQA Chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",  # \"stuff\", \"map_reduce\", \"refine\"\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 3}),\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "print(\"âœ… QA Chain å»ºç«‹å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 ä½¿ç”¨ QA Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å•ç­”æ¸¬è©¦\n",
    "question = \"What is BERT and who developed it?\"\n",
    "\n",
    "result = qa_chain({\"query\": question})\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"\\nAnswer: {result['result']}\")\n",
    "print(f\"\\nä¾†æºæ–‡æª”:\")\n",
    "for i, doc in enumerate(result['source_documents'], 1):\n",
    "    print(f\"  {i}. {doc.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: å®Œæ•´ QA ç³»çµ±é¡åˆ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntelligentQASystem:\n",
    "    \"\"\"\n",
    "    æ™ºèƒ½å•ç­”ç³»çµ±\n",
    "    \"\"\"\n",
    "    def __init__(self, knowledge_base, embedding_model_name=\"all-MiniLM-L6-v2\"):\n",
    "        print(\"åˆå§‹åŒ–å•ç­”ç³»çµ±...\")\n",
    "\n",
    "        # è¼‰å…¥ Embedding æ¨¡å‹\n",
    "        self.embedding_model = SentenceTransformer(embedding_model_name)\n",
    "\n",
    "        # è¼‰å…¥å•ç­”æ¨¡å‹\n",
    "        self.qa_model = pipeline(\n",
    "            \"question-answering\",\n",
    "            model=\"distilbert-base-cased-distilled-squad\"\n",
    "        )\n",
    "\n",
    "        # å»ºç«‹å‘é‡ç´¢å¼•\n",
    "        self.knowledge_base = knowledge_base\n",
    "        self.index = self._build_index()\n",
    "\n",
    "        print(\"âœ… ç³»çµ±åˆå§‹åŒ–å®Œæˆ\")\n",
    "\n",
    "    def _build_index(self):\n",
    "        \"\"\"å»ºç«‹ FAISS ç´¢å¼•\"\"\"\n",
    "        embeddings = self.embedding_model.encode(\n",
    "            self.knowledge_base,\n",
    "            convert_to_numpy=True\n",
    "        )\n",
    "\n",
    "        dimension = embeddings.shape[1]\n",
    "        index = faiss.IndexFlatL2(dimension)\n",
    "        index.add(embeddings.astype('float32'))\n",
    "\n",
    "        return index\n",
    "\n",
    "    def retrieve(self, query, top_k=3):\n",
    "        \"\"\"æª¢ç´¢ç›¸é—œæ–‡æª”\"\"\"\n",
    "        query_embedding = self.embedding_model.encode(\n",
    "            [query],\n",
    "            convert_to_numpy=True\n",
    "        )\n",
    "\n",
    "        distances, indices = self.index.search(\n",
    "            query_embedding.astype('float32'),\n",
    "            top_k\n",
    "        )\n",
    "\n",
    "        results = []\n",
    "        for idx, dist in zip(indices[0], distances[0]):\n",
    "            results.append({\n",
    "                'text': self.knowledge_base[idx],\n",
    "                'distance': float(dist),\n",
    "                'relevance': 1 / (1 + float(dist))  # è½‰æ›ç‚ºç›¸é—œæ€§åˆ†æ•¸\n",
    "            })\n",
    "\n",
    "        return results\n",
    "\n",
    "    def answer(self, question, top_k=3, min_confidence=0.3):\n",
    "        \"\"\"\n",
    "        å›ç­”å•é¡Œ\n",
    "\n",
    "        Args:\n",
    "            question: ç”¨æˆ¶å•é¡Œ\n",
    "            top_k: æª¢ç´¢æ–‡æª”æ•¸\n",
    "            min_confidence: æœ€ä½ä¿¡å¿ƒåº¦é–¾å€¼\n",
    "\n",
    "        Returns:\n",
    "            Dict with answer, confidence, sources\n",
    "        \"\"\"\n",
    "        # Step 1: æª¢ç´¢\n",
    "        retrieved_docs = self.retrieve(question, top_k=top_k)\n",
    "\n",
    "        # Step 2: æ§‹å»º Context\n",
    "        context = \" \".join([doc['text'] for doc in retrieved_docs])\n",
    "\n",
    "        # Step 3: æŠ½å–ç­”æ¡ˆ\n",
    "        try:\n",
    "            qa_result = self.qa_model(\n",
    "                question=question,\n",
    "                context=context\n",
    "            )\n",
    "\n",
    "            answer = qa_result['answer']\n",
    "            confidence = qa_result['score']\n",
    "\n",
    "            # æª¢æŸ¥ä¿¡å¿ƒåº¦\n",
    "            if confidence < min_confidence:\n",
    "                return {\n",
    "                    'answer': \"I'm not confident about the answer. Could you rephrase?\",\n",
    "                    'confidence': confidence,\n",
    "                    'status': 'low_confidence',\n",
    "                    'retrieved_docs': retrieved_docs\n",
    "                }\n",
    "\n",
    "            return {\n",
    "                'answer': answer,\n",
    "                'confidence': confidence,\n",
    "                'status': 'success',\n",
    "                'context': context,\n",
    "                'retrieved_docs': retrieved_docs\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'answer': \"Sorry, I encountered an error processing your question.\",\n",
    "                'confidence': 0.0,\n",
    "                'status': 'error',\n",
    "                'error': str(e)\n",
    "            }\n",
    "\n",
    "    def add_documents(self, new_documents):\n",
    "        \"\"\"å‹•æ…‹æ·»åŠ æ–°æ–‡æª”åˆ°çŸ¥è­˜åº«\"\"\"\n",
    "        # å‘é‡åŒ–æ–°æ–‡æª”\n",
    "        new_embeddings = self.embedding_model.encode(\n",
    "            new_documents,\n",
    "            convert_to_numpy=True\n",
    "        )\n",
    "\n",
    "        # æ·»åŠ åˆ°ç´¢å¼•\n",
    "        self.index.add(new_embeddings.astype('float32'))\n",
    "\n",
    "        # æ›´æ–°çŸ¥è­˜åº«\n",
    "        self.knowledge_base.extend(new_documents)\n",
    "\n",
    "        print(f\"âœ… æ·»åŠ  {len(new_documents)} ç¯‡æ–‡æª”\")\n",
    "        print(f\"   çŸ¥è­˜åº«ç¸½æ•¸: {len(self.knowledge_base)}\")\n",
    "\n",
    "\n",
    "# å‰µå»º QA ç³»çµ±å¯¦ä¾‹\n",
    "qa_system = IntelligentQASystem(knowledge_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 æ¸¬è©¦å•ç­”ç³»çµ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç¶œåˆæ¸¬è©¦\n",
    "test_questions = [\n",
    "    \"What is Natural Language Processing?\",\n",
    "    \"Who developed BERT?\",\n",
    "    \"What is the attention mechanism?\",\n",
    "    \"Explain transfer learning in NLP.\",\n",
    "    \"What platform is used for sharing NLP models?\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ¯ å•ç­”ç³»çµ±ç¶œåˆæ¸¬è©¦\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    result = qa_system.answer(question)\n",
    "\n",
    "    print(f\"\\nQ{i}: {question}\")\n",
    "    print(f\"A: {result['answer']}\")\n",
    "    print(f\"   ä¿¡å¿ƒåº¦: {result['confidence']:.2%}\")\n",
    "    print(f\"   ç‹€æ…‹: {result['status']}\")\n",
    "\n",
    "    if 'retrieved_docs' in result:\n",
    "        print(f\"   æª¢ç´¢æ–‡æª”æ•¸: {len(result['retrieved_docs'])}\")\n",
    "        print(f\"   æœ€ç›¸é—œ: {result['retrieved_docs'][0]['text'][:80]}...\")\n",
    "\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: å¾æ–‡ä»¶æ§‹å»ºçŸ¥è­˜åº«\n",
    "\n",
    "### 7.1 è¼‰å…¥é•·æ–‡æª”ä¸¦åˆ‡åˆ†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "def load_and_split_documents(file_paths, chunk_size=500, chunk_overlap=50):\n",
    "    \"\"\"\n",
    "    è¼‰å…¥æ–‡æª”ä¸¦åˆ‡åˆ†ç‚º chunks\n",
    "\n",
    "    Args:\n",
    "        file_paths: æ–‡æª”è·¯å¾‘åˆ—è¡¨\n",
    "        chunk_size: æ¯å€‹ chunk çš„å­—ç¬¦æ•¸\n",
    "        chunk_overlap: chunks é–“é‡ç–Šå­—ç¬¦æ•¸\n",
    "\n",
    "    Returns:\n",
    "        List of text chunks\n",
    "    \"\"\"\n",
    "    # æ–‡æœ¬åˆ‡åˆ†å™¨\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len\n",
    "    )\n",
    "\n",
    "    all_chunks = []\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        # è¼‰å…¥æ–‡æª”\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "\n",
    "        # åˆ‡åˆ†\n",
    "        chunks = text_splitter.split_text(text)\n",
    "        all_chunks.extend(chunks)\n",
    "\n",
    "    return all_chunks\n",
    "\n",
    "\n",
    "# ç¯„ä¾‹: è¼‰å…¥ README æ–‡æª”\n",
    "# doc_paths = ['README.md', 'docs/GUIDE.md']\n",
    "# chunks = load_and_split_documents(doc_paths)\n",
    "# print(f\"âœ… è¼‰å…¥ {len(chunks)} å€‹æ–‡æª”å¡Š\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 å¾ç¶²é æ§‹å»ºçŸ¥è­˜åº«"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç¤ºç¯„: çˆ¬å–ç¶²é å…§å®¹ (éœ€è¦ beautifulsoup4)\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_webpage(url):\n",
    "    \"\"\"\n",
    "    çˆ¬å–ç¶²é æ–‡æœ¬\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # ç§»é™¤ script å’Œ style æ¨™ç±¤\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.decompose()\n",
    "\n",
    "        # æå–æ–‡æœ¬\n",
    "        text = soup.get_text(separator='\\n', strip=True)\n",
    "\n",
    "        return text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"çˆ¬å–å¤±æ•—: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# ç¤ºç¯„: å¾ Hugging Face æ–‡æª”æ§‹å»ºçŸ¥è­˜åº«\n",
    "# urls = [\n",
    "#     'https://huggingface.co/docs/transformers/index',\n",
    "#     'https://huggingface.co/docs/transformers/quicktour'\n",
    "# ]\n",
    "#\n",
    "# web_docs = []\n",
    "# for url in urls:\n",
    "#     text = scrape_webpage(url)\n",
    "#     if text:\n",
    "#         web_docs.append(text)\n",
    "#\n",
    "# qa_system.add_documents(web_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: é€²éšåŠŸèƒ½\n",
    "\n",
    "### 8.1 æ··åˆæª¢ç´¢ (Hybrid Retrieval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class HybridRetriever:\n",
    "    \"\"\"\n",
    "    æ··åˆæª¢ç´¢: èªç¾©æª¢ç´¢ + é—œéµè©æª¢ç´¢\n",
    "    \"\"\"\n",
    "    def __init__(self, documents, semantic_weight=0.7):\n",
    "        self.documents = documents\n",
    "        self.semantic_weight = semantic_weight\n",
    "        self.keyword_weight = 1 - semantic_weight\n",
    "\n",
    "        # èªç¾©æª¢ç´¢: Sentence Transformer\n",
    "        self.embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "        self.doc_embeddings = self.embedding_model.encode(documents)\n",
    "\n",
    "        # é—œéµè©æª¢ç´¢: TF-IDF\n",
    "        self.tfidf = TfidfVectorizer()\n",
    "        self.tfidf_matrix = self.tfidf.fit_transform(documents)\n",
    "\n",
    "    def retrieve(self, query, top_k=5):\n",
    "        \"\"\"æ··åˆæª¢ç´¢\"\"\"\n",
    "        # èªç¾©æª¢ç´¢åˆ†æ•¸\n",
    "        query_embedding = self.embedding_model.encode([query])\n",
    "        semantic_scores = cosine_similarity(query_embedding, self.doc_embeddings)[0]\n",
    "\n",
    "        # é—œéµè©æª¢ç´¢åˆ†æ•¸\n",
    "        query_tfidf = self.tfidf.transform([query])\n",
    "        keyword_scores = cosine_similarity(query_tfidf, self.tfidf_matrix)[0]\n",
    "\n",
    "        # æ··åˆåˆ†æ•¸\n",
    "        hybrid_scores = (\n",
    "            self.semantic_weight * semantic_scores +\n",
    "            self.keyword_weight * keyword_scores\n",
    "        )\n",
    "\n",
    "        # æ’åº\n",
    "        top_indices = np.argsort(hybrid_scores)[::-1][:top_k]\n",
    "\n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            results.append({\n",
    "                'text': self.documents[idx],\n",
    "                'hybrid_score': hybrid_scores[idx],\n",
    "                'semantic_score': semantic_scores[idx],\n",
    "                'keyword_score': keyword_scores[idx]\n",
    "            })\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "# æ¸¬è©¦æ··åˆæª¢ç´¢\n",
    "hybrid_retriever = HybridRetriever(knowledge_base, semantic_weight=0.7)\n",
    "\n",
    "query = \"transformer attention mechanism\"\n",
    "results = hybrid_retriever.retrieve(query, top_k=3)\n",
    "\n",
    "print(f\"æŸ¥è©¢: {query}\\n\")\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"{i}. æ··åˆåˆ†æ•¸: {result['hybrid_score']:.4f}\")\n",
    "    print(f\"   (èªç¾©: {result['semantic_score']:.4f}, é—œéµè©: {result['keyword_score']:.4f})\")\n",
    "    print(f\"   {result['text'][:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 ç­”æ¡ˆé‡æ’åº (Re-ranking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# ä½¿ç”¨äº¤å‰ç·¨ç¢¼å™¨é€²è¡Œé‡æ’åº\n",
    "reranker_model_name = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "reranker_tokenizer = AutoTokenizer.from_pretrained(reranker_model_name)\n",
    "reranker_model = AutoModelForSequenceClassification.from_pretrained(reranker_model_name)\n",
    "\n",
    "def rerank_documents(query, documents, top_k=3):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨äº¤å‰ç·¨ç¢¼å™¨é‡æ’åºæ–‡æª”\n",
    "    \"\"\"\n",
    "    # æ§‹å»º query-document pairs\n",
    "    pairs = [[query, doc] for doc in documents]\n",
    "\n",
    "    # ç·¨ç¢¼\n",
    "    inputs = reranker_tokenizer(\n",
    "        pairs,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors='pt',\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "    # è¨ˆç®—ç›¸é—œæ€§åˆ†æ•¸\n",
    "    with torch.no_grad():\n",
    "        scores = reranker_model(**inputs).logits.squeeze().tolist()\n",
    "\n",
    "    # æ’åº\n",
    "    ranked = sorted(\n",
    "        zip(documents, scores),\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True\n",
    "    )\n",
    "\n",
    "    return ranked[:top_k]\n",
    "\n",
    "\n",
    "# æ¸¬è©¦é‡æ’åº\n",
    "query = \"What is BERT?\"\n",
    "candidate_docs = [doc['text'] for doc in qa_system.retrieve(query, top_k=5)]\n",
    "\n",
    "reranked = rerank_documents(query, candidate_docs, top_k=3)\n",
    "\n",
    "print(f\"æŸ¥è©¢: {query}\\n\")\n",
    "print(\"é‡æ’åºçµæœ:\")\n",
    "for i, (doc, score) in enumerate(reranked, 1):\n",
    "    print(f\"{i}. (åˆ†æ•¸: {score:.4f})\")\n",
    "    print(f\"   {doc[:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: ç”Ÿç”¢éƒ¨ç½² (FastAPI)\n",
    "\n",
    "### 9.1 FastAPI æœå‹™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile qa_api.py\n",
    "# qa_api.py - å•ç­”ç³»çµ± API\n",
    "\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "import uvicorn\n",
    "\n",
    "# åˆå§‹åŒ– FastAPI\n",
    "app = FastAPI(\n",
    "    title=\"Intelligent QA System API\",\n",
    "    description=\"RAG-based Question Answering System\",\n",
    "    version=\"1.0.0\"\n",
    ")\n",
    "\n",
    "# å…¨å±€è®Šæ•¸: QA ç³»çµ±å¯¦ä¾‹\n",
    "qa_system = None\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "async def load_qa_system():\n",
    "    \"\"\"å•Ÿå‹•æ™‚è¼‰å…¥ QA ç³»çµ±\"\"\"\n",
    "    global qa_system\n",
    "\n",
    "    print(\"è¼‰å…¥å•ç­”ç³»çµ±...\")\n",
    "    # è¼‰å…¥çŸ¥è­˜åº«\n",
    "    knowledge_base = load_knowledge_base()  # å¾æ–‡ä»¶æˆ–æ•¸æ“šåº«è¼‰å…¥\n",
    "    qa_system = IntelligentQASystem(knowledge_base)\n",
    "    print(\"âœ… å•ç­”ç³»çµ±è¼‰å…¥å®Œæˆ\")\n",
    "\n",
    "# Request/Response Models\n",
    "class QuestionInput(BaseModel):\n",
    "    question: str = Field(..., min_length=5, max_length=500)\n",
    "    top_k: Optional[int] = Field(default=3, ge=1, le=10)\n",
    "\n",
    "class AnswerResponse(BaseModel):\n",
    "    answer: str\n",
    "    confidence: float\n",
    "    status: str\n",
    "    sources: List[str]\n",
    "\n",
    "class DocumentInput(BaseModel):\n",
    "    documents: List[str]\n",
    "\n",
    "# å•ç­”ç«¯é»\n",
    "@app.post(\"/ask\", response_model=AnswerResponse)\n",
    "async def ask_question(input_data: QuestionInput):\n",
    "    \"\"\"\n",
    "    å›ç­”ç”¨æˆ¶å•é¡Œ\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = qa_system.answer(\n",
    "            question=input_data.question,\n",
    "            top_k=input_data.top_k\n",
    "        )\n",
    "\n",
    "        sources = [\n",
    "            doc['text'][:100] + \"...\"\n",
    "            for doc in result.get('retrieved_docs', [])\n",
    "        ]\n",
    "\n",
    "        return AnswerResponse(\n",
    "            answer=result['answer'],\n",
    "            confidence=result['confidence'],\n",
    "            status=result['status'],\n",
    "            sources=sources\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "# æ·»åŠ æ–‡æª”ç«¯é»\n",
    "@app.post(\"/add_documents\")\n",
    "async def add_documents(input_data: DocumentInput):\n",
    "    \"\"\"\n",
    "    å‹•æ…‹æ·»åŠ æ–°æ–‡æª”åˆ°çŸ¥è­˜åº«\n",
    "    \"\"\"\n",
    "    try:\n",
    "        qa_system.add_documents(input_data.documents)\n",
    "        return {\n",
    "            \"message\": f\"Added {len(input_data.documents)} documents\",\n",
    "            \"total_documents\": len(qa_system.knowledge_base)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "# å¥åº·æª¢æŸ¥\n",
    "@app.get(\"/health\")\n",
    "async def health_check():\n",
    "    return {\n",
    "        \"status\": \"healthy\",\n",
    "        \"qa_system_loaded\": qa_system is not None,\n",
    "        \"knowledge_base_size\": len(qa_system.knowledge_base) if qa_system else 0\n",
    "    }\n",
    "\n",
    "# ç³»çµ±è³‡è¨Š\n",
    "@app.get(\"/info\")\n",
    "async def system_info():\n",
    "    return {\n",
    "        \"knowledge_base_size\": len(qa_system.knowledge_base),\n",
    "        \"embedding_model\": \"all-MiniLM-L6-v2\",\n",
    "        \"qa_model\": \"distilbert-base-cased-distilled-squad\"\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 æ¸¬è©¦ API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile test_qa_api.py\n",
    "# test_qa_api.py\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "API_URL = \"http://localhost:8000\"\n",
    "\n",
    "# æ¸¬è©¦å•ç­”\n",
    "response = requests.post(\n",
    "    f\"{API_URL}/ask\",\n",
    "    json={\n",
    "        \"question\": \"What is BERT?\",\n",
    "        \"top_k\": 3\n",
    "    }\n",
    ")\n",
    "\n",
    "result = response.json()\n",
    "print(f\"Question: What is BERT?\")\n",
    "print(f\"Answer: {result['answer']}\")\n",
    "print(f\"Confidence: {result['confidence']:.2%}\")\n",
    "print(f\"\\nSources:\")\n",
    "for i, source in enumerate(result['sources'], 1):\n",
    "    print(f\"  {i}. {source}\")\n",
    "\n",
    "# æ·»åŠ æ–°æ–‡æª”\n",
    "response = requests.post(\n",
    "    f\"{API_URL}/add_documents\",\n",
    "    json={\n",
    "        \"documents\": [\n",
    "            \"RAG is a powerful technique that combines retrieval and generation.\"\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"\\nAdd documents: {response.json()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: ç¸½çµèˆ‡æ“´å±•\n",
    "\n",
    "### âœ… æœ¬å°ˆæ¡ˆå®Œæˆå…§å®¹\n",
    "\n",
    "1. **æ ¸å¿ƒ RAG ç³»çµ±**\n",
    "   - æ–‡æª”å‘é‡åŒ– (Sentence Transformer)\n",
    "   - FAISS å‘é‡æª¢ç´¢\n",
    "   - å•ç­”æ¨¡å‹æ•´åˆ\n",
    "\n",
    "2. **é€²éšåŠŸèƒ½**\n",
    "   - æ··åˆæª¢ç´¢ (èªç¾©+é—œéµè©)\n",
    "   - ç­”æ¡ˆé‡æ’åº\n",
    "   - ä¿¡å¿ƒåº¦éæ¿¾\n",
    "\n",
    "3. **ç”Ÿç”¢éƒ¨ç½²**\n",
    "   - FastAPI æœå‹™\n",
    "   - å‹•æ…‹çŸ¥è­˜åº«æ›´æ–°\n",
    "   - API æ–‡æª” (Swagger)\n",
    "\n",
    "### ğŸš€ é€²éšæ“´å±•æ–¹å‘\n",
    "\n",
    "#### æŠ€è¡“å„ªåŒ–\n",
    "- [ ] ä½¿ç”¨æ›´å¤§çš„ Embedding æ¨¡å‹\n",
    "- [ ] æ•´åˆ GPT-3.5/GPT-4 API\n",
    "- [ ] å¯¦ä½œå°è©±å¼å•ç­” (Multi-turn QA)\n",
    "- [ ] æ·»åŠ å¼•ç”¨ä¾†æºè¿½è¹¤\n",
    "\n",
    "#### åŠŸèƒ½æ“´å±•\n",
    "- [ ] å¤šèªè¨€æ”¯æŒ\n",
    "- [ ] çµæ§‹åŒ–æ•¸æ“šå•ç­” (è¡¨æ ¼ã€åœ–è¡¨)\n",
    "- [ ] è¤‡é›œæ¨ç† (Chain-of-Thought)\n",
    "- [ ] äº‹å¯¦é©—è­‰\n",
    "\n",
    "#### æ‡‰ç”¨å ´æ™¯\n",
    "- [ ] ä¼æ¥­çŸ¥è­˜ç®¡ç†ç³»çµ±\n",
    "- [ ] æ™ºèƒ½æ–‡æª”åŠ©æ‰‹\n",
    "- [ ] æ³•å¾‹/é†«ç™‚å•ç­”\n",
    "- [ ] æ•™è‚²è¼”åŠ©ç³»çµ±\n",
    "\n",
    "### ğŸ“š å»¶ä¼¸é–±è®€\n",
    "\n",
    "- [RAG è«–æ–‡](https://arxiv.org/abs/2005.11401)\n",
    "- [FAISS æ–‡æª”](https://github.com/facebookresearch/faiss)\n",
    "- [LangChain æ–‡æª”](https://python.langchain.com/docs/get_started/introduction)\n",
    "- [Sentence Transformers](https://www.sbert.net/)\n",
    "\n",
    "---\n",
    "\n",
    "**å°ˆæ¡ˆç‰ˆæœ¬**: v1.0\n",
    "**å»ºç«‹æ—¥æœŸ**: 2025-10-17\n",
    "**ä½œè€…**: iSpan NLP Team\n",
    "**æˆæ¬Š**: MIT License"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
