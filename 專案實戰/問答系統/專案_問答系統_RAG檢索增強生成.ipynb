{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 專案實戰: 智能問答系統 (QA System with RAG)\n",
    "\n",
    "**專案類型**: 檢索增強生成 (Retrieval-Augmented Generation)\n",
    "**難度**: ⭐⭐⭐⭐⭐ 專家級\n",
    "**預計時間**: 5-6 小時\n",
    "**技術棧**: Hugging Face, FAISS, LangChain, Sentence Transformers\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 學習目標\n",
    "\n",
    "完成本專案後,您將能夠:\n",
    "\n",
    "1. ✅ 理解問答系統的核心架構\n",
    "2. ✅ 掌握檢索增強生成 (RAG) 技術\n",
    "3. ✅ 使用 FAISS 建立向量數據庫\n",
    "4. ✅ 整合 LangChain 構建完整 QA 系統\n",
    "5. ✅ 部署生產級問答助手\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: RAG 架構概覽\n",
    "\n",
    "### 1.1 什麼是 RAG?\n",
    "\n",
    "**RAG (Retrieval-Augmented Generation)** 結合:\n",
    "- **檢索 (Retrieval)**: 從知識庫找相關資訊\n",
    "- **生成 (Generation)**: 基於檢索結果生成答案\n",
    "\n",
    "### 1.2 RAG vs 傳統 QA\n",
    "\n",
    "| 方法 | 知識來源 | 準確性 | 可擴展性 | 成本 |\n",
    "|------|---------|--------|----------|------|\n",
    "| **傳統 QA** | 模型參數 | 中 | 低 (需重新訓練) | 高 (訓練成本) |\n",
    "| **RAG** | 外部知識庫 | 高 | 高 (動態更新) | 低 (只需檢索) |\n",
    "\n",
    "### 1.3 系統架構\n",
    "\n",
    "```\n",
    "用戶問題\n",
    "    ↓\n",
    "問題編碼 (Sentence Transformer)\n",
    "    ↓\n",
    "向量檢索 (FAISS)\n",
    "    ├── 找出最相關的 K 個文檔\n",
    "    └── 基於餘弦相似度排序\n",
    "    ↓\n",
    "Context 構建\n",
    "    ├── 合併檢索到的文檔\n",
    "    └── 添加到 Prompt\n",
    "    ↓\n",
    "答案生成 (LLM)\n",
    "    ├── 基於 Context 生成\n",
    "    └── 引用來源\n",
    "    ↓\n",
    "回答輸出\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: 環境準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安裝必要套件\n",
    "# !pip install transformers sentence-transformers faiss-cpu langchain -q\n",
    "\n",
    "import transformers\n",
    "import sentence_transformers\n",
    "import faiss\n",
    "import langchain\n",
    "\n",
    "print(f\"✅ Transformers: {transformers.__version__}\")\n",
    "print(f\"✅ Sentence-Transformers: {sentence_transformers.__version__}\")\n",
    "print(f\"✅ FAISS: {faiss.__version__}\")\n",
    "print(f\"✅ LangChain: {langchain.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: 知識庫構建\n",
    "\n",
    "### 3.1 準備知識文檔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示範知識庫 (NLP 相關知識)\n",
    "knowledge_base = [\n",
    "    \"Natural Language Processing (NLP) is a field of AI that focuses on the interaction between computers and human language.\",\n",
    "    \"Transformers are a type of neural network architecture introduced in the paper 'Attention is All You Need' in 2017.\",\n",
    "    \"BERT stands for Bidirectional Encoder Representations from Transformers, developed by Google in 2018.\",\n",
    "    \"GPT (Generative Pre-trained Transformer) is an autoregressive language model developed by OpenAI.\",\n",
    "    \"Word embeddings are dense vector representations of words that capture semantic relationships.\",\n",
    "    \"Tokenization is the process of splitting text into smaller units called tokens.\",\n",
    "    \"Named Entity Recognition (NER) identifies and classifies entities like person names, organizations, and locations.\",\n",
    "    \"Sentiment analysis determines the emotional tone of a text, such as positive, negative, or neutral.\",\n",
    "    \"The attention mechanism allows models to focus on relevant parts of the input when generating output.\",\n",
    "    \"Transfer learning in NLP involves pre-training a model on large datasets and fine-tuning it for specific tasks.\",\n",
    "    \"LSTM (Long Short-Term Memory) networks are a type of RNN designed to handle long-term dependencies.\",\n",
    "    \"Hugging Face is a popular platform for sharing and using pre-trained NLP models.\",\n",
    "    \"TF-IDF (Term Frequency-Inverse Document Frequency) is a statistical measure used to evaluate word importance.\",\n",
    "    \"Machine translation automatically converts text from one language to another.\",\n",
    "    \"Text summarization condenses long documents into shorter versions while preserving key information.\"\n",
    "]\n",
    "\n",
    "print(f\"✅ 知識庫包含 {len(knowledge_base)} 篇文檔\")\n",
    "print(f\"\\n前 3 篇預覽:\")\n",
    "for i, doc in enumerate(knowledge_base[:3], 1):\n",
    "    print(f\"{i}. {doc[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 文檔向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# 載入 Sentence Transformer 模型\n",
    "embedding_model_name = \"all-MiniLM-L6-v2\"  # 輕量高效\n",
    "\n",
    "print(f\"載入 Embedding 模型: {embedding_model_name}\")\n",
    "embedding_model = SentenceTransformer(embedding_model_name)\n",
    "print(\"✅ 模型載入完成\")\n",
    "\n",
    "# 向量化知識庫\n",
    "print(\"\\n向量化知識庫...\")\n",
    "document_embeddings = embedding_model.encode(\n",
    "    knowledge_base,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True\n",
    ")\n",
    "\n",
    "print(f\"✅ 向量化完成\")\n",
    "print(f\"   文檔數量: {document_embeddings.shape[0]}\")\n",
    "print(f\"   向量維度: {document_embeddings.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 建立 FAISS 索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "# 建立 FAISS 索引\n",
    "dimension = document_embeddings.shape[1]  # 向量維度\n",
    "\n",
    "# 使用 L2 距離 (也可用 Inner Product)\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "# 添加向量到索引\n",
    "index.add(document_embeddings.astype('float32'))\n",
    "\n",
    "print(f\"✅ FAISS 索引建立完成\")\n",
    "print(f\"   索引中文檔數: {index.ntotal}\")\n",
    "print(f\"   向量維度: {dimension}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: 檢索功能實作\n",
    "\n",
    "### 4.1 語義檢索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query, top_k=3):\n",
    "    \"\"\"\n",
    "    語義檢索: 找出最相關的文檔\n",
    "\n",
    "    Args:\n",
    "        query: 查詢問題\n",
    "        top_k: 返回前 K 個結果\n",
    "\n",
    "    Returns:\n",
    "        List of (document, score) tuples\n",
    "    \"\"\"\n",
    "    # 編碼查詢\n",
    "    query_embedding = embedding_model.encode([query], convert_to_numpy=True)\n",
    "\n",
    "    # 檢索\n",
    "    distances, indices = index.search(query_embedding.astype('float32'), top_k)\n",
    "\n",
    "    # 組織結果\n",
    "    results = []\n",
    "    for idx, distance in zip(indices[0], distances[0]):\n",
    "        results.append({\n",
    "            'document': knowledge_base[idx],\n",
    "            'score': float(distance),\n",
    "            'index': int(idx)\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# 測試檢索\n",
    "test_queries = [\n",
    "    \"What is BERT?\",\n",
    "    \"How does attention mechanism work?\",\n",
    "    \"Explain word embeddings\"\n",
    "]\n",
    "\n",
    "print(\"🔍 語義檢索測試\\n\")\n",
    "for query in test_queries:\n",
    "    print(f\"查詢: {query}\")\n",
    "    results = semantic_search(query, top_k=2)\n",
    "\n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"  {i}. (距離: {result['score']:.4f})\")\n",
    "        print(f\"     {result['document'][:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: RAG 問答系統\n",
    "\n",
    "### 5.1 基礎 RAG 實作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# 載入問答模型\n",
    "qa_pipeline = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=\"distilbert-base-cased-distilled-squad\"\n",
    ")\n",
    "\n",
    "def rag_qa(question, top_k=3):\n",
    "    \"\"\"\n",
    "    RAG 問答系統\n",
    "\n",
    "    流程:\n",
    "    1. 檢索相關文檔\n",
    "    2. 合併為 Context\n",
    "    3. 從 Context 中提取答案\n",
    "    \"\"\"\n",
    "    # Step 1: 檢索相關文檔\n",
    "    retrieved_docs = semantic_search(question, top_k=top_k)\n",
    "\n",
    "    # Step 2: 構建 Context\n",
    "    context = \" \".join([doc['document'] for doc in retrieved_docs])\n",
    "\n",
    "    # Step 3: 問答\n",
    "    result = qa_pipeline(\n",
    "        question=question,\n",
    "        context=context\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'answer': result['answer'],\n",
    "        'confidence': result['score'],\n",
    "        'context': context,\n",
    "        'retrieved_docs': retrieved_docs\n",
    "    }\n",
    "\n",
    "\n",
    "# 測試 RAG QA\n",
    "questions = [\n",
    "    \"What is BERT?\",\n",
    "    \"Who developed GPT?\",\n",
    "    \"What is the purpose of tokenization?\"\n",
    "]\n",
    "\n",
    "print(\"🤖 RAG 問答系統測試\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for q in questions:\n",
    "    result = rag_qa(q)\n",
    "\n",
    "    print(f\"\\n❓ Question: {q}\")\n",
    "    print(f\"✅ Answer: {result['answer']}\")\n",
    "    print(f\"   Confidence: {result['confidence']:.2%}\")\n",
    "    print(f\"   Retrieved docs: {len(result['retrieved_docs'])}\")\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 使用 LangChain 簡化實作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS as LangChainFAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import pipeline as hf_pipeline\n",
    "\n",
    "# 1. 建立 Embeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# 2. 建立向量存儲\n",
    "vectorstore = LangChainFAISS.from_texts(\n",
    "    texts=knowledge_base,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "print(\"✅ LangChain 向量存儲建立完成\")\n",
    "\n",
    "# 3. 建立 LLM\n",
    "llm_pipeline = hf_pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=\"google/flan-t5-small\",\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=llm_pipeline)\n",
    "\n",
    "# 4. 建立 RetrievalQA Chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",  # \"stuff\", \"map_reduce\", \"refine\"\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 3}),\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "print(\"✅ QA Chain 建立完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 使用 QA Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 問答測試\n",
    "question = \"What is BERT and who developed it?\"\n",
    "\n",
    "result = qa_chain({\"query\": question})\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"\\nAnswer: {result['result']}\")\n",
    "print(f\"\\n來源文檔:\")\n",
    "for i, doc in enumerate(result['source_documents'], 1):\n",
    "    print(f\"  {i}. {doc.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: 完整 QA 系統類別"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntelligentQASystem:\n",
    "    \"\"\"\n",
    "    智能問答系統\n",
    "    \"\"\"\n",
    "    def __init__(self, knowledge_base, embedding_model_name=\"all-MiniLM-L6-v2\"):\n",
    "        print(\"初始化問答系統...\")\n",
    "\n",
    "        # 載入 Embedding 模型\n",
    "        self.embedding_model = SentenceTransformer(embedding_model_name)\n",
    "\n",
    "        # 載入問答模型\n",
    "        self.qa_model = pipeline(\n",
    "            \"question-answering\",\n",
    "            model=\"distilbert-base-cased-distilled-squad\"\n",
    "        )\n",
    "\n",
    "        # 建立向量索引\n",
    "        self.knowledge_base = knowledge_base\n",
    "        self.index = self._build_index()\n",
    "\n",
    "        print(\"✅ 系統初始化完成\")\n",
    "\n",
    "    def _build_index(self):\n",
    "        \"\"\"建立 FAISS 索引\"\"\"\n",
    "        embeddings = self.embedding_model.encode(\n",
    "            self.knowledge_base,\n",
    "            convert_to_numpy=True\n",
    "        )\n",
    "\n",
    "        dimension = embeddings.shape[1]\n",
    "        index = faiss.IndexFlatL2(dimension)\n",
    "        index.add(embeddings.astype('float32'))\n",
    "\n",
    "        return index\n",
    "\n",
    "    def retrieve(self, query, top_k=3):\n",
    "        \"\"\"檢索相關文檔\"\"\"\n",
    "        query_embedding = self.embedding_model.encode(\n",
    "            [query],\n",
    "            convert_to_numpy=True\n",
    "        )\n",
    "\n",
    "        distances, indices = self.index.search(\n",
    "            query_embedding.astype('float32'),\n",
    "            top_k\n",
    "        )\n",
    "\n",
    "        results = []\n",
    "        for idx, dist in zip(indices[0], distances[0]):\n",
    "            results.append({\n",
    "                'text': self.knowledge_base[idx],\n",
    "                'distance': float(dist),\n",
    "                'relevance': 1 / (1 + float(dist))  # 轉換為相關性分數\n",
    "            })\n",
    "\n",
    "        return results\n",
    "\n",
    "    def answer(self, question, top_k=3, min_confidence=0.3):\n",
    "        \"\"\"\n",
    "        回答問題\n",
    "\n",
    "        Args:\n",
    "            question: 用戶問題\n",
    "            top_k: 檢索文檔數\n",
    "            min_confidence: 最低信心度閾值\n",
    "\n",
    "        Returns:\n",
    "            Dict with answer, confidence, sources\n",
    "        \"\"\"\n",
    "        # Step 1: 檢索\n",
    "        retrieved_docs = self.retrieve(question, top_k=top_k)\n",
    "\n",
    "        # Step 2: 構建 Context\n",
    "        context = \" \".join([doc['text'] for doc in retrieved_docs])\n",
    "\n",
    "        # Step 3: 抽取答案\n",
    "        try:\n",
    "            qa_result = self.qa_model(\n",
    "                question=question,\n",
    "                context=context\n",
    "            )\n",
    "\n",
    "            answer = qa_result['answer']\n",
    "            confidence = qa_result['score']\n",
    "\n",
    "            # 檢查信心度\n",
    "            if confidence < min_confidence:\n",
    "                return {\n",
    "                    'answer': \"I'm not confident about the answer. Could you rephrase?\",\n",
    "                    'confidence': confidence,\n",
    "                    'status': 'low_confidence',\n",
    "                    'retrieved_docs': retrieved_docs\n",
    "                }\n",
    "\n",
    "            return {\n",
    "                'answer': answer,\n",
    "                'confidence': confidence,\n",
    "                'status': 'success',\n",
    "                'context': context,\n",
    "                'retrieved_docs': retrieved_docs\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'answer': \"Sorry, I encountered an error processing your question.\",\n",
    "                'confidence': 0.0,\n",
    "                'status': 'error',\n",
    "                'error': str(e)\n",
    "            }\n",
    "\n",
    "    def add_documents(self, new_documents):\n",
    "        \"\"\"動態添加新文檔到知識庫\"\"\"\n",
    "        # 向量化新文檔\n",
    "        new_embeddings = self.embedding_model.encode(\n",
    "            new_documents,\n",
    "            convert_to_numpy=True\n",
    "        )\n",
    "\n",
    "        # 添加到索引\n",
    "        self.index.add(new_embeddings.astype('float32'))\n",
    "\n",
    "        # 更新知識庫\n",
    "        self.knowledge_base.extend(new_documents)\n",
    "\n",
    "        print(f\"✅ 添加 {len(new_documents)} 篇文檔\")\n",
    "        print(f\"   知識庫總數: {len(self.knowledge_base)}\")\n",
    "\n",
    "\n",
    "# 創建 QA 系統實例\n",
    "qa_system = IntelligentQASystem(knowledge_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 測試問答系統"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 綜合測試\n",
    "test_questions = [\n",
    "    \"What is Natural Language Processing?\",\n",
    "    \"Who developed BERT?\",\n",
    "    \"What is the attention mechanism?\",\n",
    "    \"Explain transfer learning in NLP.\",\n",
    "    \"What platform is used for sharing NLP models?\"\n",
    "]\n",
    "\n",
    "print(\"🎯 問答系統綜合測試\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    result = qa_system.answer(question)\n",
    "\n",
    "    print(f\"\\nQ{i}: {question}\")\n",
    "    print(f\"A: {result['answer']}\")\n",
    "    print(f\"   信心度: {result['confidence']:.2%}\")\n",
    "    print(f\"   狀態: {result['status']}\")\n",
    "\n",
    "    if 'retrieved_docs' in result:\n",
    "        print(f\"   檢索文檔數: {len(result['retrieved_docs'])}\")\n",
    "        print(f\"   最相關: {result['retrieved_docs'][0]['text'][:80]}...\")\n",
    "\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: 從文件構建知識庫\n",
    "\n",
    "### 7.1 載入長文檔並切分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "def load_and_split_documents(file_paths, chunk_size=500, chunk_overlap=50):\n",
    "    \"\"\"\n",
    "    載入文檔並切分為 chunks\n",
    "\n",
    "    Args:\n",
    "        file_paths: 文檔路徑列表\n",
    "        chunk_size: 每個 chunk 的字符數\n",
    "        chunk_overlap: chunks 間重疊字符數\n",
    "\n",
    "    Returns:\n",
    "        List of text chunks\n",
    "    \"\"\"\n",
    "    # 文本切分器\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len\n",
    "    )\n",
    "\n",
    "    all_chunks = []\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        # 載入文檔\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "\n",
    "        # 切分\n",
    "        chunks = text_splitter.split_text(text)\n",
    "        all_chunks.extend(chunks)\n",
    "\n",
    "    return all_chunks\n",
    "\n",
    "\n",
    "# 範例: 載入 README 文檔\n",
    "# doc_paths = ['README.md', 'docs/GUIDE.md']\n",
    "# chunks = load_and_split_documents(doc_paths)\n",
    "# print(f\"✅ 載入 {len(chunks)} 個文檔塊\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 從網頁構建知識庫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示範: 爬取網頁內容 (需要 beautifulsoup4)\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_webpage(url):\n",
    "    \"\"\"\n",
    "    爬取網頁文本\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # 移除 script 和 style 標籤\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.decompose()\n",
    "\n",
    "        # 提取文本\n",
    "        text = soup.get_text(separator='\\n', strip=True)\n",
    "\n",
    "        return text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"爬取失敗: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# 示範: 從 Hugging Face 文檔構建知識庫\n",
    "# urls = [\n",
    "#     'https://huggingface.co/docs/transformers/index',\n",
    "#     'https://huggingface.co/docs/transformers/quicktour'\n",
    "# ]\n",
    "#\n",
    "# web_docs = []\n",
    "# for url in urls:\n",
    "#     text = scrape_webpage(url)\n",
    "#     if text:\n",
    "#         web_docs.append(text)\n",
    "#\n",
    "# qa_system.add_documents(web_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: 進階功能\n",
    "\n",
    "### 8.1 混合檢索 (Hybrid Retrieval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class HybridRetriever:\n",
    "    \"\"\"\n",
    "    混合檢索: 語義檢索 + 關鍵詞檢索\n",
    "    \"\"\"\n",
    "    def __init__(self, documents, semantic_weight=0.7):\n",
    "        self.documents = documents\n",
    "        self.semantic_weight = semantic_weight\n",
    "        self.keyword_weight = 1 - semantic_weight\n",
    "\n",
    "        # 語義檢索: Sentence Transformer\n",
    "        self.embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "        self.doc_embeddings = self.embedding_model.encode(documents)\n",
    "\n",
    "        # 關鍵詞檢索: TF-IDF\n",
    "        self.tfidf = TfidfVectorizer()\n",
    "        self.tfidf_matrix = self.tfidf.fit_transform(documents)\n",
    "\n",
    "    def retrieve(self, query, top_k=5):\n",
    "        \"\"\"混合檢索\"\"\"\n",
    "        # 語義檢索分數\n",
    "        query_embedding = self.embedding_model.encode([query])\n",
    "        semantic_scores = cosine_similarity(query_embedding, self.doc_embeddings)[0]\n",
    "\n",
    "        # 關鍵詞檢索分數\n",
    "        query_tfidf = self.tfidf.transform([query])\n",
    "        keyword_scores = cosine_similarity(query_tfidf, self.tfidf_matrix)[0]\n",
    "\n",
    "        # 混合分數\n",
    "        hybrid_scores = (\n",
    "            self.semantic_weight * semantic_scores +\n",
    "            self.keyword_weight * keyword_scores\n",
    "        )\n",
    "\n",
    "        # 排序\n",
    "        top_indices = np.argsort(hybrid_scores)[::-1][:top_k]\n",
    "\n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            results.append({\n",
    "                'text': self.documents[idx],\n",
    "                'hybrid_score': hybrid_scores[idx],\n",
    "                'semantic_score': semantic_scores[idx],\n",
    "                'keyword_score': keyword_scores[idx]\n",
    "            })\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "# 測試混合檢索\n",
    "hybrid_retriever = HybridRetriever(knowledge_base, semantic_weight=0.7)\n",
    "\n",
    "query = \"transformer attention mechanism\"\n",
    "results = hybrid_retriever.retrieve(query, top_k=3)\n",
    "\n",
    "print(f\"查詢: {query}\\n\")\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"{i}. 混合分數: {result['hybrid_score']:.4f}\")\n",
    "    print(f\"   (語義: {result['semantic_score']:.4f}, 關鍵詞: {result['keyword_score']:.4f})\")\n",
    "    print(f\"   {result['text'][:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 答案重排序 (Re-ranking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# 使用交叉編碼器進行重排序\n",
    "reranker_model_name = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "reranker_tokenizer = AutoTokenizer.from_pretrained(reranker_model_name)\n",
    "reranker_model = AutoModelForSequenceClassification.from_pretrained(reranker_model_name)\n",
    "\n",
    "def rerank_documents(query, documents, top_k=3):\n",
    "    \"\"\"\n",
    "    使用交叉編碼器重排序文檔\n",
    "    \"\"\"\n",
    "    # 構建 query-document pairs\n",
    "    pairs = [[query, doc] for doc in documents]\n",
    "\n",
    "    # 編碼\n",
    "    inputs = reranker_tokenizer(\n",
    "        pairs,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors='pt',\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "    # 計算相關性分數\n",
    "    with torch.no_grad():\n",
    "        scores = reranker_model(**inputs).logits.squeeze().tolist()\n",
    "\n",
    "    # 排序\n",
    "    ranked = sorted(\n",
    "        zip(documents, scores),\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True\n",
    "    )\n",
    "\n",
    "    return ranked[:top_k]\n",
    "\n",
    "\n",
    "# 測試重排序\n",
    "query = \"What is BERT?\"\n",
    "candidate_docs = [doc['text'] for doc in qa_system.retrieve(query, top_k=5)]\n",
    "\n",
    "reranked = rerank_documents(query, candidate_docs, top_k=3)\n",
    "\n",
    "print(f\"查詢: {query}\\n\")\n",
    "print(\"重排序結果:\")\n",
    "for i, (doc, score) in enumerate(reranked, 1):\n",
    "    print(f\"{i}. (分數: {score:.4f})\")\n",
    "    print(f\"   {doc[:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: 生產部署 (FastAPI)\n",
    "\n",
    "### 9.1 FastAPI 服務"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile qa_api.py\n",
    "# qa_api.py - 問答系統 API\n",
    "\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "import uvicorn\n",
    "\n",
    "# 初始化 FastAPI\n",
    "app = FastAPI(\n",
    "    title=\"Intelligent QA System API\",\n",
    "    description=\"RAG-based Question Answering System\",\n",
    "    version=\"1.0.0\"\n",
    ")\n",
    "\n",
    "# 全局變數: QA 系統實例\n",
    "qa_system = None\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "async def load_qa_system():\n",
    "    \"\"\"啟動時載入 QA 系統\"\"\"\n",
    "    global qa_system\n",
    "\n",
    "    print(\"載入問答系統...\")\n",
    "    # 載入知識庫\n",
    "    knowledge_base = load_knowledge_base()  # 從文件或數據庫載入\n",
    "    qa_system = IntelligentQASystem(knowledge_base)\n",
    "    print(\"✅ 問答系統載入完成\")\n",
    "\n",
    "# Request/Response Models\n",
    "class QuestionInput(BaseModel):\n",
    "    question: str = Field(..., min_length=5, max_length=500)\n",
    "    top_k: Optional[int] = Field(default=3, ge=1, le=10)\n",
    "\n",
    "class AnswerResponse(BaseModel):\n",
    "    answer: str\n",
    "    confidence: float\n",
    "    status: str\n",
    "    sources: List[str]\n",
    "\n",
    "class DocumentInput(BaseModel):\n",
    "    documents: List[str]\n",
    "\n",
    "# 問答端點\n",
    "@app.post(\"/ask\", response_model=AnswerResponse)\n",
    "async def ask_question(input_data: QuestionInput):\n",
    "    \"\"\"\n",
    "    回答用戶問題\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = qa_system.answer(\n",
    "            question=input_data.question,\n",
    "            top_k=input_data.top_k\n",
    "        )\n",
    "\n",
    "        sources = [\n",
    "            doc['text'][:100] + \"...\"\n",
    "            for doc in result.get('retrieved_docs', [])\n",
    "        ]\n",
    "\n",
    "        return AnswerResponse(\n",
    "            answer=result['answer'],\n",
    "            confidence=result['confidence'],\n",
    "            status=result['status'],\n",
    "            sources=sources\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "# 添加文檔端點\n",
    "@app.post(\"/add_documents\")\n",
    "async def add_documents(input_data: DocumentInput):\n",
    "    \"\"\"\n",
    "    動態添加新文檔到知識庫\n",
    "    \"\"\"\n",
    "    try:\n",
    "        qa_system.add_documents(input_data.documents)\n",
    "        return {\n",
    "            \"message\": f\"Added {len(input_data.documents)} documents\",\n",
    "            \"total_documents\": len(qa_system.knowledge_base)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "# 健康檢查\n",
    "@app.get(\"/health\")\n",
    "async def health_check():\n",
    "    return {\n",
    "        \"status\": \"healthy\",\n",
    "        \"qa_system_loaded\": qa_system is not None,\n",
    "        \"knowledge_base_size\": len(qa_system.knowledge_base) if qa_system else 0\n",
    "    }\n",
    "\n",
    "# 系統資訊\n",
    "@app.get(\"/info\")\n",
    "async def system_info():\n",
    "    return {\n",
    "        \"knowledge_base_size\": len(qa_system.knowledge_base),\n",
    "        \"embedding_model\": \"all-MiniLM-L6-v2\",\n",
    "        \"qa_model\": \"distilbert-base-cased-distilled-squad\"\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 測試 API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile test_qa_api.py\n",
    "# test_qa_api.py\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "API_URL = \"http://localhost:8000\"\n",
    "\n",
    "# 測試問答\n",
    "response = requests.post(\n",
    "    f\"{API_URL}/ask\",\n",
    "    json={\n",
    "        \"question\": \"What is BERT?\",\n",
    "        \"top_k\": 3\n",
    "    }\n",
    ")\n",
    "\n",
    "result = response.json()\n",
    "print(f\"Question: What is BERT?\")\n",
    "print(f\"Answer: {result['answer']}\")\n",
    "print(f\"Confidence: {result['confidence']:.2%}\")\n",
    "print(f\"\\nSources:\")\n",
    "for i, source in enumerate(result['sources'], 1):\n",
    "    print(f\"  {i}. {source}\")\n",
    "\n",
    "# 添加新文檔\n",
    "response = requests.post(\n",
    "    f\"{API_URL}/add_documents\",\n",
    "    json={\n",
    "        \"documents\": [\n",
    "            \"RAG is a powerful technique that combines retrieval and generation.\"\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"\\nAdd documents: {response.json()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: 總結與擴展\n",
    "\n",
    "### ✅ 本專案完成內容\n",
    "\n",
    "1. **核心 RAG 系統**\n",
    "   - 文檔向量化 (Sentence Transformer)\n",
    "   - FAISS 向量檢索\n",
    "   - 問答模型整合\n",
    "\n",
    "2. **進階功能**\n",
    "   - 混合檢索 (語義+關鍵詞)\n",
    "   - 答案重排序\n",
    "   - 信心度過濾\n",
    "\n",
    "3. **生產部署**\n",
    "   - FastAPI 服務\n",
    "   - 動態知識庫更新\n",
    "   - API 文檔 (Swagger)\n",
    "\n",
    "### 🚀 進階擴展方向\n",
    "\n",
    "#### 技術優化\n",
    "- [ ] 使用更大的 Embedding 模型\n",
    "- [ ] 整合 GPT-3.5/GPT-4 API\n",
    "- [ ] 實作對話式問答 (Multi-turn QA)\n",
    "- [ ] 添加引用來源追蹤\n",
    "\n",
    "#### 功能擴展\n",
    "- [ ] 多語言支持\n",
    "- [ ] 結構化數據問答 (表格、圖表)\n",
    "- [ ] 複雜推理 (Chain-of-Thought)\n",
    "- [ ] 事實驗證\n",
    "\n",
    "#### 應用場景\n",
    "- [ ] 企業知識管理系統\n",
    "- [ ] 智能文檔助手\n",
    "- [ ] 法律/醫療問答\n",
    "- [ ] 教育輔助系統\n",
    "\n",
    "### 📚 延伸閱讀\n",
    "\n",
    "- [RAG 論文](https://arxiv.org/abs/2005.11401)\n",
    "- [FAISS 文檔](https://github.com/facebookresearch/faiss)\n",
    "- [LangChain 文檔](https://python.langchain.com/docs/get_started/introduction)\n",
    "- [Sentence Transformers](https://www.sbert.net/)\n",
    "\n",
    "---\n",
    "\n",
    "**專案版本**: v1.0\n",
    "**建立日期**: 2025-10-17\n",
    "**作者**: iSpan NLP Team\n",
    "**授權**: MIT License"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
