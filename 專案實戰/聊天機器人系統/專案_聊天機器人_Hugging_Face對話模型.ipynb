{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# å°ˆæ¡ˆå¯¦æˆ°: èŠå¤©æ©Ÿå™¨äººç³»çµ± (Chatbot with Hugging Face)\n",
    "\n",
    "**å°ˆæ¡ˆé¡å‹**: å°è©±ç³»çµ± - åŸºæ–¼ Transformer çš„èŠå¤©æ©Ÿå™¨äºº\n",
    "**é›£åº¦**: â­â­â­â­ é€²éš\n",
    "**é è¨ˆæ™‚é–“**: 4-5 å°æ™‚\n",
    "**æŠ€è¡“æ£§**: Hugging Face Transformers, DialoGPT, Streamlit\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š å­¸ç¿’ç›®æ¨™\n",
    "\n",
    "å®Œæˆæœ¬å°ˆæ¡ˆå¾Œ,æ‚¨å°‡èƒ½å¤ :\n",
    "\n",
    "1. âœ… ç†è§£å°è©±ç³»çµ±çš„æ ¸å¿ƒæ¶æ§‹\n",
    "2. âœ… ä½¿ç”¨ Hugging Face å°è©±æ¨¡å‹ (DialoGPT, Blenderbot)\n",
    "3. âœ… å¯¦ä½œå¤šè¼ªå°è©±ç®¡ç†\n",
    "4. âœ… æ§‹å»ºäº’å‹•å¼èŠå¤©ä»‹é¢\n",
    "5. âœ… éƒ¨ç½²ç”Ÿç”¢ç´šèŠå¤©æ©Ÿå™¨äºº\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: å°è©±ç³»çµ±åŸºç¤\n",
    "\n",
    "### 1.1 å°è©±ç³»çµ±é¡å‹\n",
    "\n",
    "| é¡å‹ | èªªæ˜ | æ‡‰ç”¨å ´æ™¯ | ç¯„ä¾‹ |\n",
    "|------|------|---------|------|\n",
    "| **æª¢ç´¢å¼** | å¾é å®šç¾©å›è¦†ä¸­é¸æ“‡ | å®¢æœ FAQ | é¸æ“‡æœ€ç›¸é—œç­”æ¡ˆ |\n",
    "| **ç”Ÿæˆå¼** | å‹•æ…‹ç”Ÿæˆå›è¦† | é–‹æ”¾å°è©± | GPT, DialoGPT |\n",
    "| **ä»»å‹™å°å‘** | å®Œæˆç‰¹å®šä»»å‹™ | è¨‚ç¥¨ã€æŸ¥è©¢ | æ§½å¡«å……å°è©± |\n",
    "| **é–’èŠå‹** | è‡ªç”±å°è©± | é™ªä¼´æ©Ÿå™¨äºº | Blenderbot |\n",
    "\n",
    "### 1.2 æœ¬å°ˆæ¡ˆæ¶æ§‹\n",
    "\n",
    "```\n",
    "ç”¨æˆ¶è¼¸å…¥\n",
    "    â†“\n",
    "æ–‡æœ¬é è™•ç†\n",
    "    â”œâ”€â”€ Tokenization\n",
    "    â”œâ”€â”€ æ·»åŠ æ­·å²å°è©±\n",
    "    â””â”€â”€ æ§‹å»º Context\n",
    "    â†“\n",
    "å°è©±æ¨¡å‹ (DialoGPT)\n",
    "    â”œâ”€â”€ Encoder: ç†è§£è¼¸å…¥\n",
    "    â”œâ”€â”€ Decoder: ç”Ÿæˆå›è¦†\n",
    "    â””â”€â”€ Attention: é—œæ³¨é‡é»\n",
    "    â†“\n",
    "å¾Œè™•ç†\n",
    "    â”œâ”€â”€ Beam Search é¸æ“‡æœ€ä½³å›è¦†\n",
    "    â”œâ”€â”€ éæ¿¾ä¸ç•¶å…§å®¹\n",
    "    â””â”€â”€ æ ¼å¼åŒ–è¼¸å‡º\n",
    "    â†“\n",
    "æ©Ÿå™¨äººå›è¦†\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: ç’°å¢ƒæº–å‚™èˆ‡å¥—ä»¶å®‰è£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£å¿…è¦å¥—ä»¶\n",
    "# !pip install transformers torch accelerate streamlit -q\n",
    "\n",
    "# é©—è­‰å®‰è£\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "print(f\"âœ… Transformers ç‰ˆæœ¬: {transformers.__version__}\")\n",
    "print(f\"âœ… PyTorch ç‰ˆæœ¬: {torch.__version__}\")\n",
    "print(f\"âœ… CUDA å¯ç”¨: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: åŸºç¤èŠå¤©æ©Ÿå™¨äººå¯¦ä½œ\n",
    "\n",
    "### 3.1 ä½¿ç”¨ DialoGPT (Microsoft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# è¼‰å…¥ DialoGPT æ¨¡å‹ (3 ç¨®è¦æ¨¡å¯é¸)\n",
    "model_name = \"microsoft/DialoGPT-medium\"  # small, medium, large\n",
    "\n",
    "print(f\"è¼‰å…¥æ¨¡å‹: {model_name}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "print(f\"âœ… æ¨¡å‹è¼‰å…¥å®Œæˆ\")\n",
    "print(f\"   åƒæ•¸é‡: {model.num_parameters():,}\")\n",
    "print(f\"   è©å½™è¡¨å¤§å°: {tokenizer.vocab_size:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 å–®è¼ªå°è©±å¯¦ä½œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(user_input, chat_history_ids=None, max_length=1000):\n",
    "    \"\"\"\n",
    "    ç”ŸæˆèŠå¤©æ©Ÿå™¨äººå›è¦†\n",
    "\n",
    "    Args:\n",
    "        user_input: ç”¨æˆ¶è¼¸å…¥æ–‡æœ¬\n",
    "        chat_history_ids: å°è©±æ­·å² (ç”¨æ–¼å¤šè¼ªå°è©±)\n",
    "        max_length: æœ€å¤§ç”Ÿæˆé•·åº¦\n",
    "\n",
    "    Returns:\n",
    "        response: æ©Ÿå™¨äººå›è¦†\n",
    "        new_chat_history_ids: æ›´æ–°çš„å°è©±æ­·å²\n",
    "    \"\"\"\n",
    "    # ç·¨ç¢¼ç”¨æˆ¶è¼¸å…¥\n",
    "    new_input_ids = tokenizer.encode(\n",
    "        user_input + tokenizer.eos_token,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    # åˆä½µå°è©±æ­·å²\n",
    "    if chat_history_ids is not None:\n",
    "        bot_input_ids = torch.cat([chat_history_ids, new_input_ids], dim=-1)\n",
    "    else:\n",
    "        bot_input_ids = new_input_ids\n",
    "\n",
    "    # ç”Ÿæˆå›è¦†\n",
    "    chat_history_ids = model.generate(\n",
    "        bot_input_ids,\n",
    "        max_length=max_length,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,           # å•Ÿç”¨æ¡æ¨£\n",
    "        top_k=50,                 # Top-K æ¡æ¨£\n",
    "        top_p=0.95,               # Nucleus æ¡æ¨£\n",
    "        temperature=0.7           # æ§åˆ¶å‰µé€ æ€§\n",
    "    )\n",
    "\n",
    "    # è§£ç¢¼å›è¦†\n",
    "    response = tokenizer.decode(\n",
    "        chat_history_ids[:, bot_input_ids.shape[-1]:][0],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    return response, chat_history_ids\n",
    "\n",
    "\n",
    "# æ¸¬è©¦å–®è¼ªå°è©±\n",
    "user_input = \"Hello! How are you?\"\n",
    "response, history = generate_response(user_input)\n",
    "\n",
    "print(f\"User: {user_input}\")\n",
    "print(f\"Bot: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 å¤šè¼ªå°è©±å¯¦ä½œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¤šè¼ªå°è©±æ¸¬è©¦\n",
    "chat_history_ids = None\n",
    "conversation = [\n",
    "    \"Hi there!\",\n",
    "    \"What's your favorite programming language?\",\n",
    "    \"Why do you like it?\",\n",
    "    \"Do you know about NLP?\",\n",
    "    \"Goodbye!\"\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"å¤šè¼ªå°è©±ç¤ºç¯„\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, user_input in enumerate(conversation, 1):\n",
    "    response, chat_history_ids = generate_response(\n",
    "        user_input,\n",
    "        chat_history_ids=chat_history_ids\n",
    "    )\n",
    "\n",
    "    print(f\"\\nç¬¬ {i} è¼ªå°è©±:\")\n",
    "    print(f\"User: {user_input}\")\n",
    "    print(f\"Bot: {response}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: é€²éšåŠŸèƒ½å¯¦ä½œ\n",
    "\n",
    "### 4.1 å°è©±æ­·å²ç®¡ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatHistoryManager:\n",
    "    \"\"\"\n",
    "    ç®¡ç†å°è©±æ­·å²ï¼Œé˜²æ­¢è¨˜æ†¶é«”æº¢å‡º\n",
    "    \"\"\"\n",
    "    def __init__(self, max_history_length=1000):\n",
    "        self.max_history_length = max_history_length\n",
    "        self.chat_history_ids = None\n",
    "\n",
    "    def add_to_history(self, new_input_ids, response_ids):\n",
    "        \"\"\"æ·»åŠ å°è©±åˆ°æ­·å²\"\"\"\n",
    "        if self.chat_history_ids is None:\n",
    "            self.chat_history_ids = torch.cat([new_input_ids, response_ids], dim=-1)\n",
    "        else:\n",
    "            self.chat_history_ids = torch.cat(\n",
    "                [self.chat_history_ids, new_input_ids, response_ids],\n",
    "                dim=-1\n",
    "            )\n",
    "\n",
    "        # é™åˆ¶æ­·å²é•·åº¦\n",
    "        if self.chat_history_ids.shape[-1] > self.max_history_length:\n",
    "            self.chat_history_ids = self.chat_history_ids[:, -self.max_history_length:]\n",
    "\n",
    "    def get_history(self):\n",
    "        \"\"\"ç²å–ç•¶å‰æ­·å²\"\"\"\n",
    "        return self.chat_history_ids\n",
    "\n",
    "    def clear_history(self):\n",
    "        \"\"\"æ¸…ç©ºæ­·å²\"\"\"\n",
    "        self.chat_history_ids = None\n",
    "\n",
    "    def get_conversation_text(self, tokenizer):\n",
    "        \"\"\"ç²å–å°è©±æ­·å²æ–‡æœ¬\"\"\"\n",
    "        if self.chat_history_ids is None:\n",
    "            return \"\"\n",
    "        return tokenizer.decode(self.chat_history_ids[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# ä½¿ç”¨ç¯„ä¾‹\n",
    "history_manager = ChatHistoryManager(max_history_length=500)\n",
    "\n",
    "user_input = \"Tell me about Python\"\n",
    "response, history_ids = generate_response(user_input)\n",
    "\n",
    "print(f\"User: {user_input}\")\n",
    "print(f\"Bot: {response}\")\n",
    "print(f\"\\næ­·å²é•·åº¦: {history_ids.shape[-1]} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 å›è¦†å“è³ªæ§åˆ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class ResponseFilter:\n",
    "    \"\"\"\n",
    "    éæ¿¾ä¸ç•¶å›è¦†,æå‡å°è©±å“è³ª\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # ä¸ç•¶è©å½™åˆ—è¡¨ (ç¤ºç¯„,å¯¦éš›æ‡‰æ›´å®Œæ•´)\n",
    "        self.blocked_words = set(['badword1', 'badword2'])\n",
    "\n",
    "    def is_valid_response(self, response):\n",
    "        \"\"\"\n",
    "        æª¢æŸ¥å›è¦†æ˜¯å¦åˆæ ¼\n",
    "        \"\"\"\n",
    "        # æª¢æŸ¥ 1: éç©º\n",
    "        if not response or len(response.strip()) == 0:\n",
    "            return False, \"ç©ºå›è¦†\"\n",
    "\n",
    "        # æª¢æŸ¥ 2: é•·åº¦åˆç†\n",
    "        if len(response) < 3:\n",
    "            return False, \"å›è¦†éçŸ­\"\n",
    "\n",
    "        if len(response) > 500:\n",
    "            return False, \"å›è¦†éé•·\"\n",
    "\n",
    "        # æª¢æŸ¥ 3: ç„¡é‡è¤‡ (é¿å… \"I I I I...\")\n",
    "        words = response.split()\n",
    "        if len(words) > 3 and len(set(words)) < len(words) * 0.3:\n",
    "            return False, \"éåº¦é‡è¤‡\"\n",
    "\n",
    "        # æª¢æŸ¥ 4: ç„¡ä¸ç•¶è©å½™\n",
    "        if any(word in response.lower() for word in self.blocked_words):\n",
    "            return False, \"åŒ…å«ä¸ç•¶è©å½™\"\n",
    "\n",
    "        return True, \"åˆæ ¼\"\n",
    "\n",
    "    def clean_response(self, response):\n",
    "        \"\"\"\n",
    "        æ¸…ç†å›è¦†\n",
    "        \"\"\"\n",
    "        # ç§»é™¤å¤šé¤˜ç©ºç™½\n",
    "        response = re.sub(r'\\s+', ' ', response).strip()\n",
    "\n",
    "        # ç¢ºä¿å¥è™Ÿçµå°¾\n",
    "        if response and response[-1] not in '.!?':\n",
    "            response += '.'\n",
    "\n",
    "        return response\n",
    "\n",
    "\n",
    "# æ¸¬è©¦å›è¦†éæ¿¾\n",
    "filter = ResponseFilter()\n",
    "\n",
    "test_responses = [\n",
    "    \"I love Python programming!\",\n",
    "    \"I I I I I\",  # é‡è¤‡\n",
    "    \"\",  # ç©ºå›è¦†\n",
    "    \"Ok\"  # éçŸ­\n",
    "]\n",
    "\n",
    "for resp in test_responses:\n",
    "    is_valid, reason = filter.is_valid_response(resp)\n",
    "    print(f\"å›è¦†: '{resp}'\")\n",
    "    print(f\"  æœ‰æ•ˆ: {is_valid} ({reason})\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 å®Œæ•´èŠå¤©æ©Ÿå™¨äººé¡åˆ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chatbot:\n",
    "    \"\"\"\n",
    "    å®Œæ•´çš„èŠå¤©æ©Ÿå™¨äººç³»çµ±\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name=\"microsoft/DialoGPT-medium\"):\n",
    "        print(f\"åˆå§‹åŒ–èŠå¤©æ©Ÿå™¨äºº: {model_name}\")\n",
    "\n",
    "        # è¼‰å…¥æ¨¡å‹\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "        # å°è©±æ­·å²\n",
    "        self.chat_history_ids = None\n",
    "\n",
    "        # å›è¦†éæ¿¾å™¨\n",
    "        self.response_filter = ResponseFilter()\n",
    "\n",
    "        print(\"âœ… èŠå¤©æ©Ÿå™¨äººæº–å‚™å®Œæˆ!\")\n",
    "\n",
    "    def chat(self, user_input, max_retries=3):\n",
    "        \"\"\"\n",
    "        è™•ç†ç”¨æˆ¶è¼¸å…¥ä¸¦ç”Ÿæˆå›è¦†\n",
    "        \"\"\"\n",
    "        # ç·¨ç¢¼è¼¸å…¥\n",
    "        new_input_ids = self.tokenizer.encode(\n",
    "            user_input + self.tokenizer.eos_token,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # åˆä½µæ­·å²\n",
    "        if self.chat_history_ids is not None:\n",
    "            bot_input_ids = torch.cat([self.chat_history_ids, new_input_ids], dim=-1)\n",
    "        else:\n",
    "            bot_input_ids = new_input_ids\n",
    "\n",
    "        # å˜—è©¦ç”Ÿæˆåˆæ ¼å›è¦† (æœ€å¤šé‡è©¦ 3 æ¬¡)\n",
    "        for attempt in range(max_retries):\n",
    "            # ç”Ÿæˆå›è¦†\n",
    "            chat_history_ids = self.model.generate(\n",
    "                bot_input_ids,\n",
    "                max_length=1000,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "                do_sample=True,\n",
    "                top_k=50,\n",
    "                top_p=0.95,\n",
    "                temperature=0.7 + (attempt * 0.1)  # é‡è©¦æ™‚å¢åŠ æº«åº¦\n",
    "            )\n",
    "\n",
    "            # è§£ç¢¼\n",
    "            response = self.tokenizer.decode(\n",
    "                chat_history_ids[:, bot_input_ids.shape[-1]:][0],\n",
    "                skip_special_tokens=True\n",
    "            )\n",
    "\n",
    "            # æª¢æŸ¥å“è³ª\n",
    "            is_valid, reason = self.response_filter.is_valid_response(response)\n",
    "\n",
    "            if is_valid:\n",
    "                # æ›´æ–°æ­·å²\n",
    "                self.chat_history_ids = chat_history_ids\n",
    "\n",
    "                # æ¸…ç†å›è¦†\n",
    "                response = self.response_filter.clean_response(response)\n",
    "\n",
    "                return response\n",
    "            else:\n",
    "                print(f\"  å˜—è©¦ {attempt+1}: å›è¦†ç„¡æ•ˆ ({reason})\")\n",
    "\n",
    "        # æ‰€æœ‰é‡è©¦å¤±æ•—,è¿”å›å‚™ç”¨å›è¦†\n",
    "        return \"I'm sorry, I didn't quite understand that. Could you rephrase?\"\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"é‡ç½®å°è©±æ­·å²\"\"\"\n",
    "        self.chat_history_ids = None\n",
    "        print(\"âœ… å°è©±æ­·å²å·²æ¸…ç©º\")\n",
    "\n",
    "    def get_conversation_length(self):\n",
    "        \"\"\"ç²å–å°è©±é•·åº¦\"\"\"\n",
    "        if self.chat_history_ids is None:\n",
    "            return 0\n",
    "        return self.chat_history_ids.shape[-1]\n",
    "\n",
    "\n",
    "# å‰µå»ºèŠå¤©æ©Ÿå™¨äººå¯¦ä¾‹\n",
    "bot = Chatbot(model_name=\"microsoft/DialoGPT-medium\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 äº’å‹•å¼å°è©±ä»‹é¢ (Jupyter Widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML, clear_output\n",
    "import ipywidgets as widgets\n",
    "\n",
    "class ChatInterface:\n",
    "    \"\"\"\n",
    "    Jupyter Notebook èŠå¤©ä»‹é¢\n",
    "    \"\"\"\n",
    "    def __init__(self, chatbot):\n",
    "        self.chatbot = chatbot\n",
    "        self.conversation_history = []\n",
    "\n",
    "        # UI å…ƒä»¶\n",
    "        self.output = widgets.Output()\n",
    "        self.user_input = widgets.Text(\n",
    "            placeholder='è¼¸å…¥è¨Šæ¯...',\n",
    "            description='æ‚¨:',\n",
    "            layout=widgets.Layout(width='80%')\n",
    "        )\n",
    "        self.send_button = widgets.Button(\n",
    "            description='ç™¼é€',\n",
    "            button_style='primary'\n",
    "        )\n",
    "        self.reset_button = widgets.Button(\n",
    "            description='é‡ç½®å°è©±',\n",
    "            button_style='warning'\n",
    "        )\n",
    "\n",
    "        # ç¶å®šäº‹ä»¶\n",
    "        self.send_button.on_click(self.on_send)\n",
    "        self.reset_button.on_click(self.on_reset)\n",
    "        self.user_input.on_submit(self.on_send)\n",
    "\n",
    "    def on_send(self, b):\n",
    "        \"\"\"è™•ç†ç™¼é€äº‹ä»¶\"\"\"\n",
    "        user_message = self.user_input.value.strip()\n",
    "\n",
    "        if not user_message:\n",
    "            return\n",
    "\n",
    "        # æ¸…ç©ºè¼¸å…¥æ¡†\n",
    "        self.user_input.value = ''\n",
    "\n",
    "        # ç”Ÿæˆå›è¦†\n",
    "        with self.output:\n",
    "            print(f\"\\nğŸ‘¤ You: {user_message}\")\n",
    "\n",
    "        bot_response = self.chatbot.chat(user_message)\n",
    "\n",
    "        with self.output:\n",
    "            print(f\"ğŸ¤– Bot: {bot_response}\")\n",
    "            print(\"-\" * 60)\n",
    "\n",
    "        # è¨˜éŒ„å°è©±\n",
    "        self.conversation_history.append({\n",
    "            'user': user_message,\n",
    "            'bot': bot_response\n",
    "        })\n",
    "\n",
    "    def on_reset(self, b):\n",
    "        \"\"\"é‡ç½®å°è©±\"\"\"\n",
    "        self.chatbot.reset()\n",
    "        self.conversation_history = []\n",
    "\n",
    "        with self.output:\n",
    "            clear_output()\n",
    "            print(\"âœ… å°è©±å·²é‡ç½®\")\n",
    "\n",
    "    def display(self):\n",
    "        \"\"\"é¡¯ç¤ºèŠå¤©ä»‹é¢\"\"\"\n",
    "        input_box = widgets.HBox([self.user_input, self.send_button, self.reset_button])\n",
    "        chat_box = widgets.VBox([self.output, input_box])\n",
    "\n",
    "        display(HTML(\"<h3>ğŸ’¬ èŠå¤©æ©Ÿå™¨äººä»‹é¢</h3>\"))\n",
    "        display(chat_box)\n",
    "\n",
    "\n",
    "# å•Ÿå‹•èŠå¤©ä»‹é¢\n",
    "chat_interface = ChatInterface(bot)\n",
    "chat_interface.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: ä½¿ç”¨ Blenderbot (Meta)\n",
    "\n",
    "### 5.1 Blenderbot vs DialoGPT å°æ¯”\n",
    "\n",
    "| æ¨¡å‹ | é–‹ç™¼è€… | è¦æ¨¡ | ç‰¹è‰² | é©ç”¨å ´æ™¯ |\n",
    "|------|--------|------|------|----------|\n",
    "| **DialoGPT** | Microsoft | 117M-762M | åŸºæ–¼ Reddit è¨“ç·´ | é–‹æ”¾é–’èŠ |\n",
    "| **Blenderbot** | Meta (Facebook) | 90M-9.4B | å¤šæŠ€èƒ½æ•´åˆ | çŸ¥è­˜å•ç­”+é–’èŠ |\n",
    "| **DialoGPT** | ç”Ÿæˆé€Ÿåº¦å¿« | è¼ƒè¼•é‡ | è‹±æ–‡å°è©± |\n",
    "| **Blenderbot** | çŸ¥è­˜è±å¯Œ | è¼ƒé‡é‡ | å¤šèªè¨€æ”¯æŒ |\n",
    "\n",
    "### 5.2 ä½¿ç”¨ Blenderbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BlenderbotTokenizer, BlenderbotForConditionalGeneration\n",
    "\n",
    "# è¼‰å…¥ Blenderbot\n",
    "blenderbot_model_name = \"facebook/blenderbot-400M-distill\"\n",
    "\n",
    "print(f\"è¼‰å…¥ Blenderbot: {blenderbot_model_name}\")\n",
    "blenderbot_tokenizer = BlenderbotTokenizer.from_pretrained(blenderbot_model_name)\n",
    "blenderbot_model = BlenderbotForConditionalGeneration.from_pretrained(blenderbot_model_name)\n",
    "\n",
    "print(\"âœ… Blenderbot è¼‰å…¥å®Œæˆ\")\n",
    "\n",
    "\n",
    "def chat_with_blenderbot(user_input):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨ Blenderbot å°è©±\n",
    "    \"\"\"\n",
    "    inputs = blenderbot_tokenizer([user_input], return_tensors=\"pt\")\n",
    "\n",
    "    reply_ids = blenderbot_model.generate(\n",
    "        **inputs,\n",
    "        max_length=128,\n",
    "        num_beams=5,              # Beam Search\n",
    "        early_stopping=True,\n",
    "        no_repeat_ngram_size=3   # é¿å…é‡è¤‡\n",
    "    )\n",
    "\n",
    "    response = blenderbot_tokenizer.decode(\n",
    "        reply_ids[0],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "# æ¸¬è©¦ Blenderbot\n",
    "test_inputs = [\n",
    "    \"What is natural language processing?\",\n",
    "    \"Tell me about Python programming.\",\n",
    "    \"What's your favorite book?\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ¤– Blenderbot å°è©±æ¸¬è©¦\\n\")\n",
    "for inp in test_inputs:\n",
    "    response = chat_with_blenderbot(inp)\n",
    "    print(f\"User: {inp}\")\n",
    "    print(f\"Bot: {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: ç‰¹æ®ŠåŠŸèƒ½æ“´å±•\n",
    "\n",
    "### 6.1 æƒ…ç·’è­˜åˆ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# è¼‰å…¥æƒ…æ„Ÿåˆ†æå™¨\n",
    "emotion_classifier = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"j-hartmann/emotion-english-distilroberta-base\"\n",
    ")\n",
    "\n",
    "def detect_emotion(text):\n",
    "    \"\"\"\n",
    "    æª¢æ¸¬ç”¨æˆ¶æƒ…ç·’\n",
    "    \"\"\"\n",
    "    result = emotion_classifier(text)[0]\n",
    "    return result['label'], result['score']\n",
    "\n",
    "def generate_empathetic_response(user_input, emotion):\n",
    "    \"\"\"\n",
    "    æ ¹æ“šç”¨æˆ¶æƒ…ç·’èª¿æ•´å›è¦†é¢¨æ ¼\n",
    "    \"\"\"\n",
    "    # æƒ…ç·’å‰ç¶´æ¨¡æ¿\n",
    "    emotion_templates = {\n",
    "        'joy': \"That's wonderful! \",\n",
    "        'sadness': \"I'm sorry to hear that. \",\n",
    "        'anger': \"I understand your frustration. \",\n",
    "        'fear': \"Don't worry, \",\n",
    "        'surprise': \"Oh! \"\n",
    "    }\n",
    "\n",
    "    # ç²å–åŸºæœ¬å›è¦†\n",
    "    base_response = bot.chat(user_input)\n",
    "\n",
    "    # æ·»åŠ æƒ…ç·’å‰ç¶´\n",
    "    prefix = emotion_templates.get(emotion.lower(), \"\")\n",
    "    empathetic_response = prefix + base_response\n",
    "\n",
    "    return empathetic_response\n",
    "\n",
    "\n",
    "# æ¸¬è©¦æƒ…ç·’æ„ŸçŸ¥å°è©±\n",
    "test_messages = [\n",
    "    \"I just got a promotion at work!\",\n",
    "    \"I'm feeling really sad today.\",\n",
    "    \"This is so frustrating!\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ˜Š æƒ…ç·’æ„ŸçŸ¥å°è©±æ¸¬è©¦\\n\")\n",
    "for msg in test_messages:\n",
    "    emotion, confidence = detect_emotion(msg)\n",
    "    response = generate_empathetic_response(msg, emotion)\n",
    "\n",
    "    print(f\"User: {msg}\")\n",
    "    print(f\"  æª¢æ¸¬æƒ…ç·’: {emotion} ({confidence:.2%})\")\n",
    "    print(f\"Bot: {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 æ„åœ–è­˜åˆ¥èˆ‡æ§½å¡«å……"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç°¡å–®çš„æ„åœ–è­˜åˆ¥\n",
    "intent_classifier = pipeline(\n",
    "    \"zero-shot-classification\",\n",
    "    model=\"facebook/bart-large-mnli\"\n",
    ")\n",
    "\n",
    "INTENTS = [\n",
    "    \"greeting\",\n",
    "    \"weather_query\",\n",
    "    \"restaurant_search\",\n",
    "    \"general_chat\",\n",
    "    \"goodbye\"\n",
    "]\n",
    "\n",
    "def detect_intent(user_input):\n",
    "    \"\"\"\n",
    "    è­˜åˆ¥ç”¨æˆ¶æ„åœ–\n",
    "    \"\"\"\n",
    "    result = intent_classifier(user_input, INTENTS)\n",
    "    return result['labels'][0], result['scores'][0]\n",
    "\n",
    "def handle_intent(user_input, intent):\n",
    "    \"\"\"\n",
    "    æ ¹æ“šæ„åœ–è™•ç†è«‹æ±‚\n",
    "    \"\"\"\n",
    "    if intent == \"greeting\":\n",
    "        return \"Hello! How can I help you today?\"\n",
    "\n",
    "    elif intent == \"weather_query\":\n",
    "        # å¯¦éš›æ‡‰ä¸²æ¥å¤©æ°£ API\n",
    "        return \"I'm sorry, I can't check the weather right now. Try asking me about something else!\"\n",
    "\n",
    "    elif intent == \"goodbye\":\n",
    "        return \"Goodbye! Have a great day!\"\n",
    "\n",
    "    else:\n",
    "        # ä¸€èˆ¬é–’èŠ,ä½¿ç”¨å°è©±æ¨¡å‹\n",
    "        return bot.chat(user_input)\n",
    "\n",
    "\n",
    "# æ¸¬è©¦æ„åœ–è­˜åˆ¥\n",
    "test_inputs = [\n",
    "    \"Hi there!\",\n",
    "    \"What's the weather like today?\",\n",
    "    \"Can you recommend a good restaurant?\",\n",
    "    \"Tell me a joke.\",\n",
    "    \"Goodbye!\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ¯ æ„åœ–è­˜åˆ¥æ¸¬è©¦\\n\")\n",
    "for inp in test_inputs:\n",
    "    intent, confidence = detect_intent(inp)\n",
    "    response = handle_intent(inp, intent)\n",
    "\n",
    "    print(f\"User: {inp}\")\n",
    "    print(f\"  æ„åœ–: {intent} ({confidence:.2%})\")\n",
    "    print(f\"Bot: {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: ç”Ÿç”¢éƒ¨ç½² (Streamlit App)\n",
    "\n",
    "### 7.1 Streamlit èŠå¤©æ‡‰ç”¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile chatbot_app.py\n",
    "# chatbot_app.py - Streamlit èŠå¤©æ©Ÿå™¨äººæ‡‰ç”¨\n",
    "\n",
    "import streamlit as st\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "@st.cache_resource\n",
    "def load_chatbot_model():\n",
    "    \"\"\"è¼‰å…¥æ¨¡å‹ (åƒ…è¼‰å…¥ä¸€æ¬¡)\"\"\"\n",
    "    model_name = \"microsoft/DialoGPT-medium\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    return tokenizer, model\n",
    "\n",
    "def generate_response(user_input, chat_history_ids, tokenizer, model):\n",
    "    \"\"\"ç”Ÿæˆå›è¦†\"\"\"\n",
    "    new_input_ids = tokenizer.encode(\n",
    "        user_input + tokenizer.eos_token,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    if chat_history_ids is not None:\n",
    "        bot_input_ids = torch.cat([chat_history_ids, new_input_ids], dim=-1)\n",
    "    else:\n",
    "        bot_input_ids = new_input_ids\n",
    "\n",
    "    chat_history_ids = model.generate(\n",
    "        bot_input_ids,\n",
    "        max_length=1000,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        top_p=0.95,\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "    response = tokenizer.decode(\n",
    "        chat_history_ids[:, bot_input_ids.shape[-1]:][0],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    return response, chat_history_ids\n",
    "\n",
    "# Streamlit UI\n",
    "st.title(\"ğŸ’¬ AI èŠå¤©æ©Ÿå™¨äºº\")\n",
    "st.caption(\"Powered by DialoGPT (Microsoft)\")\n",
    "\n",
    "# è¼‰å…¥æ¨¡å‹\n",
    "tokenizer, model = load_chatbot_model()\n",
    "\n",
    "# åˆå§‹åŒ–å°è©±æ­·å² (ä½¿ç”¨ session state)\n",
    "if \"chat_history_ids\" not in st.session_state:\n",
    "    st.session_state.chat_history_ids = None\n",
    "\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "# é¡¯ç¤ºå°è©±æ­·å²\n",
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(message[\"role\"]):\n",
    "        st.write(message[\"content\"])\n",
    "\n",
    "# ç”¨æˆ¶è¼¸å…¥\n",
    "if user_input := st.chat_input(\"è¼¸å…¥è¨Šæ¯...\"):\n",
    "    # é¡¯ç¤ºç”¨æˆ¶è¨Šæ¯\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "    with st.chat_message(\"user\"):\n",
    "        st.write(user_input)\n",
    "\n",
    "    # ç”Ÿæˆå›è¦†\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        with st.spinner(\"æ€è€ƒä¸­...\"):\n",
    "            response, st.session_state.chat_history_ids = generate_response(\n",
    "                user_input,\n",
    "                st.session_state.chat_history_ids,\n",
    "                tokenizer,\n",
    "                model\n",
    "            )\n",
    "            st.write(response)\n",
    "\n",
    "    # è¨˜éŒ„æ©Ÿå™¨äººå›è¦†\n",
    "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "# å´é‚Šæ¬„: é‡ç½®å°è©±\n",
    "if st.sidebar.button(\"ğŸ”„ é‡ç½®å°è©±\"):\n",
    "    st.session_state.chat_history_ids = None\n",
    "    st.session_state.messages = []\n",
    "    st.rerun()\n",
    "\n",
    "# å´é‚Šæ¬„: å°è©±çµ±è¨ˆ\n",
    "st.sidebar.markdown(\"### ğŸ“Š å°è©±çµ±è¨ˆ\")\n",
    "st.sidebar.metric(\"å°è©±è¼ªæ•¸\", len(st.session_state.messages) // 2)\n",
    "\n",
    "if st.session_state.chat_history_ids is not None:\n",
    "    st.sidebar.metric(\"Token æ•¸\", st.session_state.chat_history_ids.shape[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 é‹è¡Œ Streamlit App\n",
    "\n",
    "```bash\n",
    "# å®‰è£ Streamlit\n",
    "poetry add streamlit\n",
    "\n",
    "# é‹è¡Œæ‡‰ç”¨\n",
    "poetry run streamlit run chatbot_app.py\n",
    "\n",
    "# ç€è¦½å™¨è‡ªå‹•é–‹å•Ÿ: http://localhost:8501\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: é€²éšå„ªåŒ–\n",
    "\n",
    "### 8.1 åŠ å…¥è¨˜æ†¶åŠŸèƒ½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryEnhancedChatbot:\n",
    "    \"\"\"\n",
    "    å¸¶è¨˜æ†¶åŠŸèƒ½çš„èŠå¤©æ©Ÿå™¨äºº\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name=\"microsoft/DialoGPT-medium\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "        self.chat_history_ids = None\n",
    "\n",
    "        # ç”¨æˆ¶è³‡è¨Šè¨˜æ†¶\n",
    "        self.user_info = {}\n",
    "\n",
    "    def extract_user_info(self, user_input):\n",
    "        \"\"\"\n",
    "        å¾å°è©±ä¸­æå–ç”¨æˆ¶è³‡è¨Š\n",
    "        \"\"\"\n",
    "        # ç°¡å–®ç¯„ä¾‹: æå–åå­—\n",
    "        if \"my name is\" in user_input.lower():\n",
    "            name = user_input.lower().split(\"my name is\")[-1].strip().split()[0]\n",
    "            self.user_info['name'] = name.capitalize()\n",
    "            print(f\"  ğŸ“ è¨˜ä½: ç”¨æˆ¶åå­—æ˜¯ {self.user_info['name']}\")\n",
    "\n",
    "        # æå–å…¶ä»–è³‡è¨Š (èˆˆè¶£ã€ä½ç½®ç­‰)\n",
    "        # å¯¦éš›æ‡‰ç”¨å¯ä½¿ç”¨ NER\n",
    "\n",
    "    def personalize_response(self, response):\n",
    "        \"\"\"\n",
    "        å€‹æ€§åŒ–å›è¦†\n",
    "        \"\"\"\n",
    "        if 'name' in self.user_info:\n",
    "            # åœ¨å›è¦†ä¸­ä½¿ç”¨ç”¨æˆ¶åå­—\n",
    "            response = response.replace(\"you\", self.user_info['name'], 1)\n",
    "\n",
    "        return response\n",
    "\n",
    "    def chat(self, user_input):\n",
    "        \"\"\"è™•ç†å°è©±ä¸¦è¨˜æ†¶è³‡è¨Š\"\"\"\n",
    "        # æå–ç”¨æˆ¶è³‡è¨Š\n",
    "        self.extract_user_info(user_input)\n",
    "\n",
    "        # ç”Ÿæˆå›è¦† (ä½¿ç”¨å‰é¢å®šç¾©çš„å‡½æ•¸)\n",
    "        response, self.chat_history_ids = generate_response(\n",
    "            user_input,\n",
    "            self.chat_history_ids\n",
    "        )\n",
    "\n",
    "        # å€‹æ€§åŒ–å›è¦†\n",
    "        response = self.personalize_response(response)\n",
    "\n",
    "        return response\n",
    "\n",
    "\n",
    "# æ¸¬è©¦è¨˜æ†¶åŠŸèƒ½\n",
    "memory_bot = MemoryEnhancedChatbot()\n",
    "\n",
    "dialogue = [\n",
    "    \"Hi! My name is Alice.\",\n",
    "    \"What's your favorite color?\",\n",
    "    \"Do you remember my name?\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ§  è¨˜æ†¶åŠŸèƒ½æ¸¬è©¦\\n\")\n",
    "for inp in dialogue:\n",
    "    response = memory_bot.chat(inp)\n",
    "    print(f\"User: {inp}\")\n",
    "    print(f\"Bot: {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: è©•ä¼°èˆ‡æ¸¬è©¦\n",
    "\n",
    "### 9.1 å°è©±å“è³ªè©•ä¼°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_response_quality(response, user_input):\n",
    "    \"\"\"\n",
    "    è©•ä¼°å›è¦†å“è³ª\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "\n",
    "    # 1. é•·åº¦åˆç†æ€§\n",
    "    metrics['length'] = len(response.split())\n",
    "    metrics['length_score'] = 1.0 if 5 <= metrics['length'] <= 50 else 0.5\n",
    "\n",
    "    # 2. å¤šæ¨£æ€§ (ä¸é‡è¤‡)\n",
    "    words = response.split()\n",
    "    unique_ratio = len(set(words)) / len(words) if words else 0\n",
    "    metrics['diversity_score'] = unique_ratio\n",
    "\n",
    "    # 3. ç›¸é—œæ€§ (ç°¡å–®æª¢æŸ¥æ˜¯å¦åŒ…å«ç”¨æˆ¶è¼¸å…¥é—œéµè©)\n",
    "    user_keywords = set(user_input.lower().split())\n",
    "    response_keywords = set(response.lower().split())\n",
    "    overlap = len(user_keywords & response_keywords)\n",
    "    metrics['relevance_score'] = min(overlap / len(user_keywords), 1.0) if user_keywords else 0\n",
    "\n",
    "    # 4. æ•´é«”åˆ†æ•¸\n",
    "    metrics['overall_score'] = (\n",
    "        metrics['length_score'] * 0.3 +\n",
    "        metrics['diversity_score'] * 0.4 +\n",
    "        metrics['relevance_score'] * 0.3\n",
    "    )\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "# è©•ä¼°ç¯„ä¾‹\n",
    "user_inp = \"What do you think about artificial intelligence?\"\n",
    "bot_resp = bot.chat(user_inp)\n",
    "\n",
    "metrics = evaluate_response_quality(bot_resp, user_inp)\n",
    "\n",
    "print(f\"User: {user_inp}\")\n",
    "print(f\"Bot: {bot_resp}\\n\")\n",
    "print(\"å“è³ªè©•ä¼°:\")\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"  {metric}: {value:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: ç¸½çµèˆ‡æ“´å±•\n",
    "\n",
    "### âœ… æœ¬å°ˆæ¡ˆå®Œæˆå…§å®¹\n",
    "\n",
    "1. **åŸºç¤å°è©±ç³»çµ±**\n",
    "   - DialoGPT å–®è¼ª/å¤šè¼ªå°è©±\n",
    "   - Blenderbot æ•´åˆ\n",
    "   - å°è©±æ­·å²ç®¡ç†\n",
    "\n",
    "2. **é€²éšåŠŸèƒ½**\n",
    "   - å›è¦†å“è³ªéæ¿¾\n",
    "   - æƒ…ç·’è­˜åˆ¥\n",
    "   - æ„åœ–åˆ†é¡\n",
    "   - ç”¨æˆ¶è³‡è¨Šè¨˜æ†¶\n",
    "\n",
    "3. **ç”Ÿç”¢éƒ¨ç½²**\n",
    "   - Streamlit äº’å‹•ä»‹é¢\n",
    "   - æ¨¡å‹å¿«å–å„ªåŒ–\n",
    "   - è©•ä¼°æŒ‡æ¨™\n",
    "\n",
    "### ğŸš€ é€²éšæ“´å±•æ–¹å‘\n",
    "\n",
    "#### åŠŸèƒ½æ“´å±•\n",
    "- [ ] æ•´åˆçŸ¥è­˜åº« (RAG)\n",
    "- [ ] å¤šèªè¨€æ”¯æŒ\n",
    "- [ ] èªéŸ³è¼¸å…¥/è¼¸å‡º\n",
    "- [ ] å€‹æ€§åŒ–å°è©±é¢¨æ ¼\n",
    "\n",
    "#### æŠ€è¡“å„ªåŒ–\n",
    "- [ ] ä½¿ç”¨æ›´å¤§æ¨¡å‹ (GPT-3.5, LLaMA)\n",
    "- [ ] æ¨¡å‹é‡åŒ–åŠ é€Ÿ\n",
    "- [ ] å¤šæ¨¡æ…‹å°è©± (åœ–ç‰‡+æ–‡å­—)\n",
    "- [ ] å¼·åŒ–å­¸ç¿’å¾®èª¿ (RLHF)\n",
    "\n",
    "#### æ‡‰ç”¨å ´æ™¯\n",
    "- [ ] å®¢æœæ©Ÿå™¨äºº\n",
    "- [ ] æ•™å­¸åŠ©æ‰‹\n",
    "- [ ] å¿ƒç†è«®è©¢æ©Ÿå™¨äºº\n",
    "- [ ] ç¨‹å¼ç¢¼åŠ©æ‰‹\n",
    "\n",
    "### ğŸ“š å»¶ä¼¸é–±è®€\n",
    "\n",
    "- [DialoGPT è«–æ–‡](https://arxiv.org/abs/1911.00536)\n",
    "- [Blenderbot è«–æ–‡](https://arxiv.org/abs/2004.13637)\n",
    "- [å°è©±ç³»çµ±ç¶œè¿°](https://arxiv.org/abs/2203.08745)\n",
    "- [Streamlit æ–‡æª”](https://docs.streamlit.io/)\n",
    "\n",
    "---\n",
    "\n",
    "**å°ˆæ¡ˆç‰ˆæœ¬**: v1.0\n",
    "**å»ºç«‹æ—¥æœŸ**: 2025-10-17\n",
    "**ä½œè€…**: iSpan NLP Team\n",
    "**æˆæ¬Š**: MIT License"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
