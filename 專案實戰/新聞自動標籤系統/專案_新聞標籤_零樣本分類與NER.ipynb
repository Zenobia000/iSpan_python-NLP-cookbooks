{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# å°ˆæ¡ˆå¯¦æˆ°: æ–°èè‡ªå‹•æ¨™ç±¤ç³»çµ± (News Auto-Tagging)\n",
    "\n",
    "**å°ˆæ¡ˆé¡å‹**: é›¶æ¨£æœ¬åˆ†é¡ + NER - æ™ºèƒ½æ¨™ç±¤ç”Ÿæˆ\n",
    "**é›£åº¦**: â­â­â­â­ é€²éš\n",
    "**é è¨ˆæ™‚é–“**: 3-4 å°æ™‚\n",
    "**æŠ€è¡“æ£§**: Zero-Shot Classification, NER, BART, spaCy\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š å­¸ç¿’ç›®æ¨™\n",
    "\n",
    "å®Œæˆæœ¬å°ˆæ¡ˆå¾Œ,æ‚¨å°‡èƒ½å¤ :\n",
    "\n",
    "1. âœ… æŒæ¡é›¶æ¨£æœ¬åˆ†é¡æŠ€è¡“ (ç„¡éœ€è¨“ç·´æ•¸æ“š)\n",
    "2. âœ… ä½¿ç”¨ NER è‡ªå‹•æå–å¯¦é«”æ¨™ç±¤\n",
    "3. âœ… çµåˆå¤šç¨® NLP æŠ€è¡“è‡ªå‹•ç”Ÿæˆæ¨™ç±¤\n",
    "4. âœ… æ§‹å»ºæ™ºèƒ½å…§å®¹ç®¡ç†ç³»çµ±\n",
    "5. âœ… å¯¦ä½œç”Ÿç”¢ç´šæ¨™ç±¤æ¨è–¦å¼•æ“\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ å°ˆæ¡ˆå ´æ™¯\n",
    "\n",
    "### æ¥­å‹™éœ€æ±‚\n",
    "\n",
    "**å ´æ™¯**: æ–°èåª’é«”æ¯å¤©ç™¼å¸ƒæ•¸ç™¾ç¯‡æ–‡ç« ,éœ€è¦è‡ªå‹•ç”Ÿæˆæ¨™ç±¤ä»¥ä¾¿:\n",
    "- å…§å®¹åˆ†é¡èˆ‡æ­¸æª”\n",
    "- SEO å„ªåŒ–\n",
    "- æ¨è–¦ç³»çµ±\n",
    "- æœå°‹å¼•æ“ç´¢å¼•\n",
    "\n",
    "**æŒ‘æˆ°**:\n",
    "- âŒ æ‰‹å‹•æ¨™ç±¤è€—æ™‚ (æ¯ç¯‡ 5-10 åˆ†é˜)\n",
    "- âŒ æ¨™ç±¤ä¸ä¸€è‡´\n",
    "- âŒ æ–°ä¸»é¡Œé›£ä»¥åŠæ™‚æ¨™è¨»\n",
    "\n",
    "**è§£æ±ºæ–¹æ¡ˆ**:\n",
    "- âœ… é›¶æ¨£æœ¬åˆ†é¡: ç„¡éœ€è¨“ç·´,éˆæ´»æ·»åŠ æ–°æ¨™ç±¤\n",
    "- âœ… NER: è‡ªå‹•æå–äººåã€åœ°åã€çµ„ç¹”\n",
    "- âœ… é—œéµå­—æå–: TF-IDF + RAKE\n",
    "- âœ… å¤šç­–ç•¥èåˆ: ç¶œåˆæ¨è–¦æœ€ä½³æ¨™ç±¤\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: ç’°å¢ƒæº–å‚™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install transformers torch spacy rake-nltk -q\n",
    "# !python -m spacy download en_core_web_sm\n",
    "\n",
    "import transformers\n",
    "import spacy\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "print(f\"âœ… Transformers: {transformers.__version__}\")\n",
    "print(f\"âœ… spaCy: {spacy.__version__}\")\n",
    "print(f\"âœ… PyTorch: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: æº–å‚™ç¤ºç¯„æ–°èæ•¸æ“š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample news articles\n",
    "news_articles = [\n",
    "    {\n",
    "        'id': 'N001',\n",
    "        'title': 'Apple Unveils New iPhone 15 with Advanced AI Features',\n",
    "        'content': '''Apple Inc. announced its latest iPhone 15 at a press conference in \n",
    "        Cupertino, California on September 12, 2024. CEO Tim Cook presented the new device \n",
    "        featuring advanced AI capabilities powered by the A17 Bionic chip. The phone includes \n",
    "        improved camera technology and enhanced battery life. Analysts expect strong sales \n",
    "        in the holiday season.'''\n",
    "    },\n",
    "    {\n",
    "        'id': 'N002',\n",
    "        'title': 'Lakers Win NBA Championship After Dramatic Game 7',\n",
    "        'content': '''The Los Angeles Lakers defeated the Boston Celtics 110-108 in a \n",
    "        thrilling Game 7 to win the NBA Championship. LeBron James scored 35 points, while \n",
    "        Anthony Davis added 28 points and 12 rebounds. This marks the Lakers 18th \n",
    "        championship title. The game was held at Crypto.com Arena in Los Angeles.'''\n",
    "    },\n",
    "    {\n",
    "        'id': 'N003',\n",
    "        'title': 'Federal Reserve Raises Interest Rates to Combat Inflation',\n",
    "        'content': '''The Federal Reserve announced a 0.25% interest rate increase on \n",
    "        Wednesday, bringing the federal funds rate to 5.5%. Fed Chair Jerome Powell stated \n",
    "        that the decision aims to curb persistent inflation. Economists predict this could \n",
    "        slow economic growth but is necessary to maintain price stability. Stock markets \n",
    "        reacted negatively to the news.'''\n",
    "    },\n",
    "    {\n",
    "        'id': 'N004',\n",
    "        'title': 'Scientists Discover Potential Cancer Treatment Using CRISPR',\n",
    "        'content': '''Researchers at Stanford University have made a breakthrough in cancer \n",
    "        treatment using CRISPR gene-editing technology. The study, published in Nature Medicine, \n",
    "        shows promising results in targeting and eliminating cancer cells without harming \n",
    "        healthy tissue. Clinical trials are expected to begin next year. Dr. Jennifer Doudna, \n",
    "        Nobel laureate and CRISPR pioneer, called it a significant advance.'''\n",
    "    },\n",
    "    {\n",
    "        'id': 'N005',\n",
    "        'title': 'Climate Summit in Paris Reaches Historic Agreement',\n",
    "        'content': '''World leaders gathered in Paris for the Global Climate Summit reached \n",
    "        a landmark agreement to reduce carbon emissions by 50% by 2030. UN Secretary-General \n",
    "        AntÃ³nio Guterres praised the accord as a critical step. Over 190 countries committed \n",
    "        to the new targets. Environmental activists welcomed the decision but called for \n",
    "        faster action.'''\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(news_articles)\n",
    "print(f\"âœ… Loaded {len(df)} news articles\\n\")\n",
    "print(df[['id', 'title']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: ç­–ç•¥ 1 - é›¶æ¨£æœ¬åˆ†é¡\n",
    "\n",
    "### 3.1 é å®šç¾©æ¨™ç±¤åº«"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tag categories\n",
    "TAG_CATEGORIES = {\n",
    "    'topic': [\n",
    "        'Technology', 'Sports', 'Business', 'Science', 'Politics',\n",
    "        'Entertainment', 'Health', 'Environment', 'Education'\n",
    "    ],\n",
    "    'industry': [\n",
    "        'Tech Industry', 'Finance', 'Healthcare', 'Energy',\n",
    "        'Retail', 'Manufacturing', 'Transportation'\n",
    "    ],\n",
    "    'sentiment': [\n",
    "        'Positive', 'Negative', 'Neutral'\n",
    "    ],\n",
    "    'urgency': [\n",
    "        'Breaking News', 'Regular News', 'Feature Story'\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"ğŸ“‹ Available Tag Categories:\")\n",
    "for category, tags in TAG_CATEGORIES.items():\n",
    "    print(f\"\\n{category.upper()}:\")\n",
    "    print(f\"   {', '.join(tags)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 é›¶æ¨£æœ¬åˆ†é¡å¯¦ä½œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load zero-shot classification model\n",
    "print(\"ğŸ“¦ Loading zero-shot classifier...\")\n",
    "\n",
    "zero_shot_classifier = pipeline(\n",
    "    \"zero-shot-classification\",\n",
    "    model=\"facebook/bart-large-mnli\",\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "print(\"âœ… Classifier loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_article(text, candidate_labels, top_k=3, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Classify article using zero-shot classification\n",
    "    \n",
    "    Args:\n",
    "        text: Article text\n",
    "        candidate_labels: List of possible labels\n",
    "        top_k: Number of top labels to return\n",
    "        threshold: Minimum confidence threshold\n",
    "    \n",
    "    Returns:\n",
    "        List of (label, score) tuples\n",
    "    \"\"\"\n",
    "    result = zero_shot_classifier(\n",
    "        text,\n",
    "        candidate_labels,\n",
    "        multi_label=True  # Allow multiple labels\n",
    "    )\n",
    "    \n",
    "    # Filter by threshold and get top-k\n",
    "    tags = []\n",
    "    for label, score in zip(result['labels'], result['scores']):\n",
    "        if score >= threshold:\n",
    "            tags.append((label, score))\n",
    "            if len(tags) >= top_k:\n",
    "                break\n",
    "    \n",
    "    return tags\n",
    "\n",
    "\n",
    "# Test on first article\n",
    "article = df.iloc[0]\n",
    "text = article['title'] + ' ' + article['content']\n",
    "\n",
    "print(f\"ğŸ“° Article: {article['title']}\\n\")\n",
    "\n",
    "# Classify by topic\n",
    "topic_tags = classify_article(text, TAG_CATEGORIES['topic'], top_k=3, threshold=0.5)\n",
    "\n",
    "print(\"ğŸ·ï¸ Topic Tags:\")\n",
    "for tag, score in topic_tags:\n",
    "    print(f\"   {tag}: {score:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 æ‰¹é‡æ¨™ç±¤ç”Ÿæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate tags for all articles\n",
    "print(\"ğŸ”„ Generating tags for all articles...\\n\")\n",
    "\n",
    "all_tags = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    text = row['title'] + ' ' + row['content']\n",
    "    \n",
    "    # Generate topic tags\n",
    "    topic_tags = classify_article(\n",
    "        text,\n",
    "        TAG_CATEGORIES['topic'],\n",
    "        top_k=2,\n",
    "        threshold=0.5\n",
    "    )\n",
    "    \n",
    "    # Generate industry tags\n",
    "    industry_tags = classify_article(\n",
    "        text,\n",
    "        TAG_CATEGORIES['industry'],\n",
    "        top_k=1,\n",
    "        threshold=0.6\n",
    "    )\n",
    "    \n",
    "    all_tags.append({\n",
    "        'article_id': row['id'],\n",
    "        'title': row['title'],\n",
    "        'topic_tags': [tag for tag, score in topic_tags],\n",
    "        'industry_tags': [tag for tag, score in industry_tags],\n",
    "        'confidence': np.mean([score for tag, score in topic_tags]) if topic_tags else 0\n",
    "    })\n",
    "    \n",
    "    print(f\"âœ… {row['id']}: {row['title'][:50]}...\")\n",
    "    print(f\"   Topics: {', '.join([tag for tag, _ in topic_tags])}\")\n",
    "    print(f\"   Industry: {', '.join([tag for tag, _ in industry_tags])}\\n\")\n",
    "\n",
    "tags_df = pd.DataFrame(all_tags)\n",
    "print(\"\\nâœ… Tagging completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: ç­–ç•¥ 2 - NER å¯¦é«”æå–\n",
    "\n",
    "### 4.1 æå–å‘½åå¯¦é«”ä½œç‚ºæ¨™ç±¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load NER pipeline\n",
    "print(\"ğŸ“¦ Loading NER model...\")\n",
    "\n",
    "ner_pipeline = pipeline(\n",
    "    \"ner\",\n",
    "    model=\"dslim/bert-base-NER\",\n",
    "    aggregation_strategy=\"simple\",\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "print(\"âœ… NER model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entity_tags(text, entity_types=['PER', 'ORG', 'LOC'], min_confidence=0.9):\n",
    "    \"\"\"\n",
    "    Extract named entities as tags\n",
    "    \n",
    "    Args:\n",
    "        text: Article text\n",
    "        entity_types: Entity types to extract\n",
    "        min_confidence: Minimum confidence threshold\n",
    "    \n",
    "    Returns:\n",
    "        Dict with entities by type\n",
    "    \"\"\"\n",
    "    entities = ner_pipeline(text)\n",
    "    \n",
    "    entity_tags = {et: [] for et in entity_types}\n",
    "    \n",
    "    for entity in entities:\n",
    "        if entity['score'] >= min_confidence and entity['entity_group'] in entity_types:\n",
    "            entity_tags[entity['entity_group']].append({\n",
    "                'text': entity['word'],\n",
    "                'score': entity['score']\n",
    "            })\n",
    "    \n",
    "    # Remove duplicates\n",
    "    for entity_type in entity_tags:\n",
    "        seen = set()\n",
    "        unique_entities = []\n",
    "        for ent in entity_tags[entity_type]:\n",
    "            if ent['text'] not in seen:\n",
    "                seen.add(ent['text'])\n",
    "                unique_entities.append(ent)\n",
    "        entity_tags[entity_type] = unique_entities\n",
    "    \n",
    "    return entity_tags\n",
    "\n",
    "\n",
    "# Test NER on first article\n",
    "article = df.iloc[0]\n",
    "text = article['title'] + ' ' + article['content']\n",
    "\n",
    "entity_tags = extract_entity_tags(text)\n",
    "\n",
    "print(f\"ğŸ“° Article: {article['title']}\\n\")\n",
    "print(\"ğŸ·ï¸ Extracted Entity Tags:\\n\")\n",
    "\n",
    "for entity_type, entities in entity_tags.items():\n",
    "    if entities:\n",
    "        print(f\"{entity_type} (People/Organizations/Locations):\")\n",
    "        for ent in entities:\n",
    "            print(f\"   - {ent['text']} (confidence: {ent['score']:.2%})\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 ç‚ºæ‰€æœ‰æ–‡ç« æå–å¯¦é«”æ¨™ç±¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract entities for all articles\n",
    "print(\"ğŸ”„ Extracting entities from all articles...\\n\")\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    text = row['title'] + ' ' + row['content']\n",
    "    entities = extract_entity_tags(text)\n",
    "    \n",
    "    # Flatten entities\n",
    "    all_entity_names = []\n",
    "    for entity_type, ents in entities.items():\n",
    "        all_entity_names.extend([e['text'] for e in ents])\n",
    "    \n",
    "    # Add to tags DataFrame\n",
    "    tags_df.loc[tags_df['article_id'] == row['id'], 'entity_tags'] = ', '.join(all_entity_names)\n",
    "    \n",
    "    print(f\"âœ… {row['id']}: {', '.join(all_entity_names[:5])}\")\n",
    "\n",
    "print(\"\\nâœ… Entity extraction completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: ç­–ç•¥ 3 - é—œéµå­—æå– (RAKE)\n",
    "\n",
    "### 5.1 ä½¿ç”¨ RAKE æ¼”ç®—æ³•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rake_nltk import Rake\n",
    "import nltk\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "# Initialize RAKE\n",
    "rake = Rake()\n",
    "\n",
    "def extract_keywords_rake(text, top_n=5):\n",
    "    \"\"\"\n",
    "    Extract keywords using RAKE algorithm\n",
    "    \n",
    "    Args:\n",
    "        text: Article text\n",
    "        top_n: Number of top keywords\n",
    "    \n",
    "    Returns:\n",
    "        List of (keyword, score) tuples\n",
    "    \"\"\"\n",
    "    rake.extract_keywords_from_text(text)\n",
    "    keywords = rake.get_ranked_phrases_with_scores()\n",
    "    \n",
    "    # Return top N\n",
    "    return keywords[:top_n]\n",
    "\n",
    "\n",
    "# Test RAKE\n",
    "article = df.iloc[0]\n",
    "text = article['title'] + ' ' + article['content']\n",
    "\n",
    "keywords = extract_keywords_rake(text, top_n=5)\n",
    "\n",
    "print(f\"ğŸ“° Article: {article['title']}\\n\")\n",
    "print(\"ğŸ”‘ RAKE Keywords:\\n\")\n",
    "for keyword, score in keywords:\n",
    "    print(f\"   {keyword} (score: {score:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: å®Œæ•´æ¨™ç±¤ç³»çµ±æ•´åˆ\n",
    "\n",
    "### 6.1 å¤šç­–ç•¥æ¨™ç±¤ç”Ÿæˆå™¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsAutoTagger:\n",
    "    \"\"\"\n",
    "    æ™ºèƒ½æ–°èè‡ªå‹•æ¨™ç±¤ç³»çµ±\n",
    "    çµåˆé›¶æ¨£æœ¬åˆ†é¡ã€NERã€é—œéµå­—æå–\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        print(\"ğŸš€ Initializing News Auto-Tagger...\")\n",
    "        \n",
    "        # Load models\n",
    "        self.zero_shot = pipeline(\n",
    "            \"zero-shot-classification\",\n",
    "            model=\"facebook/bart-large-mnli\",\n",
    "            device=-1\n",
    "        )\n",
    "        \n",
    "        self.ner = pipeline(\n",
    "            \"ner\",\n",
    "            model=\"dslim/bert-base-NER\",\n",
    "            aggregation_strategy=\"simple\",\n",
    "            device=-1\n",
    "        )\n",
    "        \n",
    "        self.rake = Rake()\n",
    "        \n",
    "        print(\"âœ… Auto-Tagger ready!\")\n",
    "    \n",
    "    def generate_tags(self, article, tag_categories=None):\n",
    "        \"\"\"\n",
    "        Generate comprehensive tags for an article\n",
    "        \n",
    "        Args:\n",
    "            article: Dict with 'title' and 'content'\n",
    "            tag_categories: Dict of tag categories\n",
    "        \n",
    "        Returns:\n",
    "            Dict with different types of tags\n",
    "        \"\"\"\n",
    "        if tag_categories is None:\n",
    "            tag_categories = TAG_CATEGORIES\n",
    "        \n",
    "        text = article['title'] + ' ' + article['content']\n",
    "        \n",
    "        tags = {\n",
    "            'article_id': article.get('id', 'unknown'),\n",
    "            'title': article['title']\n",
    "        }\n",
    "        \n",
    "        # 1. Topic classification\n",
    "        topic_result = self.zero_shot(\n",
    "            text,\n",
    "            tag_categories['topic'],\n",
    "            multi_label=True\n",
    "        )\n",
    "        tags['topics'] = [\n",
    "            {'tag': label, 'score': score}\n",
    "            for label, score in zip(topic_result['labels'][:3], topic_result['scores'][:3])\n",
    "            if score > 0.5\n",
    "        ]\n",
    "        \n",
    "        # 2. Entity extraction\n",
    "        entities = self.ner(text[:512])  # Limit text length for NER\n",
    "        entity_tags = []\n",
    "        seen = set()\n",
    "        for ent in entities:\n",
    "            if ent['score'] > 0.9 and ent['word'] not in seen:\n",
    "                entity_tags.append({\n",
    "                    'tag': ent['word'],\n",
    "                    'type': ent['entity_group'],\n",
    "                    'score': ent['score']\n",
    "                })\n",
    "                seen.add(ent['word'])\n",
    "        tags['entities'] = entity_tags[:5]\n",
    "        \n",
    "        # 3. Keyword extraction\n",
    "        self.rake.extract_keywords_from_text(text)\n",
    "        keywords = self.rake.get_ranked_phrases()[:5]\n",
    "        tags['keywords'] = keywords\n",
    "        \n",
    "        # 4. Generate final tag list\n",
    "        final_tags = []\n",
    "        \n",
    "        # Add top topics\n",
    "        final_tags.extend([t['tag'] for t in tags['topics'][:2]])\n",
    "        \n",
    "        # Add top entities\n",
    "        final_tags.extend([e['tag'] for e in tags['entities'][:3]])\n",
    "        \n",
    "        # Add top keyword\n",
    "        if keywords:\n",
    "            final_tags.append(keywords[0])\n",
    "        \n",
    "        tags['final_tags'] = list(set(final_tags))  # Remove duplicates\n",
    "        \n",
    "        return tags\n",
    "    \n",
    "    def batch_generate_tags(self, articles):\n",
    "        \"\"\"\n",
    "        Generate tags for multiple articles\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        for article in articles:\n",
    "            tags = self.generate_tags(article)\n",
    "            results.append(tags)\n",
    "        return results\n",
    "\n",
    "\n",
    "# Initialize tagger\n",
    "auto_tagger = NewsAutoTagger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 æ¸¬è©¦è‡ªå‹•æ¨™ç±¤ç³»çµ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on all articles\n",
    "print(\"ğŸ·ï¸ Generating comprehensive tags...\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "tagging_results = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    article = row.to_dict()\n",
    "    tags = auto_tagger.generate_tags(article)\n",
    "    tagging_results.append(tags)\n",
    "    \n",
    "    print(f\"\\nğŸ“° {tags['article_id']}: {tags['title']}\")\n",
    "    print(f\"\\nğŸ·ï¸ Recommended Tags:\")\n",
    "    for tag in tags['final_tags']:\n",
    "        print(f\"   â€¢ {tag}\")\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Details:\")\n",
    "    print(f\"   Topics: {[t['tag'] for t in tags['topics']]}\")\n",
    "    print(f\"   Entities: {[e['tag'] for e in tags['entities']]}\")\n",
    "    print(f\"   Keywords: {tags['keywords'][:3]}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "print(\"\\nâœ… All articles tagged!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: æ¨™ç±¤åˆ†æèˆ‡å¯è¦–åŒ–\n",
    "\n",
    "### 7.1 æ¨™ç±¤çµ±è¨ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze tag distribution\n",
    "all_final_tags = []\n",
    "for result in tagging_results:\n",
    "    all_final_tags.extend(result['final_tags'])\n",
    "\n",
    "tag_freq = Counter(all_final_tags)\n",
    "\n",
    "print(\"ğŸ“Š Tag Frequency Distribution:\\n\")\n",
    "for tag, count in tag_freq.most_common(15):\n",
    "    print(f\"   {tag:30} : {count:2d} articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top tags\n",
    "top_tags = tag_freq.most_common(10)\n",
    "tags, counts = zip(*top_tags)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(tags, counts, color='steelblue')\n",
    "plt.xlabel('Frequency')\n",
    "plt.title('Top 10 Most Frequent Tags', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 æ¨™ç±¤å…±ç¾åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tag co-occurrence matrix\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "# Prepare tag lists\n",
    "tag_lists = [result['final_tags'] for result in tagging_results]\n",
    "\n",
    "# Binary encoding\n",
    "mlb = MultiLabelBinarizer()\n",
    "tag_matrix = mlb.fit_transform(tag_lists)\n",
    "\n",
    "# Calculate co-occurrence\n",
    "co_occurrence = tag_matrix.T @ tag_matrix\n",
    "\n",
    "# Visualize (top 10 tags)\n",
    "top_tag_names = [tag for tag, _ in tag_freq.most_common(10)]\n",
    "top_tag_indices = [list(mlb.classes_).index(tag) for tag in top_tag_names if tag in mlb.classes_]\n",
    "\n",
    "co_occurrence_subset = co_occurrence[np.ix_(top_tag_indices, top_tag_indices)]\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(\n",
    "    co_occurrence_subset,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='YlOrRd',\n",
    "    xticklabels=top_tag_names,\n",
    "    yticklabels=top_tag_names,\n",
    "    cbar_kws={'label': 'Co-occurrence Count'}\n",
    ")\n",
    "plt.title('Tag Co-occurrence Matrix', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: ç”Ÿç”¢éƒ¨ç½²\n",
    "\n",
    "### 8.1 FastAPI æ¨™ç±¤æœå‹™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile news_tagging_api.py\n",
    "# news_tagging_api.py\n",
    "\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Dict\n",
    "import uvicorn\n",
    "\n",
    "app = FastAPI(\n",
    "    title=\"News Auto-Tagging API\",\n",
    "    description=\"Automatic tag generation for news articles\",\n",
    "    version=\"1.0.0\"\n",
    ")\n",
    "\n",
    "# Global auto-tagger\n",
    "auto_tagger = None\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "async def load_models():\n",
    "    global auto_tagger\n",
    "    print(\"Loading models...\")\n",
    "    auto_tagger = NewsAutoTagger()\n",
    "    print(\"âœ… Models loaded\")\n",
    "\n",
    "class ArticleInput(BaseModel):\n",
    "    title: str = Field(..., min_length=5, max_length=500)\n",
    "    content: str = Field(..., min_length=50, max_length=10000)\n",
    "    id: str = Field(default=\"AUTO\")\n",
    "\n",
    "class TagResponse(BaseModel):\n",
    "    article_id: str\n",
    "    recommended_tags: List[str]\n",
    "    topics: List[Dict]\n",
    "    entities: List[Dict]\n",
    "    keywords: List[str]\n",
    "\n",
    "@app.post(\"/tag\", response_model=TagResponse)\n",
    "async def tag_article(article: ArticleInput):\n",
    "    \"\"\"Generate tags for an article\"\"\"\n",
    "    try:\n",
    "        tags = auto_tagger.generate_tags(article.dict())\n",
    "        \n",
    "        return TagResponse(\n",
    "            article_id=tags['article_id'],\n",
    "            recommended_tags=tags['final_tags'],\n",
    "            topics=tags['topics'],\n",
    "            entities=tags['entities'],\n",
    "            keywords=tags['keywords']\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.get(\"/available_tags\")\n",
    "async def get_available_tags():\n",
    "    \"\"\"Get all available tag categories\"\"\"\n",
    "    return TAG_CATEGORIES\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health_check():\n",
    "    return {\"status\": \"healthy\", \"tagger_loaded\": auto_tagger is not None}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: æ¨™ç±¤å“è³ªè©•ä¼°\n",
    "\n",
    "### 9.1 æ¨™ç±¤ä¸€è‡´æ€§åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate tagging statistics\n",
    "tag_counts_per_article = [len(result['final_tags']) for result in tagging_results]\n",
    "\n",
    "print(\"ğŸ“Š Tagging Statistics:\\n\")\n",
    "print(f\"   Average tags per article: {np.mean(tag_counts_per_article):.2f}\")\n",
    "print(f\"   Min tags: {np.min(tag_counts_per_article)}\")\n",
    "print(f\"   Max tags: {np.max(tag_counts_per_article)}\")\n",
    "print(f\"   Median tags: {np.median(tag_counts_per_article)}\")\n",
    "\n",
    "# Unique tags\n",
    "all_unique_tags = set()\n",
    "for result in tagging_results:\n",
    "    all_unique_tags.update(result['final_tags'])\n",
    "\n",
    "print(f\"\\n   Total unique tags: {len(all_unique_tags)}\")\n",
    "print(f\"   Tag vocabulary size: {len(tag_freq)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: ç¸½çµèˆ‡æ“´å±•\n",
    "\n",
    "### âœ… æœ¬å°ˆæ¡ˆå®Œæˆå…§å®¹\n",
    "\n",
    "1. **å¤šç­–ç•¥æ¨™ç±¤ç”Ÿæˆ**\n",
    "   - é›¶æ¨£æœ¬åˆ†é¡ (ä¸»é¡Œã€ç”¢æ¥­åˆ†é¡)\n",
    "   - NER å¯¦é«”æå– (äººåã€åœ°åã€çµ„ç¹”)\n",
    "   - é—œéµå­—æå– (RAKE æ¼”ç®—æ³•)\n",
    "\n",
    "2. **æ™ºèƒ½èåˆ**\n",
    "   - å¤šæ¨¡å‹çµæœæ•´åˆ\n",
    "   - ä¿¡å¿ƒåº¦éæ¿¾\n",
    "   - å»é‡èˆ‡æ’åº\n",
    "\n",
    "3. **å¯è¦–åŒ–åˆ†æ**\n",
    "   - æ¨™ç±¤é »ç‡åˆ†å¸ƒ\n",
    "   - æ¨™ç±¤å…±ç¾çŸ©é™£\n",
    "   - å“è³ªçµ±è¨ˆåˆ†æ\n",
    "\n",
    "4. **ç”Ÿç”¢éƒ¨ç½²**\n",
    "   - FastAPI æœå‹™\n",
    "   - RESTful API è¨­è¨ˆ\n",
    "   - æ‰¹é‡æ¨™ç±¤ç”Ÿæˆ\n",
    "\n",
    "### ğŸš€ é€²éšæ“´å±•\n",
    "\n",
    "#### æŠ€è¡“å„ªåŒ–\n",
    "- [ ] ä½¿ç”¨ BERT é€²è¡Œé—œéµå­—æå– (KeyBERT)\n",
    "- [ ] æ•´åˆ LLM ç”Ÿæˆæè¿°æ€§æ¨™ç±¤\n",
    "- [ ] å¯¦ä½œæ¨™ç±¤éšå±¤çµæ§‹ (çˆ¶æ¨™ç±¤-å­æ¨™ç±¤)\n",
    "- [ ] å€‹æ€§åŒ–æ¨™ç±¤æ¨è–¦ (åŸºæ–¼ç”¨æˆ¶é–±è®€æ­·å²)\n",
    "\n",
    "#### åŠŸèƒ½æ“´å±•\n",
    "- [ ] ä¸­æ–‡æ–°èæ¨™ç±¤æ”¯æŒ\n",
    "- [ ] è‡ªå‹•ç”Ÿæˆ SEO meta tags\n",
    "- [ ] æ¨™ç±¤è¶¨å‹¢åˆ†æ (ç†±é–€æ¨™ç±¤è¿½è¹¤)\n",
    "- [ ] æ¨™ç±¤ç›¸ä¼¼åº¦è¨ˆç®— (tag embeddings)\n",
    "\n",
    "#### æ‡‰ç”¨å ´æ™¯\n",
    "- [ ] CMS (å…§å®¹ç®¡ç†ç³»çµ±) æ•´åˆ\n",
    "- [ ] ç¤¾äº¤åª’é«”è‡ªå‹• hashtag\n",
    "- [ ] é›»å•†ç”¢å“æ¨™ç±¤ç”Ÿæˆ\n",
    "- [ ] å­¸è¡“è«–æ–‡é—œéµè©æå–\n",
    "\n",
    "---\n",
    "\n",
    "**å°ˆæ¡ˆç‰ˆæœ¬**: v1.0\n",
    "**å»ºç«‹æ—¥æœŸ**: 2025-10-17\n",
    "**ä½œè€…**: iSpan NLP Team\n",
    "**æˆæ¬Š**: MIT License"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
