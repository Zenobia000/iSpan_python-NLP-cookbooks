{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Â∞àÊ°àÂØ¶Êà∞: Êñ∞ËÅûËá™ÂãïÊ®ôÁ±§Á≥ªÁµ± (News Auto-Tagging)\n",
    "\n",
    "**Â∞àÊ°àÈ°ûÂûã**: Èõ∂Ê®£Êú¨ÂàÜÈ°û + NER - Êô∫ËÉΩÊ®ôÁ±§ÁîüÊàê\n",
    "**Èõ£Â∫¶**: ‚≠ê‚≠ê‚≠ê‚≠ê ÈÄ≤Èöé\n",
    "**È†êË®àÊôÇÈñì**: 3-4 Â∞èÊôÇ\n",
    "**ÊäÄË°ìÊ£ß**: Zero-Shot Classification, NER, BART, spaCy\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Â≠∏ÁøíÁõÆÊ®ô\n",
    "\n",
    "ÂÆåÊàêÊú¨Â∞àÊ°àÂæå,ÊÇ®Â∞áËÉΩÂ§†:\n",
    "\n",
    "1. ‚úÖ ÊéåÊè°Èõ∂Ê®£Êú¨ÂàÜÈ°ûÊäÄË°ì (ÁÑ°ÈúÄË®ìÁ∑¥Êï∏Êìö)\n",
    "2. ‚úÖ ‰ΩøÁî® NER Ëá™ÂãïÊèêÂèñÂØ¶È´îÊ®ôÁ±§\n",
    "3. ‚úÖ ÁµêÂêàÂ§öÁ®Æ NLP ÊäÄË°ìËá™ÂãïÁîüÊàêÊ®ôÁ±§\n",
    "4. ‚úÖ ÊßãÂª∫Êô∫ËÉΩÂÖßÂÆπÁÆ°ÁêÜÁ≥ªÁµ±\n",
    "5. ‚úÖ ÂØ¶‰ΩúÁîüÁî¢Á¥öÊ®ôÁ±§Êé®Ëñ¶ÂºïÊìé\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Â∞àÊ°àÂ†¥ÊôØ\n",
    "\n",
    "### Ê•≠ÂãôÈúÄÊ±Ç\n",
    "\n",
    "**Â†¥ÊôØ**: Êñ∞ËÅûÂ™íÈ´îÊØèÂ§©ÁôºÂ∏ÉÊï∏ÁôæÁØáÊñáÁ´†,ÈúÄË¶ÅËá™ÂãïÁîüÊàêÊ®ôÁ±§‰ª•‰æø:\n",
    "- ÂÖßÂÆπÂàÜÈ°ûËàáÊ≠∏Ê™î\n",
    "- SEO ÂÑ™Âåñ\n",
    "- Êé®Ëñ¶Á≥ªÁµ±\n",
    "- ÊêúÂ∞ãÂºïÊìéÁ¥¢Âºï\n",
    "\n",
    "**ÊåëÊà∞**:\n",
    "- ‚ùå ÊâãÂãïÊ®ôÁ±§ËÄóÊôÇ (ÊØèÁØá 5-10 ÂàÜÈêò)\n",
    "- ‚ùå Ê®ôÁ±§‰∏ç‰∏ÄËá¥\n",
    "- ‚ùå Êñ∞‰∏ªÈ°åÈõ£‰ª•ÂèäÊôÇÊ®ôË®ª\n",
    "\n",
    "**Ëß£Ê±∫ÊñπÊ°à**:\n",
    "- ‚úÖ Èõ∂Ê®£Êú¨ÂàÜÈ°û: ÁÑ°ÈúÄË®ìÁ∑¥,ÈùàÊ¥ªÊ∑ªÂä†Êñ∞Ê®ôÁ±§\n",
    "- ‚úÖ NER: Ëá™ÂãïÊèêÂèñ‰∫∫Âêç„ÄÅÂú∞Âêç„ÄÅÁµÑÁπî\n",
    "- ‚úÖ ÈóúÈçµÂ≠óÊèêÂèñ: TF-IDF + RAKE\n",
    "- ‚úÖ Â§öÁ≠ñÁï•ËûçÂêà: Á∂úÂêàÊé®Ëñ¶ÊúÄ‰Ω≥Ê®ôÁ±§\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Áí∞Â¢ÉÊ∫ñÂÇô"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install transformers torch spacy rake-nltk -q\n",
    "# !python -m spacy download en_core_web_sm\n",
    "\n",
    "import transformers\n",
    "import spacy\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "print(f\"‚úÖ Transformers: {transformers.__version__}\")\n",
    "print(f\"‚úÖ spaCy: {spacy.__version__}\")\n",
    "print(f\"‚úÖ PyTorch: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Ê∫ñÂÇôÁ§∫ÁØÑÊñ∞ËÅûÊï∏Êìö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample news articles\n",
    "news_articles = [\n",
    "    {\n",
    "        'id': 'N001',\n",
    "        'title': 'Apple Unveils New iPhone 15 with Advanced AI Features',\n",
    "        'content': '''Apple Inc. announced its latest iPhone 15 at a press conference in \n",
    "        Cupertino, California on September 12, 2024. CEO Tim Cook presented the new device \n",
    "        featuring advanced AI capabilities powered by the A17 Bionic chip. The phone includes \n",
    "        improved camera technology and enhanced battery life. Analysts expect strong sales \n",
    "        in the holiday season.'''\n",
    "    },\n",
    "    {\n",
    "        'id': 'N002',\n",
    "        'title': 'Lakers Win NBA Championship After Dramatic Game 7',\n",
    "        'content': '''The Los Angeles Lakers defeated the Boston Celtics 110-108 in a \n",
    "        thrilling Game 7 to win the NBA Championship. LeBron James scored 35 points, while \n",
    "        Anthony Davis added 28 points and 12 rebounds. This marks the Lakers 18th \n",
    "        championship title. The game was held at Crypto.com Arena in Los Angeles.'''\n",
    "    },\n",
    "    {\n",
    "        'id': 'N003',\n",
    "        'title': 'Federal Reserve Raises Interest Rates to Combat Inflation',\n",
    "        'content': '''The Federal Reserve announced a 0.25% interest rate increase on \n",
    "        Wednesday, bringing the federal funds rate to 5.5%. Fed Chair Jerome Powell stated \n",
    "        that the decision aims to curb persistent inflation. Economists predict this could \n",
    "        slow economic growth but is necessary to maintain price stability. Stock markets \n",
    "        reacted negatively to the news.'''\n",
    "    },\n",
    "    {\n",
    "        'id': 'N004',\n",
    "        'title': 'Scientists Discover Potential Cancer Treatment Using CRISPR',\n",
    "        'content': '''Researchers at Stanford University have made a breakthrough in cancer \n",
    "        treatment using CRISPR gene-editing technology. The study, published in Nature Medicine, \n",
    "        shows promising results in targeting and eliminating cancer cells without harming \n",
    "        healthy tissue. Clinical trials are expected to begin next year. Dr. Jennifer Doudna, \n",
    "        Nobel laureate and CRISPR pioneer, called it a significant advance.'''\n",
    "    },\n",
    "    {\n",
    "        'id': 'N005',\n",
    "        'title': 'Climate Summit in Paris Reaches Historic Agreement',\n",
    "        'content': '''World leaders gathered in Paris for the Global Climate Summit reached \n",
    "        a landmark agreement to reduce carbon emissions by 50% by 2030. UN Secretary-General \n",
    "        Ant√≥nio Guterres praised the accord as a critical step. Over 190 countries committed \n",
    "        to the new targets. Environmental activists welcomed the decision but called for \n",
    "        faster action.'''\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(news_articles)\n",
    "print(f\"‚úÖ Loaded {len(df)} news articles\\n\")\n",
    "print(df[['id', 'title']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Á≠ñÁï• 1 - Èõ∂Ê®£Êú¨ÂàÜÈ°û\n",
    "\n",
    "### 3.1 È†êÂÆöÁæ©Ê®ôÁ±§Â∫´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tag categories\n",
    "TAG_CATEGORIES = {\n",
    "    'topic': [\n",
    "        'Technology', 'Sports', 'Business', 'Science', 'Politics',\n",
    "        'Entertainment', 'Health', 'Environment', 'Education'\n",
    "    ],\n",
    "    'industry': [\n",
    "        'Tech Industry', 'Finance', 'Healthcare', 'Energy',\n",
    "        'Retail', 'Manufacturing', 'Transportation'\n",
    "    ],\n",
    "    'sentiment': [\n",
    "        'Positive', 'Negative', 'Neutral'\n",
    "    ],\n",
    "    'urgency': [\n",
    "        'Breaking News', 'Regular News', 'Feature Story'\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"üìã Available Tag Categories:\")\n",
    "for category, tags in TAG_CATEGORIES.items():\n",
    "    print(f\"\\n{category.upper()}:\")\n",
    "    print(f\"   {', '.join(tags)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Èõ∂Ê®£Êú¨ÂàÜÈ°ûÂØ¶‰Ωú"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load zero-shot classification model\n",
    "print(\"üì¶ Loading zero-shot classifier...\")\n",
    "\n",
    "zero_shot_classifier = pipeline(\n",
    "    \"zero-shot-classification\",\n",
    "    model=\"facebook/bart-large-mnli\",\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Classifier loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_article(text, candidate_labels, top_k=3, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Classify article using zero-shot classification\n",
    "    \n",
    "    Args:\n",
    "        text: Article text\n",
    "        candidate_labels: List of possible labels\n",
    "        top_k: Number of top labels to return\n",
    "        threshold: Minimum confidence threshold\n",
    "    \n",
    "    Returns:\n",
    "        List of (label, score) tuples\n",
    "    \"\"\"\n",
    "    result = zero_shot_classifier(\n",
    "        text,\n",
    "        candidate_labels,\n",
    "        multi_label=True  # Allow multiple labels\n",
    "    )\n",
    "    \n",
    "    # Filter by threshold and get top-k\n",
    "    tags = []\n",
    "    for label, score in zip(result['labels'], result['scores']):\n",
    "        if score >= threshold:\n",
    "            tags.append((label, score))\n",
    "            if len(tags) >= top_k:\n",
    "                break\n",
    "    \n",
    "    return tags\n",
    "\n",
    "\n",
    "# Test on first article\n",
    "article = df.iloc[0]\n",
    "text = article['title'] + ' ' + article['content']\n",
    "\n",
    "print(f\"üì∞ Article: {article['title']}\\n\")\n",
    "\n",
    "# Classify by topic\n",
    "topic_tags = classify_article(text, TAG_CATEGORIES['topic'], top_k=3, threshold=0.5)\n",
    "\n",
    "print(\"üè∑Ô∏è Topic Tags:\")\n",
    "for tag, score in topic_tags:\n",
    "    print(f\"   {tag}: {score:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 ÊâπÈáèÊ®ôÁ±§ÁîüÊàê"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate tags for all articles\n",
    "print(\"üîÑ Generating tags for all articles...\\n\")\n",
    "\n",
    "all_tags = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    text = row['title'] + ' ' + row['content']\n",
    "    \n",
    "    # Generate topic tags\n",
    "    topic_tags = classify_article(\n",
    "        text,\n",
    "        TAG_CATEGORIES['topic'],\n",
    "        top_k=2,\n",
    "        threshold=0.5\n",
    "    )\n",
    "    \n",
    "    # Generate industry tags\n",
    "    industry_tags = classify_article(\n",
    "        text,\n",
    "        TAG_CATEGORIES['industry'],\n",
    "        top_k=1,\n",
    "        threshold=0.6\n",
    "    )\n",
    "    \n",
    "    all_tags.append({\n",
    "        'article_id': row['id'],\n",
    "        'title': row['title'],\n",
    "        'topic_tags': [tag for tag, score in topic_tags],\n",
    "        'industry_tags': [tag for tag, score in industry_tags],\n",
    "        'confidence': np.mean([score for tag, score in topic_tags]) if topic_tags else 0\n",
    "    })\n",
    "    \n",
    "    print(f\"‚úÖ {row['id']}: {row['title'][:50]}...\")\n",
    "    print(f\"   Topics: {', '.join([tag for tag, _ in topic_tags])}\")\n",
    "    print(f\"   Industry: {', '.join([tag for tag, _ in industry_tags])}\\n\")\n",
    "\n",
    "tags_df = pd.DataFrame(all_tags)\n",
    "print(\"\\n‚úÖ Tagging completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Á≠ñÁï• 2 - NER ÂØ¶È´îÊèêÂèñ\n",
    "\n",
    "### 4.1 ÊèêÂèñÂëΩÂêçÂØ¶È´î‰ΩúÁÇ∫Ê®ôÁ±§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load NER pipeline\n",
    "print(\"üì¶ Loading NER model...\")\n",
    "\n",
    "ner_pipeline = pipeline(\n",
    "    \"ner\",\n",
    "    model=\"dslim/bert-base-NER\",\n",
    "    aggregation_strategy=\"simple\",\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "print(\"‚úÖ NER model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entity_tags(text, entity_types=['PER', 'ORG', 'LOC'], min_confidence=0.9):\n",
    "    \"\"\"\n",
    "    Extract named entities as tags\n",
    "    \n",
    "    Args:\n",
    "        text: Article text\n",
    "        entity_types: Entity types to extract\n",
    "        min_confidence: Minimum confidence threshold\n",
    "    \n",
    "    Returns:\n",
    "        Dict with entities by type\n",
    "    \"\"\"\n",
    "    entities = ner_pipeline(text)\n",
    "    \n",
    "    entity_tags = {et: [] for et in entity_types}\n",
    "    \n",
    "    for entity in entities:\n",
    "        if entity['score'] >= min_confidence and entity['entity_group'] in entity_types:\n",
    "            entity_tags[entity['entity_group']].append({\n",
    "                'text': entity['word'],\n",
    "                'score': entity['score']\n",
    "            })\n",
    "    \n",
    "    # Remove duplicates\n",
    "    for entity_type in entity_tags:\n",
    "        seen = set()\n",
    "        unique_entities = []\n",
    "        for ent in entity_tags[entity_type]:\n",
    "            if ent['text'] not in seen:\n",
    "                seen.add(ent['text'])\n",
    "                unique_entities.append(ent)\n",
    "        entity_tags[entity_type] = unique_entities\n",
    "    \n",
    "    return entity_tags\n",
    "\n",
    "\n",
    "# Test NER on first article\n",
    "article = df.iloc[0]\n",
    "text = article['title'] + ' ' + article['content']\n",
    "\n",
    "entity_tags = extract_entity_tags(text)\n",
    "\n",
    "print(f\"üì∞ Article: {article['title']}\\n\")\n",
    "print(\"üè∑Ô∏è Extracted Entity Tags:\\n\")\n",
    "\n",
    "for entity_type, entities in entity_tags.items():\n",
    "    if entities:\n",
    "        print(f\"{entity_type} (People/Organizations/Locations):\")\n",
    "        for ent in entities:\n",
    "            print(f\"   - {ent['text']} (confidence: {ent['score']:.2%})\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 ÁÇ∫ÊâÄÊúâÊñáÁ´†ÊèêÂèñÂØ¶È´îÊ®ôÁ±§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract entities for all articles\n",
    "print(\"üîÑ Extracting entities from all articles...\\n\")\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    text = row['title'] + ' ' + row['content']\n",
    "    entities = extract_entity_tags(text)\n",
    "    \n",
    "    # Flatten entities\n",
    "    all_entity_names = []\n",
    "    for entity_type, ents in entities.items():\n",
    "        all_entity_names.extend([e['text'] for e in ents])\n",
    "    \n",
    "    # Add to tags DataFrame\n",
    "    tags_df.loc[tags_df['article_id'] == row['id'], 'entity_tags'] = ', '.join(all_entity_names)\n",
    "    \n",
    "    print(f\"‚úÖ {row['id']}: {', '.join(all_entity_names[:5])}\")\n",
    "\n",
    "print(\"\\n‚úÖ Entity extraction completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Á≠ñÁï• 3 - ÈóúÈçµÂ≠óÊèêÂèñ (RAKE)\n",
    "\n",
    "### 5.1 ‰ΩøÁî® RAKE ÊºîÁÆóÊ≥ï"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rake_nltk import Rake\n",
    "import nltk\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "# Initialize RAKE\n",
    "rake = Rake()\n",
    "\n",
    "def extract_keywords_rake(text, top_n=5):\n",
    "    \"\"\"\n",
    "    Extract keywords using RAKE algorithm\n",
    "    \n",
    "    Args:\n",
    "        text: Article text\n",
    "        top_n: Number of top keywords\n",
    "    \n",
    "    Returns:\n",
    "        List of (keyword, score) tuples\n",
    "    \"\"\"\n",
    "    rake.extract_keywords_from_text(text)\n",
    "    keywords = rake.get_ranked_phrases_with_scores()\n",
    "    \n",
    "    # Return top N\n",
    "    return keywords[:top_n]\n",
    "\n",
    "\n",
    "# Test RAKE\n",
    "article = df.iloc[0]\n",
    "text = article['title'] + ' ' + article['content']\n",
    "\n",
    "keywords = extract_keywords_rake(text, top_n=5)\n",
    "\n",
    "print(f\"üì∞ Article: {article['title']}\\n\")\n",
    "print(\"üîë RAKE Keywords:\\n\")\n",
    "for keyword, score in keywords:\n",
    "    print(f\"   {keyword} (score: {score:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: ÂÆåÊï¥Ê®ôÁ±§Á≥ªÁµ±Êï¥Âêà\n",
    "\n",
    "### 6.1 Â§öÁ≠ñÁï•Ê®ôÁ±§ÁîüÊàêÂô®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsAutoTagger:\n",
    "    \"\"\"\n",
    "    Êô∫ËÉΩÊñ∞ËÅûËá™ÂãïÊ®ôÁ±§Á≥ªÁµ±\n",
    "    ÁµêÂêàÈõ∂Ê®£Êú¨ÂàÜÈ°û„ÄÅNER„ÄÅÈóúÈçµÂ≠óÊèêÂèñ\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        print(\"üöÄ Initializing News Auto-Tagger...\")\n",
    "        \n",
    "        # Load models\n",
    "        self.zero_shot = pipeline(\n",
    "            \"zero-shot-classification\",\n",
    "            model=\"facebook/bart-large-mnli\",\n",
    "            device=-1\n",
    "        )\n",
    "        \n",
    "        self.ner = pipeline(\n",
    "            \"ner\",\n",
    "            model=\"dslim/bert-base-NER\",\n",
    "            aggregation_strategy=\"simple\",\n",
    "            device=-1\n",
    "        )\n",
    "        \n",
    "        self.rake = Rake()\n",
    "        \n",
    "        print(\"‚úÖ Auto-Tagger ready!\")\n",
    "    \n",
    "    def generate_tags(self, article, tag_categories=None):\n",
    "        \"\"\"\n",
    "        Generate comprehensive tags for an article\n",
    "        \n",
    "        Args:\n",
    "            article: Dict with 'title' and 'content'\n",
    "            tag_categories: Dict of tag categories\n",
    "        \n",
    "        Returns:\n",
    "            Dict with different types of tags\n",
    "        \"\"\"\n",
    "        if tag_categories is None:\n",
    "            tag_categories = TAG_CATEGORIES\n",
    "        \n",
    "        text = article['title'] + ' ' + article['content']\n",
    "        \n",
    "        tags = {\n",
    "            'article_id': article.get('id', 'unknown'),\n",
    "            'title': article['title']\n",
    "        }\n",
    "        \n",
    "        # 1. Topic classification\n",
    "        topic_result = self.zero_shot(\n",
    "            text,\n",
    "            tag_categories['topic'],\n",
    "            multi_label=True\n",
    "        )\n",
    "        tags['topics'] = [\n",
    "            {'tag': label, 'score': score}\n",
    "            for label, score in zip(topic_result['labels'][:3], topic_result['scores'][:3])\n",
    "            if score > 0.5\n",
    "        ]\n",
    "        \n",
    "        # 2. Entity extraction\n",
    "        entities = self.ner(text[:512])  # Limit text length for NER\n",
    "        entity_tags = []\n",
    "        seen = set()\n",
    "        for ent in entities:\n",
    "            if ent['score'] > 0.9 and ent['word'] not in seen:\n",
    "                entity_tags.append({\n",
    "                    'tag': ent['word'],\n",
    "                    'type': ent['entity_group'],\n",
    "                    'score': ent['score']\n",
    "                })\n",
    "                seen.add(ent['word'])\n",
    "        tags['entities'] = entity_tags[:5]\n",
    "        \n",
    "        # 3. Keyword extraction\n",
    "        self.rake.extract_keywords_from_text(text)\n",
    "        keywords = self.rake.get_ranked_phrases()[:5]\n",
    "        tags['keywords'] = keywords\n",
    "        \n",
    "        # 4. Generate final tag list\n",
    "        final_tags = []\n",
    "        \n",
    "        # Add top topics\n",
    "        final_tags.extend([t['tag'] for t in tags['topics'][:2]])\n",
    "        \n",
    "        # Add top entities\n",
    "        final_tags.extend([e['tag'] for e in tags['entities'][:3]])\n",
    "        \n",
    "        # Add top keyword\n",
    "        if keywords:\n",
    "            final_tags.append(keywords[0])\n",
    "        \n",
    "        tags['final_tags'] = list(set(final_tags))  # Remove duplicates\n",
    "        \n",
    "        return tags\n",
    "    \n",
    "    def batch_generate_tags(self, articles):\n",
    "        \"\"\"\n",
    "        Generate tags for multiple articles\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        for article in articles:\n",
    "            tags = self.generate_tags(article)\n",
    "            results.append(tags)\n",
    "        return results\n",
    "\n",
    "\n",
    "# Initialize tagger\n",
    "auto_tagger = NewsAutoTagger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Ê∏¨Ë©¶Ëá™ÂãïÊ®ôÁ±§Á≥ªÁµ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on all articles\n",
    "print(\"üè∑Ô∏è Generating comprehensive tags...\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "tagging_results = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    article = row.to_dict()\n",
    "    tags = auto_tagger.generate_tags(article)\n",
    "    tagging_results.append(tags)\n",
    "    \n",
    "    print(f\"\\nüì∞ {tags['article_id']}: {tags['title']}\")\n",
    "    print(f\"\\nüè∑Ô∏è Recommended Tags:\")\n",
    "    for tag in tags['final_tags']:\n",
    "        print(f\"   ‚Ä¢ {tag}\")\n",
    "    \n",
    "    print(f\"\\nüìä Details:\")\n",
    "    print(f\"   Topics: {[t['tag'] for t in tags['topics']]}\")\n",
    "    print(f\"   Entities: {[e['tag'] for e in tags['entities']]}\")\n",
    "    print(f\"   Keywords: {tags['keywords'][:3]}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "print(\"\\n‚úÖ All articles tagged!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Ê®ôÁ±§ÂàÜÊûêËàáÂèØË¶ñÂåñ\n",
    "\n",
    "### 7.1 Ê®ôÁ±§Áµ±Ë®à"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze tag distribution\n",
    "all_final_tags = []\n",
    "for result in tagging_results:\n",
    "    all_final_tags.extend(result['final_tags'])\n",
    "\n",
    "tag_freq = Counter(all_final_tags)\n",
    "\n",
    "print(\"üìä Tag Frequency Distribution:\\n\")\n",
    "for tag, count in tag_freq.most_common(15):\n",
    "    print(f\"   {tag:30} : {count:2d} articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top tags\n",
    "top_tags = tag_freq.most_common(10)\n",
    "tags, counts = zip(*top_tags)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(tags, counts, color='steelblue')\n",
    "plt.xlabel('Frequency')\n",
    "plt.title('Top 10 Most Frequent Tags', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Ê®ôÁ±§ÂÖ±ÁèæÂàÜÊûê"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tag co-occurrence matrix\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "# Prepare tag lists\n",
    "tag_lists = [result['final_tags'] for result in tagging_results]\n",
    "\n",
    "# Binary encoding\n",
    "mlb = MultiLabelBinarizer()\n",
    "tag_matrix = mlb.fit_transform(tag_lists)\n",
    "\n",
    "# Calculate co-occurrence\n",
    "co_occurrence = tag_matrix.T @ tag_matrix\n",
    "\n",
    "# Visualize (top 10 tags)\n",
    "top_tag_names = [tag for tag, _ in tag_freq.most_common(10)]\n",
    "top_tag_indices = [list(mlb.classes_).index(tag) for tag in top_tag_names if tag in mlb.classes_]\n",
    "\n",
    "co_occurrence_subset = co_occurrence[np.ix_(top_tag_indices, top_tag_indices)]\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(\n",
    "    co_occurrence_subset,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='YlOrRd',\n",
    "    xticklabels=top_tag_names,\n",
    "    yticklabels=top_tag_names,\n",
    "    cbar_kws={'label': 'Co-occurrence Count'}\n",
    ")\n",
    "plt.title('Tag Co-occurrence Matrix', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: ÁîüÁî¢ÈÉ®ÁΩ≤\n",
    "\n",
    "### 8.1 FastAPI Ê®ôÁ±§ÊúçÂãô"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile news_tagging_api.py\n",
    "# news_tagging_api.py\n",
    "\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Dict\n",
    "import uvicorn\n",
    "\n",
    "app = FastAPI(\n",
    "    title=\"News Auto-Tagging API\",\n",
    "    description=\"Automatic tag generation for news articles\",\n",
    "    version=\"1.0.0\"\n",
    ")\n",
    "\n",
    "# Global auto-tagger\n",
    "auto_tagger = None\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "async def load_models():\n",
    "    global auto_tagger\n",
    "    print(\"Loading models...\")\n",
    "    auto_tagger = NewsAutoTagger()\n",
    "    print(\"‚úÖ Models loaded\")\n",
    "\n",
    "class ArticleInput(BaseModel):\n",
    "    title: str = Field(..., min_length=5, max_length=500)\n",
    "    content: str = Field(..., min_length=50, max_length=10000)\n",
    "    id: str = Field(default=\"AUTO\")\n",
    "\n",
    "class TagResponse(BaseModel):\n",
    "    article_id: str\n",
    "    recommended_tags: List[str]\n",
    "    topics: List[Dict]\n",
    "    entities: List[Dict]\n",
    "    keywords: List[str]\n",
    "\n",
    "@app.post(\"/tag\", response_model=TagResponse)\n",
    "async def tag_article(article: ArticleInput):\n",
    "    \"\"\"Generate tags for an article\"\"\"\n",
    "    try:\n",
    "        tags = auto_tagger.generate_tags(article.dict())\n",
    "        \n",
    "        return TagResponse(\n",
    "            article_id=tags['article_id'],\n",
    "            recommended_tags=tags['final_tags'],\n",
    "            topics=tags['topics'],\n",
    "            entities=tags['entities'],\n",
    "            keywords=tags['keywords']\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.get(\"/available_tags\")\n",
    "async def get_available_tags():\n",
    "    \"\"\"Get all available tag categories\"\"\"\n",
    "    return TAG_CATEGORIES\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health_check():\n",
    "    return {\"status\": \"healthy\", \"tagger_loaded\": auto_tagger is not None}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Ê®ôÁ±§ÂìÅË≥™Ë©ï‰º∞\n",
    "\n",
    "### 9.1 Ê®ôÁ±§‰∏ÄËá¥ÊÄßÂàÜÊûê"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate tagging statistics\n",
    "tag_counts_per_article = [len(result['final_tags']) for result in tagging_results]\n",
    "\n",
    "print(\"üìä Tagging Statistics:\\n\")\n",
    "print(f\"   Average tags per article: {np.mean(tag_counts_per_article):.2f}\")\n",
    "print(f\"   Min tags: {np.min(tag_counts_per_article)}\")\n",
    "print(f\"   Max tags: {np.max(tag_counts_per_article)}\")\n",
    "print(f\"   Median tags: {np.median(tag_counts_per_article)}\")\n",
    "\n",
    "# Unique tags\n",
    "all_unique_tags = set()\n",
    "for result in tagging_results:\n",
    "    all_unique_tags.update(result['final_tags'])\n",
    "\n",
    "print(f\"\\n   Total unique tags: {len(all_unique_tags)}\")\n",
    "print(f\"   Tag vocabulary size: {len(tag_freq)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Á∏ΩÁµêËàáÊì¥Â±ï\n",
    "\n",
    "### ‚úÖ Êú¨Â∞àÊ°àÂÆåÊàêÂÖßÂÆπ\n",
    "\n",
    "1. **Â§öÁ≠ñÁï•Ê®ôÁ±§ÁîüÊàê**\n",
    "   - Èõ∂Ê®£Êú¨ÂàÜÈ°û (‰∏ªÈ°å„ÄÅÁî¢Ê•≠ÂàÜÈ°û)\n",
    "   - NER ÂØ¶È´îÊèêÂèñ (‰∫∫Âêç„ÄÅÂú∞Âêç„ÄÅÁµÑÁπî)\n",
    "   - ÈóúÈçµÂ≠óÊèêÂèñ (RAKE ÊºîÁÆóÊ≥ï)\n",
    "\n",
    "2. **Êô∫ËÉΩËûçÂêà**\n",
    "   - Â§öÊ®°ÂûãÁµêÊûúÊï¥Âêà\n",
    "   - ‰ø°ÂøÉÂ∫¶ÈÅéÊøæ\n",
    "   - ÂéªÈáçËàáÊéíÂ∫è\n",
    "\n",
    "3. **ÂèØË¶ñÂåñÂàÜÊûê**\n",
    "   - Ê®ôÁ±§È†ªÁéáÂàÜÂ∏É\n",
    "   - Ê®ôÁ±§ÂÖ±ÁèæÁü©Èô£\n",
    "   - ÂìÅË≥™Áµ±Ë®àÂàÜÊûê\n",
    "\n",
    "4. **ÁîüÁî¢ÈÉ®ÁΩ≤**\n",
    "   - FastAPI ÊúçÂãô\n",
    "   - RESTful API Ë®≠Ë®à\n",
    "   - ÊâπÈáèÊ®ôÁ±§ÁîüÊàê\n",
    "\n",
    "### üöÄ ÈÄ≤ÈöéÊì¥Â±ï\n",
    "\n",
    "#### ÊäÄË°ìÂÑ™Âåñ\n",
    "- [ ] ‰ΩøÁî® BERT ÈÄ≤Ë°åÈóúÈçµÂ≠óÊèêÂèñ (KeyBERT)\n",
    "- [ ] Êï¥Âêà LLM ÁîüÊàêÊèèËø∞ÊÄßÊ®ôÁ±§\n",
    "- [ ] ÂØ¶‰ΩúÊ®ôÁ±§ÈöéÂ±§ÁµêÊßã (Áà∂Ê®ôÁ±§-Â≠êÊ®ôÁ±§)\n",
    "- [ ] ÂÄãÊÄßÂåñÊ®ôÁ±§Êé®Ëñ¶ (Âü∫ÊñºÁî®Êà∂Èñ±ËÆÄÊ≠∑Âè≤)\n",
    "\n",
    "#### ÂäüËÉΩÊì¥Â±ï\n",
    "- [ ] ‰∏≠ÊñáÊñ∞ËÅûÊ®ôÁ±§ÊîØÊåÅ\n",
    "- [ ] Ëá™ÂãïÁîüÊàê SEO meta tags\n",
    "- [ ] Ê®ôÁ±§Ë∂®Âã¢ÂàÜÊûê (ÁÜ±ÈñÄÊ®ôÁ±§ËøΩËπ§)\n",
    "- [ ] Ê®ôÁ±§Áõ∏‰ººÂ∫¶Ë®àÁÆó (tag embeddings)\n",
    "\n",
    "#### ÊáâÁî®Â†¥ÊôØ\n",
    "- [ ] CMS (ÂÖßÂÆπÁÆ°ÁêÜÁ≥ªÁµ±) Êï¥Âêà\n",
    "- [ ] Á§æ‰∫§Â™íÈ´îËá™Âãï hashtag\n",
    "- [ ] ÈõªÂïÜÁî¢ÂìÅÊ®ôÁ±§ÁîüÊàê\n",
    "- [ ] Â≠∏Ë°ìË´ñÊñáÈóúÈçµË©ûÊèêÂèñ\n",
    "\n",
    "---\n",
    "\n",
    "**Â∞àÊ°àÁâàÊú¨**: v1.0\n",
    "**Âª∫Á´ãÊó•Êúü**: 2025-10-17\n",
    "**‰ΩúËÄÖ**: iSpan NLP Team\n",
    "**ÊéàÊ¨ä**: MIT License"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
