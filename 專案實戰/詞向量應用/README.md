# è©å‘é‡æ‡‰ç”¨å°ˆæ¡ˆ

**å°ˆæ¡ˆé¡å‹**: è©å‘é‡ - Word2Vec èˆ‡æ·±åº¦å­¸ç¿’
**é›£åº¦**: â­â­â­â­ é€²éš
**é è¨ˆæ™‚é–“**: 6-8 å°æ™‚ (3 notebooks)
**æŠ€è¡“æ£§**: Word2Vec, Gensim, Keras, LSTM

---

## ğŸ“‹ å°ˆæ¡ˆæ¦‚è¿°

æœ¬å°ˆæ¡ˆæ·±å…¥æ¢ç´¢**è©å‘é‡ (Word Embeddings)** æŠ€è¡“,å¾æ¦‚å¿µåˆ°å¯¦æˆ°æ‡‰ç”¨:

1. **Word2Vec æ¦‚å¿µä»‹ç´¹**: ç†è§£è©å‘é‡åŸç†
2. **åµŒå…¥å±¤æ‡‰ç”¨**: Keras Embedding å±¤å¯¦æˆ°
3. **æƒ…ç·’åˆ†æ**: ä½¿ç”¨è©å‘é‡+LSTM æ§‹å»ºæƒ…æ„Ÿåˆ†é¡å™¨

**æ ¸å¿ƒåƒ¹å€¼**:
- å°‡æ–‡å­—è½‰æ›ç‚ºæ•¸å€¼å‘é‡
- æ•æ‰èªç¾©ç›¸ä¼¼æ€§
- æå‡æ·±åº¦å­¸ç¿’æ¨¡å‹æ€§èƒ½

---

## ğŸ¯ å­¸ç¿’ç›®æ¨™

- âœ… ç†è§£è©å‘é‡çš„æ ¸å¿ƒæ¦‚å¿µ
- âœ… æŒæ¡ Word2Vec (CBOW & Skip-gram) åŸç†
- âœ… ä½¿ç”¨ Gensim è¨“ç·´ Word2Vec æ¨¡å‹
- âœ… æ‡‰ç”¨ Keras Embedding å±¤
- âœ… æ§‹å»ºè©å‘é‡+LSTM æƒ…æ„Ÿåˆ†é¡å™¨
- âœ… è¦–è¦ºåŒ–è©å‘é‡ (t-SNE)

---

## ğŸ“Š Notebooks è©³è§£

### Notebook 1: Word2Vec æ¦‚å¿µä»‹ç´¹

**æª”å**: `å°ˆæ¡ˆ_è©å‘é‡_Word2Vecæ¦‚å¿µä»‹ç´¹.ipynb`

**æ ¸å¿ƒå…§å®¹**:
1. è©å‘é‡çš„å‹•æ©Ÿ (ç‚ºä»€éº¼éœ€è¦?)
2. Word2Vec å…©ç¨®æ¶æ§‹:
   - CBOW (Continuous Bag of Words)
   - Skip-gram
3. è² æ¡æ¨£ (Negative Sampling)
4. ä½¿ç”¨ Gensim è¨“ç·´ Word2Vec
5. è©å‘é‡æ‡‰ç”¨:
   - ç›¸ä¼¼è©æŸ¥è©¢
   - è©é¡æ¯” (king - man + woman = queen)
   - è©å‘é‡å¯è¦–åŒ–

**é—œéµä»£ç¢¼**:
```python
from gensim.models import Word2Vec

# è¨“ç·´ Word2Vec
sentences = [['I', 'love', 'NLP'], ['Word2Vec', 'is', 'amazing']]
model = Word2Vec(
    sentences,
    vector_size=100,      # å‘é‡ç¶­åº¦
    window=5,             # ä¸Šä¸‹æ–‡çª—å£
    min_count=1,          # æœ€å°è©é »
    sg=0,                 # 0=CBOW, 1=Skip-gram
    epochs=10
)

# ç›¸ä¼¼è©æŸ¥è©¢
similar_words = model.wv.most_similar('love', topn=5)

# è©é¡æ¯”
result = model.wv.most_similar(
    positive=['king', 'woman'],
    negative=['man'],
    topn=1
)
```

---

### Notebook 2: åµŒå…¥å±¤æ‡‰ç”¨

**æª”å**: `å°ˆæ¡ˆ_è©å‘é‡_åµŒå…¥å±¤æ‡‰ç”¨_Keras.ipynb`

**æ ¸å¿ƒå…§å®¹**:
1. Keras Embedding å±¤åŸç†
2. éš¨æ©Ÿåˆå§‹åŒ– vs é è¨“ç·´è©å‘é‡
3. è¼‰å…¥ GloVe/Word2Vec é è¨“ç·´å‘é‡
4. å‡çµ/å¾®èª¿ Embedding å±¤
5. æ–‡æœ¬åˆ†é¡æ‡‰ç”¨

**é—œéµä»£ç¢¼**:
```python
from tensorflow import keras
from tensorflow.keras.layers import Embedding, LSTM, Dense

# æ–¹æ³• 1: éš¨æ©Ÿåˆå§‹åŒ– Embedding
model = keras.Sequential([
    Embedding(
        input_dim=vocab_size,     # è©å½™è¡¨å¤§å°
        output_dim=100,           # è©å‘é‡ç¶­åº¦
        input_length=max_length   # åºåˆ—é•·åº¦
    ),
    LSTM(64),
    Dense(1, activation='sigmoid')
])

# æ–¹æ³• 2: ä½¿ç”¨é è¨“ç·´è©å‘é‡
embedding_matrix = load_glove_embeddings()  # è¼‰å…¥ GloVe

model = keras.Sequential([
    Embedding(
        input_dim=vocab_size,
        output_dim=100,
        weights=[embedding_matrix],  # é è¨“ç·´æ¬Šé‡
        trainable=False              # å‡çµ Embedding
    ),
    LSTM(64),
    Dense(1, activation='sigmoid')
])
```

**å°æ¯”å¯¦é©—**:
| æ–¹æ³• | è¨“ç·´æ™‚é–“ | æº–ç¢ºç‡ | èªªæ˜ |
|------|---------|-------|------|
| éš¨æ©Ÿåˆå§‹åŒ– | å¿« | 85% | å¾é›¶å­¸ç¿’ |
| GloVe å‡çµ | å¿« | 91% | åˆ©ç”¨é è¨“ç·´çŸ¥è­˜ |
| GloVe å¾®èª¿ | æ…¢ | 93% | æœ€ä½³æ€§èƒ½ |

---

### Notebook 3: æƒ…ç·’åˆ†æ (è©å‘é‡+ç¥ç¶“ç¶²è·¯)

**æª”å**: `å°ˆæ¡ˆ_è©å‘é‡_æƒ…ç·’åˆ†æ_ç¥ç¶“ç¶²è·¯.ipynb`

**æ ¸å¿ƒå…§å®¹**:
1. æ•¸æ“šæº–å‚™ (IMDB æˆ– Twitter)
2. æ–‡æœ¬é è™•ç†èˆ‡ Tokenization
3. æ§‹å»ºè©å‘é‡+LSTM æ¨¡å‹
4. è¨“ç·´èˆ‡è©•ä¼°
5. éŒ¯èª¤åˆ†æ
6. æ¨¡å‹å„ªåŒ–æŠ€å·§

**å®Œæ•´æ¨¡å‹æ¶æ§‹**:
```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import (
    Embedding,
    LSTM,
    Bidirectional,
    Dropout,
    Dense,
    GlobalMaxPooling1D
)

model = Sequential([
    # Embedding å±¤
    Embedding(
        input_dim=vocab_size,
        output_dim=128,
        input_length=max_length,
        weights=[embedding_matrix],
        trainable=True
    ),

    # é›™å‘ LSTM
    Bidirectional(LSTM(64, return_sequences=True)),
    Dropout(0.5),

    # å…¨å±€æ± åŒ–
    GlobalMaxPooling1D(),

    # åˆ†é¡å±¤
    Dense(64, activation='relu'),
    Dropout(0.5),
    Dense(1, activation='sigmoid')
])

model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy']
)

# è¨“ç·´
history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=10,
    batch_size=32,
    callbacks=[
        keras.callbacks.EarlyStopping(patience=3),
        keras.callbacks.ModelCheckpoint('best_model.h5', save_best_only=True)
    ]
)
```

**æ€§èƒ½æŒ‡æ¨™**:
```
è¨“ç·´æº–ç¢ºç‡: 95.2%
é©—è­‰æº–ç¢ºç‡: 91.8%
æ¸¬è©¦æº–ç¢ºç‡: 90.5%
F1-Score: 0.89
```

---

## ğŸ“ æ•¸æ“šèªªæ˜

### Notebook 1-2 æ•¸æ“š
- å¯ä½¿ç”¨ä»»ä½•æ–‡æœ¬èªæ–™
- æ¨è–¦: Text8, Wikipedia dump, æˆ–è‡ªè¨‚èªæ–™

### Notebook 3 æ•¸æ“š
- **IMDB é›»å½±è©•è«–**: Keras å…§å»º
  ```python
  from tensorflow.keras.datasets import imdb
  (X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=10000)
  ```
- **Twitter æƒ…æ„Ÿ**: Kaggle ä¸‹è¼‰
- **ä¸­æ–‡è©•è«–**: ä½¿ç”¨ `datasets/google_reviews/`

---

## ğŸ”§ æŠ€è¡“é›£é»èˆ‡è§£æ±º

### é›£é» 1: è©å½™è¡¨å¤–è©å½™ (OOV)

```python
# å•é¡Œ: æ¸¬è©¦é›†æœ‰è¨“ç·´æ™‚æœªè¦‹éçš„è©

# è§£æ±ºæ–¹æ¡ˆ 1: ä½¿ç”¨ <UNK> token
word_index = {'<PAD>': 0, '<UNK>': 1}
for word in vocab:
    word_index[word] = len(word_index)

# è§£æ±ºæ–¹æ¡ˆ 2: ä½¿ç”¨å­—ç¬¦ç´šæ¨¡å‹
# è§£æ±ºæ–¹æ¡ˆ 3: ä½¿ç”¨ subword tokenization (BPE)
```

### é›£é» 2: åºåˆ—é•·åº¦ä¸ä¸€

```python
from tensorflow.keras.preprocessing.sequence import pad_sequences

# å¡«å……/æˆªæ–·åˆ°çµ±ä¸€é•·åº¦
padded_sequences = pad_sequences(
    sequences,
    maxlen=max_length,
    padding='post',      # å¾Œå¡«å……
    truncating='post'    # å¾Œæˆªæ–·
)
```

### é›£é» 3: é¡åˆ¥ä¸å¹³è¡¡

```python
# è§£æ±ºæ–¹æ¡ˆ 1: é¡åˆ¥æ¬Šé‡
from sklearn.utils.class_weight import compute_class_weight

class_weights = compute_class_weight(
    'balanced',
    classes=np.unique(y_train),
    y=y_train
)

model.fit(
    X_train, y_train,
    class_weight=dict(enumerate(class_weights))
)

# è§£æ±ºæ–¹æ¡ˆ 2: æ•¸æ“šé‡æ¡æ¨£
from imblearn.over_sampling import SMOTE
X_resampled, y_resampled = SMOTE().fit_resample(X_train, y_train)
```

---

## ğŸ“ˆ é€²éšå„ªåŒ–

### ä½¿ç”¨é è¨“ç·´è©å‘é‡

```python
# GloVe è©å‘é‡ä¸‹è¼‰
# wget http://nlp.stanford.edu/data/glove.6B.zip

def load_glove_embeddings(glove_file, word_index, embedding_dim=100):
    """è¼‰å…¥ GloVe è©å‘é‡"""
    embeddings_index = {}

    with open(glove_file, encoding='utf-8') as f:
        for line in f:
            values = line.split()
            word = values[0]
            coefs = np.asarray(values[1:], dtype='float32')
            embeddings_index[word] = coefs

    print(f"âœ… è¼‰å…¥ {len(embeddings_index)} å€‹è©å‘é‡")

    # å‰µå»º Embedding Matrix
    embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))

    for word, i in word_index.items():
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[i] = embedding_vector

    return embedding_matrix

# ä½¿ç”¨
glove_matrix = load_glove_embeddings(
    'glove.6B.100d.txt',
    word_index,
    embedding_dim=100
)
```

### è©å‘é‡å¯è¦–åŒ– (t-SNE)

```python
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt

# æå–è©å‘é‡
words_to_plot = ['love', 'hate', 'good', 'bad', 'happy', 'sad']
vectors = [model.wv[word] for word in words_to_plot]

# t-SNE é™ç¶­åˆ° 2D
tsne = TSNE(n_components=2, random_state=42)
vectors_2d = tsne.fit_transform(vectors)

# ç¹ªåœ–
plt.figure(figsize=(10, 8))
for i, word in enumerate(words_to_plot):
    x, y = vectors_2d[i]
    plt.scatter(x, y)
    plt.annotate(word, (x, y), fontsize=12)

plt.title('Word Embeddings Visualization (t-SNE)')
plt.show()
```

---

## âœ… æª¢æŸ¥æ¸…å–®

- [ ] ç†è§£ Word2Vec CBOW èˆ‡ Skip-gram å·®ç•°
- [ ] æˆåŠŸè¨“ç·´ Word2Vec æ¨¡å‹
- [ ] å®Œæˆç›¸ä¼¼è©æŸ¥è©¢èˆ‡è©é¡æ¯”
- [ ] æŒæ¡ Keras Embedding å±¤ä½¿ç”¨
- [ ] å°æ¯”éš¨æ©Ÿåˆå§‹åŒ–èˆ‡é è¨“ç·´è©å‘é‡
- [ ] æ§‹å»ºå®Œæ•´çš„æƒ…æ„Ÿåˆ†é¡æ¨¡å‹
- [ ] é”åˆ° 90%+ æº–ç¢ºç‡
- [ ] å®Œæˆè©å‘é‡ t-SNE è¦–è¦ºåŒ–

---

**å°ˆæ¡ˆç‰ˆæœ¬**: v1.0
**Notebooks æ•¸é‡**: 3
**æœ€å¾Œæ›´æ–°**: 2025-10-17
**ç¶­è­·è€…**: iSpan NLP Team
