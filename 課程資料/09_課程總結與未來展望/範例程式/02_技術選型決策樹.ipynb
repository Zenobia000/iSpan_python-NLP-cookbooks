{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH09-02: 技術選型決策樹\n",
    "\n",
    "**課程目標:**\n",
    "- 掌握不同 NLP 任務的技術選型決策流程\n",
    "- 理解模型大小與效能權衡\n",
    "- 學會根據實際需求選擇最合適的技術方案\n",
    "- 建立系統化的技術決策框架\n",
    "\n",
    "**學習時間:** 約 90 分鐘\n",
    "\n",
    "**前置知識:**\n",
    "- 已完成 CH09-01 技術體系回顧\n",
    "- 熟悉各類 NLP 技術與模型\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 目錄\n",
    "\n",
    "1. [技術選型核心原則](#1)\n",
    "2. [文本分類任務選型決策樹](#2)\n",
    "3. [命名實體識別 (NER) 選型決策樹](#3)\n",
    "4. [文本生成任務選型決策樹](#4)\n",
    "5. [問答系統選型決策樹](#5)\n",
    "6. [模型大小與效能權衡](#6)\n",
    "7. [實際案例選型分析](#7)\n",
    "8. [決策樹實戰工具](#8)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 環境設定與套件導入\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import HTML, display, Markdown\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 設定中文顯示\n",
    "plt.rcParams['font.sans-serif'] = ['Microsoft JhengHei', 'SimHei', 'Arial Unicode MS']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 設定顯示風格\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_palette('Set2')\n",
    "\n",
    "print(\"✅ 環境設定完成\")\n",
    "print(f\"NumPy 版本: {np.__version__}\")\n",
    "print(f\"Pandas 版本: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "## 1. 技術選型核心原則\n",
    "\n",
    "### 1.1 選型決策的六大維度\n",
    "\n",
    "在選擇 NLP 技術方案時,需要綜合考慮以下維度:\n",
    "\n",
    "#### 1. 任務類型 (Task Type)\n",
    "- **理解型任務:** 分類、NER、關係抽取、情感分析\n",
    "- **生成型任務:** 機器翻譯、摘要、對話、創作\n",
    "- **混合型任務:** 問答系統、文本改寫\n",
    "\n",
    "#### 2. 數據規模 (Data Scale)\n",
    "- **極少 (<1K):** 零樣本學習、少樣本學習\n",
    "- **少量 (1K-10K):** 預訓練模型微調\n",
    "- **中等 (10K-100K):** 預訓練模型微調或輕量級訓練\n",
    "- **大量 (>100K):** 從頭訓練或大規模微調\n",
    "\n",
    "#### 3. 計算資源 (Computing Resources)\n",
    "- **低算力 (CPU only):** 傳統 ML、小型模型\n",
    "- **中等算力 (單 GPU):** BERT Base、小型 Transformer\n",
    "- **高算力 (多 GPU/TPU):** BERT Large、GPT 系列\n",
    "\n",
    "#### 4. 效能要求 (Performance Requirements)\n",
    "- **基礎要求:** 傳統 ML、小型預訓練模型\n",
    "- **高效能要求:** 大型預訓練模型、集成方法\n",
    "- **SOTA 要求:** 最新 LLM、精細調優\n",
    "\n",
    "#### 5. 延遲限制 (Latency Constraints)\n",
    "- **實時 (<100ms):** 模型壓縮、量化、蒸餾\n",
    "- **近實時 (<1s):** 中型模型、優化推理\n",
    "- **批處理 (>1s):** 大型模型、高精度優先\n",
    "\n",
    "#### 6. 可解釋性需求 (Interpretability)\n",
    "- **高可解釋性:** 傳統 ML、規則系統\n",
    "- **中等可解釋性:** Attention 視覺化、特徵重要性\n",
    "- **低可解釋性:** 端到端深度學習\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化技術選型六大維度\n",
    "dimensions = ['任務類型', '數據規模', '計算資源', '效能要求', '延遲限制', '可解釋性']\n",
    "\n",
    "# 不同方案在各維度的評分 (1-10)\n",
    "traditional_ml = [5, 3, 10, 5, 10, 9]\n",
    "bert_base = [8, 6, 6, 8, 6, 5]\n",
    "bert_large = [9, 7, 4, 9, 4, 5]\n",
    "gpt_small = [9, 5, 5, 9, 5, 3]\n",
    "gpt_large = [10, 8, 2, 10, 2, 2]\n",
    "\n",
    "# 繪製雷達圖\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "ax = fig.add_subplot(111, projection='polar')\n",
    "\n",
    "num_vars = len(dimensions)\n",
    "angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()\n",
    "angles += angles[:1]\n",
    "\n",
    "# 繪製各方案\n",
    "solutions = [\n",
    "    ('傳統 ML', traditional_ml, 'blue'),\n",
    "    ('BERT Base', bert_base, 'green'),\n",
    "    ('BERT Large', bert_large, 'orange'),\n",
    "    ('GPT (小型)', gpt_small, 'purple'),\n",
    "    ('GPT (大型)', gpt_large, 'red')\n",
    "]\n",
    "\n",
    "for name, values, color in solutions:\n",
    "    values_plot = values + values[:1]\n",
    "    ax.plot(angles, values_plot, 'o-', linewidth=2, label=name, color=color, markersize=6)\n",
    "    ax.fill(angles, values_plot, alpha=0.15, color=color)\n",
    "\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(dimensions, fontsize=12)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.set_yticks([2, 4, 6, 8, 10])\n",
    "ax.set_yticklabels(['2', '4', '6', '8', '10'], fontsize=10)\n",
    "ax.set_title('不同技術方案在六大維度的表現對比', fontsize=16, fontweight='bold', pad=30)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.15), fontsize=11)\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n💡 選型洞察:\")\n",
    "print(\"  1. 傳統 ML: 資源友好,延遲低,可解釋性強,但效能有限\")\n",
    "print(\"  2. BERT Base: 平衡方案,適合大多數任務\")\n",
    "print(\"  3. BERT Large: 高效能,但資源需求高\")\n",
    "print(\"  4. GPT 小型: 生成能力強,資源適中\")\n",
    "print(\"  5. GPT 大型: 最強效能,但資源與延遲是瓶頸\")\n",
    "print(\"\\n🎯 選型建議: 根據實際約束條件選擇最平衡的方案\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 技術選型決策流程\n",
    "\n",
    "通用的技術選型決策流程:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 Mermaid 語法繪製通用決策流程\n",
    "mermaid_general_flow = '''\n",
    "graph TD\n",
    "    Start[開始技術選型] --> Q1{任務類型?}\n",
    "    \n",
    "    Q1 -->|理解型| Understanding[文本分類/NER/關係抽取]\n",
    "    Q1 -->|生成型| Generation[翻譯/摘要/對話]\n",
    "    Q1 -->|混合型| Hybrid[問答/改寫]\n",
    "    \n",
    "    Understanding --> Q2{數據量?}\n",
    "    Generation --> Q2\n",
    "    Hybrid --> Q2\n",
    "    \n",
    "    Q2 -->|<1K| ZeroFew[零樣本/少樣本學習]\n",
    "    Q2 -->|1K-10K| SmallData[小數據微調]\n",
    "    Q2 -->|>10K| LargeData[充足數據]\n",
    "    \n",
    "    ZeroFew --> Q3{計算資源?}\n",
    "    SmallData --> Q3\n",
    "    LargeData --> Q3\n",
    "    \n",
    "    Q3 -->|CPU only| LowCompute[傳統ML/小型模型]\n",
    "    Q3 -->|單GPU| MidCompute[BERT Base/GPT小型]\n",
    "    Q3 -->|多GPU| HighCompute[BERT Large/GPT大型]\n",
    "    \n",
    "    LowCompute --> Q4{延遲要求?}\n",
    "    MidCompute --> Q4\n",
    "    HighCompute --> Q4\n",
    "    \n",
    "    Q4 -->|實時<100ms| Realtime[模型壓縮/量化]\n",
    "    Q4 -->|近實時<1s| NearRealtime[優化推理]\n",
    "    Q4 -->|批處理>1s| Batch[高精度優先]\n",
    "    \n",
    "    Realtime --> Final[最終技術方案]\n",
    "    NearRealtime --> Final\n",
    "    Batch --> Final\n",
    "    \n",
    "    style Start fill:#FFD700,stroke:#333,stroke-width:3px\n",
    "    style Final fill:#4CAF50,stroke:#333,stroke-width:3px\n",
    "    style Q1 fill:#87CEEB,stroke:#333,stroke-width:2px\n",
    "    style Q2 fill:#87CEEB,stroke:#333,stroke-width:2px\n",
    "    style Q3 fill:#87CEEB,stroke:#333,stroke-width:2px\n",
    "    style Q4 fill:#87CEEB,stroke:#333,stroke-width:2px\n",
    "'''\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"border:3px solid #4CAF50; padding:20px; border-radius:15px; background-color:#F1F8E9;\">\n",
    "    <h3 style=\"color:#2E7D32;\">🌳 通用技術選型決策流程</h3>\n",
    "    <p><strong>四步決策法:</strong></p>\n",
    "    <ol style=\"line-height:1.8;\">\n",
    "        <li><strong>步驟 1:</strong> 確定任務類型 (理解/生成/混合)</li>\n",
    "        <li><strong>步驟 2:</strong> 評估數據規模 (少/中/多)</li>\n",
    "        <li><strong>步驟 3:</strong> 考慮計算資源 (CPU/單GPU/多GPU)</li>\n",
    "        <li><strong>步驟 4:</strong> 確定延遲要求 (實時/近實時/批處理)</li>\n",
    "    </ol>\n",
    "    <p style=\"color:#666; font-style:italic; margin-top:15px;\">💡 提示: 在 Markdown 編輯器中使用上方 Mermaid 代碼可視覺化完整決策樹</p>\n",
    "</div>\n",
    "\"\"\"))\n",
    "\n",
    "print(\"\\n🔍 決策要點:\")\n",
    "print(\"  1. 任務類型決定模型架構 (Encoder/Decoder/Encoder-Decoder)\")\n",
    "print(\"  2. 數據規模決定訓練策略 (零樣本/微調/從頭訓練)\")\n",
    "print(\"  3. 計算資源決定模型大小\")\n",
    "print(\"  4. 延遲要求決定優化策略\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "## 2. 文本分類任務選型決策樹\n",
    "\n",
    "### 2.1 文本分類任務特點\n",
    "\n",
    "**任務定義:** 將文本分配到預定義的類別\n",
    "\n",
    "**常見應用:**\n",
    "- 情感分析 (正面/負面/中性)\n",
    "- 主題分類 (新聞分類、內容審核)\n",
    "- 垃圾郵件檢測\n",
    "- 意圖識別\n",
    "\n",
    "**評估指標:**\n",
    "- Accuracy, Precision, Recall, F1-Score\n",
    "- Confusion Matrix\n",
    "- ROC-AUC (二分類)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文本分類決策樹\n",
    "text_classification_tree = '''\n",
    "graph TD\n",
    "    Start[文本分類任務] --> Q1{數據量?}\n",
    "    \n",
    "    Q1 -->|<500 樣本| Few[極少數據]\n",
    "    Q1 -->|500-5K| Small[少量數據]\n",
    "    Q1 -->|5K-50K| Medium[中等數據]\n",
    "    Q1 -->|>50K| Large[充足數據]\n",
    "    \n",
    "    Few --> F1[零樣本學習<br/>GPT-3/4 Prompt]\n",
    "    Few --> F2[少樣本學習<br/>Few-shot Prompt]\n",
    "    \n",
    "    Small --> S1{類別數?}\n",
    "    S1 -->|2-5 類| S1A[BERT Base 微調<br/>DistilBERT]\n",
    "    S1 -->|>5 類| S1B[數據增強<br/>+ BERT 微調]\n",
    "    \n",
    "    Medium --> M1{效能要求?}\n",
    "    M1 -->|基礎| M1A[TF-IDF + LR<br/>FastText]\n",
    "    M1 -->|高| M1B[BERT Base 微調<br/>RoBERTa]\n",
    "    \n",
    "    Large --> L1{計算資源?}\n",
    "    L1 -->|有限| L1A[BERT Base<br/>DistilBERT]\n",
    "    L1 -->|充足| L1B[BERT Large<br/>RoBERTa Large]\n",
    "    L1 -->|極高| L1C[GPT-3 微調<br/>或從頭訓練]\n",
    "    \n",
    "    style Start fill:#FFD700,stroke:#333,stroke-width:3px\n",
    "    style F1 fill:#90EE90,stroke:#333,stroke-width:2px\n",
    "    style F2 fill:#90EE90,stroke:#333,stroke-width:2px\n",
    "    style S1A fill:#87CEEB,stroke:#333,stroke-width:2px\n",
    "    style S1B fill:#87CEEB,stroke:#333,stroke-width:2px\n",
    "    style M1A fill:#FFB6C1,stroke:#333,stroke-width:2px\n",
    "    style M1B fill:#FFB6C1,stroke:#333,stroke-width:2px\n",
    "    style L1A fill:#DDA0DD,stroke:#333,stroke-width:2px\n",
    "    style L1B fill:#DDA0DD,stroke:#333,stroke-width:2px\n",
    "    style L1C fill:#DDA0DD,stroke:#333,stroke-width:2px\n",
    "'''\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"border:3px solid #2196F3; padding:20px; border-radius:15px; background-color:#E3F2FD;\">\n",
    "    <h3 style=\"color:#1976D2;\">📊 文本分類任務選型決策樹</h3>\n",
    "    <p><strong>決策邏輯:</strong></p>\n",
    "    <ul style=\"line-height:1.8;\">\n",
    "        <li>🟢 <strong>極少數據 (<500):</strong> 零樣本/少樣本學習 (GPT Prompt)</li>\n",
    "        <li>🔵 <strong>少量數據 (500-5K):</strong> BERT Base 微調 + 數據增強</li>\n",
    "        <li>🔴 <strong>中等數據 (5K-50K):</strong> TF-IDF (基礎需求) 或 BERT (高效能)</li>\n",
    "        <li>🟣 <strong>充足數據 (>50K):</strong> 根據資源選擇 BERT Base/Large 或 GPT</li>\n",
    "    </ul>\n",
    "</div>\n",
    "\"\"\"))\n",
    "\n",
    "print(\"\\n🎯 文本分類選型建議:\")\n",
    "print(\"  1. 數據<500: GPT-3/4 零樣本學習 (快速驗證想法)\")\n",
    "print(\"  2. 數據 500-5K: BERT Base 微調 (性價比最高)\")\n",
    "print(\"  3. 數據 5K-50K: TF-IDF+LR (快速) 或 BERT (高精度)\")\n",
    "print(\"  4. 數據>50K: BERT Large 或從頭訓練 (追求極致效能)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 文本分類方案對比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文本分類不同方案對比\n",
    "classification_solutions = {\n",
    "    '方案': ['TF-IDF + LR', 'FastText', 'BERT Base 微調', 'BERT Large 微調', 'GPT-3 零樣本', 'GPT-3 微調'],\n",
    "    '數據需求': ['5K+', '10K+', '1K+', '5K+', '<100', '1K+'],\n",
    "    '訓練時間': ['< 1 分鐘', '< 5 分鐘', '30-60 分鐘', '2-4 小時', '0 (無訓練)', '1-2 小時'],\n",
    "    '推理速度': ['極快 (ms)', '極快 (ms)', '快 (10-50ms)', '中 (50-100ms)', '慢 (1-3s)', '慢 (1-3s)'],\n",
    "    '準確率 (%)': ['85-90', '88-92', '92-95', '94-97', '80-90', '93-96'],\n",
    "    '資源需求': ['CPU', 'CPU/GPU', '單 GPU', '單/多 GPU', 'API', 'API/多 GPU'],\n",
    "    '適用場景': ['快速原型', '大規模部署', '通用方案', '高精度需求', '快速驗證', '高精度+靈活']\n",
    "}\n",
    "\n",
    "df_class = pd.DataFrame(classification_solutions)\n",
    "\n",
    "# 美化表格\n",
    "def color_by_performance(val):\n",
    "    if '94-97' in str(val) or '93-96' in str(val):\n",
    "        return 'background-color: lightgreen; font-weight: bold'\n",
    "    elif '92-95' in str(val) or '88-92' in str(val):\n",
    "        return 'background-color: lightyellow'\n",
    "    return ''\n",
    "\n",
    "styled_class = df_class.style.applymap(color_by_performance, subset=['準確率 (%)'])\n",
    "\n",
    "display(HTML(\"<h3>📋 文本分類方案全面對比</h3>\"))\n",
    "display(styled_class.hide(axis='index').set_properties(**{'text-align': 'center', 'font-size': '10pt'}))\n",
    "\n",
    "print(\"\\n💡 方案選擇建議:\")\n",
    "print(\"  - 快速原型/成本敏感: TF-IDF + LR\")\n",
    "print(\"  - 大規模部署: FastText (速度+效能平衡)\")\n",
    "print(\"  - 通用推薦: BERT Base 微調 (性價比最高)\")\n",
    "print(\"  - 高精度需求: BERT Large 微調\")\n",
    "print(\"  - 快速驗證/極少數據: GPT-3 零樣本\")\n",
    "print(\"  - 靈活性+高精度: GPT-3 微調\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "## 3. 命名實體識別 (NER) 選型決策樹\n",
    "\n",
    "### 3.1 NER 任務特點\n",
    "\n",
    "**任務定義:** 從文本中識別並分類實體 (人名、地名、機構名等)\n",
    "\n",
    "**常見實體類型:**\n",
    "- 人名 (PER)\n",
    "- 地名 (LOC)\n",
    "- 機構名 (ORG)\n",
    "- 時間 (TIME)\n",
    "- 金額 (MONEY)\n",
    "\n",
    "**評估指標:**\n",
    "- Entity-level F1-Score\n",
    "- Token-level Accuracy\n",
    "- Precision, Recall\n",
    "\n",
    "**技術特點:**\n",
    "- 序列標註任務 (BIO 標註)\n",
    "- 上下文依賴強\n",
    "- 需要處理實體邊界\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NER 任務選型決策樹\n",
    "ner_decision_tree = '''\n",
    "graph TD\n",
    "    Start[NER 任務] --> Q1{領域?}\n",
    "    \n",
    "    Q1 -->|通用領域| General[通用 NER]\n",
    "    Q1 -->|專業領域| Domain[領域 NER<br/>醫療/金融/法律]\n",
    "    \n",
    "    General --> G1{數據量?}\n",
    "    G1 -->|<1K| G1A[零樣本<br/>spaCy/Flair]\n",
    "    G1 -->|1K-10K| G1B[BERT 微調<br/>SpanBERT]\n",
    "    G1 -->|>10K| G1C[BERT Large<br/>或 RoBERTa]\n",
    "    \n",
    "    Domain --> D1{有標註數據?}\n",
    "    D1 -->|無| D1A[弱監督學習<br/>遠程監督]\n",
    "    D1 -->|少量<1K| D1B[數據增強<br/>+ BERT 微調]\n",
    "    D1 -->|充足>5K| D1C[領域 BERT<br/>BioBERT/FinBERT]\n",
    "    \n",
    "    G1A --> Final1[spaCy<br/>預訓練模型]\n",
    "    G1B --> Final2[BERT-NER<br/>微調]\n",
    "    G1C --> Final3[BERT Large<br/>或 RoBERTa-NER]\n",
    "    D1A --> Final4[遠程監督<br/>+ 自訓練]\n",
    "    D1B --> Final5[數據增強<br/>+ BERT]\n",
    "    D1C --> Final6[領域預訓練<br/>模型微調]\n",
    "    \n",
    "    style Start fill:#FFD700,stroke:#333,stroke-width:3px\n",
    "    style Final1 fill:#90EE90,stroke:#333,stroke-width:2px\n",
    "    style Final2 fill:#90EE90,stroke:#333,stroke-width:2px\n",
    "    style Final3 fill:#90EE90,stroke:#333,stroke-width:2px\n",
    "    style Final4 fill:#87CEEB,stroke:#333,stroke-width:2px\n",
    "    style Final5 fill:#87CEEB,stroke:#333,stroke-width:2px\n",
    "    style Final6 fill:#87CEEB,stroke:#333,stroke-width:2px\n",
    "'''\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"border:3px solid #FF9800; padding:20px; border-radius:15px; background-color:#FFF3E0;\">\n",
    "    <h3 style=\"color:#E65100;\">🏷️ NER 任務選型決策樹</h3>\n",
    "    <p><strong>關鍵決策點:</strong></p>\n",
    "    <ol style=\"line-height:1.8;\">\n",
    "        <li><strong>領域判斷:</strong> 通用 vs 專業領域 (醫療/金融/法律)</li>\n",
    "        <li><strong>通用領域:</strong> 根據數據量選擇 spaCy/BERT Base/BERT Large</li>\n",
    "        <li><strong>專業領域:</strong> 優先使用領域預訓練模型 (BioBERT/FinBERT)</li>\n",
    "        <li><strong>無標註數據:</strong> 遠程監督、弱監督學習</li>\n",
    "    </ol>\n",
    "</div>\n",
    "\"\"\"))\n",
    "\n",
    "print(\"\\n🎯 NER 選型建議:\")\n",
    "print(\"  1. 通用領域 + 少量數據: BERT Base 微調 (CoNLL-2003 預訓練)\")\n",
    "print(\"  2. 醫療領域: BioBERT, SciBERT (生物醫學預訓練)\")\n",
    "print(\"  3. 金融領域: FinBERT (金融新聞預訓練)\")\n",
    "print(\"  4. 中文 NER: BERT-Chinese, RoBERTa-Chinese\")\n",
    "print(\"  5. 快速部署: spaCy 預訓練模型 (開箱即用)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 NER 方案對比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NER 方案對比\n",
    "ner_solutions = {\n",
    "    '方案': ['CRF', 'BiLSTM-CRF', 'spaCy', 'BERT-NER', 'BioBERT', 'SpanBERT'],\n",
    "    '技術基礎': ['統計模型', 'RNN+CRF', 'CNN+規則', 'Transformer', '領域Transformer', 'Span預測'],\n",
    "    '數據需求': ['10K+', '5K+', '0 (預訓練)', '1K+', '2K+ (領域)', '2K+'],\n",
    "    '訓練時間': ['< 10 分鐘', '30-60 分鐘', '0', '1-2 小時', '1-3 小時', '2-4 小時'],\n",
    "    'F1-Score (%)': ['75-82', '82-88', '85-90', '90-93', '92-95 (領域)', '91-94'],\n",
    "    '優勢': ['快速訓練', '處理序列', '開箱即用', '通用性強', '領域專精', '邊界準確'],\n",
    "    '劣勢': ['特徵工程', '訓練慢', '難以自定義', '需要數據', '領域受限', '計算量大']\n",
    "}\n",
    "\n",
    "df_ner = pd.DataFrame(ner_solutions)\n",
    "\n",
    "# 視覺化 F1-Score 對比\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "f1_scores = [78.5, 85, 87.5, 91.5, 93.5, 92.5]\n",
    "colors_bars = ['coral', 'skyblue', 'lightgreen', 'gold', 'plum', 'pink']\n",
    "\n",
    "bars = ax.bar(df_ner['方案'], f1_scores, color=colors_bars, alpha=0.8, edgecolor='black', linewidth=2)\n",
    "\n",
    "# 添加數值標籤\n",
    "for bar, score in zip(bars, f1_scores):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "            f'{score:.1f}%', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "ax.set_ylabel('F1-Score (%)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('NER 不同方案效能對比', fontsize=15, fontweight='bold')\n",
    "ax.set_ylim(0, 100)\n",
    "ax.axhline(y=90, color='red', linestyle='--', linewidth=1.5, alpha=0.7, label='90% 基準線')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.xticks(rotation=15, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "display(HTML(\"<h3>📊 NER 方案詳細對比</h3>\"))\n",
    "display(df_ner.style.hide(axis='index').set_properties(**{'text-align': 'center', 'font-size': '10pt'}))\n",
    "\n",
    "print(\"\\n💡 實戰建議:\")\n",
    "print(\"  - 快速原型: spaCy 預訓練模型\")\n",
    "print(\"  - 通用 NER: BERT-NER 微調\")\n",
    "print(\"  - 醫療領域: BioBERT (F1 可達 93%+)\")\n",
    "print(\"  - 高精度需求: SpanBERT (邊界識別更準確)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "## 4. 文本生成任務選型決策樹\n",
    "\n",
    "### 4.1 文本生成任務類型\n",
    "\n",
    "文本生成任務可分為多種類型:\n",
    "\n",
    "#### 1. 機器翻譯\n",
    "- **任務:** 將源語言翻譯為目標語言\n",
    "- **評估:** BLEU, METEOR, chrF\n",
    "- **推薦:** mT5, mBART, NLLB\n",
    "\n",
    "#### 2. 文本摘要\n",
    "- **任務:** 生成簡潔摘要\n",
    "- **類型:** 抽取式 vs 生成式\n",
    "- **推薦:** BART, PEGASUS, T5\n",
    "\n",
    "#### 3. 對話生成\n",
    "- **任務:** 生成自然對話回復\n",
    "- **評估:** BLEU, Perplexity, 人工評估\n",
    "- **推薦:** GPT-3/4, DialoGPT, Blenderbot\n",
    "\n",
    "#### 4. 創意寫作\n",
    "- **任務:** 故事、詩歌、文章創作\n",
    "- **評估:** 人工評估、多樣性指標\n",
    "- **推薦:** GPT-3/4, Claude\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文本生成任務選型決策樹\n",
    "generation_tree = '''\n",
    "graph TD\n",
    "    Start[文本生成任務] --> Q1{任務類型?}\n",
    "    \n",
    "    Q1 -->|機器翻譯| Translation[翻譯任務]\n",
    "    Q1 -->|文本摘要| Summarization[摘要任務]\n",
    "    Q1 -->|對話生成| Dialogue[對話任務]\n",
    "    Q1 -->|創意寫作| Creative[創作任務]\n",
    "    \n",
    "    Translation --> T1{語言對?}\n",
    "    T1 -->|英中/中英| T1A[mT5<br/>mBART]\n",
    "    T1 -->|多語言| T1B[NLLB<br/>mT5-XXL]\n",
    "    T1 -->|低資源語言| T1C[mBART50<br/>遷移學習]\n",
    "    \n",
    "    Summarization --> S1{摘要類型?}\n",
    "    S1 -->|抽取式| S1A[BERT<br/>TextRank]\n",
    "    S1 -->|生成式| S1B[BART<br/>PEGASUS]\n",
    "    S1 -->|混合| S1C[T5<br/>Longformer]\n",
    "    \n",
    "    Dialogue --> D1{對話場景?}\n",
    "    D1 -->|任務導向| D1A[T5<br/>BERT+生成]\n",
    "    D1 -->|開放域| D1B[GPT-3/4<br/>DialoGPT]\n",
    "    D1 -->|多輪對話| D1C[Blenderbot<br/>ChatGPT]\n",
    "    \n",
    "    Creative --> C1{創作類型?}\n",
    "    C1 -->|故事/小說| C1A[GPT-3/4<br/>Claude]\n",
    "    C1 -->|詩歌/歌詞| C1B[GPT-3<br/>專用模型]\n",
    "    C1 -->|技術文檔| C1C[GPT-4<br/>Claude 2]\n",
    "    \n",
    "    style Start fill:#FFD700,stroke:#333,stroke-width:3px\n",
    "    style T1A fill:#90EE90,stroke:#333,stroke-width:2px\n",
    "    style T1B fill:#90EE90,stroke:#333,stroke-width:2px\n",
    "    style T1C fill:#90EE90,stroke:#333,stroke-width:2px\n",
    "    style S1A fill:#87CEEB,stroke:#333,stroke-width:2px\n",
    "    style S1B fill:#87CEEB,stroke:#333,stroke-width:2px\n",
    "    style S1C fill:#87CEEB,stroke:#333,stroke-width:2px\n",
    "    style D1A fill:#FFB6C1,stroke:#333,stroke-width:2px\n",
    "    style D1B fill:#FFB6C1,stroke:#333,stroke-width:2px\n",
    "    style D1C fill:#FFB6C1,stroke:#333,stroke-width:2px\n",
    "    style C1A fill:#DDA0DD,stroke:#333,stroke-width:2px\n",
    "    style C1B fill:#DDA0DD,stroke:#333,stroke-width:2px\n",
    "    style C1C fill:#DDA0DD,stroke:#333,stroke-width:2px\n",
    "'''\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"border:3px solid #9C27B0; padding:20px; border-radius:15px; background-color:#F3E5F5;\">\n",
    "    <h3 style=\"color:#6A1B9A;\">✍️ 文本生成任務選型決策樹</h3>\n",
    "    <p><strong>生成任務選型關鍵:</strong></p>\n",
    "    <ul style=\"line-height:1.8;\">\n",
    "        <li>🟢 <strong>機器翻譯:</strong> mT5 (通用), NLLB (多語言), mBART (低資源)</li>\n",
    "        <li>🔵 <strong>文本摘要:</strong> BERT (抽取式), BART/PEGASUS (生成式), T5 (混合)</li>\n",
    "        <li>🔴 <strong>對話生成:</strong> T5 (任務導向), GPT-3/4 (開放域), Blenderbot (多輪)</li>\n",
    "        <li>🟣 <strong>創意寫作:</strong> GPT-3/4, Claude (通用創作), 專用模型 (詩歌)</li>\n",
    "    </ul>\n",
    "</div>\n",
    "\"\"\"))\n",
    "\n",
    "print(\"\\n🎯 文本生成選型建議:\")\n",
    "print(\"  1. 機器翻譯: mT5 (多語言通用), NLLB (200+ 語言)\")\n",
    "print(\"  2. 文本摘要: PEGASUS (專為摘要設計), BART (通用生成)\")\n",
    "print(\"  3. 對話生成: GPT-3/4 (開放域), DialoGPT (預訓練對話模型)\")\n",
    "print(\"  4. 創意寫作: GPT-4 (最強), Claude (安全性高)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 生成任務方案對比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文本生成方案對比\n",
    "generation_solutions = {\n",
    "    '任務類型': ['機器翻譯', '機器翻譯', '文本摘要', '文本摘要', '對話生成', '對話生成', '創意寫作'],\n",
    "    '推薦方案': ['mT5', 'NLLB', 'BART', 'PEGASUS', 'DialoGPT', 'GPT-3/4', 'GPT-4'],\n",
    "    '模型大小': ['580M-13B', '600M-54B', '400M', '568M', '345M', '175B-1T', '1T+'],\n",
    "    'BLEU/得分': ['35-40', '40-45', '45-50', '48-52', '-', '-', '-'],\n",
    "    '訓練成本': ['中', '高', '中', '中', '中', '極高', '極高'],\n",
    "    '推理速度': ['快', '中', '快', '快', '快', '慢', '慢'],\n",
    "    '特點': ['多語言', '200+語言', '通用生成', '摘要專用', '對話預訓練', '零樣本', '創作能力強']\n",
    "}\n",
    "\n",
    "df_gen = pd.DataFrame(generation_solutions)\n",
    "\n",
    "display(HTML(\"<h3>📋 文本生成方案對比</h3>\"))\n",
    "display(df_gen.style.hide(axis='index').set_properties(**{'text-align': 'center', 'font-size': '10pt'}))\n",
    "\n",
    "# 視覺化模型大小與效能關係\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "model_sizes = [580, 600, 400, 568, 345, 175000, 1000000]  # 單位: 百萬參數\n",
    "performance = [37.5, 42.5, 47.5, 50, 85, 90, 95]  # 模擬綜合效能\n",
    "labels = df_gen['推薦方案'].tolist()\n",
    "\n",
    "scatter = ax.scatter(model_sizes, performance, s=300, alpha=0.6, \n",
    "                     c=range(len(labels)), cmap='viridis', edgecolors='black', linewidth=2)\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    ax.annotate(label, (model_sizes[i], performance[i]), \n",
    "                textcoords=\"offset points\", xytext=(0,10), ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('模型參數量 (百萬)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('綜合效能評分', fontsize=12, fontweight='bold')\n",
    "ax.set_title('文本生成模型: 參數規模 vs 效能', fontsize=15, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n💡 生成任務關鍵洞察:\")\n",
    "print(\"  1. 模型越大,生成質量越高 (但非線性增長)\")\n",
    "print(\"  2. 專用模型 (PEGASUS) 在特定任務上優於通用大模型\")\n",
    "print(\"  3. GPT-3/4 適合零樣本場景,但成本高\")\n",
    "print(\"  4. 中小型模型 (BART/mT5) 微調後性價比高\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "## 5. 問答系統選型決策樹\n",
    "\n",
    "### 5.1 問答系統分類\n",
    "\n",
    "#### 1. 抽取式問答 (Extractive QA)\n",
    "- **任務:** 從給定文本中抽取答案片段\n",
    "- **代表:** SQuAD, CMRC 2018\n",
    "- **推薦:** BERT, RoBERTa, ALBERT\n",
    "\n",
    "#### 2. 生成式問答 (Generative QA)\n",
    "- **任務:** 生成自然語言答案\n",
    "- **代表:** MS MARCO, Natural Questions\n",
    "- **推薦:** T5, BART, GPT-3\n",
    "\n",
    "#### 3. 檢索式問答 (Retrieval-based QA)\n",
    "- **任務:** 從文檔庫檢索相關答案\n",
    "- **代表:** DPR, ColBERT\n",
    "- **推薦:** Dense Retrieval + Reader\n",
    "\n",
    "#### 4. 知識庫問答 (KB-QA)\n",
    "- **任務:** 基於知識圖譜回答問題\n",
    "- **代表:** WebQuestions, ComplexWebQuestions\n",
    "- **推薦:** KBQA 專用模型\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 問答系統選型決策樹\n",
    "qa_tree = '''\n",
    "graph TD\n",
    "    Start[問答系統] --> Q1{QA 類型?}\n",
    "    \n",
    "    Q1 -->|抽取式| Extractive[從文本抽取答案]\n",
    "    Q1 -->|生成式| Generative[生成自然語言答案]\n",
    "    Q1 -->|檢索式| Retrieval[從文檔庫檢索]\n",
    "    Q1 -->|知識庫| KB[基於知識圖譜]\n",
    "    \n",
    "    Extractive --> E1{數據量?}\n",
    "    E1 -->|<5K| E1A[BERT Base<br/>微調]\n",
    "    E1 -->|5K-50K| E1B[RoBERTa<br/>ALBERT]\n",
    "    E1 -->|>50K| E1C[BERT Large<br/>ELECTRA]\n",
    "    \n",
    "    Generative --> G1{場景?}\n",
    "    G1 -->|單文檔| G1A[T5<br/>BART]\n",
    "    G1 -->|多文檔| G1B[FiD<br/>RAG]\n",
    "    G1 -->|開放域| G1C[GPT-3/4<br/>ChatGPT]\n",
    "    \n",
    "    Retrieval --> R1{文檔規模?}\n",
    "    R1 -->|小<10K| R1A[BM25<br/>+ BERT Reader]\n",
    "    R1 -->|中 10K-1M| R1B[DPR<br/>+ FiD]\n",
    "    R1 -->|大>1M| R1C[ColBERT<br/>+ GPT Reader]\n",
    "    \n",
    "    KB --> K1{知識庫?}\n",
    "    K1 -->|結構化| K1A[KBQA<br/>SPARQL生成]\n",
    "    K1 -->|半結構化| K1B[混合檢索<br/>+ 生成]\n",
    "    K1 -->|非結構化| K1C[RAG<br/>GPT-3/4]\n",
    "    \n",
    "    style Start fill:#FFD700,stroke:#333,stroke-width:3px\n",
    "'''\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"border:3px solid #00BCD4; padding:20px; border-radius:15px; background-color:#E0F7FA;\">\n",
    "    <h3 style=\"color:#006064;\">❓ 問答系統選型決策樹</h3>\n",
    "    <p><strong>四大 QA 類型選型:</strong></p>\n",
    "    <ul style=\"line-height:1.8;\">\n",
    "        <li><strong>抽取式 QA:</strong> BERT (基礎), RoBERTa (進階), ELECTRA (高精度)</li>\n",
    "        <li><strong>生成式 QA:</strong> T5 (單文檔), FiD/RAG (多文檔), GPT-3/4 (開放域)</li>\n",
    "        <li><strong>檢索式 QA:</strong> BM25+BERT (小規模), DPR (中規模), ColBERT (大規模)</li>\n",
    "        <li><strong>知識庫 QA:</strong> KBQA (結構化), 混合方法 (半結構化), RAG (非結構化)</li>\n",
    "    </ul>\n",
    "</div>\n",
    "\"\"\"))\n",
    "\n",
    "print(\"\\n🎯 問答系統選型建議:\")\n",
    "print(\"  1. 抽取式 QA (SQuAD類): BERT/RoBERTa 微調 (F1: 88-93%)\")\n",
    "print(\"  2. 生成式 QA: T5 (可控性好), GPT-3 (開放域強)\")\n",
    "print(\"  3. 檢索式 QA: DPR + FiD (平衡方案)\")\n",
    "print(\"  4. 知識庫 QA: RAG (檢索增強生成) + GPT-3/4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 問答系統方案對比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 問答系統方案對比\n",
    "qa_solutions = {\n",
    "    'QA 類型': ['抽取式', '抽取式', '生成式', '生成式', '檢索式', '檢索式', '知識庫'],\n",
    "    '方案': ['BERT-QA', 'RoBERTa-QA', 'T5-QA', 'GPT-3 QA', 'DPR+FiD', 'ColBERT', 'RAG'],\n",
    "    'F1/EM (%)': ['88/81', '91/85', '-', '-', '42 (EM)', '45 (EM)', '-'],\n",
    "    '優勢': ['簡單高效', '高準確率', '靈活生成', '零樣本', '可擴展', '高效檢索', '知識更新快'],\n",
    "    '劣勢': ['需答案在文本', '訓練慢', '需大量數據', '成本高', '兩階段複雜', '計算量大', '檢索質量依賴'],\n",
    "    '適用場景': ['單文檔閱讀理解', '高精度需求', '多樣化答案', '快速原型', '大規模文檔', '億級檢索', '動態知識']\n",
    "}\n",
    "\n",
    "df_qa = pd.DataFrame(qa_solutions)\n",
    "\n",
    "display(HTML(\"<h3>📋 問答系統方案全面對比</h3>\"))\n",
    "display(df_qa.style.hide(axis='index').set_properties(**{'text-align': 'center', 'font-size': '10pt'}))\n",
    "\n",
    "# 視覺化不同 QA 類型的效能\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "qa_types = ['抽取式\\nBERT', '抽取式\\nRoBERTa', '檢索式\\nDPR+FiD', '檢索式\\nColBERT']\n",
    "em_scores = [81, 85, 42, 45]\n",
    "colors_qa = ['skyblue', 'lightgreen', 'coral', 'plum']\n",
    "\n",
    "bars = ax.bar(qa_types, em_scores, color=colors_qa, alpha=0.8, edgecolor='black', linewidth=2)\n",
    "\n",
    "for bar, score in zip(bars, em_scores):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 2,\n",
    "            f'{score}%', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "ax.set_ylabel('Exact Match (%)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('不同 QA 方案效能對比 (SQuAD/Natural Questions)', fontsize=14, fontweight='bold')\n",
    "ax.set_ylim(0, 100)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n💡 QA 系統選型關鍵:\")\n",
    "print(\"  1. 答案在給定文本: 抽取式 QA (BERT/RoBERTa)\")\n",
    "print(\"  2. 需生成完整答案: 生成式 QA (T5/GPT-3)\")\n",
    "print(\"  3. 大規模文檔檢索: 檢索式 QA (DPR+FiD)\")\n",
    "print(\"  4. 知識動態更新: RAG (檢索增強生成)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a>\n",
    "## 6. 模型大小與效能權衡\n",
    "\n",
    "### 6.1 模型規模分類\n",
    "\n",
    "根據參數量,可將模型分為:\n",
    "\n",
    "#### 1. 微型模型 (Tiny: <50M)\n",
    "- **代表:** DistilBERT (66M), TinyBERT (14.5M)\n",
    "- **優勢:** 極快推理,可部署在移動端\n",
    "- **劣勢:** 效能損失 5-10%\n",
    "- **適用:** 邊緣設備,實時應用\n",
    "\n",
    "#### 2. 小型模型 (Small: 50M-200M)\n",
    "- **代表:** BERT-Base (110M), RoBERTa-Base (125M)\n",
    "- **優勢:** 效能與速度平衡\n",
    "- **劣勢:** 複雜任務表現有限\n",
    "- **適用:** 大多數 NLP 任務\n",
    "\n",
    "#### 3. 中型模型 (Medium: 200M-1B)\n",
    "- **代表:** BERT-Large (340M), GPT-2 (1.5B)\n",
    "- **優勢:** 高效能,可處理複雜任務\n",
    "- **劣勢:** 推理較慢,需 GPU\n",
    "- **適用:** 高精度需求\n",
    "\n",
    "#### 4. 大型模型 (Large: 1B-100B)\n",
    "- **代表:** GPT-3 (175B), PaLM (540B)\n",
    "- **優勢:** 零樣本學習,涌現能力\n",
    "- **劣勢:** 極高成本,推理慢\n",
    "- **適用:** 通用智能,研究\n",
    "\n",
    "#### 5. 超大型模型 (XL: >100B)\n",
    "- **代表:** GPT-4 (1T+), PaLM 2 (340B+)\n",
    "- **優勢:** 最強能力,多模態\n",
    "- **劣勢:** 僅 API 可用,成本極高\n",
    "- **適用:** 尖端應用,對話系統\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型規模與效能權衡分析\n",
    "model_scale_data = {\n",
    "    '模型': ['TinyBERT', 'DistilBERT', 'BERT-Base', 'BERT-Large', 'GPT-2', 'GPT-3', 'GPT-4'],\n",
    "    '參數量 (M)': [14.5, 66, 110, 340, 1500, 175000, 1000000],\n",
    "    '相對效能 (%)': [85, 92, 100, 105, 110, 125, 135],\n",
    "    '推理速度 (相對)': [10, 5, 1, 0.3, 0.2, 0.05, 0.02],\n",
    "    '訓練成本 ($)': [100, 500, 2000, 10000, 50000, 10000000, 100000000],\n",
    "    '部署難度': ['極易', '易', '中', '中', '難', '極難', '極難']\n",
    "}\n",
    "\n",
    "df_scale = pd.DataFrame(model_scale_data)\n",
    "\n",
    "# 繪製參數量 vs 效能 vs 速度\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# 左圖: 參數量 vs 效能\n",
    "ax1.plot(df_scale['參數量 (M)'], df_scale['相對效能 (%)'], 'o-', linewidth=2, markersize=10, color='royalblue')\n",
    "for i, model in enumerate(df_scale['模型']):\n",
    "    ax1.annotate(model, (df_scale['參數量 (M)'][i], df_scale['相對效能 (%)'][i]),\n",
    "                textcoords=\"offset points\", xytext=(0,8), ha='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_xlabel('參數量 (百萬)', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('相對效能 (%)', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('模型規模 vs 效能', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.axhline(y=100, color='red', linestyle='--', linewidth=1.5, alpha=0.7, label='BERT-Base 基準')\n",
    "ax1.legend()\n",
    "\n",
    "# 右圖: 參數量 vs 推理速度\n",
    "ax2.plot(df_scale['參數量 (M)'], df_scale['推理速度 (相對)'], 's-', linewidth=2, markersize=10, color='darkorange')\n",
    "for i, model in enumerate(df_scale['模型']):\n",
    "    ax2.annotate(model, (df_scale['參數量 (M)'][i], df_scale['推理速度 (相對)'][i]),\n",
    "                textcoords=\"offset points\", xytext=(0,0.3), ha='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "ax2.set_xscale('log')\n",
    "ax2.set_yscale('log')\n",
    "ax2.set_xlabel('參數量 (百萬)', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('推理速度 (相對,越大越快)', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('模型規模 vs 推理速度', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.axhline(y=1, color='red', linestyle='--', linewidth=1.5, alpha=0.7, label='BERT-Base 基準')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n📊 模型規模權衡洞察:\")\n",
    "print(\"  1. 效能增長: 參數量增加 → 效能提升 (邊際遞減)\")\n",
    "print(\"  2. 速度下降: 參數量增加 → 推理速度大幅下降\")\n",
    "print(\"  3. 成本飆升: GPT-3 訓練成本 > $10M, GPT-4 > $100M\")\n",
    "print(\"  4. 甜蜜點: BERT-Base/DistilBERT (效能與速度平衡)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 模型壓縮技術\n",
    "\n",
    "當需要在資源受限環境部署大模型時,可使用模型壓縮技術:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型壓縮技術對比\n",
    "compression_methods = {\n",
    "    '壓縮技術': ['知識蒸餾\\n(Distillation)', '量化\\n(Quantization)', '剪枝\\n(Pruning)', \n",
    "                '低秩分解\\n(Factorization)', '混合精度\\n(Mixed Precision)'],\n",
    "    '原理': ['大模型教小模型', '降低數值精度', '移除冗餘參數', '矩陣分解降維', 'FP16/INT8 訓練'],\n",
    "    '壓縮率': ['2-4x', '2-8x', '2-10x', '1.5-3x', '2x'],\n",
    "    '效能損失 (%)': ['2-5', '1-3', '3-8', '2-4', '0.5-2'],\n",
    "    '速度提升': ['2-3x', '2-4x', '2-5x', '1.5-2x', '2x'],\n",
    "    '代表方法': ['DistilBERT', 'BERT-INT8', 'Lottery Ticket', 'ALBERT', 'NVIDIA Apex']\n",
    "}\n",
    "\n",
    "df_compress = pd.DataFrame(compression_methods)\n",
    "\n",
    "display(HTML(\"<h3>🔧 模型壓縮技術對比</h3>\"))\n",
    "display(df_compress.style.hide(axis='index').set_properties(**{'text-align': 'center', 'font-size': '10pt'}))\n",
    "\n",
    "# 視覺化壓縮率 vs 效能損失\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "compression_rates = [3, 5, 6, 2, 2]\n",
    "performance_loss = [3.5, 2, 5.5, 3, 1.25]\n",
    "methods_short = ['蒸餾', '量化', '剪枝', '低秩分解', '混合精度']\n",
    "\n",
    "scatter = ax.scatter(compression_rates, performance_loss, s=400, alpha=0.6,\n",
    "                     c=range(len(methods_short)), cmap='Set2', edgecolors='black', linewidth=2)\n",
    "\n",
    "for i, method in enumerate(methods_short):\n",
    "    ax.annotate(method, (compression_rates[i], performance_loss[i]),\n",
    "                textcoords=\"offset points\", xytext=(0,-15), ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "ax.set_xlabel('壓縮率 (倍數)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('效能損失 (%)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('模型壓縮技術: 壓縮率 vs 效能損失', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 理想區域 (高壓縮+低損失)\n",
    "ax.axhline(y=3, color='green', linestyle='--', linewidth=1.5, alpha=0.5, label='可接受效能損失線')\n",
    "ax.axvline(x=4, color='blue', linestyle='--', linewidth=1.5, alpha=0.5, label='理想壓縮率線')\n",
    "ax.legend(fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n💡 壓縮技術選擇建議:\")\n",
    "print(\"  1. 追求極致速度: 量化 (INT8) + 剪枝 (最高 10x 壓縮)\")\n",
    "print(\"  2. 平衡方案: 知識蒸餾 (DistilBERT, 僅損失 3% 效能)\")\n",
    "print(\"  3. 最小損失: 混合精度訓練 (FP16, 損失 <2%)\")\n",
    "print(\"  4. 組合使用: 蒸餾 + 量化 + 剪枝 (可達 20x+ 壓縮)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"7\"></a>\n",
    "## 7. 實際案例選型分析\n",
    "\n",
    "### 7.1 案例 1: 電商評論情感分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 案例 1: 電商評論情感分析\n",
    "case1 = {\n",
    "    '需求描述': '分析用戶評論情感 (正面/負面/中性)',\n",
    "    '數據量': '50,000 條標註評論',\n",
    "    '計算資源': '單塊 GPU (V100)',\n",
    "    '延遲要求': '推理 < 100ms',\n",
    "    '準確率要求': '> 90%'\n",
    "}\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"border:2px solid #4CAF50; padding:15px; border-radius:10px; background-color:#F1F8E9; margin:10px 0;\">\n",
    "    <h4 style=\"color:#2E7D32;\">📦 案例 1: 電商評論情感分析</h4>\n",
    "    <p><strong>需求:</strong> 分析用戶評論情感 (正面/負面/中性)</p>\n",
    "    <p><strong>數據量:</strong> 50,000 條標註評論</p>\n",
    "    <p><strong>計算資源:</strong> 單塊 GPU (V100)</p>\n",
    "    <p><strong>延遲要求:</strong> 推理 < 100ms</p>\n",
    "    <p><strong>準確率要求:</strong> > 90%</p>\n",
    "</div>\n",
    "\"\"\"))\n",
    "\n",
    "print(\"\\n🔍 決策流程:\")\n",
    "print(\"  步驟 1: 任務類型 → 文本分類 (三分類)\")\n",
    "print(\"  步驟 2: 數據量 50K → 充足數據\")\n",
    "print(\"  步驟 3: 單 GPU → 可使用 BERT Base/Large\")\n",
    "print(\"  步驟 4: 延遲 <100ms → 需要模型優化\")\n",
    "print(\"\\n✅ 推薦方案:\")\n",
    "print(\"  【方案 A】 BERT-Base 微調 + 量化 (INT8)\")\n",
    "print(\"    - 訓練: 2-3 小時\")\n",
    "print(\"    - 準確率: 92-94%\")\n",
    "print(\"    - 推理: 30-50ms (量化後)\")\n",
    "print(\"  【方案 B】 DistilBERT 微調\")\n",
    "print(\"    - 訓練: 1-2 小時\")\n",
    "print(\"    - 準確率: 90-92%\")\n",
    "print(\"    - 推理: 15-30ms\")\n",
    "print(\"\\n💡 最終選擇: DistilBERT (速度優先,準確率滿足要求)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 案例 2: 醫療文本命名實體識別"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 案例 2: 醫療文本 NER\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"border:2px solid #2196F3; padding:15px; border-radius:10px; background-color:#E3F2FD; margin:10px 0;\">\n",
    "    <h4 style=\"color:#1976D2;\">🏥 案例 2: 醫療文本命名實體識別</h4>\n",
    "    <p><strong>需求:</strong> 從醫療記錄中識別疾病、藥物、症狀</p>\n",
    "    <p><strong>數據量:</strong> 3,000 條標註文本 (專業領域)</p>\n",
    "    <p><strong>計算資源:</strong> 多 GPU (4x A100)</p>\n",
    "    <p><strong>準確率要求:</strong> F1 > 85% (專業領域高標準)</p>\n",
    "</div>\n",
    "\"\"\"))\n",
    "\n",
    "print(\"\\n🔍 決策流程:\")\n",
    "print(\"  步驟 1: 任務類型 → NER (序列標註)\")\n",
    "print(\"  步驟 2: 領域 → 醫療專業領域\")\n",
    "print(\"  步驟 3: 數據量 3K → 少量領域數據\")\n",
    "print(\"  步驟 4: 多 GPU → 可訓練大模型\")\n",
    "print(\"\\n✅ 推薦方案:\")\n",
    "print(\"  【方案 A】 BioBERT 微調\")\n",
    "print(\"    - 預訓練: 生物醫學文獻 (PubMed, PMC)\")\n",
    "print(\"    - 訓練: 2-4 小時\")\n",
    "print(\"    - F1-Score: 87-91% (領域專精)\")\n",
    "print(\"  【方案 B】 SciBERT 微調\")\n",
    "print(\"    - 預訓練: 科學文獻\")\n",
    "print(\"    - F1-Score: 85-89%\")\n",
    "print(\"  【方案 C】 數據增強 + BERT-Base\")\n",
    "print(\"    - 使用遠程監督擴充數據至 10K\")\n",
    "print(\"    - F1-Score: 82-86%\")\n",
    "print(\"\\n💡 最終選擇: BioBERT (領域預訓練優勢明顯)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 案例 3: 智能客服對話系統"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 案例 3: 智能客服\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"border:2px solid #FF9800; padding:15px; border-radius:10px; background-color:#FFF3E0; margin:10px 0;\">\n",
    "    <h4 style=\"color:#E65100;\">💬 案例 3: 智能客服對話系統</h4>\n",
    "    <p><strong>需求:</strong> 7x24小時自動回答客戶問題</p>\n",
    "    <p><strong>對話數據:</strong> 10,000 條歷史對話 (FAQ 1,000 條)</p>\n",
    "    <p><strong>計算資源:</strong> 成本敏感,優先 CPU 部署</p>\n",
    "    <p><strong>要求:</strong> 快速響應 (<1s), 準確率 >80%</p>\n",
    "</div>\n",
    "\"\"\"))\n",
    "\n",
    "print(\"\\n🔍 決策流程:\")\n",
    "print(\"  步驟 1: 任務類型 → 對話生成 (檢索式 + 生成式)\")\n",
    "print(\"  步驟 2: 場景 → 任務導向對話 (FAQ)\")\n",
    "print(\"  步驟 3: 資源 → CPU 部署,成本敏感\")\n",
    "print(\"  步驟 4: 延遲 <1s → 必須優化\")\n",
    "print(\"\\n✅ 推薦方案:\")\n",
    "print(\"  【階段 1】 FAQ 檢索 (BM25 + BERT Reranking)\")\n",
    "print(\"    - BM25 快速召回 Top-20\")\n",
    "print(\"    - DistilBERT 重排序 Top-3\")\n",
    "print(\"    - 響應時間: 200-500ms\")\n",
    "print(\"    - 準確率: 85-90% (FAQ 覆蓋場景)\")\n",
    "print(\"  【階段 2】 生成式回答 (DialoGPT 微調)\")\n",
    "print(\"    - 處理 FAQ 外問題\")\n",
    "print(\"    - 響應時間: 1-2s\")\n",
    "print(\"    - 準確率: 70-80%\")\n",
    "print(\"  【架構】 混合式: FAQ 檢索 (主) + 生成式 (輔)\")\n",
    "print(\"\\n💡 最終選擇: BM25 + DistilBERT + DialoGPT 混合架構\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"8\"></a>\n",
    "## 8. 決策樹實戰工具\n",
    "\n",
    "### 8.1 互動式技術選型工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 互動式技術選型決策工具\n",
    "def recommend_solution(task_type, data_size, compute_resource, latency_req):\n",
    "    \"\"\"\n",
    "    基於輸入參數推薦技術方案\n",
    "    \n",
    "    Args:\n",
    "        task_type: 任務類型 ('classification', 'ner', 'generation', 'qa')\n",
    "        data_size: 數據量 ('tiny', 'small', 'medium', 'large')\n",
    "        compute_resource: 計算資源 ('cpu', 'single_gpu', 'multi_gpu')\n",
    "        latency_req: 延遲要求 ('realtime', 'near_realtime', 'batch')\n",
    "    \n",
    "    Returns:\n",
    "        推薦方案字典\n",
    "    \"\"\"\n",
    "    recommendations = []\n",
    "    \n",
    "    # 文本分類決策樹\n",
    "    if task_type == 'classification':\n",
    "        if data_size == 'tiny':\n",
    "            recommendations.append({\n",
    "                'solution': 'GPT-3/4 零樣本學習',\n",
    "                'reason': '數據極少,使用大模型零樣本',\n",
    "                'expected_acc': '80-90%',\n",
    "                'cost': '高 (API 調用)'\n",
    "            })\n",
    "        elif data_size == 'small':\n",
    "            if latency_req == 'realtime':\n",
    "                recommendations.append({\n",
    "                    'solution': 'DistilBERT 微調',\n",
    "                    'reason': '數據少,延遲要求高',\n",
    "                    'expected_acc': '88-92%',\n",
    "                    'cost': '低'\n",
    "                })\n",
    "            else:\n",
    "                recommendations.append({\n",
    "                    'solution': 'BERT-Base 微調',\n",
    "                    'reason': '平衡方案,性價比高',\n",
    "                    'expected_acc': '92-95%',\n",
    "                    'cost': '中'\n",
    "                })\n",
    "        elif data_size == 'medium':\n",
    "            if compute_resource == 'cpu':\n",
    "                recommendations.append({\n",
    "                    'solution': 'TF-IDF + Logistic Regression',\n",
    "                    'reason': 'CPU 友好,數據充足',\n",
    "                    'expected_acc': '85-90%',\n",
    "                    'cost': '極低'\n",
    "                })\n",
    "            else:\n",
    "                recommendations.append({\n",
    "                    'solution': 'BERT-Base 微調',\n",
    "                    'reason': '數據充足,可訓練高質量模型',\n",
    "                    'expected_acc': '93-96%',\n",
    "                    'cost': '中'\n",
    "                })\n",
    "        else:  # large\n",
    "            if compute_resource == 'multi_gpu':\n",
    "                recommendations.append({\n",
    "                    'solution': 'BERT-Large 微調',\n",
    "                    'reason': '數據與資源充足,追求極致效能',\n",
    "                    'expected_acc': '95-97%',\n",
    "                    'cost': '高'\n",
    "                })\n",
    "            else:\n",
    "                recommendations.append({\n",
    "                    'solution': 'RoBERTa-Base 微調',\n",
    "                    'reason': '單 GPU 最優選擇',\n",
    "                    'expected_acc': '94-96%',\n",
    "                    'cost': '中'\n",
    "                })\n",
    "    \n",
    "    # NER 決策樹\n",
    "    elif task_type == 'ner':\n",
    "        if data_size in ['tiny', 'small']:\n",
    "            recommendations.append({\n",
    "                'solution': 'BERT-NER 微調 + 數據增強',\n",
    "                'reason': 'NER 需要較多數據,建議數據增強',\n",
    "                'expected_f1': '88-91%',\n",
    "                'cost': '中'\n",
    "            })\n",
    "        else:\n",
    "            recommendations.append({\n",
    "                'solution': 'SpanBERT 微調',\n",
    "                'reason': '數據充足,SpanBERT 邊界識別更準確',\n",
    "                'expected_f1': '91-94%',\n",
    "                'cost': '中高'\n",
    "            })\n",
    "    \n",
    "    # 文本生成決策樹\n",
    "    elif task_type == 'generation':\n",
    "        if data_size == 'tiny':\n",
    "            recommendations.append({\n",
    "                'solution': 'GPT-3/4 Prompt Engineering',\n",
    "                'reason': '數據少,使用大模型零樣本生成',\n",
    "                'expected_quality': '高',\n",
    "                'cost': '高 (API)'\n",
    "            })\n",
    "        elif compute_resource == 'multi_gpu':\n",
    "            recommendations.append({\n",
    "                'solution': 'T5-Large 微調',\n",
    "                'reason': '資源充足,T5 生成質量高',\n",
    "                'expected_quality': '高',\n",
    "                'cost': '高'\n",
    "            })\n",
    "        else:\n",
    "            recommendations.append({\n",
    "                'solution': 'BART-Base 微調',\n",
    "                'reason': '平衡方案,生成質量好',\n",
    "                'expected_quality': '中高',\n",
    "                'cost': '中'\n",
    "            })\n",
    "    \n",
    "    # 問答系統決策樹\n",
    "    elif task_type == 'qa':\n",
    "        if data_size in ['tiny', 'small']:\n",
    "            recommendations.append({\n",
    "                'solution': 'BERT-QA 微調',\n",
    "                'reason': '抽取式 QA,BERT 表現優秀',\n",
    "                'expected_em': '85-90%',\n",
    "                'cost': '中'\n",
    "            })\n",
    "        else:\n",
    "            recommendations.append({\n",
    "                'solution': 'RoBERTa-QA 微調',\n",
    "                'reason': '數據充足,RoBERTa 效能更好',\n",
    "                'expected_em': '88-93%',\n",
    "                'cost': '中高'\n",
    "            })\n",
    "    \n",
    "    # 如果有延遲要求,額外推薦優化方案\n",
    "    if latency_req == 'realtime' and recommendations:\n",
    "        recommendations.append({\n",
    "            'solution': f\"{recommendations[0]['solution']} + 量化 (INT8)\",\n",
    "            'reason': '實時要求高,建議模型量化',\n",
    "            'speedup': '2-4x',\n",
    "            'cost': '低 (優化成本)'\n",
    "        })\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# 測試案例\n",
    "print(\"🔧 互動式技術選型工具\\n\")\n",
    "\n",
    "test_cases = [\n",
    "    {\n",
    "        'name': '案例 1: 情感分析',\n",
    "        'params': ('classification', 'medium', 'single_gpu', 'near_realtime')\n",
    "    },\n",
    "    {\n",
    "        'name': '案例 2: 醫療 NER',\n",
    "        'params': ('ner', 'small', 'multi_gpu', 'batch')\n",
    "    },\n",
    "    {\n",
    "        'name': '案例 3: 文本摘要',\n",
    "        'params': ('generation', 'medium', 'single_gpu', 'batch')\n",
    "    }\n",
    "]\n",
    "\n",
    "for case in test_cases:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"📋 {case['name']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    recommendations = recommend_solution(*case['params'])\n",
    "    for i, rec in enumerate(recommendations, 1):\n",
    "        print(f\"\\n推薦方案 {i}:\")\n",
    "        for key, value in rec.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\n\\n💡 使用方式:\")\n",
    "print(\"  可根據實際需求調用 recommend_solution() 函數獲得推薦方案\")\n",
    "print(\"  參數說明:\")\n",
    "print(\"    - task_type: 'classification', 'ner', 'generation', 'qa'\")\n",
    "print(\"    - data_size: 'tiny'(<1K), 'small'(1K-10K), 'medium'(10K-100K), 'large'(>100K)\")\n",
    "print(\"    - compute_resource: 'cpu', 'single_gpu', 'multi_gpu'\")\n",
    "print(\"    - latency_req: 'realtime'(<100ms), 'near_realtime'(<1s), 'batch'(>1s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 📚 本課總結\n",
    "\n",
    "### 核心要點回顧:\n",
    "\n",
    "1. **技術選型六大維度:**\n",
    "   - 任務類型、數據規模、計算資源\n",
    "   - 效能要求、延遲限制、可解釋性\n",
    "\n",
    "2. **四大任務決策樹:**\n",
    "   - **文本分類:** BERT Base (通用), DistilBERT (速度), GPT-3 (零樣本)\n",
    "   - **NER:** BERT-NER (通用), BioBERT (醫療), SpanBERT (高精度)\n",
    "   - **文本生成:** T5 (通用), BART (摘要), GPT-3/4 (開放域)\n",
    "   - **問答系統:** BERT-QA (抽取式), DPR+FiD (檢索式), RAG (知識庫)\n",
    "\n",
    "3. **模型規模權衡:**\n",
    "   - 參數量增加 → 效能提升 (邊際遞減)\n",
    "   - 推理速度下降,成本飆升\n",
    "   - BERT-Base/DistilBERT 是甜蜜點\n",
    "\n",
    "4. **模型壓縮技術:**\n",
    "   - 知識蒸餾、量化、剪枝、低秩分解\n",
    "   - 可達 2-10x 壓縮,損失 <5% 效能\n",
    "\n",
    "5. **實戰案例:**\n",
    "   - 電商情感分析: DistilBERT\n",
    "   - 醫療 NER: BioBERT\n",
    "   - 智能客服: BM25 + DistilBERT + DialoGPT\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 下節預告\n",
    "\n",
    "**CH09-03: 進階學習路徑**\n",
    "\n",
    "我們將探討:\n",
    "- NLP 工程師技能樹\n",
    "- 推薦書籍、課程、論文清單\n",
    "- 開源專案推薦\n",
    "- 從初學者到專家的學習路線圖\n",
    "\n",
    "---\n",
    "\n",
    "## 📖 延伸閱讀\n",
    "\n",
    "1. **模型對比:**\n",
    "   - [Papers with Code - NLP Benchmarks](https://paperswithcode.com/area/natural-language-processing)\n",
    "   - [Hugging Face Model Hub](https://huggingface.co/models)\n",
    "\n",
    "2. **模型壓縮:**\n",
    "   - [DistilBERT Paper](https://arxiv.org/abs/1910.01108)\n",
    "   - [Model Compression Survey](https://arxiv.org/abs/2006.04884)\n",
    "\n",
    "3. **技術選型指南:**\n",
    "   - [Google ML Practitioners Guide](https://developers.google.com/machine-learning/guides)\n",
    "   - [AWS ML Best Practices](https://aws.amazon.com/machine-learning/)\n",
    "\n",
    "---\n",
    "\n",
    "### 🙋 問題討論\n",
    "\n",
    "有任何問題嗎?歡迎在討論區提問!\n",
    "\n",
    "---\n",
    "\n",
    "**課程資訊:**\n",
    "- **作者:** iSpan NLP Team\n",
    "- **版本:** v1.0\n",
    "- **最後更新:** 2025-10-17\n",
    "- **授權:** MIT License (僅供教學使用)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
