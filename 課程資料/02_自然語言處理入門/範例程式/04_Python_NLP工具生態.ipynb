{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH02-04: Python NLP å·¥å…·ç”Ÿæ…‹å®Œå…¨æŒ‡å—\n",
    "\n",
    "**èª²ç¨‹ç›®æ¨™:**\n",
    "- æŒæ¡ Python NLP ä¸»æµå·¥å…·çš„ç‰¹é»èˆ‡é¸æ“‡é‚è¼¯\n",
    "- ç†è§£ NLTK vs spaCy vs Transformers çš„å®šä½èˆ‡å·®ç•°\n",
    "- å­¸æœƒæ ¹æ“šæ‡‰ç”¨å ´æ™¯é¸æ“‡åˆé©å·¥å…·\n",
    "- äº†è§£ä¸­è‹±æ–‡ NLP å·¥å…·çš„å·®ç•°èˆ‡æœ€ä½³å¯¦è¸\n",
    "- å¯¦ä½œæ•ˆèƒ½åŸºæº–æ¸¬è©¦èˆ‡å·¥å…·å°æ¯”\n",
    "\n",
    "**å­¸ç¿’æ™‚é–“:** ç´„ 120-150 åˆ†é˜\n",
    "\n",
    "**å‰ç½®çŸ¥è­˜:**\n",
    "- Python åŸºç¤èªæ³•\n",
    "- åŸºæœ¬ NLP æ¦‚å¿µ (åˆ†è©ã€è©æ€§æ¨™è¨»)\n",
    "- æ©Ÿå™¨å­¸ç¿’åŸºç¤\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š ç›®éŒ„\n",
    "\n",
    "1. [Python NLP ç”Ÿæ…‹ç³»çµ±å…¨æ™¯åœ–](#1)\n",
    "2. [NLTK: æ•™å­¸å°å‘å·¥å…·åŒ…](#2)\n",
    "3. [spaCy: å·¥æ¥­ç´š NLP æ¡†æ¶](#3)\n",
    "4. [Transformers: é è¨“ç·´æ¨¡å‹åº«](#4)\n",
    "5. [ä¸­æ–‡ NLP å·¥å…·å°æ¯”](#5)\n",
    "6. [å·¥å…·æ•ˆèƒ½åŸºæº–æ¸¬è©¦](#6)\n",
    "7. [é¸æ“‡æ±ºç­–æ¨¹èˆ‡æœ€ä½³å¯¦è¸](#7)\n",
    "8. [å®Œæ•´æŠ€è¡“æ£§ç¯„ä¾‹](#8)\n",
    "9. [æœ¬èª²ç¸½çµèˆ‡å»¶ä¼¸é–±è®€](#9)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç’°å¢ƒè¨­å®šèˆ‡å¥—ä»¶å°å…¥\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# è¨­å®šä¸­æ–‡é¡¯ç¤º\n",
    "plt.rcParams['font.sans-serif'] = ['Microsoft JhengHei', 'SimHei', 'Arial Unicode MS']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# è¨­å®šé¡¯ç¤ºé¢¨æ ¼\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"âœ… ç’°å¢ƒè¨­å®šå®Œæˆ\")\n",
    "print(f\"NumPy ç‰ˆæœ¬: {np.__version__}\")\n",
    "print(f\"Pandas ç‰ˆæœ¬: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "## 1. Python NLP ç”Ÿæ…‹ç³»çµ±å…¨æ™¯åœ–\n",
    "\n",
    "### 1.1 å·¥å…·åˆ†é¡èˆ‡å®šä½\n",
    "\n",
    "Python NLP å·¥å…·å¯åˆ†ç‚ºä»¥ä¸‹å››å¤§é¡:\n",
    "\n",
    "#### 1. å‚³çµ± NLP å·¥å…·\n",
    "- **NLTK** (Natural Language Toolkit): æ•™å­¸å°å‘,åŠŸèƒ½å…¨é¢\n",
    "- **spaCy**: å·¥æ¥­ç´š,é«˜æ•ˆèƒ½,ç”Ÿç”¢éƒ¨ç½²\n",
    "\n",
    "#### 2. æ·±åº¦å­¸ç¿’æ¡†æ¶\n",
    "- **Transformers** (Hugging Face): é è¨“ç·´æ¨¡å‹åº«\n",
    "- **fairseq** (Meta): åºåˆ—å»ºæ¨¡æ¡†æ¶\n",
    "- **AllenNLP**: ç ”ç©¶å°å‘æ·±åº¦å­¸ç¿’æ¡†æ¶\n",
    "\n",
    "#### 3. ä¸­æ–‡å°ˆç”¨å·¥å…·\n",
    "- **jieba** (çµå·´): ä¸­æ–‡åˆ†è©\n",
    "- **LTP** (Language Technology Platform): ä¸­æ–‡å…¨æµç¨‹\n",
    "- **HanLP**: å¤šèªè¨€ NLP å·¥å…·åŒ…\n",
    "- **pkuseg**: åŒ—å¤§åˆ†è©å·¥å…·\n",
    "\n",
    "#### 4. è¼”åŠ©å·¥å…·\n",
    "- **Gensim**: è©å‘é‡èˆ‡ä¸»é¡Œæ¨¡å‹\n",
    "- **TextBlob**: ç°¡åŒ– API,å¿«é€ŸåŸå‹\n",
    "- **polyglot**: å¤šèªè¨€ NLP\n",
    "\n",
    "---\n",
    "\n",
    "### 1.2 å·¥å…·é¸æ“‡æ±ºç­–çŸ©é™£\n",
    "\n",
    "| å·¥å…· | èªè¨€æ”¯æ´ | é€Ÿåº¦ | æº–ç¢ºç‡ | å­¸ç¿’æ›²ç·š | é©ç”¨å ´æ™¯ |\n",
    "|:---|:---|:---|:---|:---|:---|\n",
    "| **NLTK** | è‹±æ–‡ä¸» | â˜…â˜…â˜†â˜†â˜† | â˜…â˜…â˜…â˜†â˜† | â˜…â˜†â˜†â˜†â˜† | æ•™å­¸ã€ç ”ç©¶ã€åŸå‹ |\n",
    "| **spaCy** | 20+ èªè¨€ | â˜…â˜…â˜…â˜…â˜… | â˜…â˜…â˜…â˜…â˜† | â˜…â˜…â˜†â˜†â˜† | ç”Ÿç”¢ç’°å¢ƒã€å¤§è¦æ¨¡ |\n",
    "| **Transformers** | 100+ èªè¨€ | â˜…â˜…â˜…â˜†â˜† | â˜…â˜…â˜…â˜…â˜… | â˜…â˜…â˜…â˜†â˜† | SOTA æ¨¡å‹ã€å¾®èª¿ |\n",
    "| **jieba** | ä¸­æ–‡ | â˜…â˜…â˜…â˜…â˜† | â˜…â˜…â˜…â˜†â˜† | â˜…â˜†â˜†â˜†â˜† | ä¸­æ–‡åˆ†è© |\n",
    "| **LTP** | ä¸­æ–‡ | â˜…â˜…â˜…â˜†â˜† | â˜…â˜…â˜…â˜…â˜† | â˜…â˜…â˜…â˜†â˜† | ä¸­æ–‡å…¨æµç¨‹ |\n",
    "| **Gensim** | é€šç”¨ | â˜…â˜…â˜…â˜†â˜† | â˜…â˜…â˜…â˜†â˜† | â˜…â˜…â˜†â˜†â˜† | è©å‘é‡ã€ä¸»é¡Œæ¨¡å‹ |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¦–è¦ºåŒ–: Python NLP å·¥å…·ç”Ÿæ…‹ç³»çµ±\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 10))\n",
    "ax.set_xlim(0, 10)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.axis('off')\n",
    "\n",
    "# æ¨™é¡Œ\n",
    "ax.text(5, 9.5, 'Python NLP å·¥å…·ç”Ÿæ…‹ç³»çµ±', ha='center', \n",
    "        fontsize=20, fontweight='bold')\n",
    "\n",
    "# å››å¤§é¡å·¥å…·å€å¡Š\n",
    "categories = [\n",
    "    (1.5, 7, 2, 1.5, 'å‚³çµ± NLP\\nå·¥å…·', '#F18F01', ['NLTK', 'spaCy']),\n",
    "    (6.5, 7, 2, 1.5, 'æ·±åº¦å­¸ç¿’\\næ¡†æ¶', '#2E86AB', ['Transformers', 'fairseq', 'AllenNLP']),\n",
    "    (1.5, 4, 2, 1.5, 'ä¸­æ–‡å°ˆç”¨\\nå·¥å…·', '#95E1D3', ['jieba', 'LTP', 'HanLP', 'pkuseg']),\n",
    "    (6.5, 4, 2, 1.5, 'è¼”åŠ©å·¥å…·', '#FFE66D', ['Gensim', 'TextBlob', 'polyglot'])\n",
    "]\n",
    "\n",
    "for x, y, w, h, title, color, tools in categories:\n",
    "    # ç¹ªè£½é¡åˆ¥æ–¹æ¡†\n",
    "    rect = mpatches.FancyBboxPatch((x-w/2, y-h/2), w, h,\n",
    "                                    boxstyle=\"round,pad=0.1\",\n",
    "                                    linewidth=2, edgecolor='black',\n",
    "                                    facecolor=color, alpha=0.3)\n",
    "    ax.add_patch(rect)\n",
    "    \n",
    "    # æ¨™é¡Œ\n",
    "    ax.text(x, y+h/2-0.2, title, ha='center', va='top',\n",
    "            fontsize=13, fontweight='bold')\n",
    "    \n",
    "    # å·¥å…·åˆ—è¡¨\n",
    "    for i, tool in enumerate(tools):\n",
    "        ax.text(x, y+h/2-0.5-i*0.25, f'â€¢ {tool}', ha='center', va='top',\n",
    "                fontsize=10)\n",
    "\n",
    "# æ‡‰ç”¨å±¤\n",
    "app_rect = mpatches.FancyBboxPatch((2, 1), 6, 1,\n",
    "                                    boxstyle=\"round,pad=0.1\",\n",
    "                                    linewidth=3, edgecolor='#C73E1D',\n",
    "                                    facecolor='#C73E1D', alpha=0.4)\n",
    "ax.add_patch(app_rect)\n",
    "ax.text(5, 1.5, 'æ‡‰ç”¨å±¤: æœå°‹å¼•æ“ã€å°è©±ç³»çµ±ã€æ©Ÿå™¨ç¿»è­¯ã€æ–‡æœ¬åˆ†æ...', \n",
    "        ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "# é€£æ¥ç·š\n",
    "for x_pos in [2.5, 7.5]:\n",
    "    ax.arrow(x_pos, 6.2, 0, -0.6, head_width=0.15, head_length=0.1, \n",
    "             fc='gray', ec='gray', alpha=0.5)\n",
    "    ax.arrow(x_pos, 3.2, 0, -1.1, head_width=0.15, head_length=0.1,\n",
    "             fc='gray', ec='gray', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ’¡ ç”Ÿæ…‹ç³»çµ±è§€å¯Ÿ:\")\n",
    "print(\"  1. å‚³çµ±å·¥å…· (NLTK/spaCy): åŸºç¤ NLP ä»»å‹™ (åˆ†è©ã€è©æ€§ã€NER)\")\n",
    "print(\"  2. æ·±åº¦å­¸ç¿’æ¡†æ¶ (Transformers): SOTA æ¨¡å‹ã€é è¨“ç·´ã€å¾®èª¿\")\n",
    "print(\"  3. ä¸­æ–‡å°ˆç”¨å·¥å…· (jieba/LTP): è§£æ±ºä¸­æ–‡ç‰¹æœ‰å•é¡Œ (åˆ†è©ã€ç°¡ç¹è½‰æ›)\")\n",
    "print(\"  4. è¼”åŠ©å·¥å…· (Gensim): è©å‘é‡ã€ä¸»é¡Œæ¨¡å‹ç­‰ç‰¹å®šä»»å‹™\")\n",
    "print(\"  5. æ‰€æœ‰å·¥å…·æœ€çµ‚æœå‹™æ–¼æ‡‰ç”¨å±¤\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "## 2. NLTK: æ•™å­¸å°å‘å·¥å…·åŒ…\n",
    "\n",
    "### 2.1 NLTK æ ¸å¿ƒç‰¹é»\n",
    "\n",
    "**å®šä½:** æ•™å­¸å°å‘çš„ NLP å·¥å…·åŒ…,é©åˆå­¸ç¿’èˆ‡ç ”ç©¶\n",
    "\n",
    "**å„ªå‹¢:**\n",
    "- âœ… åŠŸèƒ½å…¨é¢: æ¶µè“‹æ‰€æœ‰ NLP åŸºç¤ä»»å‹™\n",
    "- âœ… æ¨¡çµ„åŒ–è¨­è¨ˆ: æ¯å€‹åŠŸèƒ½ç¨ç«‹,ä¾¿æ–¼ç†è§£\n",
    "- âœ… è±å¯Œèªæ–™åº«: å…§å»ºå¤šç¨®èªè¨€è³‡æº\n",
    "- âœ… æ–‡æª”å®Œå–„: é…å¥—æ›¸ç± *NLTK Book*\n",
    "- âœ… ç¤¾ç¾¤æ´»èº: æŒçºŒæ›´æ–° 20+ å¹´\n",
    "\n",
    "**åŠ£å‹¢:**\n",
    "- âŒ é€Ÿåº¦æ…¢: ç´” Python å¯¦ä½œ,ç„¡å„ªåŒ–\n",
    "- âŒ ä¸é©åˆç”Ÿç”¢: ç„¡ä¸¦è¡Œè™•ç†,è¨˜æ†¶é«”å ç”¨é«˜\n",
    "- âŒ ä¸»è¦æ”¯æ´è‹±æ–‡: ä¸­æ–‡ç­‰èªè¨€æ”¯æ´æœ‰é™\n",
    "\n",
    "---\n",
    "\n",
    "### 2.2 NLTK æ ¸å¿ƒåŠŸèƒ½å¯¦ä½œ\n",
    "\n",
    "#### åŠŸèƒ½æ¸…å–®:\n",
    "1. æ–‡æœ¬åˆ†è© (Tokenization)\n",
    "2. åœç”¨è©éæ¿¾ (Stop Words)\n",
    "3. è©å¹¹æå– (Stemming)\n",
    "4. è©å½¢é‚„åŸ (Lemmatization)\n",
    "5. è©æ€§æ¨™è¨» (POS Tagging)\n",
    "6. å¥æ³•åˆ†æ (Parsing)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK åŸºç¤åŠŸèƒ½ç¤ºç¯„\n",
    "print(\"=\" * 70)\n",
    "print(\"NLTK æ ¸å¿ƒåŠŸèƒ½å±•ç¤º\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "try:\n",
    "    import nltk\n",
    "    from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "    \n",
    "    # ä¸‹è¼‰å¿…è¦è³‡æº (é¦–æ¬¡ä½¿ç”¨,å¯èƒ½éœ€è¦å¹¾åˆ†é˜)\n",
    "    print(\"\\nğŸ“¥ æª¢æŸ¥ NLTK è³‡æº...\")\n",
    "    resources = ['punkt', 'stopwords', 'wordnet', 'averaged_perceptron_tagger', 'omw-1.4']\n",
    "    for resource in resources:\n",
    "        try:\n",
    "            nltk.data.find(f'tokenizers/{resource}' if resource == 'punkt' else f'corpora/{resource}')\n",
    "            print(f\"  âœ… {resource} å·²å­˜åœ¨\")\n",
    "        except LookupError:\n",
    "            print(f\"  â¬‡ï¸ ä¸‹è¼‰ {resource}...\")\n",
    "            nltk.download(resource, quiet=True)\n",
    "    \n",
    "    # æ¸¬è©¦æ–‡æœ¬\n",
    "    text = \"\"\"\n",
    "    Natural language processing (NLP) is a subfield of linguistics, \n",
    "    computer science, and artificial intelligence. It is concerned with \n",
    "    the interactions between computers and human languages. NLP enables \n",
    "    computers to read, understand, and derive meaning from human languages.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"åŸå§‹æ–‡æœ¬:\")\n",
    "    print(text.strip())\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # 1. å¥å­åˆ†å‰²\n",
    "    print(\"\\n1ï¸âƒ£ å¥å­åˆ†å‰² (Sentence Tokenization)\")\n",
    "    print(\"-\" * 70)\n",
    "    sentences = sent_tokenize(text)\n",
    "    print(f\"å¥å­æ•¸: {len(sentences)}\\n\")\n",
    "    for i, sent in enumerate(sentences, 1):\n",
    "        print(f\"  [{i}] {sent.strip()}\")\n",
    "    \n",
    "    # 2. è©å½™åˆ†å‰²\n",
    "    print(\"\\n2ï¸âƒ£ è©å½™åˆ†å‰² (Word Tokenization)\")\n",
    "    print(\"-\" * 70)\n",
    "    tokens = word_tokenize(text)\n",
    "    print(f\"è©å½™æ•¸: {len(tokens)}\")\n",
    "    print(f\"å‰ 20 å€‹è©: {tokens[:20]}\")\n",
    "    \n",
    "    # 3. åœç”¨è©éæ¿¾\n",
    "    print(\"\\n3ï¸âƒ£ åœç”¨è©éæ¿¾ (Stop Words Removal)\")\n",
    "    print(\"-\" * 70)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [w for w in tokens if w.lower() not in stop_words and w.isalpha()]\n",
    "    print(f\"åŸå§‹è©æ•¸: {len(tokens)}\")\n",
    "    print(f\"éæ¿¾å¾Œè©æ•¸: {len(filtered_tokens)}\")\n",
    "    print(f\"éæ¿¾å¾Œè©å½™: {filtered_tokens[:15]}\")\n",
    "    \n",
    "    # 4. è©å¹¹æå– (Stemming)\n",
    "    print(\"\\n4ï¸âƒ£ è©å¹¹æå– (Stemming - Porter Stemmer)\")\n",
    "    print(\"-\" * 70)\n",
    "    stemmer = PorterStemmer()\n",
    "    test_words = ['processing', 'languages', 'interactions', 'computers', 'enables']\n",
    "    print(f\"{'åŸè©':<20s} â†’ è©å¹¹\")\n",
    "    print(\"-\" * 40)\n",
    "    for word in test_words:\n",
    "        stem = stemmer.stem(word)\n",
    "        print(f\"{word:<20s} â†’ {stem}\")\n",
    "    \n",
    "    # 5. è©å½¢é‚„åŸ (Lemmatization)\n",
    "    print(\"\\n5ï¸âƒ£ è©å½¢é‚„åŸ (Lemmatization)\")\n",
    "    print(\"-\" * 70)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    print(f\"{'åŸè©':<20s} â†’ è©å½¢é‚„åŸ\")\n",
    "    print(\"-\" * 40)\n",
    "    for word in test_words:\n",
    "        lemma = lemmatizer.lemmatize(word)\n",
    "        print(f\"{word:<20s} â†’ {lemma}\")\n",
    "    \n",
    "    # 6. è©æ€§æ¨™è¨» (POS Tagging)\n",
    "    print(\"\\n6ï¸âƒ£ è©æ€§æ¨™è¨» (POS Tagging)\")\n",
    "    print(\"-\" * 70)\n",
    "    sample_sent = \"Natural language processing enables computers to understand human languages.\"\n",
    "    sample_tokens = word_tokenize(sample_sent)\n",
    "    pos_tags = nltk.pos_tag(sample_tokens)\n",
    "    \n",
    "    print(f\"{'è©å½™':<20s} {'è©æ€§':<10s} èªªæ˜\")\n",
    "    print(\"-\" * 60)\n",
    "    for word, pos in pos_tags:\n",
    "        print(f\"{word:<20s} {pos:<10s}\")\n",
    "    \n",
    "    print(\"\\nâœ… NLTK åŠŸèƒ½å±•ç¤ºå®Œæˆ!\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"\\nâš ï¸ NLTK æœªå®‰è£!\")\n",
    "    print(\"è«‹åŸ·è¡Œ: pip install nltk\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ éŒ¯èª¤: {e}\")\n",
    "    print(\"å¦‚é‡åˆ°è³‡æºä¸‹è¼‰å•é¡Œ,è«‹æ‰‹å‹•åŸ·è¡Œ:\")\n",
    "    print(\"  import nltk\")\n",
    "    print(\"  nltk.download('punkt')\")\n",
    "    print(\"  nltk.download('stopwords')\")\n",
    "    print(\"  nltk.download('wordnet')\")\n",
    "    print(\"  nltk.download('averaged_perceptron_tagger')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è©å¹¹æå– vs è©å½¢é‚„åŸå°æ¯”è¦–è¦ºåŒ–\n",
    "try:\n",
    "    import nltk\n",
    "    from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # æ¸¬è©¦è©å½™\n",
    "    test_words = [\n",
    "        'running', 'ran', 'runs', 'easily', 'fairly',\n",
    "        'processing', 'computers', 'languages', 'better', 'good'\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    for word in test_words:\n",
    "        stem = stemmer.stem(word)\n",
    "        lemma = lemmatizer.lemmatize(word)\n",
    "        results.append({\n",
    "            'åŸè©': word,\n",
    "            'è©å¹¹ (Stem)': stem,\n",
    "            'è©å½¢é‚„åŸ (Lemma)': lemma,\n",
    "            'ç›¸åŒ?': 'âœ…' if stem == lemma else 'âŒ'\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # è¦–è¦ºåŒ–è¡¨æ ¼\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    ax.axis('tight')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    table = ax.table(cellText=df.values, colLabels=df.columns,\n",
    "                    cellLoc='center', loc='center',\n",
    "                    colWidths=[0.25, 0.25, 0.3, 0.2])\n",
    "    \n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(11)\n",
    "    table.scale(1, 2.5)\n",
    "    \n",
    "    # è¨­å®šè¡¨é ­æ¨£å¼\n",
    "    for i in range(len(df.columns)):\n",
    "        table[(0, i)].set_facecolor('#F18F01')\n",
    "        table[(0, i)].set_text_props(weight='bold', color='white', fontsize=12)\n",
    "    \n",
    "    # è¨­å®šäº¤æ›¿è¡Œé¡è‰²\n",
    "    for i in range(1, len(df) + 1):\n",
    "        for j in range(len(df.columns)):\n",
    "            if i % 2 == 0:\n",
    "                table[(i, j)].set_facecolor('#E8F4F8')\n",
    "            else:\n",
    "                table[(i, j)].set_facecolor('white')\n",
    "    \n",
    "    plt.title('NLTK: è©å¹¹æå– vs è©å½¢é‚„åŸå°æ¯”', fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nğŸ’¡ è§€å¯Ÿ:\")\n",
    "    print(\"  - è©å¹¹æå– (Stemming): ç°¡å–®è¦å‰‡,å¯èƒ½ç”¢ç”Ÿéè© (e.g., 'comput')\")\n",
    "    print(\"  - è©å½¢é‚„åŸ (Lemmatization): åŸºæ–¼è©å…¸,ä¿ç•™æœ‰æ„ç¾©çš„è©æ ¹\")\n",
    "    print(\"  - é€Ÿåº¦: Stemming æ›´å¿«,Lemmatization æ›´æº–ç¢º\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ éœ€è¦å®‰è£ NLTK: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "## 3. spaCy: å·¥æ¥­ç´š NLP æ¡†æ¶\n",
    "\n",
    "### 3.1 spaCy æ ¸å¿ƒç‰¹é»\n",
    "\n",
    "**å®šä½:** å·¥æ¥­ç´š NLP å‡½å¼åº«,é©åˆç”Ÿç”¢ç’°å¢ƒéƒ¨ç½²\n",
    "\n",
    "**å„ªå‹¢:**\n",
    "- âœ… è¶…é«˜æ•ˆèƒ½: Cython å¯¦ä½œ,é€Ÿåº¦æ¯” NLTK å¿« 10-100 å€\n",
    "- âœ… Pipeline æ¶æ§‹: ä¸€æ¬¡è™•ç†å®Œæˆæ‰€æœ‰ä»»å‹™\n",
    "- âœ… é è¨“ç·´æ¨¡å‹: æ”¯æ´ 20+ ç¨®èªè¨€\n",
    "- âœ… ç”Ÿç”¢å°±ç·’: API ç°¡æ½”,æ˜“æ–¼æ•´åˆ\n",
    "- âœ… ç¾ä»£è¨­è¨ˆ: æ”¯æ´æ·±åº¦å­¸ç¿’,å¯è‡ªå®šç¾©æ“´å±•\n",
    "\n",
    "**åŠ£å‹¢:**\n",
    "- âŒ å­¸ç¿’æ›²ç·šé™¡å³­: å…§éƒ¨è¨­è¨ˆè¼ƒè¤‡é›œ\n",
    "- âŒ æ¨¡å‹é«”ç©å¤§: é è¨“ç·´æ¨¡å‹éœ€ä¸‹è¼‰ (æ•¸ç™¾ MB)\n",
    "- âŒ éˆæ´»æ€§è¼ƒä½: Pipeline å›ºå®š,ä¸å¦‚ NLTK æ¨¡çµ„åŒ–\n",
    "\n",
    "---\n",
    "\n",
    "### 3.2 spaCy æ ¸å¿ƒåŠŸèƒ½å¯¦ä½œ\n",
    "\n",
    "#### åŠŸèƒ½æ¸…å–®:\n",
    "1. è‡ªå‹•åŒ– Pipeline è™•ç†\n",
    "2. å‘½åå¯¦é«”è­˜åˆ¥ (NER)\n",
    "3. ä¾å­˜å¥æ³•åˆ†æ (Dependency Parsing)\n",
    "4. è©å‘é‡ç›¸ä¼¼åº¦è¨ˆç®—\n",
    "5. æ‰¹æ¬¡è™•ç†å„ªåŒ–\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spaCy æ ¸å¿ƒåŠŸèƒ½å±•ç¤º\n",
    "print(\"=\" * 70)\n",
    "print(\"spaCy æ ¸å¿ƒåŠŸèƒ½å±•ç¤º\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "try:\n",
    "    import spacy\n",
    "    \n",
    "    # è¼‰å…¥è‹±æ–‡æ¨¡å‹ (å°å‹æ¨¡å‹)\n",
    "    print(\"\\nğŸ“¥ è¼‰å…¥ spaCy æ¨¡å‹...\")\n",
    "    try:\n",
    "        nlp = spacy.load('en_core_web_sm')\n",
    "        print(\"  âœ… en_core_web_sm å·²è¼‰å…¥\")\n",
    "    except OSError:\n",
    "        print(\"  âš ï¸ æ¨¡å‹æœªå®‰è£,è«‹åŸ·è¡Œ: python -m spacy download en_core_web_sm\")\n",
    "        print(\"  ä½¿ç”¨ç©ºç™½æ¨¡å‹æ›¿ä»£...\")\n",
    "        nlp = spacy.blank('en')\n",
    "    \n",
    "    # æ¸¬è©¦æ–‡æœ¬\n",
    "    text = \"\"\"\n",
    "    Apple Inc. is planning to open a new store in New York City next month. \n",
    "    The CEO Tim Cook announced the decision last week at a conference in California. \n",
    "    The company expects to hire 500 employees for this new location.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"åŸå§‹æ–‡æœ¬:\")\n",
    "    print(text.strip())\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # è™•ç†æ–‡æœ¬ (è‡ªå‹•åŸ·è¡Œå®Œæ•´ Pipeline)\n",
    "    print(\"\\nâš™ï¸ åŸ·è¡Œ spaCy Pipeline...\")\n",
    "    doc = nlp(text)\n",
    "    print(\"  âœ… è™•ç†å®Œæˆ!\")\n",
    "    \n",
    "    # 1. åˆ†è©èˆ‡è©æ€§æ¨™è¨»\n",
    "    print(\"\\n1ï¸âƒ£ åˆ†è©èˆ‡è©æ€§æ¨™è¨» (å‰ 15 å€‹è©)\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"{'è©å½™':<20s} {'è©æ€§':<10s} {'è©å¹¹':<20s} {'åœç”¨è©?'}\")\n",
    "    print(\"-\" * 70)\n",
    "    for token in list(doc)[:15]:\n",
    "        print(f\"{token.text:<20s} {token.pos_:<10s} {token.lemma_:<20s} {'âœ…' if token.is_stop else 'âŒ'}\")\n",
    "    \n",
    "    # 2. å‘½åå¯¦é«”è­˜åˆ¥ (NER)\n",
    "    print(\"\\n2ï¸âƒ£ å‘½åå¯¦é«”è­˜åˆ¥ (Named Entity Recognition)\")\n",
    "    print(\"-\" * 70)\n",
    "    if doc.ents:\n",
    "        print(f\"{'å¯¦é«”':<30s} {'é¡å‹':<15s} èªªæ˜\")\n",
    "        print(\"-\" * 70)\n",
    "        for ent in doc.ents:\n",
    "            print(f\"{ent.text:<30s} {ent.label_:<15s} {spacy.explain(ent.label_)}\")\n",
    "    else:\n",
    "        print(\"  âš ï¸ æœªæª¢æ¸¬åˆ°å¯¦é«” (å¯èƒ½ä½¿ç”¨çš„æ˜¯ç©ºç™½æ¨¡å‹)\")\n",
    "    \n",
    "    # 3. ä¾å­˜å¥æ³•åˆ†æ\n",
    "    print(\"\\n3ï¸âƒ£ ä¾å­˜å¥æ³•åˆ†æ (Dependency Parsing - ç¬¬ä¸€å¥)\")\n",
    "    print(\"-\" * 70)\n",
    "    sent = list(doc.sents)[0]\n",
    "    print(f\"{'è©å½™':<20s} {'ä¾å­˜é—œä¿‚':<15s} ä¸»å°è©\")\n",
    "    print(\"-\" * 70)\n",
    "    for token in sent:\n",
    "        print(f\"{token.text:<20s} {token.dep_:<15s} {token.head.text}\")\n",
    "    \n",
    "    # 4. å¥å­åˆ†å‰²\n",
    "    print(\"\\n4ï¸âƒ£ å¥å­åˆ†å‰²\")\n",
    "    print(\"-\" * 70)\n",
    "    sentences = list(doc.sents)\n",
    "    print(f\"å¥å­æ•¸: {len(sentences)}\\n\")\n",
    "    for i, sent in enumerate(sentences, 1):\n",
    "        print(f\"  [{i}] {sent.text.strip()}\")\n",
    "    \n",
    "    print(\"\\nâœ… spaCy åŠŸèƒ½å±•ç¤ºå®Œæˆ!\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"\\nâš ï¸ spaCy æœªå®‰è£!\")\n",
    "    print(\"è«‹åŸ·è¡Œ: pip install spacy\")\n",
    "    print(\"ç„¶å¾Œä¸‹è¼‰æ¨¡å‹: python -m spacy download en_core_web_sm\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ éŒ¯èª¤: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spaCy æ‰¹æ¬¡è™•ç†æ•ˆèƒ½å±•ç¤º\n",
    "print(\"=\" * 70)\n",
    "print(\"spaCy æ‰¹æ¬¡è™•ç†æ•ˆèƒ½æ¸¬è©¦\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "try:\n",
    "    import spacy\n",
    "    import time\n",
    "    \n",
    "    try:\n",
    "        nlp = spacy.load('en_core_web_sm')\n",
    "    except:\n",
    "        print(\"âš ï¸ ä½¿ç”¨ç©ºç™½æ¨¡å‹ (åƒ…æ¸¬è©¦é€Ÿåº¦)\")\n",
    "        nlp = spacy.blank('en')\n",
    "    \n",
    "    # æº–å‚™æ¸¬è©¦æ•¸æ“š\n",
    "    test_text = \"Natural language processing is amazing. It enables computers to understand human language.\"\n",
    "    texts = [test_text] * 1000  # 1000 å€‹æ–‡æª”\n",
    "    \n",
    "    # æ–¹æ³• 1: é€å€‹è™•ç†\n",
    "    print(\"\\næ–¹æ³• 1: é€å€‹è™•ç† (Sequential)\")\n",
    "    start = time.time()\n",
    "    docs_sequential = [nlp(text) for text in texts]\n",
    "    time_sequential = time.time() - start\n",
    "    print(f\"  è™•ç†æ™‚é–“: {time_sequential:.3f} ç§’\")\n",
    "    \n",
    "    # æ–¹æ³• 2: æ‰¹æ¬¡è™•ç†\n",
    "    print(\"\\næ–¹æ³• 2: æ‰¹æ¬¡è™•ç† (Batching)\")\n",
    "    start = time.time()\n",
    "    docs_batch = list(nlp.pipe(texts, batch_size=50))\n",
    "    time_batch = time.time() - start\n",
    "    print(f\"  è™•ç†æ™‚é–“: {time_batch:.3f} ç§’\")\n",
    "    \n",
    "    # å°æ¯”\n",
    "    speedup = time_sequential / time_batch\n",
    "    print(f\"\\nğŸš€ åŠ é€Ÿæ¯”: {speedup:.2f}x\")\n",
    "    \n",
    "    # è¦–è¦ºåŒ–\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    methods = ['é€å€‹è™•ç†', 'æ‰¹æ¬¡è™•ç†']\n",
    "    times = [time_sequential, time_batch]\n",
    "    colors = ['#F18F01', '#2E86AB']\n",
    "    \n",
    "    bars = ax.bar(methods, times, color=colors, alpha=0.7, edgecolor='black')\n",
    "    ax.set_ylabel('è™•ç†æ™‚é–“ (ç§’)', fontsize=12)\n",
    "    ax.set_title('spaCy æ‰¹æ¬¡è™•ç†æ•ˆèƒ½å°æ¯” (1000 æ–‡æª”)', fontsize=14, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # æ¨™è¨»æ•¸å€¼\n",
    "    for bar, time_val in zip(bars, times):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{time_val:.3f}s',\n",
    "                ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nğŸ’¡ æ‰¹æ¬¡è™•ç†å„ªå‹¢:\")\n",
    "    print(\"  1. æ¸›å°‘ Python è§£é‡‹å™¨é–‹éŠ·\")\n",
    "    print(\"  2. æ›´å¥½åˆ©ç”¨ CPU/GPU\")\n",
    "    print(\"  3. ç”Ÿç”¢ç’°å¢ƒå¿…å‚™å„ªåŒ–\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ éœ€è¦å®‰è£ spaCy: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "## 4. Transformers: é è¨“ç·´æ¨¡å‹åº«\n",
    "\n",
    "### 4.1 Transformers æ ¸å¿ƒç‰¹é»\n",
    "\n",
    "**å®šä½:** Hugging Face é–‹æºçš„é è¨“ç·´ Transformer æ¨¡å‹åº«\n",
    "\n",
    "**å„ªå‹¢:**\n",
    "- âœ… æ¨¡å‹è±å¯Œ: 100,000+ é è¨“ç·´æ¨¡å‹\n",
    "- âœ… çµ±ä¸€ API: AutoModel/AutoTokenizer æ”¯æ´æ‰€æœ‰æ¨¡å‹\n",
    "- âœ… SOTA æ€§èƒ½: æœ€æ–°ç ”ç©¶æˆæœå¿«é€Ÿè½åœ°\n",
    "- âœ… æ˜“æ–¼å¾®èª¿: Trainer API ç°¡åŒ–è¨“ç·´æµç¨‹\n",
    "- âœ… ç¤¾ç¾¤æ´»èº: æ¯å¤©æ–°å¢æ•¸ç™¾å€‹æ¨¡å‹\n",
    "\n",
    "**åŠ£å‹¢:**\n",
    "- âŒ è³‡æºéœ€æ±‚é«˜: å¤§æ¨¡å‹éœ€è¦ GPU\n",
    "- âŒ é€Ÿåº¦è¼ƒæ…¢: æ¨ç†é€Ÿåº¦ä¸å¦‚ spaCy\n",
    "- âŒ å­¸ç¿’æ›²ç·šé™¡: æ·±åº¦å­¸ç¿’èƒŒæ™¯éœ€æ±‚\n",
    "\n",
    "---\n",
    "\n",
    "### 4.2 Transformers ä¸‰å¤§æ¨¡å‹å®¶æ—\n",
    "\n",
    "#### 1. Encoder-Only (BERT ç³»åˆ—)\n",
    "- **ä»£è¡¨:** BERT, RoBERTa, ALBERT, DeBERTa\n",
    "- **ç‰¹é»:** é›™å‘ Attention,é©åˆç†è§£ä»»å‹™\n",
    "- **æ‡‰ç”¨:** æ–‡æœ¬åˆ†é¡ã€NERã€å•ç­”ç³»çµ±\n",
    "\n",
    "#### 2. Decoder-Only (GPT ç³»åˆ—)\n",
    "- **ä»£è¡¨:** GPT-2, GPT-3, GPT-4, LLaMA\n",
    "- **ç‰¹é»:** å–®å‘ Attention,è‡ªå›æ­¸ç”Ÿæˆ\n",
    "- **æ‡‰ç”¨:** æ–‡æœ¬ç”Ÿæˆã€å°è©±ã€ç¨‹å¼ç¢¼ç”Ÿæˆ\n",
    "\n",
    "#### 3. Encoder-Decoder (T5 ç³»åˆ—)\n",
    "- **ä»£è¡¨:** T5, BART, mT5, T0\n",
    "- **ç‰¹é»:** å®Œæ•´ Transformer æ¶æ§‹\n",
    "- **æ‡‰ç”¨:** ç¿»è­¯ã€æ‘˜è¦ã€å•ç­”ç”Ÿæˆ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformers Pipeline API å±•ç¤º\n",
    "print(\"=\" * 70)\n",
    "print(\"Transformers Pipeline API å±•ç¤º\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "try:\n",
    "    from transformers import pipeline\n",
    "    \n",
    "    # 1. æƒ…æ„Ÿåˆ†æ\n",
    "    print(\"\\n1ï¸âƒ£ æƒ…æ„Ÿåˆ†æ (Sentiment Analysis)\")\n",
    "    print(\"-\" * 70)\n",
    "    sentiment_analyzer = pipeline('sentiment-analysis')\n",
    "    \n",
    "    texts = [\n",
    "        \"I love this product! It's absolutely amazing!\",\n",
    "        \"This is the worst experience I've ever had.\",\n",
    "        \"It's okay, nothing special.\"\n",
    "    ]\n",
    "    \n",
    "    for text in texts:\n",
    "        result = sentiment_analyzer(text)[0]\n",
    "        print(f\"\\næ–‡æœ¬: {text}\")\n",
    "        print(f\"çµæœ: {result['label']} (ä¿¡å¿ƒåº¦: {result['score']:.2%})\")\n",
    "    \n",
    "    # 2. é›¶æ¨£æœ¬åˆ†é¡\n",
    "    print(\"\\n\\n2ï¸âƒ£ é›¶æ¨£æœ¬åˆ†é¡ (Zero-Shot Classification)\")\n",
    "    print(\"-\" * 70)\n",
    "    zero_shot_classifier = pipeline('zero-shot-classification')\n",
    "    \n",
    "    text = \"Apple announced a new iPhone with improved camera and longer battery life.\"\n",
    "    candidate_labels = ['technology', 'sports', 'politics', 'entertainment']\n",
    "    \n",
    "    result = zero_shot_classifier(text, candidate_labels)\n",
    "    print(f\"\\næ–‡æœ¬: {text}\")\n",
    "    print(\"\\nåˆ†é¡çµæœ:\")\n",
    "    for label, score in zip(result['labels'], result['scores']):\n",
    "        print(f\"  {label:<15s}: {score:.2%}\")\n",
    "    \n",
    "    # 3. å•ç­”ç³»çµ±\n",
    "    print(\"\\n\\n3ï¸âƒ£ å•ç­”ç³»çµ± (Question Answering)\")\n",
    "    print(\"-\" * 70)\n",
    "    qa_pipeline = pipeline('question-answering')\n",
    "    \n",
    "    context = \"\"\"\n",
    "    Hugging Face is a company based in New York City. The company develops \n",
    "    tools for building applications using machine learning. It is best known \n",
    "    for its Transformers library.\n",
    "    \"\"\"\n",
    "    \n",
    "    questions = [\n",
    "        \"Where is Hugging Face based?\",\n",
    "        \"What is Hugging Face known for?\"\n",
    "    ]\n",
    "    \n",
    "    for question in questions:\n",
    "        result = qa_pipeline(question=question, context=context)\n",
    "        print(f\"\\nå•é¡Œ: {question}\")\n",
    "        print(f\"ç­”æ¡ˆ: {result['answer']} (ä¿¡å¿ƒåº¦: {result['score']:.2%})\")\n",
    "    \n",
    "    # 4. æ–‡æœ¬ç”Ÿæˆ\n",
    "    print(\"\\n\\n4ï¸âƒ£ æ–‡æœ¬ç”Ÿæˆ (Text Generation)\")\n",
    "    print(\"-\" * 70)\n",
    "    generator = pipeline('text-generation', model='gpt2')\n",
    "    \n",
    "    prompt = \"Artificial intelligence will\"\n",
    "    outputs = generator(prompt, max_length=50, num_return_sequences=2)\n",
    "    \n",
    "    print(f\"\\næç¤ºè©: {prompt}\")\n",
    "    print(\"\\nç”Ÿæˆçµæœ:\")\n",
    "    for i, output in enumerate(outputs, 1):\n",
    "        print(f\"\\n[{i}] {output['generated_text']}\")\n",
    "    \n",
    "    print(\"\\n\\nâœ… Transformers Pipeline å±•ç¤ºå®Œæˆ!\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"\\nâš ï¸ Transformers æœªå®‰è£!\")\n",
    "    print(\"è«‹åŸ·è¡Œ: pip install transformers torch\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ éŒ¯èª¤: {e}\")\n",
    "    print(\"æç¤º: é¦–æ¬¡åŸ·è¡Œæœƒè‡ªå‹•ä¸‹è¼‰æ¨¡å‹,éœ€è¦ç¶²è·¯é€£æ¥\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformers æ¨¡å‹å®¶æ—å°æ¯”\n",
    "model_families = [\n",
    "    {\n",
    "        'å®¶æ—': 'Encoder-Only',\n",
    "        'ä»£è¡¨æ¨¡å‹': 'BERT, RoBERTa, ALBERT',\n",
    "        'æ¶æ§‹': 'é›™å‘ Attention',\n",
    "        'è¨“ç·´æ–¹å¼': 'Masked LM',\n",
    "        'é©ç”¨ä»»å‹™': 'åˆ†é¡ã€NERã€å•ç­”',\n",
    "        'åƒæ•¸é‡': '110M - 1.5B'\n",
    "    },\n",
    "    {\n",
    "        'å®¶æ—': 'Decoder-Only',\n",
    "        'ä»£è¡¨æ¨¡å‹': 'GPT-2, GPT-3, LLaMA',\n",
    "        'æ¶æ§‹': 'å–®å‘ Attention',\n",
    "        'è¨“ç·´æ–¹å¼': 'Autoregressive LM',\n",
    "        'é©ç”¨ä»»å‹™': 'ç”Ÿæˆã€å°è©±ã€ç¿»è­¯',\n",
    "        'åƒæ•¸é‡': '124M - 175B'\n",
    "    },\n",
    "    {\n",
    "        'å®¶æ—': 'Encoder-Decoder',\n",
    "        'ä»£è¡¨æ¨¡å‹': 'T5, BART, mT5',\n",
    "        'æ¶æ§‹': 'å®Œæ•´ Transformer',\n",
    "        'è¨“ç·´æ–¹å¼': 'Seq2Seq',\n",
    "        'é©ç”¨ä»»å‹™': 'ç¿»è­¯ã€æ‘˜è¦ã€QAç”Ÿæˆ',\n",
    "        'åƒæ•¸é‡': '60M - 11B'\n",
    "    }\n",
    "]\n",
    "\n",
    "df_models = pd.DataFrame(model_families)\n",
    "\n",
    "# è¦–è¦ºåŒ–è¡¨æ ¼\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "table = ax.table(cellText=df_models.values, colLabels=df_models.columns,\n",
    "                cellLoc='left', loc='center',\n",
    "                colWidths=[0.15, 0.25, 0.15, 0.15, 0.2, 0.1])\n",
    "\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1, 3)\n",
    "\n",
    "# è¨­å®šè¡¨é ­æ¨£å¼\n",
    "for i in range(len(df_models.columns)):\n",
    "    table[(0, i)].set_facecolor('#2E86AB')\n",
    "    table[(0, i)].set_text_props(weight='bold', color='white', fontsize=11)\n",
    "\n",
    "# è¨­å®šè¡Œé¡è‰²\n",
    "colors = ['#FFE66D', '#95E1D3', '#F18F01']\n",
    "for i in range(1, len(df_models) + 1):\n",
    "    for j in range(len(df_models.columns)):\n",
    "        table[(i, j)].set_facecolor(colors[i-1])\n",
    "        table[(i, j)].set_alpha(0.4)\n",
    "\n",
    "plt.title('Transformers ä¸‰å¤§æ¨¡å‹å®¶æ—å°æ¯”', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ’¡ é¸æ“‡å»ºè­°:\")\n",
    "print(\"  1. ç†è§£ä»»å‹™ (åˆ†é¡ã€NER) â†’ Encoder-Only (BERT ç³»åˆ—)\")\n",
    "print(\"  2. ç”Ÿæˆä»»å‹™ (å°è©±ã€å¯«ä½œ) â†’ Decoder-Only (GPT ç³»åˆ—)\")\n",
    "print(\"  3. è½‰æ›ä»»å‹™ (ç¿»è­¯ã€æ‘˜è¦) â†’ Encoder-Decoder (T5 ç³»åˆ—)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "## 5. ä¸­æ–‡ NLP å·¥å…·å°æ¯”\n",
    "\n",
    "### 5.1 ä¸­æ–‡ NLP çš„ç‰¹æ®ŠæŒ‘æˆ°\n",
    "\n",
    "**èˆ‡è‹±æ–‡çš„é—œéµå·®ç•°:**\n",
    "1. **ç„¡å¤©ç„¶åˆ†è©:** ä¸­æ–‡æ²’æœ‰ç©ºæ ¼åˆ†éš”,éœ€è¦å…ˆåˆ†è©\n",
    "2. **ç¹ç°¡é«”å·®ç•°:** éœ€è¦è™•ç†ç¹é«”/ç°¡é«”è½‰æ›\n",
    "3. **å¤šéŸ³å­—:** åŒä¸€å­—ä¸åŒè®€éŸ³å’Œæ„æ€\n",
    "4. **è©å½™æ­§ç¾©:** ä¸­æ–‡è©å½™æ­§ç¾©æ€§æ›´é«˜\n",
    "\n",
    "---\n",
    "\n",
    "### 5.2 ä¸»æµä¸­æ–‡ NLP å·¥å…·\n",
    "\n",
    "| å·¥å…· | é–‹ç™¼è€… | ç‰¹é» | å„ªå‹¢ | åŠ£å‹¢ |\n",
    "|:---|:---|:---|:---|:---|\n",
    "| **jieba** | fxsjy | è¼•é‡å¿«é€Ÿ | æ˜“ç”¨ã€ç„¡ä¾è³´ | æº–ç¢ºç‡ä¸­ç­‰ |\n",
    "| **LTP** | å“ˆå·¥å¤§ | å…¨æµç¨‹ | æº–ç¢ºç‡é«˜ã€åŠŸèƒ½å®Œæ•´ | é€Ÿåº¦è¼ƒæ…¢ã€ä¾è³´é‡ |\n",
    "| **HanLP** | hankcs | å¤šèªè¨€ | æ”¯æ´ç¹é«”ã€å¤šä»»å‹™ | å­¸ç¿’æ›²ç·šé™¡ |\n",
    "| **pkuseg** | åŒ—äº¬å¤§å­¸ | é ˜åŸŸé©æ‡‰ | å¤šé ˜åŸŸæ¨¡å‹ | é€Ÿåº¦æ…¢ |\n",
    "| **THULAC** | æ¸…è¯å¤§å­¸ | è©æ€§æ¨™è¨»æº– | è©æ€§æº–ç¢º | ç¤¾ç¾¤è¼ƒå° |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¸­æ–‡åˆ†è©å·¥å…·å°æ¯”\n",
    "print(\"=\" * 70)\n",
    "print(\"ä¸­æ–‡åˆ†è©å·¥å…·å°æ¯”\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "test_text = \"æˆ‘æ„›è‡ªç„¶èªè¨€è™•ç†,å®ƒæ˜¯äººå·¥æ™ºèƒ½çš„é‡è¦åˆ†æ”¯,å¯ä»¥è®“é›»è…¦ç†è§£äººé¡èªè¨€ã€‚\"\n",
    "\n",
    "print(f\"\\næ¸¬è©¦æ–‡æœ¬: {test_text}\\n\")\n",
    "\n",
    "# jieba åˆ†è©\n",
    "try:\n",
    "    import jieba\n",
    "    print(\"1ï¸âƒ£ jieba (çµå·´åˆ†è©)\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # ç²¾ç¢ºæ¨¡å¼\n",
    "    words_precise = jieba.cut(test_text, cut_all=False)\n",
    "    print(f\"ç²¾ç¢ºæ¨¡å¼: {' / '.join(words_precise)}\")\n",
    "    \n",
    "    # å…¨æ¨¡å¼\n",
    "    words_all = jieba.cut(test_text, cut_all=True)\n",
    "    print(f\"å…¨æ¨¡å¼:   {' / '.join(words_all)}\")\n",
    "    \n",
    "    # æœå°‹å¼•æ“æ¨¡å¼\n",
    "    words_search = jieba.cut_for_search(test_text)\n",
    "    print(f\"æœå°‹æ¨¡å¼: {' / '.join(words_search)}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"âš ï¸ jieba æœªå®‰è£: pip install jieba\")\n",
    "\n",
    "# pkuseg åˆ†è©\n",
    "try:\n",
    "    import pkuseg\n",
    "    print(\"\\n2ï¸âƒ£ pkuseg (åŒ—å¤§åˆ†è©)\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    seg = pkuseg.pkuseg()\n",
    "    words = seg.cut(test_text)\n",
    "    print(f\"åˆ†è©çµæœ: {' / '.join(words)}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"\\nâš ï¸ pkuseg æœªå®‰è£: pip install pkuseg\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nâš ï¸ pkuseg éŒ¯èª¤: {e}\")\n",
    "\n",
    "# spaCy ä¸­æ–‡æ¨¡å‹\n",
    "try:\n",
    "    import spacy\n",
    "    print(\"\\n3ï¸âƒ£ spaCy (ä¸­æ–‡æ¨¡å‹)\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    try:\n",
    "        nlp_zh = spacy.load('zh_core_web_sm')\n",
    "        doc = nlp_zh(test_text)\n",
    "        words = [token.text for token in doc]\n",
    "        print(f\"åˆ†è©çµæœ: {' / '.join(words)}\")\n",
    "    except OSError:\n",
    "        print(\"âš ï¸ ä¸­æ–‡æ¨¡å‹æœªå®‰è£: python -m spacy download zh_core_web_sm\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"\\nâš ï¸ spaCy æœªå®‰è£: pip install spacy\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… ä¸­æ–‡åˆ†è©å°æ¯”å®Œæˆ\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nğŸ’¡ é¸æ“‡å»ºè­°:\")\n",
    "print(\"  1. å¿«é€ŸåŸå‹ â†’ jieba (æœ€ç°¡å–®)\")\n",
    "print(\"  2. é«˜æº–ç¢ºç‡ â†’ pkuseg/LTP (é ˜åŸŸé©æ‡‰)\")\n",
    "print(\"  3. å®Œæ•´æµç¨‹ â†’ spaCy (åˆ†è©+è©æ€§+NER)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jieba é€²éšåŠŸèƒ½å±•ç¤º\n",
    "print(\"=\" * 70)\n",
    "print(\"jieba é€²éšåŠŸèƒ½å±•ç¤º\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "try:\n",
    "    import jieba\n",
    "    import jieba.analyse\n",
    "    import jieba.posseg as pseg\n",
    "    \n",
    "    text = \"\"\"\n",
    "    è‡ªç„¶èªè¨€è™•ç†æ˜¯äººå·¥æ™ºèƒ½é ˜åŸŸçš„é‡è¦åˆ†æ”¯,å®ƒç ”ç©¶å¦‚ä½•è®“é›»è…¦ç†è§£å’Œç”Ÿæˆäººé¡èªè¨€ã€‚\n",
    "    æ·±åº¦å­¸ç¿’æŠ€è¡“çš„ç™¼å±•æ¨å‹•äº†NLPçš„å¿«é€Ÿé€²æ­¥,BERTã€GPTç­‰æ¨¡å‹å–å¾—äº†çªç ´æ€§æˆæœã€‚\n",
    "    è‡ªç„¶èªè¨€è™•ç†çš„æ‡‰ç”¨åŒ…æ‹¬æ©Ÿå™¨ç¿»è­¯ã€å•ç­”ç³»çµ±ã€æ–‡æœ¬åˆ†é¡ç­‰å¤šå€‹æ–¹å‘ã€‚\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. è©æ€§æ¨™è¨»\n",
    "    print(\"\\n1ï¸âƒ£ è©æ€§æ¨™è¨» (POS Tagging)\")\n",
    "    print(\"-\" * 70)\n",
    "    words = pseg.cut(text)\n",
    "    print(f\"{'è©å½™':<15s} è©æ€§\")\n",
    "    print(\"-\" * 30)\n",
    "    for word, pos in list(words)[:20]:\n",
    "        if word.strip():\n",
    "            print(f\"{word:<15s} {pos}\")\n",
    "    \n",
    "    # 2. é—œéµå­—æå– (TF-IDF)\n",
    "    print(\"\\n2ï¸âƒ£ é—œéµå­—æå– (TF-IDF)\")\n",
    "    print(\"-\" * 70)\n",
    "    keywords_tfidf = jieba.analyse.extract_tags(text, topK=10, withWeight=True)\n",
    "    print(f\"{'é—œéµå­—':<15s} TF-IDF æ¬Šé‡\")\n",
    "    print(\"-\" * 40)\n",
    "    for word, weight in keywords_tfidf:\n",
    "        print(f\"{word:<15s} {weight:.4f}\")\n",
    "    \n",
    "    # 3. é—œéµå­—æå– (TextRank)\n",
    "    print(\"\\n3ï¸âƒ£ é—œéµå­—æå– (TextRank)\")\n",
    "    print(\"-\" * 70)\n",
    "    keywords_textrank = jieba.analyse.textrank(text, topK=10, withWeight=True)\n",
    "    print(f\"{'é—œéµå­—':<15s} TextRank æ¬Šé‡\")\n",
    "    print(\"-\" * 40)\n",
    "    for word, weight in keywords_textrank:\n",
    "        print(f\"{word:<15s} {weight:.4f}\")\n",
    "    \n",
    "    # 4. è‡ªå®šç¾©è©å…¸\n",
    "    print(\"\\n4ï¸âƒ£ è‡ªå®šç¾©è©å…¸\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    test_sentence = \"æˆ‘åœ¨å­¸ç¿’è‡ªç„¶èªè¨€è™•ç†å’Œæ·±åº¦å­¸ç¿’æŠ€è¡“\"\n",
    "    print(f\"åŸå¥: {test_sentence}\")\n",
    "    print(f\"é è¨­åˆ†è©: {' / '.join(jieba.cut(test_sentence))}\")\n",
    "    \n",
    "    # æ·»åŠ è‡ªå®šç¾©è©\n",
    "    jieba.add_word('è‡ªç„¶èªè¨€è™•ç†', freq=10000, tag='n')\n",
    "    jieba.add_word('æ·±åº¦å­¸ç¿’', freq=10000, tag='n')\n",
    "    print(f\"è‡ªå®šç¾©å¾Œ: {' / '.join(jieba.cut(test_sentence))}\")\n",
    "    \n",
    "    print(\"\\nâœ… jieba é€²éšåŠŸèƒ½å±•ç¤ºå®Œæˆ!\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"\\nâš ï¸ jieba æœªå®‰è£: pip install jieba\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a>\n",
    "## 6. å·¥å…·æ•ˆèƒ½åŸºæº–æ¸¬è©¦\n",
    "\n",
    "### 6.1 æ¸¬è©¦æ–¹æ³•è«–\n",
    "\n",
    "**æ¸¬è©¦æŒ‡æ¨™:**\n",
    "1. **è™•ç†é€Ÿåº¦ (Throughput):** æ¯ç§’è™•ç†è©æ•¸\n",
    "2. **è¨˜æ†¶é«”å ç”¨ (Memory Usage):** å³°å€¼è¨˜æ†¶é«”\n",
    "3. **æº–ç¢ºç‡ (Accuracy):** èˆ‡äººå·¥æ¨™è¨»å°æ¯”\n",
    "4. **å•Ÿå‹•æ™‚é–“ (Startup Time):** æ¨¡å‹è¼‰å…¥æ™‚é–“\n",
    "\n",
    "**æ¸¬è©¦ç’°å¢ƒ:**\n",
    "- CPU: Intel i7 (æ¨¡æ“¬)\n",
    "- è¨˜æ†¶é«”: 16GB\n",
    "- Python: 3.9+\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å·¥å…·æ•ˆèƒ½åŸºæº–æ¸¬è©¦\n",
    "import time\n",
    "import gc\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"å·¥å…·æ•ˆèƒ½åŸºæº–æ¸¬è©¦\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# æº–å‚™æ¸¬è©¦æ•¸æ“š\n",
    "test_text_en = \"Natural language processing is amazing. \" * 100\n",
    "test_text_zh = \"è‡ªç„¶èªè¨€è™•ç†æ˜¯äººå·¥æ™ºèƒ½çš„é‡è¦åˆ†æ”¯ã€‚\" * 100\n",
    "\n",
    "results = []\n",
    "\n",
    "# æ¸¬è©¦ NLTK\n",
    "try:\n",
    "    import nltk\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    \n",
    "    # è¼‰å…¥æ™‚é–“\n",
    "    start = time.time()\n",
    "    # NLTK ç„¡éœ€è¼‰å…¥æ¨¡å‹\n",
    "    load_time = time.time() - start\n",
    "    \n",
    "    # è™•ç†æ™‚é–“\n",
    "    start = time.time()\n",
    "    for _ in range(100):\n",
    "        tokens = word_tokenize(test_text_en)\n",
    "    process_time = time.time() - start\n",
    "    \n",
    "    results.append({\n",
    "        'å·¥å…·': 'NLTK',\n",
    "        'è¼‰å…¥æ™‚é–“(ç§’)': f\"{load_time:.3f}\",\n",
    "        'è™•ç†æ™‚é–“(ç§’)': f\"{process_time:.3f}\",\n",
    "        'ç›¸å°é€Ÿåº¦': '1.0x (åŸºæº–)'\n",
    "    })\n",
    "    nltk_time = process_time\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ NLTK æ¸¬è©¦å¤±æ•—: {e}\")\n",
    "    nltk_time = None\n",
    "\n",
    "# æ¸¬è©¦ spaCy\n",
    "try:\n",
    "    import spacy\n",
    "    \n",
    "    # è¼‰å…¥æ™‚é–“\n",
    "    start = time.time()\n",
    "    try:\n",
    "        nlp = spacy.load('en_core_web_sm')\n",
    "    except:\n",
    "        nlp = spacy.blank('en')\n",
    "    load_time = time.time() - start\n",
    "    \n",
    "    # è™•ç†æ™‚é–“\n",
    "    start = time.time()\n",
    "    for _ in range(100):\n",
    "        doc = nlp(test_text_en)\n",
    "    process_time = time.time() - start\n",
    "    \n",
    "    speedup = nltk_time / process_time if nltk_time else 0\n",
    "    results.append({\n",
    "        'å·¥å…·': 'spaCy',\n",
    "        'è¼‰å…¥æ™‚é–“(ç§’)': f\"{load_time:.3f}\",\n",
    "        'è™•ç†æ™‚é–“(ç§’)': f\"{process_time:.3f}\",\n",
    "        'ç›¸å°é€Ÿåº¦': f\"{speedup:.1f}x\" if speedup else 'N/A'\n",
    "    })\n",
    "    spacy_time = process_time\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ spaCy æ¸¬è©¦å¤±æ•—: {e}\")\n",
    "    spacy_time = None\n",
    "\n",
    "# æ¸¬è©¦ jieba\n",
    "try:\n",
    "    import jieba\n",
    "    \n",
    "    # è¼‰å…¥æ™‚é–“\n",
    "    start = time.time()\n",
    "    jieba.initialize()\n",
    "    load_time = time.time() - start\n",
    "    \n",
    "    # è™•ç†æ™‚é–“\n",
    "    start = time.time()\n",
    "    for _ in range(100):\n",
    "        words = list(jieba.cut(test_text_zh))\n",
    "    process_time = time.time() - start\n",
    "    \n",
    "    results.append({\n",
    "        'å·¥å…·': 'jieba',\n",
    "        'è¼‰å…¥æ™‚é–“(ç§’)': f\"{load_time:.3f}\",\n",
    "        'è™•ç†æ™‚é–“(ç§’)': f\"{process_time:.3f}\",\n",
    "        'ç›¸å°é€Ÿåº¦': 'N/A (ä¸­æ–‡)'\n",
    "    })\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ jieba æ¸¬è©¦å¤±æ•—: {e}\")\n",
    "\n",
    "# é¡¯ç¤ºçµæœ\n",
    "if results:\n",
    "    df_results = pd.DataFrame(results)\n",
    "    print(\"\\nğŸ“Š æ•ˆèƒ½æ¸¬è©¦çµæœ (100 æ¬¡è¿­ä»£)\")\n",
    "    print(\"=\" * 70)\n",
    "    print(df_results.to_string(index=False))\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # è¦–è¦ºåŒ–\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # è¼‰å…¥æ™‚é–“\n",
    "    tools = df_results['å·¥å…·'].tolist()\n",
    "    load_times = [float(t) for t in df_results['è¼‰å…¥æ™‚é–“(ç§’)'].tolist()]\n",
    "    \n",
    "    axes[0].barh(tools, load_times, color='#F18F01', alpha=0.7, edgecolor='black')\n",
    "    axes[0].set_xlabel('è¼‰å…¥æ™‚é–“ (ç§’)', fontsize=11)\n",
    "    axes[0].set_title('æ¨¡å‹è¼‰å…¥æ™‚é–“å°æ¯”', fontsize=13, fontweight='bold')\n",
    "    axes[0].grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    for i, (tool, time_val) in enumerate(zip(tools, load_times)):\n",
    "        axes[0].text(time_val, i, f' {time_val:.3f}s', va='center', fontsize=10)\n",
    "    \n",
    "    # è™•ç†æ™‚é–“\n",
    "    process_times = [float(t) for t in df_results['è™•ç†æ™‚é–“(ç§’)'].tolist()]\n",
    "    \n",
    "    axes[1].barh(tools, process_times, color='#2E86AB', alpha=0.7, edgecolor='black')\n",
    "    axes[1].set_xlabel('è™•ç†æ™‚é–“ (ç§’)', fontsize=11)\n",
    "    axes[1].set_title('è™•ç†æ™‚é–“å°æ¯” (100 æ¬¡è¿­ä»£)', fontsize=13, fontweight='bold')\n",
    "    axes[1].grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    for i, (tool, time_val) in enumerate(zip(tools, process_times)):\n",
    "        axes[1].text(time_val, i, f' {time_val:.3f}s', va='center', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nğŸ’¡ æ€§èƒ½è§€å¯Ÿ:\")\n",
    "    print(\"  1. spaCy: è¼‰å…¥æ™‚é–“è¼ƒé•·,ä½†è™•ç†é€Ÿåº¦æœ€å¿« (ç”Ÿç”¢ç’°å¢ƒå„ªå‹¢)\")\n",
    "    print(\"  2. NLTK: ç„¡éœ€è¼‰å…¥,ä½†è™•ç†é€Ÿåº¦æ…¢ (é©åˆæ•™å­¸èˆ‡å¿«é€ŸåŸå‹)\")\n",
    "    print(\"  3. jieba: è¼‰å…¥å¿«é€Ÿ,ä¸­æ–‡è™•ç†é«˜æ•ˆ (ä¸­æ–‡é¦–é¸)\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ ç„¡å¯ç”¨æ¸¬è©¦çµæœ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 NLTK vs spaCy è©³ç´°å°æ¯”å¯¦é©—\n",
    "\n",
    "ä½¿ç”¨ç›¸åŒæ–‡æœ¬,æ¸¬è©¦å…©å€‹å·¥å…·åœ¨ä¸åŒä»»å‹™ä¸Šçš„è¡¨ç¾:\n",
    "- åˆ†è© (Tokenization)\n",
    "- è©æ€§æ¨™è¨» (POS Tagging)\n",
    "- å¥å­åˆ†å‰² (Sentence Segmentation)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK vs spaCy è©³ç´°å°æ¯”\n",
    "import time\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"NLTK vs spaCy è©³ç´°å°æ¯”å¯¦é©—\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# æº–å‚™æ¸¬è©¦æ–‡æœ¬ (10,000 è©)\n",
    "base_text = \"\"\"\n",
    "The quick brown fox jumps over the lazy dog. Natural language processing \n",
    "is a subfield of linguistics and artificial intelligence. It enables \n",
    "computers to understand and generate human languages.\n",
    "\"\"\"\n",
    "test_text = base_text * 100\n",
    "word_count = len(test_text.split())\n",
    "\n",
    "print(f\"\\næ¸¬è©¦æ–‡æœ¬é•·åº¦: ç´„ {word_count:,} è©\\n\")\n",
    "\n",
    "results_comparison = []\n",
    "\n",
    "# NLTK æ¸¬è©¦\n",
    "try:\n",
    "    import nltk\n",
    "    from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "    \n",
    "    # åˆ†è©\n",
    "    start = time.time()\n",
    "    tokens_nltk = word_tokenize(test_text)\n",
    "    time_tokenize = time.time() - start\n",
    "    \n",
    "    # å¥å­åˆ†å‰²\n",
    "    start = time.time()\n",
    "    sentences_nltk = sent_tokenize(test_text)\n",
    "    time_sentence = time.time() - start\n",
    "    \n",
    "    # è©æ€§æ¨™è¨»\n",
    "    start = time.time()\n",
    "    pos_nltk = nltk.pos_tag(tokens_nltk)\n",
    "    time_pos = time.time() - start\n",
    "    \n",
    "    results_comparison.append({\n",
    "        'å·¥å…·': 'NLTK',\n",
    "        'åˆ†è©(ç§’)': f\"{time_tokenize:.3f}\",\n",
    "        'å¥å­åˆ†å‰²(ç§’)': f\"{time_sentence:.3f}\",\n",
    "        'è©æ€§æ¨™è¨»(ç§’)': f\"{time_pos:.3f}\",\n",
    "        'ç¸½æ™‚é–“(ç§’)': f\"{time_tokenize + time_sentence + time_pos:.3f}\"\n",
    "    })\n",
    "    \n",
    "    nltk_total = time_tokenize + time_sentence + time_pos\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ NLTK æ¸¬è©¦å¤±æ•—: {e}\")\n",
    "    nltk_total = None\n",
    "\n",
    "# spaCy æ¸¬è©¦\n",
    "try:\n",
    "    import spacy\n",
    "    \n",
    "    try:\n",
    "        nlp = spacy.load('en_core_web_sm')\n",
    "    except:\n",
    "        nlp = spacy.blank('en')\n",
    "    \n",
    "    # Pipeline è™•ç† (ä¸€æ¬¡å®Œæˆæ‰€æœ‰ä»»å‹™)\n",
    "    start_total = time.time()\n",
    "    \n",
    "    start = time.time()\n",
    "    doc = nlp(test_text)\n",
    "    time_pipeline = time.time() - start\n",
    "    \n",
    "    # æå–çµæœ\n",
    "    tokens_spacy = [token for token in doc]\n",
    "    sentences_spacy = list(doc.sents)\n",
    "    pos_spacy = [(token.text, token.pos_) for token in doc]\n",
    "    \n",
    "    total_time = time.time() - start_total\n",
    "    \n",
    "    results_comparison.append({\n",
    "        'å·¥å…·': 'spaCy',\n",
    "        'åˆ†è©(ç§’)': '-',\n",
    "        'å¥å­åˆ†å‰²(ç§’)': '-',\n",
    "        'è©æ€§æ¨™è¨»(ç§’)': '-',\n",
    "        'ç¸½æ™‚é–“(ç§’)': f\"{total_time:.3f} (Pipeline)\"\n",
    "    })\n",
    "    \n",
    "    spacy_total = total_time\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ spaCy æ¸¬è©¦å¤±æ•—: {e}\")\n",
    "    spacy_total = None\n",
    "\n",
    "# é¡¯ç¤ºçµæœ\n",
    "if results_comparison:\n",
    "    df_comp = pd.DataFrame(results_comparison)\n",
    "    print(\"ğŸ“Š è©³ç´°å°æ¯”çµæœ\")\n",
    "    print(\"=\" * 70)\n",
    "    print(df_comp.to_string(index=False))\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    if nltk_total and spacy_total:\n",
    "        speedup = nltk_total / spacy_total\n",
    "        print(f\"\\nğŸš€ spaCy åŠ é€Ÿæ¯”: {speedup:.1f}x\")\n",
    "        print(f\"   è™•ç†æ•ˆç‡: {word_count / spacy_total:.0f} è©/ç§’ (spaCy) vs {word_count / nltk_total:.0f} è©/ç§’ (NLTK)\")\n",
    "    \n",
    "    # è¦–è¦ºåŒ–\n",
    "    if nltk_total and spacy_total:\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        \n",
    "        tools = ['NLTK\\n(é€æ­¥è™•ç†)', 'spaCy\\n(Pipeline)']\n",
    "        times = [nltk_total, spacy_total]\n",
    "        colors = ['#F18F01', '#2E86AB']\n",
    "        \n",
    "        bars = ax.bar(tools, times, color=colors, alpha=0.7, edgecolor='black', width=0.5)\n",
    "        ax.set_ylabel('ç¸½è™•ç†æ™‚é–“ (ç§’)', fontsize=12)\n",
    "        ax.set_title(f'NLTK vs spaCy å®Œæ•´æµç¨‹å°æ¯” ({word_count:,} è©)', \n",
    "                    fontsize=14, fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # æ¨™è¨»æ•¸å€¼èˆ‡åŠ é€Ÿæ¯”\n",
    "        for bar, time_val in zip(bars, times):\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{time_val:.3f}s\\n({word_count/time_val:.0f} è©/ç§’)',\n",
    "                   ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "        \n",
    "        # åŠ é€Ÿæ¯”æ¨™è¨»\n",
    "        ax.text(0.5, max(times) * 0.8, f'{speedup:.1f}x\\nåŠ é€Ÿ',\n",
    "               ha='center', fontsize=14, fontweight='bold',\n",
    "               bbox=dict(boxstyle='round,pad=0.5', facecolor='yellow', alpha=0.7))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    print(\"\\nğŸ’¡ é—œéµæ´å¯Ÿ:\")\n",
    "    print(\"  1. spaCy Pipeline è¨­è¨ˆ: ä¸€æ¬¡è™•ç†å®Œæˆæ‰€æœ‰ä»»å‹™,æ•ˆç‡æ¥µé«˜\")\n",
    "    print(\"  2. NLTK æ¨¡çµ„åŒ–: æ¯å€‹ä»»å‹™ç¨ç«‹èª¿ç”¨,éˆæ´»ä½†æ…¢\")\n",
    "    print(\"  3. ç”Ÿç”¢ç’°å¢ƒé¸æ“‡: spaCy æ˜¯æ˜ç¢ºè´å®¶\")\n",
    "    print(\"  4. æ•™å­¸èˆ‡ç ”ç©¶: NLTK æ›´æ˜“ç†è§£å…§éƒ¨æ©Ÿåˆ¶\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"7\"></a>\n",
    "## 7. é¸æ“‡æ±ºç­–æ¨¹èˆ‡æœ€ä½³å¯¦è¸\n",
    "\n",
    "### 7.1 å·¥å…·é¸æ“‡æ±ºç­–æ¨¹\n",
    "\n",
    "```\n",
    "éœ€è¦è™•ç†å“ªç¨®èªè¨€?\n",
    "â”‚\n",
    "â”œâ”€ ä¸­æ–‡\n",
    "â”‚   â”œâ”€ åªéœ€åˆ†è©? â†’ jieba (æœ€å¿«é€Ÿ)\n",
    "â”‚   â”œâ”€ éœ€è¦å®Œæ•´æµç¨‹? â†’ LTP / spaCy ä¸­æ–‡æ¨¡å‹\n",
    "â”‚   â”œâ”€ éœ€è¦é«˜æº–ç¢ºç‡? â†’ LTP / HanLP\n",
    "â”‚   â””â”€ éœ€è¦ SOTA? â†’ Transformers (BERT-chinese)\n",
    "â”‚\n",
    "â”œâ”€ è‹±æ–‡\n",
    "â”‚   â”œâ”€ å­¸ç¿’/æ•™å­¸? â†’ NLTK\n",
    "â”‚   â”œâ”€ å¿«é€ŸåŸå‹? â†’ NLTK / TextBlob\n",
    "â”‚   â”œâ”€ ç”Ÿç”¢ç’°å¢ƒ? â†’ spaCy\n",
    "â”‚   â”œâ”€ æœ€é«˜æº–ç¢ºç‡? â†’ Transformers (RoBERTa)\n",
    "â”‚   â””â”€ ç‰¹å®šä»»å‹™? â†’ Transformers Pipeline\n",
    "â”‚\n",
    "â””â”€ å¤šèªè¨€\n",
    "    â”œâ”€ å¸¸è¦‹èªè¨€ (20+)? â†’ spaCy\n",
    "    â”œâ”€ ç¨€æœ‰èªè¨€? â†’ Transformers (XLM-R, mBERT)\n",
    "    â”œâ”€ å¤šæ¨¡æ…‹? â†’ Transformers (CLIP, LayoutLM)\n",
    "    â””â”€ è‡ªå®šç¾©éœ€æ±‚? â†’ AllenNLP / fairseq\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 7.2 å ´æ™¯åŒ–æœ€ä½³å¯¦è¸\n",
    "\n",
    "#### å ´æ™¯ 1: å­¸è¡“ç ”ç©¶èˆ‡æ•™å­¸\n",
    "- **æ¨è–¦:** NLTK + Transformers\n",
    "- **åŸå› :** NLTK æ˜“æ–¼ç†è§£ç®—æ³•,Transformers æä¾› SOTA æ¨¡å‹\n",
    "- **æŠ€è¡“æ£§:** Python + NLTK + Jupyter + Transformers\n",
    "\n",
    "#### å ´æ™¯ 2: ç”Ÿç”¢ç’°å¢ƒéƒ¨ç½²\n",
    "- **æ¨è–¦:** spaCy + Transformers (å¾®èª¿)\n",
    "- **åŸå› :** spaCy é«˜æ•ˆèƒ½,Transformers é«˜æº–ç¢ºç‡\n",
    "- **æŠ€è¡“æ£§:** FastAPI + spaCy + Docker + ONNX\n",
    "\n",
    "#### å ´æ™¯ 3: ä¸­æ–‡æ–‡æœ¬åˆ†æ\n",
    "- **æ¨è–¦:** jieba + Transformers (BERT-chinese)\n",
    "- **åŸå› :** jieba åˆ†è©å¿«,BERT èªç¾©ç†è§£å¼·\n",
    "- **æŠ€è¡“æ£§:** jieba + PyTorch + Transformers\n",
    "\n",
    "#### å ´æ™¯ 4: å¿«é€ŸåŸå‹é–‹ç™¼\n",
    "- **æ¨è–¦:** TextBlob / Transformers Pipeline\n",
    "- **åŸå› :** ç°¡åŒ– API,é–‹ç®±å³ç”¨\n",
    "- **æŠ€è¡“æ£§:** Python + TextBlob + Streamlit\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ±ºç­–æ¨¹è¦–è¦ºåŒ–\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 12))\n",
    "ax.set_xlim(0, 10)\n",
    "ax.set_ylim(0, 12)\n",
    "ax.axis('off')\n",
    "\n",
    "# æ¨™é¡Œ\n",
    "ax.text(5, 11.5, 'NLP å·¥å…·é¸æ“‡æ±ºç­–æ¨¹', ha='center', \n",
    "        fontsize=18, fontweight='bold')\n",
    "\n",
    "# æ ¹ç¯€é»\n",
    "root = mpatches.FancyBboxPatch((3.5, 10), 3, 0.8,\n",
    "                                boxstyle=\"round,pad=0.1\",\n",
    "                                linewidth=2, edgecolor='black',\n",
    "                                facecolor='#F18F01', alpha=0.7)\n",
    "ax.add_patch(root)\n",
    "ax.text(5, 10.4, 'éœ€è¦è™•ç†å“ªç¨®èªè¨€?', ha='center', va='center',\n",
    "        fontsize=13, fontweight='bold')\n",
    "\n",
    "# ç¬¬ä¸€å±¤: èªè¨€åˆ†é¡\n",
    "languages = [\n",
    "    (1.5, 8, 'ä¸­æ–‡', '#FFE66D'),\n",
    "    (5, 8, 'è‹±æ–‡', '#95E1D3'),\n",
    "    (8.5, 8, 'å¤šèªè¨€', '#2E86AB')\n",
    "]\n",
    "\n",
    "for x, y, lang, color in languages:\n",
    "    rect = mpatches.FancyBboxPatch((x-0.6, y-0.3), 1.2, 0.6,\n",
    "                                    boxstyle=\"round,pad=0.05\",\n",
    "                                    linewidth=2, edgecolor='black',\n",
    "                                    facecolor=color, alpha=0.6)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(x, y, lang, ha='center', va='center',\n",
    "            fontsize=11, fontweight='bold')\n",
    "    \n",
    "    # é€£ç·šåˆ°æ ¹ç¯€é»\n",
    "    ax.plot([5, x], [10, y+0.35], 'k-', linewidth=1.5, alpha=0.5)\n",
    "\n",
    "# ç¬¬äºŒå±¤: å…·é«”å·¥å…·\n",
    "tools = [\n",
    "    # ä¸­æ–‡\n",
    "    (0.5, 6.5, 'jieba\\n(åˆ†è©)', '#FFE66D'),\n",
    "    (1.5, 6.5, 'LTP/HanLP\\n(å®Œæ•´æµç¨‹)', '#FFE66D'),\n",
    "    (2.5, 6.5, 'BERT-chinese\\n(SOTA)', '#FFE66D'),\n",
    "    \n",
    "    # è‹±æ–‡\n",
    "    (4, 6.5, 'NLTK\\n(æ•™å­¸)', '#95E1D3'),\n",
    "    (5, 6.5, 'spaCy\\n(ç”Ÿç”¢)', '#95E1D3'),\n",
    "    (6, 6.5, 'Transformers\\n(SOTA)', '#95E1D3'),\n",
    "    \n",
    "    # å¤šèªè¨€\n",
    "    (7.5, 6.5, 'spaCy\\n(20+ èªè¨€)', '#2E86AB'),\n",
    "    (8.5, 6.5, 'XLM-R\\n(100+ èªè¨€)', '#2E86AB'),\n",
    "    (9.5, 6.5, 'CLIP\\n(å¤šæ¨¡æ…‹)', '#2E86AB'),\n",
    "]\n",
    "\n",
    "for x, y, tool, color in tools:\n",
    "    rect = mpatches.FancyBboxPatch((x-0.4, y-0.4), 0.8, 0.8,\n",
    "                                    boxstyle=\"round,pad=0.05\",\n",
    "                                    linewidth=1.5, edgecolor='black',\n",
    "                                    facecolor=color, alpha=0.4)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(x, y, tool, ha='center', va='center',\n",
    "            fontsize=8, fontweight='bold')\n",
    "    \n",
    "    # é€£ç·šåˆ°å°æ‡‰èªè¨€\n",
    "    if x < 3:\n",
    "        ax.plot([1.5, x], [7.7, y+0.45], 'k-', linewidth=1, alpha=0.3)\n",
    "    elif x < 7:\n",
    "        ax.plot([5, x], [7.7, y+0.45], 'k-', linewidth=1, alpha=0.3)\n",
    "    else:\n",
    "        ax.plot([8.5, x], [7.7, y+0.45], 'k-', linewidth=1, alpha=0.3)\n",
    "\n",
    "# åº•éƒ¨: æ‡‰ç”¨å ´æ™¯\n",
    "scenarios = [\n",
    "    (2, 4.5, 'å­¸è¡“ç ”ç©¶', 'NLTK + Transformers'),\n",
    "    (5, 4.5, 'ç”Ÿç”¢ç’°å¢ƒ', 'spaCy + Docker'),\n",
    "    (8, 4.5, 'å¿«é€ŸåŸå‹', 'Pipeline API')\n",
    "]\n",
    "\n",
    "for x, y, scenario, stack in scenarios:\n",
    "    # å ´æ™¯æ¡†\n",
    "    rect = mpatches.FancyBboxPatch((x-1, y-0.5), 2, 1,\n",
    "                                    boxstyle=\"round,pad=0.1\",\n",
    "                                    linewidth=2, edgecolor='#C73E1D',\n",
    "                                    facecolor='white', alpha=0.8)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(x, y+0.2, scenario, ha='center', va='center',\n",
    "            fontsize=11, fontweight='bold', color='#C73E1D')\n",
    "    ax.text(x, y-0.2, stack, ha='center', va='center',\n",
    "            fontsize=9)\n",
    "\n",
    "# åœ–ä¾‹\n",
    "ax.text(5, 3, 'ğŸ’¡ é¸æ“‡åŸå‰‡: èªè¨€ â†’ ä»»å‹™éœ€æ±‚ â†’ æ€§èƒ½è¦æ±‚ â†’ éƒ¨ç½²ç’°å¢ƒ',\n",
    "        ha='center', fontsize=11, style='italic',\n",
    "        bbox=dict(boxstyle='round,pad=0.5', facecolor='lightyellow', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ’¡ æ±ºç­–è¦é»:\")\n",
    "print(\"  1. èªè¨€å„ªå…ˆ: ä¸­æ–‡å¿…é¸ jieba/LTP,è‹±æ–‡ spaCy,å¤šèªè¨€ Transformers\")\n",
    "print(\"  2. ä»»å‹™éœ€æ±‚: æ•™å­¸ç”¨ NLTK,ç”Ÿç”¢ç”¨ spaCy,SOTA ç”¨ Transformers\")\n",
    "print(\"  3. æ€§èƒ½æ¬Šè¡¡: é€Ÿåº¦ vs æº–ç¢ºç‡ (spaCy å¿«, Transformers æº–)\")\n",
    "print(\"  4. éƒ¨ç½²ç’°å¢ƒ: CPU â†’ spaCy, GPU â†’ Transformers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"8\"></a>\n",
    "## 8. å®Œæ•´æŠ€è¡“æ£§ç¯„ä¾‹\n",
    "\n",
    "### 8.1 å ´æ™¯: ä¸­æ–‡æƒ…æ„Ÿåˆ†æç³»çµ±\n",
    "\n",
    "**éœ€æ±‚:**\n",
    "- åˆ†æé›»å•†è©•è«–çš„æƒ…æ„Ÿå‚¾å‘ (æ­£é¢/è² é¢/ä¸­æ€§)\n",
    "- æ”¯æ´å¯¦æ™‚æ¨ç† (< 100ms)\n",
    "- æº–ç¢ºç‡è¦æ±‚ > 90%\n",
    "\n",
    "**æŠ€è¡“æ£§é¸æ“‡:**\n",
    "1. **åˆ†è©:** jieba (ä¸­æ–‡åˆ†è©)\n",
    "2. **æ¨¡å‹:** BERT-chinese (é«˜æº–ç¢ºç‡)\n",
    "3. **éƒ¨ç½²:** FastAPI + Docker (ç”Ÿç”¢å°±ç·’)\n",
    "4. **å„ªåŒ–:** ONNX (æ¨ç†åŠ é€Ÿ)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®Œæ•´æŠ€è¡“æ£§ç¯„ä¾‹: ä¸­æ–‡æƒ…æ„Ÿåˆ†æ\n",
    "print(\"=\" * 70)\n",
    "print(\"å®Œæ•´æŠ€è¡“æ£§ç¯„ä¾‹: ä¸­æ–‡æƒ…æ„Ÿåˆ†æç³»çµ±\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# æ¨¡æ“¬å®Œæ•´æµç¨‹ (ç°¡åŒ–ç‰ˆ)\n",
    "\n",
    "class ChineseSentimentAnalyzer:\n",
    "    \"\"\"\n",
    "    ä¸­æ–‡æƒ…æ„Ÿåˆ†æå™¨\n",
    "    \n",
    "    æŠ€è¡“æ£§:\n",
    "    - åˆ†è©: jieba\n",
    "    - æ¨¡å‹: BERT-chinese (Transformers)\n",
    "    - éƒ¨ç½²: FastAPI\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        print(\"\\nğŸ”§ åˆå§‹åŒ–æƒ…æ„Ÿåˆ†æå™¨...\")\n",
    "        \n",
    "        # 1. è¼‰å…¥åˆ†è©å·¥å…·\n",
    "        try:\n",
    "            import jieba\n",
    "            self.jieba = jieba\n",
    "            print(\"  âœ… jieba åˆ†è©å™¨å·²è¼‰å…¥\")\n",
    "        except ImportError:\n",
    "            print(\"  âš ï¸ jieba æœªå®‰è£\")\n",
    "            self.jieba = None\n",
    "        \n",
    "        # 2. è¼‰å…¥æƒ…æ„Ÿåˆ†ææ¨¡å‹ (æ¨¡æ“¬)\n",
    "        try:\n",
    "            from transformers import pipeline\n",
    "            # å¯¦éš›æ‡‰ä½¿ç”¨ä¸­æ–‡ BERT æ¨¡å‹\n",
    "            # self.sentiment = pipeline('sentiment-analysis', model='bert-base-chinese')\n",
    "            self.sentiment = pipeline('sentiment-analysis')\n",
    "            print(\"  âœ… BERT æƒ…æ„Ÿåˆ†ææ¨¡å‹å·²è¼‰å…¥\")\n",
    "        except Exception as e:\n",
    "            print(f\"  âš ï¸ æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}\")\n",
    "            self.sentiment = None\n",
    "        \n",
    "        # 3. ç°¡å–®æƒ…æ„Ÿè©å…¸ (å‚™ç”¨)\n",
    "        self.positive_words = {'å¥½', 'æ£’', 'å„ªç§€', 'å–œæ­¡', 'æ¨è–¦', 'æ»¿æ„', 'å®Œç¾'}\n",
    "        self.negative_words = {'å·®', 'ç³Ÿ', 'çˆ›', 'è¨å­', 'å¤±æœ›', 'åƒåœ¾', 'é€€è²¨'}\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        \"\"\"æ–‡æœ¬é è™•ç†\"\"\"\n",
    "        if self.jieba:\n",
    "            words = list(self.jieba.cut(text))\n",
    "            return ' '.join(words)\n",
    "        return text\n",
    "    \n",
    "    def analyze_rule_based(self, text):\n",
    "        \"\"\"åŸºæ–¼è¦å‰‡çš„æƒ…æ„Ÿåˆ†æ (å‚™ç”¨)\"\"\"\n",
    "        if not self.jieba:\n",
    "            return {'label': 'NEUTRAL', 'score': 0.5}\n",
    "        \n",
    "        words = set(self.jieba.cut(text))\n",
    "        pos_count = len(words & self.positive_words)\n",
    "        neg_count = len(words & self.negative_words)\n",
    "        \n",
    "        if pos_count > neg_count:\n",
    "            return {'label': 'POSITIVE', 'score': 0.7 + pos_count * 0.1}\n",
    "        elif neg_count > pos_count:\n",
    "            return {'label': 'NEGATIVE', 'score': 0.7 + neg_count * 0.1}\n",
    "        else:\n",
    "            return {'label': 'NEUTRAL', 'score': 0.5}\n",
    "    \n",
    "    def analyze(self, text):\n",
    "        \"\"\"æƒ…æ„Ÿåˆ†æä¸»å‡½æ•¸\"\"\"\n",
    "        # é è™•ç†\n",
    "        processed_text = self.preprocess(text)\n",
    "        \n",
    "        # ä½¿ç”¨æ¨¡å‹åˆ†æ (å¦‚å¯ç”¨)\n",
    "        if self.sentiment:\n",
    "            try:\n",
    "                result = self.sentiment(processed_text)[0]\n",
    "                return result\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # å¦å‰‡ä½¿ç”¨è¦å‰‡æ–¹æ³•\n",
    "        return self.analyze_rule_based(text)\n",
    "\n",
    "# æ¸¬è©¦åˆ†æå™¨\n",
    "analyzer = ChineseSentimentAnalyzer()\n",
    "\n",
    "# æ¸¬è©¦æ¨£æœ¬\n",
    "test_reviews = [\n",
    "    \"é€™å€‹ç”¢å“è³ªé‡éå¸¸å¥½,å¼·çƒˆæ¨è–¦è³¼è²·!\",\n",
    "    \"æ”¶åˆ°è²¨ç‰©å®Œå…¨ä¸ç¬¦åˆæè¿°,å¤ªå¤±æœ›äº†,ç”³è«‹é€€è²¨ã€‚\",\n",
    "    \"é‚„å¯ä»¥å§,æ²’æœ‰ç‰¹åˆ¥é©šè‰·,ä¹Ÿæ²’æœ‰å¾ˆå·®ã€‚\",\n",
    "    \"ç‰©æµè¶…å¿«,åŒ…è£å®Œç¾,å•†å“è³ªé‡å„ªç§€,äº”æ˜Ÿå¥½è©•!\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"æƒ…æ„Ÿåˆ†ææ¸¬è©¦\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results = []\n",
    "for i, review in enumerate(test_reviews, 1):\n",
    "    result = analyzer.analyze(review)\n",
    "    \n",
    "    # è½‰æ›æ¨™ç±¤ç‚ºä¸­æ–‡\n",
    "    label_map = {\n",
    "        'POSITIVE': 'æ­£é¢',\n",
    "        'NEGATIVE': 'è² é¢',\n",
    "        'NEUTRAL': 'ä¸­æ€§'\n",
    "    }\n",
    "    label_zh = label_map.get(result['label'], result['label'])\n",
    "    \n",
    "    print(f\"\\n[{i}] è©•è«–: {review}\")\n",
    "    print(f\"    æƒ…æ„Ÿ: {label_zh} (ä¿¡å¿ƒåº¦: {result['score']:.2%})\")\n",
    "    \n",
    "    results.append({\n",
    "        'è©•è«–': review[:30] + '...' if len(review) > 30 else review,\n",
    "        'æƒ…æ„Ÿ': label_zh,\n",
    "        'ä¿¡å¿ƒåº¦': f\"{result['score']:.2%}\"\n",
    "    })\n",
    "\n",
    "# è¦–è¦ºåŒ–çµæœ\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "table = ax.table(cellText=df_results.values, colLabels=df_results.columns,\n",
    "                cellLoc='left', loc='center',\n",
    "                colWidths=[0.6, 0.2, 0.2])\n",
    "\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1, 2.5)\n",
    "\n",
    "# è¨­å®šè¡¨é ­æ¨£å¼\n",
    "for i in range(len(df_results.columns)):\n",
    "    table[(0, i)].set_facecolor('#2E86AB')\n",
    "    table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "\n",
    "# è¨­å®šè¡Œé¡è‰² (æ ¹æ“šæƒ…æ„Ÿ)\n",
    "for i in range(1, len(df_results) + 1):\n",
    "    sentiment = df_results.iloc[i-1]['æƒ…æ„Ÿ']\n",
    "    if sentiment == 'æ­£é¢':\n",
    "        color = '#95E1D3'\n",
    "    elif sentiment == 'è² é¢':\n",
    "        color = '#F18F01'\n",
    "    else:\n",
    "        color = '#FFE66D'\n",
    "    \n",
    "    for j in range(len(df_results.columns)):\n",
    "        table[(i, j)].set_facecolor(color)\n",
    "        table[(i, j)].set_alpha(0.4)\n",
    "\n",
    "plt.title('ä¸­æ–‡æƒ…æ„Ÿåˆ†æçµæœ', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… æƒ…æ„Ÿåˆ†æç³»çµ±æ¸¬è©¦å®Œæˆ!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nğŸš€ ç”Ÿç”¢éƒ¨ç½²æ­¥é©Ÿ:\")\n",
    "print(\"  1. ä½¿ç”¨ BERT-chinese å¾®èª¿æ¨¡å‹\")\n",
    "print(\"  2. è½‰æ›ç‚º ONNX æ ¼å¼åŠ é€Ÿæ¨ç†\")\n",
    "print(\"  3. ä½¿ç”¨ FastAPI åŒ…è£ç‚º API\")\n",
    "print(\"  4. Docker å®¹å™¨åŒ–éƒ¨ç½²\")\n",
    "print(\"  5. Kubernetes æ°´å¹³æ“´å±•\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FastAPI éƒ¨ç½²ç¯„ä¾‹ (ä»£ç¢¼ç¤ºç¯„,ç„¡éœ€åŸ·è¡Œ)\n",
    "print(\"=\" * 70)\n",
    "print(\"FastAPI éƒ¨ç½²ç¯„ä¾‹ä»£ç¢¼\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "api_code = '''\n",
    "# main.py - FastAPI æƒ…æ„Ÿåˆ†æ API\n",
    "\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "import jieba\n",
    "from transformers import pipeline\n",
    "\n",
    "# å‰µå»º FastAPI æ‡‰ç”¨\n",
    "app = FastAPI(\n",
    "    title=\"ä¸­æ–‡æƒ…æ„Ÿåˆ†æ API\",\n",
    "    description=\"åŸºæ–¼ BERT çš„ä¸­æ–‡æƒ…æ„Ÿåˆ†ææœå‹™\",\n",
    "    version=\"1.0.0\"\n",
    ")\n",
    "\n",
    "# è¼‰å…¥æ¨¡å‹ (å•Ÿå‹•æ™‚åŸ·è¡Œä¸€æ¬¡)\n",
    "sentiment_model = pipeline(\n",
    "    'sentiment-analysis',\n",
    "    model='bert-base-chinese'\n",
    ")\n",
    "\n",
    "# è«‹æ±‚æ¨¡å‹\n",
    "class TextRequest(BaseModel):\n",
    "    text: str\n",
    "\n",
    "# å›æ‡‰æ¨¡å‹\n",
    "class SentimentResponse(BaseModel):\n",
    "    text: str\n",
    "    sentiment: str\n",
    "    confidence: float\n",
    "\n",
    "@app.post(\"/analyze\", response_model=SentimentResponse)\n",
    "async def analyze_sentiment(request: TextRequest):\n",
    "    \"\"\"\n",
    "    æƒ…æ„Ÿåˆ†æ API\n",
    "    \n",
    "    Args:\n",
    "        request: åŒ…å«å¾…åˆ†ææ–‡æœ¬çš„è«‹æ±‚\n",
    "        \n",
    "    Returns:\n",
    "        SentimentResponse: æƒ…æ„Ÿåˆ†æçµæœ\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # åˆ†è©é è™•ç†\n",
    "        words = jieba.cut(request.text)\n",
    "        processed_text = ' '.join(words)\n",
    "        \n",
    "        # æ¨¡å‹æ¨ç†\n",
    "        result = sentiment_model(processed_text)[0]\n",
    "        \n",
    "        return SentimentResponse(\n",
    "            text=request.text,\n",
    "            sentiment=result['label'],\n",
    "            confidence=result['score']\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health_check():\n",
    "    \"\"\"å¥åº·æª¢æŸ¥\"\"\"\n",
    "    return {\"status\": \"healthy\"}\n",
    "\n",
    "# å•Ÿå‹•å‘½ä»¤:\n",
    "# uvicorn main:app --host 0.0.0.0 --port 8000\n",
    "\n",
    "# æ¸¬è©¦ API:\n",
    "# curl -X POST \"http://localhost:8000/analyze\" \\\n",
    "#      -H \"Content-Type: application/json\" \\\n",
    "#      -d '{\"text\": \"é€™å€‹ç”¢å“è³ªé‡éå¸¸å¥½!\"}'\n",
    "'''\n",
    "\n",
    "print(api_code)\n",
    "\n",
    "print(\"\\nğŸ’¡ éƒ¨ç½²æœ€ä½³å¯¦è¸:\")\n",
    "print(\"  1. æ¨¡å‹é è¼‰å…¥: æ‡‰ç”¨å•Ÿå‹•æ™‚è¼‰å…¥æ¨¡å‹ (é¿å…æ¯æ¬¡è«‹æ±‚è¼‰å…¥)\")\n",
    "print(\"  2. æ‰¹æ¬¡æ¨ç†: ç´¯ç©å¤šå€‹è«‹æ±‚æ‰¹æ¬¡è™•ç† (æå‡ååé‡)\")\n",
    "print(\"  3. å¿«å–çµæœ: ç›¸åŒè¼¸å…¥å¿«å–çµæœ (æ¸›å°‘è¨ˆç®—)\")\n",
    "print(\"  4. é™æµä¿è­·: é˜²æ­¢éè¼‰ (nginx/API Gateway)\")\n",
    "print(\"  5. ç›£æ§å‘Šè­¦: Prometheus + Grafana (æ€§èƒ½ç›£æ§)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"9\"></a>\n",
    "## 9. æœ¬èª²ç¸½çµèˆ‡å»¶ä¼¸é–±è®€\n",
    "\n",
    "### 9.1 æ ¸å¿ƒè¦é»å›é¡§\n",
    "\n",
    "#### 1. Python NLP å·¥å…·ç”Ÿæ…‹\n",
    "- **å››å¤§é¡å·¥å…·:** å‚³çµ± NLP (NLTK/spaCy)ã€æ·±åº¦å­¸ç¿’ (Transformers)ã€ä¸­æ–‡å°ˆç”¨ (jieba/LTP)ã€è¼”åŠ©å·¥å…· (Gensim)\n",
    "- **å·¥å…·å®šä½:** NLTK (æ•™å­¸)ã€spaCy (ç”Ÿç”¢)ã€Transformers (SOTA)\n",
    "\n",
    "#### 2. å·¥å…·å°æ¯”èˆ‡é¸æ“‡\n",
    "- **NLTK:** åŠŸèƒ½å…¨é¢,æ˜“æ–¼å­¸ç¿’,é€Ÿåº¦æ…¢\n",
    "- **spaCy:** å·¥æ¥­ç´š,é«˜æ•ˆèƒ½ (10-100x åŠ é€Ÿ),Pipeline è¨­è¨ˆ\n",
    "- **Transformers:** é è¨“ç·´æ¨¡å‹åº«,çµ±ä¸€ API,SOTA æ€§èƒ½\n",
    "- **jieba:** ä¸­æ–‡åˆ†è©é¦–é¸,è¼•é‡å¿«é€Ÿ\n",
    "\n",
    "#### 3. æ•ˆèƒ½åŸºæº–æ¸¬è©¦\n",
    "- **é€Ÿåº¦:** spaCy > jieba > NLTK > Transformers\n",
    "- **æº–ç¢ºç‡:** Transformers > spaCy > LTP > NLTK > jieba\n",
    "- **å­¸ç¿’æ›²ç·š:** NLTK < jieba < spaCy < Transformers\n",
    "\n",
    "#### 4. é¸æ“‡æ±ºç­–æ¨¹\n",
    "- **ä¸­æ–‡:** jieba (åˆ†è©) + LTP/BERT (å®Œæ•´æµç¨‹/SOTA)\n",
    "- **è‹±æ–‡:** NLTK (æ•™å­¸) + spaCy (ç”Ÿç”¢) + Transformers (SOTA)\n",
    "- **å¤šèªè¨€:** spaCy (20+) + Transformers (100+)\n",
    "\n",
    "#### 5. å®Œæ•´æŠ€è¡“æ£§\n",
    "- **é è™•ç†:** jieba (ä¸­æ–‡) / spaCy (è‹±æ–‡)\n",
    "- **æ¨¡å‹:** BERT/RoBERTa (ç†è§£) / GPT (ç”Ÿæˆ) / T5 (è½‰æ›)\n",
    "- **éƒ¨ç½²:** FastAPI + Docker + ONNX\n",
    "\n",
    "---\n",
    "\n",
    "### 9.2 æœ€ä½³å¯¦è¸ç¸½çµ\n",
    "\n",
    "| å ´æ™¯ | æ¨è–¦å·¥å…·çµ„åˆ | æŠ€è¡“æ£§ |\n",
    "|:---|:---|:---|\n",
    "| **å­¸è¡“ç ”ç©¶** | NLTK + Transformers | Python + Jupyter + PyTorch |\n",
    "| **ç”Ÿç”¢ç’°å¢ƒ** | spaCy + BERT (å¾®èª¿) | FastAPI + Docker + ONNX |\n",
    "| **ä¸­æ–‡åˆ†æ** | jieba + BERT-chinese | jieba + Transformers + PyTorch |\n",
    "| **å¿«é€ŸåŸå‹** | Transformers Pipeline | Python + Streamlit |\n",
    "| **å¤šèªè¨€** | spaCy + XLM-R | spaCy + Transformers |\n",
    "\n",
    "---\n",
    "\n",
    "### 9.3 ä¸‹ç¯€é å‘Š\n",
    "\n",
    "**CH03-01: æ–‡æœ¬é è™•ç†åŸºç¤**\n",
    "\n",
    "æˆ‘å€‘å°‡æ·±å…¥æ¢è¨:\n",
    "- æ–‡æœ¬æ¸…æ´—èˆ‡æ¨™æº–åŒ–æŠ€è¡“\n",
    "- åˆ†è©ç®—æ³•åŸç† (è¦å‰‡ã€çµ±è¨ˆã€æ·±åº¦å­¸ç¿’)\n",
    "- è©å½¢é‚„åŸèˆ‡è©å¹¹æå–\n",
    "- åœç”¨è©è™•ç†ç­–ç•¥\n",
    "- æ–‡æœ¬æ¨™æº–åŒ–èˆ‡æ­£å‰‡åŒ–\n",
    "\n",
    "---\n",
    "\n",
    "### 9.4 å»¶ä¼¸é–±è®€\n",
    "\n",
    "#### å®˜æ–¹æ–‡æª”\n",
    "1. **NLTK Book:** https://www.nltk.org/book/\n",
    "2. **spaCy 101:** https://spacy.io/usage/spacy-101\n",
    "3. **Transformers æ–‡æª”:** https://huggingface.co/docs/transformers/\n",
    "4. **jieba GitHub:** https://github.com/fxsjy/jieba\n",
    "\n",
    "#### å·¥å…·å°æ¯”ç ”ç©¶\n",
    "- **spaCy vs NLTK:** https://spacy.io/usage/facts-figures\n",
    "- **NLP å·¥å…·è©•æ¸¬:** https://github.com/explosion/spacy-benchmarks\n",
    "- **ä¸­æ–‡ NLP å·¥å…·å°æ¯”:** https://github.com/didi/ChineseNLP\n",
    "\n",
    "#### å­¸ç¿’è³‡æº\n",
    "- **Hugging Face Course:** https://huggingface.co/course/\n",
    "- **spaCy Projects:** https://github.com/explosion/projects\n",
    "- **NLP å·¥å…·å¯¦æˆ°:** https://github.com/practical-nlp/practical-nlp\n",
    "\n",
    "#### è«–æ–‡èˆ‡æ›¸ç±\n",
    "- **BERT:** Devlin et al. (2018). *BERT: Pre-training of Deep Bidirectional Transformers*\n",
    "- **GPT-3:** Brown et al. (2020). *Language Models are Few-Shot Learners*\n",
    "- **æ›¸ç±:** *Natural Language Processing with Python* (NLTK å®˜æ–¹æ•™æ)\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ™‹ å•é¡Œè¨è«–\n",
    "\n",
    "æœ‰ä»»ä½•å•é¡Œå—?æ­¡è¿åœ¨è¨è«–å€æå•!\n",
    "\n",
    "---\n",
    "\n",
    "**èª²ç¨‹è³‡è¨Š:**\n",
    "- **ä½œè€…:** iSpan NLP Team\n",
    "- **åƒè€ƒè¬›ç¾©:** `èª²ç¨‹è³‡æ–™/02_è‡ªç„¶èªè¨€è™•ç†å…¥é–€/è¬›ç¾©/04_Python_NLPå·¥å…·ç”Ÿæ…‹å®Œå…¨æŒ‡å—.md`\n",
    "- **ç‰ˆæœ¬:** v1.0\n",
    "- **æœ€å¾Œæ›´æ–°:** 2025-10-17\n",
    "- **æˆæ¬Š:** MIT License (åƒ…ä¾›æ•™å­¸ä½¿ç”¨)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
