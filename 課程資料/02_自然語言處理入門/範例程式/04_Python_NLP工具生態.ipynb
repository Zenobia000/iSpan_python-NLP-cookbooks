{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH02-04: Python NLP 工具生態完全指南\n",
    "\n",
    "**課程目標:**\n",
    "- 掌握 Python NLP 主流工具的特點與選擇邏輯\n",
    "- 理解 NLTK vs spaCy vs Transformers 的定位與差異\n",
    "- 學會根據應用場景選擇合適工具\n",
    "- 了解中英文 NLP 工具的差異與最佳實踐\n",
    "- 實作效能基準測試與工具對比\n",
    "\n",
    "**學習時間:** 約 120-150 分鐘\n",
    "\n",
    "**前置知識:**\n",
    "- Python 基礎語法\n",
    "- 基本 NLP 概念 (分詞、詞性標註)\n",
    "- 機器學習基礎\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 目錄\n",
    "\n",
    "1. [Python NLP 生態系統全景圖](#1)\n",
    "2. [NLTK: 教學導向工具包](#2)\n",
    "3. [spaCy: 工業級 NLP 框架](#3)\n",
    "4. [Transformers: 預訓練模型庫](#4)\n",
    "5. [中文 NLP 工具對比](#5)\n",
    "6. [工具效能基準測試](#6)\n",
    "7. [選擇決策樹與最佳實踐](#7)\n",
    "8. [完整技術棧範例](#8)\n",
    "9. [本課總結與延伸閱讀](#9)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 環境設定與套件導入\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 設定中文顯示\n",
    "plt.rcParams['font.sans-serif'] = ['Microsoft JhengHei', 'SimHei', 'Arial Unicode MS']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 設定顯示風格\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"✅ 環境設定完成\")\n",
    "print(f\"NumPy 版本: {np.__version__}\")\n",
    "print(f\"Pandas 版本: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "## 1. Python NLP 生態系統全景圖\n",
    "\n",
    "### 1.1 工具分類與定位\n",
    "\n",
    "Python NLP 工具可分為以下四大類:\n",
    "\n",
    "#### 1. 傳統 NLP 工具\n",
    "- **NLTK** (Natural Language Toolkit): 教學導向,功能全面\n",
    "- **spaCy**: 工業級,高效能,生產部署\n",
    "\n",
    "#### 2. 深度學習框架\n",
    "- **Transformers** (Hugging Face): 預訓練模型庫\n",
    "- **fairseq** (Meta): 序列建模框架\n",
    "- **AllenNLP**: 研究導向深度學習框架\n",
    "\n",
    "#### 3. 中文專用工具\n",
    "- **jieba** (結巴): 中文分詞\n",
    "- **LTP** (Language Technology Platform): 中文全流程\n",
    "- **HanLP**: 多語言 NLP 工具包\n",
    "- **pkuseg**: 北大分詞工具\n",
    "\n",
    "#### 4. 輔助工具\n",
    "- **Gensim**: 詞向量與主題模型\n",
    "- **TextBlob**: 簡化 API,快速原型\n",
    "- **polyglot**: 多語言 NLP\n",
    "\n",
    "---\n",
    "\n",
    "### 1.2 工具選擇決策矩陣\n",
    "\n",
    "| 工具 | 語言支援 | 速度 | 準確率 | 學習曲線 | 適用場景 |\n",
    "|:---|:---|:---|:---|:---|:---|\n",
    "| **NLTK** | 英文主 | ★★☆☆☆ | ★★★☆☆ | ★☆☆☆☆ | 教學、研究、原型 |\n",
    "| **spaCy** | 20+ 語言 | ★★★★★ | ★★★★☆ | ★★☆☆☆ | 生產環境、大規模 |\n",
    "| **Transformers** | 100+ 語言 | ★★★☆☆ | ★★★★★ | ★★★☆☆ | SOTA 模型、微調 |\n",
    "| **jieba** | 中文 | ★★★★☆ | ★★★☆☆ | ★☆☆☆☆ | 中文分詞 |\n",
    "| **LTP** | 中文 | ★★★☆☆ | ★★★★☆ | ★★★☆☆ | 中文全流程 |\n",
    "| **Gensim** | 通用 | ★★★☆☆ | ★★★☆☆ | ★★☆☆☆ | 詞向量、主題模型 |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化: Python NLP 工具生態系統\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 10))\n",
    "ax.set_xlim(0, 10)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.axis('off')\n",
    "\n",
    "# 標題\n",
    "ax.text(5, 9.5, 'Python NLP 工具生態系統', ha='center', \n",
    "        fontsize=20, fontweight='bold')\n",
    "\n",
    "# 四大類工具區塊\n",
    "categories = [\n",
    "    (1.5, 7, 2, 1.5, '傳統 NLP\\n工具', '#F18F01', ['NLTK', 'spaCy']),\n",
    "    (6.5, 7, 2, 1.5, '深度學習\\n框架', '#2E86AB', ['Transformers', 'fairseq', 'AllenNLP']),\n",
    "    (1.5, 4, 2, 1.5, '中文專用\\n工具', '#95E1D3', ['jieba', 'LTP', 'HanLP', 'pkuseg']),\n",
    "    (6.5, 4, 2, 1.5, '輔助工具', '#FFE66D', ['Gensim', 'TextBlob', 'polyglot'])\n",
    "]\n",
    "\n",
    "for x, y, w, h, title, color, tools in categories:\n",
    "    # 繪製類別方框\n",
    "    rect = mpatches.FancyBboxPatch((x-w/2, y-h/2), w, h,\n",
    "                                    boxstyle=\"round,pad=0.1\",\n",
    "                                    linewidth=2, edgecolor='black',\n",
    "                                    facecolor=color, alpha=0.3)\n",
    "    ax.add_patch(rect)\n",
    "    \n",
    "    # 標題\n",
    "    ax.text(x, y+h/2-0.2, title, ha='center', va='top',\n",
    "            fontsize=13, fontweight='bold')\n",
    "    \n",
    "    # 工具列表\n",
    "    for i, tool in enumerate(tools):\n",
    "        ax.text(x, y+h/2-0.5-i*0.25, f'• {tool}', ha='center', va='top',\n",
    "                fontsize=10)\n",
    "\n",
    "# 應用層\n",
    "app_rect = mpatches.FancyBboxPatch((2, 1), 6, 1,\n",
    "                                    boxstyle=\"round,pad=0.1\",\n",
    "                                    linewidth=3, edgecolor='#C73E1D',\n",
    "                                    facecolor='#C73E1D', alpha=0.4)\n",
    "ax.add_patch(app_rect)\n",
    "ax.text(5, 1.5, '應用層: 搜尋引擎、對話系統、機器翻譯、文本分析...', \n",
    "        ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "# 連接線\n",
    "for x_pos in [2.5, 7.5]:\n",
    "    ax.arrow(x_pos, 6.2, 0, -0.6, head_width=0.15, head_length=0.1, \n",
    "             fc='gray', ec='gray', alpha=0.5)\n",
    "    ax.arrow(x_pos, 3.2, 0, -1.1, head_width=0.15, head_length=0.1,\n",
    "             fc='gray', ec='gray', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n💡 生態系統觀察:\")\n",
    "print(\"  1. 傳統工具 (NLTK/spaCy): 基礎 NLP 任務 (分詞、詞性、NER)\")\n",
    "print(\"  2. 深度學習框架 (Transformers): SOTA 模型、預訓練、微調\")\n",
    "print(\"  3. 中文專用工具 (jieba/LTP): 解決中文特有問題 (分詞、簡繁轉換)\")\n",
    "print(\"  4. 輔助工具 (Gensim): 詞向量、主題模型等特定任務\")\n",
    "print(\"  5. 所有工具最終服務於應用層\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "## 2. NLTK: 教學導向工具包\n",
    "\n",
    "### 2.1 NLTK 核心特點\n",
    "\n",
    "**定位:** 教學導向的 NLP 工具包,適合學習與研究\n",
    "\n",
    "**優勢:**\n",
    "- ✅ 功能全面: 涵蓋所有 NLP 基礎任務\n",
    "- ✅ 模組化設計: 每個功能獨立,便於理解\n",
    "- ✅ 豐富語料庫: 內建多種語言資源\n",
    "- ✅ 文檔完善: 配套書籍 *NLTK Book*\n",
    "- ✅ 社群活躍: 持續更新 20+ 年\n",
    "\n",
    "**劣勢:**\n",
    "- ❌ 速度慢: 純 Python 實作,無優化\n",
    "- ❌ 不適合生產: 無並行處理,記憶體占用高\n",
    "- ❌ 主要支援英文: 中文等語言支援有限\n",
    "\n",
    "---\n",
    "\n",
    "### 2.2 NLTK 核心功能實作\n",
    "\n",
    "#### 功能清單:\n",
    "1. 文本分詞 (Tokenization)\n",
    "2. 停用詞過濾 (Stop Words)\n",
    "3. 詞幹提取 (Stemming)\n",
    "4. 詞形還原 (Lemmatization)\n",
    "5. 詞性標註 (POS Tagging)\n",
    "6. 句法分析 (Parsing)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK 基礎功能示範\n",
    "print(\"=\" * 70)\n",
    "print(\"NLTK 核心功能展示\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "try:\n",
    "    import nltk\n",
    "    from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "    \n",
    "    # 下載必要資源 (首次使用,可能需要幾分鐘)\n",
    "    print(\"\\n📥 檢查 NLTK 資源...\")\n",
    "    resources = ['punkt', 'stopwords', 'wordnet', 'averaged_perceptron_tagger', 'omw-1.4']\n",
    "    for resource in resources:\n",
    "        try:\n",
    "            nltk.data.find(f'tokenizers/{resource}' if resource == 'punkt' else f'corpora/{resource}')\n",
    "            print(f\"  ✅ {resource} 已存在\")\n",
    "        except LookupError:\n",
    "            print(f\"  ⬇️ 下載 {resource}...\")\n",
    "            nltk.download(resource, quiet=True)\n",
    "    \n",
    "    # 測試文本\n",
    "    text = \"\"\"\n",
    "    Natural language processing (NLP) is a subfield of linguistics, \n",
    "    computer science, and artificial intelligence. It is concerned with \n",
    "    the interactions between computers and human languages. NLP enables \n",
    "    computers to read, understand, and derive meaning from human languages.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"原始文本:\")\n",
    "    print(text.strip())\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # 1. 句子分割\n",
    "    print(\"\\n1️⃣ 句子分割 (Sentence Tokenization)\")\n",
    "    print(\"-\" * 70)\n",
    "    sentences = sent_tokenize(text)\n",
    "    print(f\"句子數: {len(sentences)}\\n\")\n",
    "    for i, sent in enumerate(sentences, 1):\n",
    "        print(f\"  [{i}] {sent.strip()}\")\n",
    "    \n",
    "    # 2. 詞彙分割\n",
    "    print(\"\\n2️⃣ 詞彙分割 (Word Tokenization)\")\n",
    "    print(\"-\" * 70)\n",
    "    tokens = word_tokenize(text)\n",
    "    print(f\"詞彙數: {len(tokens)}\")\n",
    "    print(f\"前 20 個詞: {tokens[:20]}\")\n",
    "    \n",
    "    # 3. 停用詞過濾\n",
    "    print(\"\\n3️⃣ 停用詞過濾 (Stop Words Removal)\")\n",
    "    print(\"-\" * 70)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [w for w in tokens if w.lower() not in stop_words and w.isalpha()]\n",
    "    print(f\"原始詞數: {len(tokens)}\")\n",
    "    print(f\"過濾後詞數: {len(filtered_tokens)}\")\n",
    "    print(f\"過濾後詞彙: {filtered_tokens[:15]}\")\n",
    "    \n",
    "    # 4. 詞幹提取 (Stemming)\n",
    "    print(\"\\n4️⃣ 詞幹提取 (Stemming - Porter Stemmer)\")\n",
    "    print(\"-\" * 70)\n",
    "    stemmer = PorterStemmer()\n",
    "    test_words = ['processing', 'languages', 'interactions', 'computers', 'enables']\n",
    "    print(f\"{'原詞':<20s} → 詞幹\")\n",
    "    print(\"-\" * 40)\n",
    "    for word in test_words:\n",
    "        stem = stemmer.stem(word)\n",
    "        print(f\"{word:<20s} → {stem}\")\n",
    "    \n",
    "    # 5. 詞形還原 (Lemmatization)\n",
    "    print(\"\\n5️⃣ 詞形還原 (Lemmatization)\")\n",
    "    print(\"-\" * 70)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    print(f\"{'原詞':<20s} → 詞形還原\")\n",
    "    print(\"-\" * 40)\n",
    "    for word in test_words:\n",
    "        lemma = lemmatizer.lemmatize(word)\n",
    "        print(f\"{word:<20s} → {lemma}\")\n",
    "    \n",
    "    # 6. 詞性標註 (POS Tagging)\n",
    "    print(\"\\n6️⃣ 詞性標註 (POS Tagging)\")\n",
    "    print(\"-\" * 70)\n",
    "    sample_sent = \"Natural language processing enables computers to understand human languages.\"\n",
    "    sample_tokens = word_tokenize(sample_sent)\n",
    "    pos_tags = nltk.pos_tag(sample_tokens)\n",
    "    \n",
    "    print(f\"{'詞彙':<20s} {'詞性':<10s} 說明\")\n",
    "    print(\"-\" * 60)\n",
    "    for word, pos in pos_tags:\n",
    "        print(f\"{word:<20s} {pos:<10s}\")\n",
    "    \n",
    "    print(\"\\n✅ NLTK 功能展示完成!\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"\\n⚠️ NLTK 未安裝!\")\n",
    "    print(\"請執行: pip install nltk\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ 錯誤: {e}\")\n",
    "    print(\"如遇到資源下載問題,請手動執行:\")\n",
    "    print(\"  import nltk\")\n",
    "    print(\"  nltk.download('punkt')\")\n",
    "    print(\"  nltk.download('stopwords')\")\n",
    "    print(\"  nltk.download('wordnet')\")\n",
    "    print(\"  nltk.download('averaged_perceptron_tagger')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 詞幹提取 vs 詞形還原對比視覺化\n",
    "try:\n",
    "    import nltk\n",
    "    from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # 測試詞彙\n",
    "    test_words = [\n",
    "        'running', 'ran', 'runs', 'easily', 'fairly',\n",
    "        'processing', 'computers', 'languages', 'better', 'good'\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    for word in test_words:\n",
    "        stem = stemmer.stem(word)\n",
    "        lemma = lemmatizer.lemmatize(word)\n",
    "        results.append({\n",
    "            '原詞': word,\n",
    "            '詞幹 (Stem)': stem,\n",
    "            '詞形還原 (Lemma)': lemma,\n",
    "            '相同?': '✅' if stem == lemma else '❌'\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # 視覺化表格\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    ax.axis('tight')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    table = ax.table(cellText=df.values, colLabels=df.columns,\n",
    "                    cellLoc='center', loc='center',\n",
    "                    colWidths=[0.25, 0.25, 0.3, 0.2])\n",
    "    \n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(11)\n",
    "    table.scale(1, 2.5)\n",
    "    \n",
    "    # 設定表頭樣式\n",
    "    for i in range(len(df.columns)):\n",
    "        table[(0, i)].set_facecolor('#F18F01')\n",
    "        table[(0, i)].set_text_props(weight='bold', color='white', fontsize=12)\n",
    "    \n",
    "    # 設定交替行顏色\n",
    "    for i in range(1, len(df) + 1):\n",
    "        for j in range(len(df.columns)):\n",
    "            if i % 2 == 0:\n",
    "                table[(i, j)].set_facecolor('#E8F4F8')\n",
    "            else:\n",
    "                table[(i, j)].set_facecolor('white')\n",
    "    \n",
    "    plt.title('NLTK: 詞幹提取 vs 詞形還原對比', fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n💡 觀察:\")\n",
    "    print(\"  - 詞幹提取 (Stemming): 簡單規則,可能產生非詞 (e.g., 'comput')\")\n",
    "    print(\"  - 詞形還原 (Lemmatization): 基於詞典,保留有意義的詞根\")\n",
    "    print(\"  - 速度: Stemming 更快,Lemmatization 更準確\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️ 需要安裝 NLTK: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "## 3. spaCy: 工業級 NLP 框架\n",
    "\n",
    "### 3.1 spaCy 核心特點\n",
    "\n",
    "**定位:** 工業級 NLP 函式庫,適合生產環境部署\n",
    "\n",
    "**優勢:**\n",
    "- ✅ 超高效能: Cython 實作,速度比 NLTK 快 10-100 倍\n",
    "- ✅ Pipeline 架構: 一次處理完成所有任務\n",
    "- ✅ 預訓練模型: 支援 20+ 種語言\n",
    "- ✅ 生產就緒: API 簡潔,易於整合\n",
    "- ✅ 現代設計: 支援深度學習,可自定義擴展\n",
    "\n",
    "**劣勢:**\n",
    "- ❌ 學習曲線陡峭: 內部設計較複雜\n",
    "- ❌ 模型體積大: 預訓練模型需下載 (數百 MB)\n",
    "- ❌ 靈活性較低: Pipeline 固定,不如 NLTK 模組化\n",
    "\n",
    "---\n",
    "\n",
    "### 3.2 spaCy 核心功能實作\n",
    "\n",
    "#### 功能清單:\n",
    "1. 自動化 Pipeline 處理\n",
    "2. 命名實體識別 (NER)\n",
    "3. 依存句法分析 (Dependency Parsing)\n",
    "4. 詞向量相似度計算\n",
    "5. 批次處理優化\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spaCy 核心功能展示\n",
    "print(\"=\" * 70)\n",
    "print(\"spaCy 核心功能展示\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "try:\n",
    "    import spacy\n",
    "    \n",
    "    # 載入英文模型 (小型模型)\n",
    "    print(\"\\n📥 載入 spaCy 模型...\")\n",
    "    try:\n",
    "        nlp = spacy.load('en_core_web_sm')\n",
    "        print(\"  ✅ en_core_web_sm 已載入\")\n",
    "    except OSError:\n",
    "        print(\"  ⚠️ 模型未安裝,請執行: python -m spacy download en_core_web_sm\")\n",
    "        print(\"  使用空白模型替代...\")\n",
    "        nlp = spacy.blank('en')\n",
    "    \n",
    "    # 測試文本\n",
    "    text = \"\"\"\n",
    "    Apple Inc. is planning to open a new store in New York City next month. \n",
    "    The CEO Tim Cook announced the decision last week at a conference in California. \n",
    "    The company expects to hire 500 employees for this new location.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"原始文本:\")\n",
    "    print(text.strip())\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # 處理文本 (自動執行完整 Pipeline)\n",
    "    print(\"\\n⚙️ 執行 spaCy Pipeline...\")\n",
    "    doc = nlp(text)\n",
    "    print(\"  ✅ 處理完成!\")\n",
    "    \n",
    "    # 1. 分詞與詞性標註\n",
    "    print(\"\\n1️⃣ 分詞與詞性標註 (前 15 個詞)\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"{'詞彙':<20s} {'詞性':<10s} {'詞幹':<20s} {'停用詞?'}\")\n",
    "    print(\"-\" * 70)\n",
    "    for token in list(doc)[:15]:\n",
    "        print(f\"{token.text:<20s} {token.pos_:<10s} {token.lemma_:<20s} {'✅' if token.is_stop else '❌'}\")\n",
    "    \n",
    "    # 2. 命名實體識別 (NER)\n",
    "    print(\"\\n2️⃣ 命名實體識別 (Named Entity Recognition)\")\n",
    "    print(\"-\" * 70)\n",
    "    if doc.ents:\n",
    "        print(f\"{'實體':<30s} {'類型':<15s} 說明\")\n",
    "        print(\"-\" * 70)\n",
    "        for ent in doc.ents:\n",
    "            print(f\"{ent.text:<30s} {ent.label_:<15s} {spacy.explain(ent.label_)}\")\n",
    "    else:\n",
    "        print(\"  ⚠️ 未檢測到實體 (可能使用的是空白模型)\")\n",
    "    \n",
    "    # 3. 依存句法分析\n",
    "    print(\"\\n3️⃣ 依存句法分析 (Dependency Parsing - 第一句)\")\n",
    "    print(\"-\" * 70)\n",
    "    sent = list(doc.sents)[0]\n",
    "    print(f\"{'詞彙':<20s} {'依存關係':<15s} 主導詞\")\n",
    "    print(\"-\" * 70)\n",
    "    for token in sent:\n",
    "        print(f\"{token.text:<20s} {token.dep_:<15s} {token.head.text}\")\n",
    "    \n",
    "    # 4. 句子分割\n",
    "    print(\"\\n4️⃣ 句子分割\")\n",
    "    print(\"-\" * 70)\n",
    "    sentences = list(doc.sents)\n",
    "    print(f\"句子數: {len(sentences)}\\n\")\n",
    "    for i, sent in enumerate(sentences, 1):\n",
    "        print(f\"  [{i}] {sent.text.strip()}\")\n",
    "    \n",
    "    print(\"\\n✅ spaCy 功能展示完成!\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"\\n⚠️ spaCy 未安裝!\")\n",
    "    print(\"請執行: pip install spacy\")\n",
    "    print(\"然後下載模型: python -m spacy download en_core_web_sm\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ 錯誤: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spaCy 批次處理效能展示\n",
    "print(\"=\" * 70)\n",
    "print(\"spaCy 批次處理效能測試\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "try:\n",
    "    import spacy\n",
    "    import time\n",
    "    \n",
    "    try:\n",
    "        nlp = spacy.load('en_core_web_sm')\n",
    "    except:\n",
    "        print(\"⚠️ 使用空白模型 (僅測試速度)\")\n",
    "        nlp = spacy.blank('en')\n",
    "    \n",
    "    # 準備測試數據\n",
    "    test_text = \"Natural language processing is amazing. It enables computers to understand human language.\"\n",
    "    texts = [test_text] * 1000  # 1000 個文檔\n",
    "    \n",
    "    # 方法 1: 逐個處理\n",
    "    print(\"\\n方法 1: 逐個處理 (Sequential)\")\n",
    "    start = time.time()\n",
    "    docs_sequential = [nlp(text) for text in texts]\n",
    "    time_sequential = time.time() - start\n",
    "    print(f\"  處理時間: {time_sequential:.3f} 秒\")\n",
    "    \n",
    "    # 方法 2: 批次處理\n",
    "    print(\"\\n方法 2: 批次處理 (Batching)\")\n",
    "    start = time.time()\n",
    "    docs_batch = list(nlp.pipe(texts, batch_size=50))\n",
    "    time_batch = time.time() - start\n",
    "    print(f\"  處理時間: {time_batch:.3f} 秒\")\n",
    "    \n",
    "    # 對比\n",
    "    speedup = time_sequential / time_batch\n",
    "    print(f\"\\n🚀 加速比: {speedup:.2f}x\")\n",
    "    \n",
    "    # 視覺化\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    methods = ['逐個處理', '批次處理']\n",
    "    times = [time_sequential, time_batch]\n",
    "    colors = ['#F18F01', '#2E86AB']\n",
    "    \n",
    "    bars = ax.bar(methods, times, color=colors, alpha=0.7, edgecolor='black')\n",
    "    ax.set_ylabel('處理時間 (秒)', fontsize=12)\n",
    "    ax.set_title('spaCy 批次處理效能對比 (1000 文檔)', fontsize=14, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # 標註數值\n",
    "    for bar, time_val in zip(bars, times):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{time_val:.3f}s',\n",
    "                ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n💡 批次處理優勢:\")\n",
    "    print(\"  1. 減少 Python 解釋器開銷\")\n",
    "    print(\"  2. 更好利用 CPU/GPU\")\n",
    "    print(\"  3. 生產環境必備優化\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️ 需要安裝 spaCy: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "## 4. Transformers: 預訓練模型庫\n",
    "\n",
    "### 4.1 Transformers 核心特點\n",
    "\n",
    "**定位:** Hugging Face 開源的預訓練 Transformer 模型庫\n",
    "\n",
    "**優勢:**\n",
    "- ✅ 模型豐富: 100,000+ 預訓練模型\n",
    "- ✅ 統一 API: AutoModel/AutoTokenizer 支援所有模型\n",
    "- ✅ SOTA 性能: 最新研究成果快速落地\n",
    "- ✅ 易於微調: Trainer API 簡化訓練流程\n",
    "- ✅ 社群活躍: 每天新增數百個模型\n",
    "\n",
    "**劣勢:**\n",
    "- ❌ 資源需求高: 大模型需要 GPU\n",
    "- ❌ 速度較慢: 推理速度不如 spaCy\n",
    "- ❌ 學習曲線陡: 深度學習背景需求\n",
    "\n",
    "---\n",
    "\n",
    "### 4.2 Transformers 三大模型家族\n",
    "\n",
    "#### 1. Encoder-Only (BERT 系列)\n",
    "- **代表:** BERT, RoBERTa, ALBERT, DeBERTa\n",
    "- **特點:** 雙向 Attention,適合理解任務\n",
    "- **應用:** 文本分類、NER、問答系統\n",
    "\n",
    "#### 2. Decoder-Only (GPT 系列)\n",
    "- **代表:** GPT-2, GPT-3, GPT-4, LLaMA\n",
    "- **特點:** 單向 Attention,自回歸生成\n",
    "- **應用:** 文本生成、對話、程式碼生成\n",
    "\n",
    "#### 3. Encoder-Decoder (T5 系列)\n",
    "- **代表:** T5, BART, mT5, T0\n",
    "- **特點:** 完整 Transformer 架構\n",
    "- **應用:** 翻譯、摘要、問答生成\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformers Pipeline API 展示\n",
    "print(\"=\" * 70)\n",
    "print(\"Transformers Pipeline API 展示\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "try:\n",
    "    from transformers import pipeline\n",
    "    \n",
    "    # 1. 情感分析\n",
    "    print(\"\\n1️⃣ 情感分析 (Sentiment Analysis)\")\n",
    "    print(\"-\" * 70)\n",
    "    sentiment_analyzer = pipeline('sentiment-analysis')\n",
    "    \n",
    "    texts = [\n",
    "        \"I love this product! It's absolutely amazing!\",\n",
    "        \"This is the worst experience I've ever had.\",\n",
    "        \"It's okay, nothing special.\"\n",
    "    ]\n",
    "    \n",
    "    for text in texts:\n",
    "        result = sentiment_analyzer(text)[0]\n",
    "        print(f\"\\n文本: {text}\")\n",
    "        print(f\"結果: {result['label']} (信心度: {result['score']:.2%})\")\n",
    "    \n",
    "    # 2. 零樣本分類\n",
    "    print(\"\\n\\n2️⃣ 零樣本分類 (Zero-Shot Classification)\")\n",
    "    print(\"-\" * 70)\n",
    "    zero_shot_classifier = pipeline('zero-shot-classification')\n",
    "    \n",
    "    text = \"Apple announced a new iPhone with improved camera and longer battery life.\"\n",
    "    candidate_labels = ['technology', 'sports', 'politics', 'entertainment']\n",
    "    \n",
    "    result = zero_shot_classifier(text, candidate_labels)\n",
    "    print(f\"\\n文本: {text}\")\n",
    "    print(\"\\n分類結果:\")\n",
    "    for label, score in zip(result['labels'], result['scores']):\n",
    "        print(f\"  {label:<15s}: {score:.2%}\")\n",
    "    \n",
    "    # 3. 問答系統\n",
    "    print(\"\\n\\n3️⃣ 問答系統 (Question Answering)\")\n",
    "    print(\"-\" * 70)\n",
    "    qa_pipeline = pipeline('question-answering')\n",
    "    \n",
    "    context = \"\"\"\n",
    "    Hugging Face is a company based in New York City. The company develops \n",
    "    tools for building applications using machine learning. It is best known \n",
    "    for its Transformers library.\n",
    "    \"\"\"\n",
    "    \n",
    "    questions = [\n",
    "        \"Where is Hugging Face based?\",\n",
    "        \"What is Hugging Face known for?\"\n",
    "    ]\n",
    "    \n",
    "    for question in questions:\n",
    "        result = qa_pipeline(question=question, context=context)\n",
    "        print(f\"\\n問題: {question}\")\n",
    "        print(f\"答案: {result['answer']} (信心度: {result['score']:.2%})\")\n",
    "    \n",
    "    # 4. 文本生成\n",
    "    print(\"\\n\\n4️⃣ 文本生成 (Text Generation)\")\n",
    "    print(\"-\" * 70)\n",
    "    generator = pipeline('text-generation', model='gpt2')\n",
    "    \n",
    "    prompt = \"Artificial intelligence will\"\n",
    "    outputs = generator(prompt, max_length=50, num_return_sequences=2)\n",
    "    \n",
    "    print(f\"\\n提示詞: {prompt}\")\n",
    "    print(\"\\n生成結果:\")\n",
    "    for i, output in enumerate(outputs, 1):\n",
    "        print(f\"\\n[{i}] {output['generated_text']}\")\n",
    "    \n",
    "    print(\"\\n\\n✅ Transformers Pipeline 展示完成!\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"\\n⚠️ Transformers 未安裝!\")\n",
    "    print(\"請執行: pip install transformers torch\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ 錯誤: {e}\")\n",
    "    print(\"提示: 首次執行會自動下載模型,需要網路連接\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformers 模型家族對比\n",
    "model_families = [\n",
    "    {\n",
    "        '家族': 'Encoder-Only',\n",
    "        '代表模型': 'BERT, RoBERTa, ALBERT',\n",
    "        '架構': '雙向 Attention',\n",
    "        '訓練方式': 'Masked LM',\n",
    "        '適用任務': '分類、NER、問答',\n",
    "        '參數量': '110M - 1.5B'\n",
    "    },\n",
    "    {\n",
    "        '家族': 'Decoder-Only',\n",
    "        '代表模型': 'GPT-2, GPT-3, LLaMA',\n",
    "        '架構': '單向 Attention',\n",
    "        '訓練方式': 'Autoregressive LM',\n",
    "        '適用任務': '生成、對話、翻譯',\n",
    "        '參數量': '124M - 175B'\n",
    "    },\n",
    "    {\n",
    "        '家族': 'Encoder-Decoder',\n",
    "        '代表模型': 'T5, BART, mT5',\n",
    "        '架構': '完整 Transformer',\n",
    "        '訓練方式': 'Seq2Seq',\n",
    "        '適用任務': '翻譯、摘要、QA生成',\n",
    "        '參數量': '60M - 11B'\n",
    "    }\n",
    "]\n",
    "\n",
    "df_models = pd.DataFrame(model_families)\n",
    "\n",
    "# 視覺化表格\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "table = ax.table(cellText=df_models.values, colLabels=df_models.columns,\n",
    "                cellLoc='left', loc='center',\n",
    "                colWidths=[0.15, 0.25, 0.15, 0.15, 0.2, 0.1])\n",
    "\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1, 3)\n",
    "\n",
    "# 設定表頭樣式\n",
    "for i in range(len(df_models.columns)):\n",
    "    table[(0, i)].set_facecolor('#2E86AB')\n",
    "    table[(0, i)].set_text_props(weight='bold', color='white', fontsize=11)\n",
    "\n",
    "# 設定行顏色\n",
    "colors = ['#FFE66D', '#95E1D3', '#F18F01']\n",
    "for i in range(1, len(df_models) + 1):\n",
    "    for j in range(len(df_models.columns)):\n",
    "        table[(i, j)].set_facecolor(colors[i-1])\n",
    "        table[(i, j)].set_alpha(0.4)\n",
    "\n",
    "plt.title('Transformers 三大模型家族對比', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n💡 選擇建議:\")\n",
    "print(\"  1. 理解任務 (分類、NER) → Encoder-Only (BERT 系列)\")\n",
    "print(\"  2. 生成任務 (對話、寫作) → Decoder-Only (GPT 系列)\")\n",
    "print(\"  3. 轉換任務 (翻譯、摘要) → Encoder-Decoder (T5 系列)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "## 5. 中文 NLP 工具對比\n",
    "\n",
    "### 5.1 中文 NLP 的特殊挑戰\n",
    "\n",
    "**與英文的關鍵差異:**\n",
    "1. **無天然分詞:** 中文沒有空格分隔,需要先分詞\n",
    "2. **繁簡體差異:** 需要處理繁體/簡體轉換\n",
    "3. **多音字:** 同一字不同讀音和意思\n",
    "4. **詞彙歧義:** 中文詞彙歧義性更高\n",
    "\n",
    "---\n",
    "\n",
    "### 5.2 主流中文 NLP 工具\n",
    "\n",
    "| 工具 | 開發者 | 特點 | 優勢 | 劣勢 |\n",
    "|:---|:---|:---|:---|:---|\n",
    "| **jieba** | fxsjy | 輕量快速 | 易用、無依賴 | 準確率中等 |\n",
    "| **LTP** | 哈工大 | 全流程 | 準確率高、功能完整 | 速度較慢、依賴重 |\n",
    "| **HanLP** | hankcs | 多語言 | 支援繁體、多任務 | 學習曲線陡 |\n",
    "| **pkuseg** | 北京大學 | 領域適應 | 多領域模型 | 速度慢 |\n",
    "| **THULAC** | 清華大學 | 詞性標註準 | 詞性準確 | 社群較小 |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 中文分詞工具對比\n",
    "print(\"=\" * 70)\n",
    "print(\"中文分詞工具對比\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "test_text = \"我愛自然語言處理,它是人工智能的重要分支,可以讓電腦理解人類語言。\"\n",
    "\n",
    "print(f\"\\n測試文本: {test_text}\\n\")\n",
    "\n",
    "# jieba 分詞\n",
    "try:\n",
    "    import jieba\n",
    "    print(\"1️⃣ jieba (結巴分詞)\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # 精確模式\n",
    "    words_precise = jieba.cut(test_text, cut_all=False)\n",
    "    print(f\"精確模式: {' / '.join(words_precise)}\")\n",
    "    \n",
    "    # 全模式\n",
    "    words_all = jieba.cut(test_text, cut_all=True)\n",
    "    print(f\"全模式:   {' / '.join(words_all)}\")\n",
    "    \n",
    "    # 搜尋引擎模式\n",
    "    words_search = jieba.cut_for_search(test_text)\n",
    "    print(f\"搜尋模式: {' / '.join(words_search)}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"⚠️ jieba 未安裝: pip install jieba\")\n",
    "\n",
    "# pkuseg 分詞\n",
    "try:\n",
    "    import pkuseg\n",
    "    print(\"\\n2️⃣ pkuseg (北大分詞)\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    seg = pkuseg.pkuseg()\n",
    "    words = seg.cut(test_text)\n",
    "    print(f\"分詞結果: {' / '.join(words)}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"\\n⚠️ pkuseg 未安裝: pip install pkuseg\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n⚠️ pkuseg 錯誤: {e}\")\n",
    "\n",
    "# spaCy 中文模型\n",
    "try:\n",
    "    import spacy\n",
    "    print(\"\\n3️⃣ spaCy (中文模型)\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    try:\n",
    "        nlp_zh = spacy.load('zh_core_web_sm')\n",
    "        doc = nlp_zh(test_text)\n",
    "        words = [token.text for token in doc]\n",
    "        print(f\"分詞結果: {' / '.join(words)}\")\n",
    "    except OSError:\n",
    "        print(\"⚠️ 中文模型未安裝: python -m spacy download zh_core_web_sm\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"\\n⚠️ spaCy 未安裝: pip install spacy\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✅ 中文分詞對比完成\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n💡 選擇建議:\")\n",
    "print(\"  1. 快速原型 → jieba (最簡單)\")\n",
    "print(\"  2. 高準確率 → pkuseg/LTP (領域適應)\")\n",
    "print(\"  3. 完整流程 → spaCy (分詞+詞性+NER)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jieba 進階功能展示\n",
    "print(\"=\" * 70)\n",
    "print(\"jieba 進階功能展示\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "try:\n",
    "    import jieba\n",
    "    import jieba.analyse\n",
    "    import jieba.posseg as pseg\n",
    "    \n",
    "    text = \"\"\"\n",
    "    自然語言處理是人工智能領域的重要分支,它研究如何讓電腦理解和生成人類語言。\n",
    "    深度學習技術的發展推動了NLP的快速進步,BERT、GPT等模型取得了突破性成果。\n",
    "    自然語言處理的應用包括機器翻譯、問答系統、文本分類等多個方向。\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. 詞性標註\n",
    "    print(\"\\n1️⃣ 詞性標註 (POS Tagging)\")\n",
    "    print(\"-\" * 70)\n",
    "    words = pseg.cut(text)\n",
    "    print(f\"{'詞彙':<15s} 詞性\")\n",
    "    print(\"-\" * 30)\n",
    "    for word, pos in list(words)[:20]:\n",
    "        if word.strip():\n",
    "            print(f\"{word:<15s} {pos}\")\n",
    "    \n",
    "    # 2. 關鍵字提取 (TF-IDF)\n",
    "    print(\"\\n2️⃣ 關鍵字提取 (TF-IDF)\")\n",
    "    print(\"-\" * 70)\n",
    "    keywords_tfidf = jieba.analyse.extract_tags(text, topK=10, withWeight=True)\n",
    "    print(f\"{'關鍵字':<15s} TF-IDF 權重\")\n",
    "    print(\"-\" * 40)\n",
    "    for word, weight in keywords_tfidf:\n",
    "        print(f\"{word:<15s} {weight:.4f}\")\n",
    "    \n",
    "    # 3. 關鍵字提取 (TextRank)\n",
    "    print(\"\\n3️⃣ 關鍵字提取 (TextRank)\")\n",
    "    print(\"-\" * 70)\n",
    "    keywords_textrank = jieba.analyse.textrank(text, topK=10, withWeight=True)\n",
    "    print(f\"{'關鍵字':<15s} TextRank 權重\")\n",
    "    print(\"-\" * 40)\n",
    "    for word, weight in keywords_textrank:\n",
    "        print(f\"{word:<15s} {weight:.4f}\")\n",
    "    \n",
    "    # 4. 自定義詞典\n",
    "    print(\"\\n4️⃣ 自定義詞典\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    test_sentence = \"我在學習自然語言處理和深度學習技術\"\n",
    "    print(f\"原句: {test_sentence}\")\n",
    "    print(f\"預設分詞: {' / '.join(jieba.cut(test_sentence))}\")\n",
    "    \n",
    "    # 添加自定義詞\n",
    "    jieba.add_word('自然語言處理', freq=10000, tag='n')\n",
    "    jieba.add_word('深度學習', freq=10000, tag='n')\n",
    "    print(f\"自定義後: {' / '.join(jieba.cut(test_sentence))}\")\n",
    "    \n",
    "    print(\"\\n✅ jieba 進階功能展示完成!\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"\\n⚠️ jieba 未安裝: pip install jieba\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a>\n",
    "## 6. 工具效能基準測試\n",
    "\n",
    "### 6.1 測試方法論\n",
    "\n",
    "**測試指標:**\n",
    "1. **處理速度 (Throughput):** 每秒處理詞數\n",
    "2. **記憶體占用 (Memory Usage):** 峰值記憶體\n",
    "3. **準確率 (Accuracy):** 與人工標註對比\n",
    "4. **啟動時間 (Startup Time):** 模型載入時間\n",
    "\n",
    "**測試環境:**\n",
    "- CPU: Intel i7 (模擬)\n",
    "- 記憶體: 16GB\n",
    "- Python: 3.9+\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 工具效能基準測試\n",
    "import time\n",
    "import gc\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"工具效能基準測試\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 準備測試數據\n",
    "test_text_en = \"Natural language processing is amazing. \" * 100\n",
    "test_text_zh = \"自然語言處理是人工智能的重要分支。\" * 100\n",
    "\n",
    "results = []\n",
    "\n",
    "# 測試 NLTK\n",
    "try:\n",
    "    import nltk\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    \n",
    "    # 載入時間\n",
    "    start = time.time()\n",
    "    # NLTK 無需載入模型\n",
    "    load_time = time.time() - start\n",
    "    \n",
    "    # 處理時間\n",
    "    start = time.time()\n",
    "    for _ in range(100):\n",
    "        tokens = word_tokenize(test_text_en)\n",
    "    process_time = time.time() - start\n",
    "    \n",
    "    results.append({\n",
    "        '工具': 'NLTK',\n",
    "        '載入時間(秒)': f\"{load_time:.3f}\",\n",
    "        '處理時間(秒)': f\"{process_time:.3f}\",\n",
    "        '相對速度': '1.0x (基準)'\n",
    "    })\n",
    "    nltk_time = process_time\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️ NLTK 測試失敗: {e}\")\n",
    "    nltk_time = None\n",
    "\n",
    "# 測試 spaCy\n",
    "try:\n",
    "    import spacy\n",
    "    \n",
    "    # 載入時間\n",
    "    start = time.time()\n",
    "    try:\n",
    "        nlp = spacy.load('en_core_web_sm')\n",
    "    except:\n",
    "        nlp = spacy.blank('en')\n",
    "    load_time = time.time() - start\n",
    "    \n",
    "    # 處理時間\n",
    "    start = time.time()\n",
    "    for _ in range(100):\n",
    "        doc = nlp(test_text_en)\n",
    "    process_time = time.time() - start\n",
    "    \n",
    "    speedup = nltk_time / process_time if nltk_time else 0\n",
    "    results.append({\n",
    "        '工具': 'spaCy',\n",
    "        '載入時間(秒)': f\"{load_time:.3f}\",\n",
    "        '處理時間(秒)': f\"{process_time:.3f}\",\n",
    "        '相對速度': f\"{speedup:.1f}x\" if speedup else 'N/A'\n",
    "    })\n",
    "    spacy_time = process_time\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️ spaCy 測試失敗: {e}\")\n",
    "    spacy_time = None\n",
    "\n",
    "# 測試 jieba\n",
    "try:\n",
    "    import jieba\n",
    "    \n",
    "    # 載入時間\n",
    "    start = time.time()\n",
    "    jieba.initialize()\n",
    "    load_time = time.time() - start\n",
    "    \n",
    "    # 處理時間\n",
    "    start = time.time()\n",
    "    for _ in range(100):\n",
    "        words = list(jieba.cut(test_text_zh))\n",
    "    process_time = time.time() - start\n",
    "    \n",
    "    results.append({\n",
    "        '工具': 'jieba',\n",
    "        '載入時間(秒)': f\"{load_time:.3f}\",\n",
    "        '處理時間(秒)': f\"{process_time:.3f}\",\n",
    "        '相對速度': 'N/A (中文)'\n",
    "    })\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️ jieba 測試失敗: {e}\")\n",
    "\n",
    "# 顯示結果\n",
    "if results:\n",
    "    df_results = pd.DataFrame(results)\n",
    "    print(\"\\n📊 效能測試結果 (100 次迭代)\")\n",
    "    print(\"=\" * 70)\n",
    "    print(df_results.to_string(index=False))\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # 視覺化\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # 載入時間\n",
    "    tools = df_results['工具'].tolist()\n",
    "    load_times = [float(t) for t in df_results['載入時間(秒)'].tolist()]\n",
    "    \n",
    "    axes[0].barh(tools, load_times, color='#F18F01', alpha=0.7, edgecolor='black')\n",
    "    axes[0].set_xlabel('載入時間 (秒)', fontsize=11)\n",
    "    axes[0].set_title('模型載入時間對比', fontsize=13, fontweight='bold')\n",
    "    axes[0].grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    for i, (tool, time_val) in enumerate(zip(tools, load_times)):\n",
    "        axes[0].text(time_val, i, f' {time_val:.3f}s', va='center', fontsize=10)\n",
    "    \n",
    "    # 處理時間\n",
    "    process_times = [float(t) for t in df_results['處理時間(秒)'].tolist()]\n",
    "    \n",
    "    axes[1].barh(tools, process_times, color='#2E86AB', alpha=0.7, edgecolor='black')\n",
    "    axes[1].set_xlabel('處理時間 (秒)', fontsize=11)\n",
    "    axes[1].set_title('處理時間對比 (100 次迭代)', fontsize=13, fontweight='bold')\n",
    "    axes[1].grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    for i, (tool, time_val) in enumerate(zip(tools, process_times)):\n",
    "        axes[1].text(time_val, i, f' {time_val:.3f}s', va='center', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n💡 性能觀察:\")\n",
    "    print(\"  1. spaCy: 載入時間較長,但處理速度最快 (生產環境優勢)\")\n",
    "    print(\"  2. NLTK: 無需載入,但處理速度慢 (適合教學與快速原型)\")\n",
    "    print(\"  3. jieba: 載入快速,中文處理高效 (中文首選)\")\n",
    "else:\n",
    "    print(\"\\n⚠️ 無可用測試結果\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 NLTK vs spaCy 詳細對比實驗\n",
    "\n",
    "使用相同文本,測試兩個工具在不同任務上的表現:\n",
    "- 分詞 (Tokenization)\n",
    "- 詞性標註 (POS Tagging)\n",
    "- 句子分割 (Sentence Segmentation)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK vs spaCy 詳細對比\n",
    "import time\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"NLTK vs spaCy 詳細對比實驗\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 準備測試文本 (10,000 詞)\n",
    "base_text = \"\"\"\n",
    "The quick brown fox jumps over the lazy dog. Natural language processing \n",
    "is a subfield of linguistics and artificial intelligence. It enables \n",
    "computers to understand and generate human languages.\n",
    "\"\"\"\n",
    "test_text = base_text * 100\n",
    "word_count = len(test_text.split())\n",
    "\n",
    "print(f\"\\n測試文本長度: 約 {word_count:,} 詞\\n\")\n",
    "\n",
    "results_comparison = []\n",
    "\n",
    "# NLTK 測試\n",
    "try:\n",
    "    import nltk\n",
    "    from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "    \n",
    "    # 分詞\n",
    "    start = time.time()\n",
    "    tokens_nltk = word_tokenize(test_text)\n",
    "    time_tokenize = time.time() - start\n",
    "    \n",
    "    # 句子分割\n",
    "    start = time.time()\n",
    "    sentences_nltk = sent_tokenize(test_text)\n",
    "    time_sentence = time.time() - start\n",
    "    \n",
    "    # 詞性標註\n",
    "    start = time.time()\n",
    "    pos_nltk = nltk.pos_tag(tokens_nltk)\n",
    "    time_pos = time.time() - start\n",
    "    \n",
    "    results_comparison.append({\n",
    "        '工具': 'NLTK',\n",
    "        '分詞(秒)': f\"{time_tokenize:.3f}\",\n",
    "        '句子分割(秒)': f\"{time_sentence:.3f}\",\n",
    "        '詞性標註(秒)': f\"{time_pos:.3f}\",\n",
    "        '總時間(秒)': f\"{time_tokenize + time_sentence + time_pos:.3f}\"\n",
    "    })\n",
    "    \n",
    "    nltk_total = time_tokenize + time_sentence + time_pos\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️ NLTK 測試失敗: {e}\")\n",
    "    nltk_total = None\n",
    "\n",
    "# spaCy 測試\n",
    "try:\n",
    "    import spacy\n",
    "    \n",
    "    try:\n",
    "        nlp = spacy.load('en_core_web_sm')\n",
    "    except:\n",
    "        nlp = spacy.blank('en')\n",
    "    \n",
    "    # Pipeline 處理 (一次完成所有任務)\n",
    "    start_total = time.time()\n",
    "    \n",
    "    start = time.time()\n",
    "    doc = nlp(test_text)\n",
    "    time_pipeline = time.time() - start\n",
    "    \n",
    "    # 提取結果\n",
    "    tokens_spacy = [token for token in doc]\n",
    "    sentences_spacy = list(doc.sents)\n",
    "    pos_spacy = [(token.text, token.pos_) for token in doc]\n",
    "    \n",
    "    total_time = time.time() - start_total\n",
    "    \n",
    "    results_comparison.append({\n",
    "        '工具': 'spaCy',\n",
    "        '分詞(秒)': '-',\n",
    "        '句子分割(秒)': '-',\n",
    "        '詞性標註(秒)': '-',\n",
    "        '總時間(秒)': f\"{total_time:.3f} (Pipeline)\"\n",
    "    })\n",
    "    \n",
    "    spacy_total = total_time\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️ spaCy 測試失敗: {e}\")\n",
    "    spacy_total = None\n",
    "\n",
    "# 顯示結果\n",
    "if results_comparison:\n",
    "    df_comp = pd.DataFrame(results_comparison)\n",
    "    print(\"📊 詳細對比結果\")\n",
    "    print(\"=\" * 70)\n",
    "    print(df_comp.to_string(index=False))\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    if nltk_total and spacy_total:\n",
    "        speedup = nltk_total / spacy_total\n",
    "        print(f\"\\n🚀 spaCy 加速比: {speedup:.1f}x\")\n",
    "        print(f\"   處理效率: {word_count / spacy_total:.0f} 詞/秒 (spaCy) vs {word_count / nltk_total:.0f} 詞/秒 (NLTK)\")\n",
    "    \n",
    "    # 視覺化\n",
    "    if nltk_total and spacy_total:\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        \n",
    "        tools = ['NLTK\\n(逐步處理)', 'spaCy\\n(Pipeline)']\n",
    "        times = [nltk_total, spacy_total]\n",
    "        colors = ['#F18F01', '#2E86AB']\n",
    "        \n",
    "        bars = ax.bar(tools, times, color=colors, alpha=0.7, edgecolor='black', width=0.5)\n",
    "        ax.set_ylabel('總處理時間 (秒)', fontsize=12)\n",
    "        ax.set_title(f'NLTK vs spaCy 完整流程對比 ({word_count:,} 詞)', \n",
    "                    fontsize=14, fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # 標註數值與加速比\n",
    "        for bar, time_val in zip(bars, times):\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{time_val:.3f}s\\n({word_count/time_val:.0f} 詞/秒)',\n",
    "                   ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "        \n",
    "        # 加速比標註\n",
    "        ax.text(0.5, max(times) * 0.8, f'{speedup:.1f}x\\n加速',\n",
    "               ha='center', fontsize=14, fontweight='bold',\n",
    "               bbox=dict(boxstyle='round,pad=0.5', facecolor='yellow', alpha=0.7))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    print(\"\\n💡 關鍵洞察:\")\n",
    "    print(\"  1. spaCy Pipeline 設計: 一次處理完成所有任務,效率極高\")\n",
    "    print(\"  2. NLTK 模組化: 每個任務獨立調用,靈活但慢\")\n",
    "    print(\"  3. 生產環境選擇: spaCy 是明確贏家\")\n",
    "    print(\"  4. 教學與研究: NLTK 更易理解內部機制\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"7\"></a>\n",
    "## 7. 選擇決策樹與最佳實踐\n",
    "\n",
    "### 7.1 工具選擇決策樹\n",
    "\n",
    "```\n",
    "需要處理哪種語言?\n",
    "│\n",
    "├─ 中文\n",
    "│   ├─ 只需分詞? → jieba (最快速)\n",
    "│   ├─ 需要完整流程? → LTP / spaCy 中文模型\n",
    "│   ├─ 需要高準確率? → LTP / HanLP\n",
    "│   └─ 需要 SOTA? → Transformers (BERT-chinese)\n",
    "│\n",
    "├─ 英文\n",
    "│   ├─ 學習/教學? → NLTK\n",
    "│   ├─ 快速原型? → NLTK / TextBlob\n",
    "│   ├─ 生產環境? → spaCy\n",
    "│   ├─ 最高準確率? → Transformers (RoBERTa)\n",
    "│   └─ 特定任務? → Transformers Pipeline\n",
    "│\n",
    "└─ 多語言\n",
    "    ├─ 常見語言 (20+)? → spaCy\n",
    "    ├─ 稀有語言? → Transformers (XLM-R, mBERT)\n",
    "    ├─ 多模態? → Transformers (CLIP, LayoutLM)\n",
    "    └─ 自定義需求? → AllenNLP / fairseq\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 7.2 場景化最佳實踐\n",
    "\n",
    "#### 場景 1: 學術研究與教學\n",
    "- **推薦:** NLTK + Transformers\n",
    "- **原因:** NLTK 易於理解算法,Transformers 提供 SOTA 模型\n",
    "- **技術棧:** Python + NLTK + Jupyter + Transformers\n",
    "\n",
    "#### 場景 2: 生產環境部署\n",
    "- **推薦:** spaCy + Transformers (微調)\n",
    "- **原因:** spaCy 高效能,Transformers 高準確率\n",
    "- **技術棧:** FastAPI + spaCy + Docker + ONNX\n",
    "\n",
    "#### 場景 3: 中文文本分析\n",
    "- **推薦:** jieba + Transformers (BERT-chinese)\n",
    "- **原因:** jieba 分詞快,BERT 語義理解強\n",
    "- **技術棧:** jieba + PyTorch + Transformers\n",
    "\n",
    "#### 場景 4: 快速原型開發\n",
    "- **推薦:** TextBlob / Transformers Pipeline\n",
    "- **原因:** 簡化 API,開箱即用\n",
    "- **技術棧:** Python + TextBlob + Streamlit\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 決策樹視覺化\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 12))\n",
    "ax.set_xlim(0, 10)\n",
    "ax.set_ylim(0, 12)\n",
    "ax.axis('off')\n",
    "\n",
    "# 標題\n",
    "ax.text(5, 11.5, 'NLP 工具選擇決策樹', ha='center', \n",
    "        fontsize=18, fontweight='bold')\n",
    "\n",
    "# 根節點\n",
    "root = mpatches.FancyBboxPatch((3.5, 10), 3, 0.8,\n",
    "                                boxstyle=\"round,pad=0.1\",\n",
    "                                linewidth=2, edgecolor='black',\n",
    "                                facecolor='#F18F01', alpha=0.7)\n",
    "ax.add_patch(root)\n",
    "ax.text(5, 10.4, '需要處理哪種語言?', ha='center', va='center',\n",
    "        fontsize=13, fontweight='bold')\n",
    "\n",
    "# 第一層: 語言分類\n",
    "languages = [\n",
    "    (1.5, 8, '中文', '#FFE66D'),\n",
    "    (5, 8, '英文', '#95E1D3'),\n",
    "    (8.5, 8, '多語言', '#2E86AB')\n",
    "]\n",
    "\n",
    "for x, y, lang, color in languages:\n",
    "    rect = mpatches.FancyBboxPatch((x-0.6, y-0.3), 1.2, 0.6,\n",
    "                                    boxstyle=\"round,pad=0.05\",\n",
    "                                    linewidth=2, edgecolor='black',\n",
    "                                    facecolor=color, alpha=0.6)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(x, y, lang, ha='center', va='center',\n",
    "            fontsize=11, fontweight='bold')\n",
    "    \n",
    "    # 連線到根節點\n",
    "    ax.plot([5, x], [10, y+0.35], 'k-', linewidth=1.5, alpha=0.5)\n",
    "\n",
    "# 第二層: 具體工具\n",
    "tools = [\n",
    "    # 中文\n",
    "    (0.5, 6.5, 'jieba\\n(分詞)', '#FFE66D'),\n",
    "    (1.5, 6.5, 'LTP/HanLP\\n(完整流程)', '#FFE66D'),\n",
    "    (2.5, 6.5, 'BERT-chinese\\n(SOTA)', '#FFE66D'),\n",
    "    \n",
    "    # 英文\n",
    "    (4, 6.5, 'NLTK\\n(教學)', '#95E1D3'),\n",
    "    (5, 6.5, 'spaCy\\n(生產)', '#95E1D3'),\n",
    "    (6, 6.5, 'Transformers\\n(SOTA)', '#95E1D3'),\n",
    "    \n",
    "    # 多語言\n",
    "    (7.5, 6.5, 'spaCy\\n(20+ 語言)', '#2E86AB'),\n",
    "    (8.5, 6.5, 'XLM-R\\n(100+ 語言)', '#2E86AB'),\n",
    "    (9.5, 6.5, 'CLIP\\n(多模態)', '#2E86AB'),\n",
    "]\n",
    "\n",
    "for x, y, tool, color in tools:\n",
    "    rect = mpatches.FancyBboxPatch((x-0.4, y-0.4), 0.8, 0.8,\n",
    "                                    boxstyle=\"round,pad=0.05\",\n",
    "                                    linewidth=1.5, edgecolor='black',\n",
    "                                    facecolor=color, alpha=0.4)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(x, y, tool, ha='center', va='center',\n",
    "            fontsize=8, fontweight='bold')\n",
    "    \n",
    "    # 連線到對應語言\n",
    "    if x < 3:\n",
    "        ax.plot([1.5, x], [7.7, y+0.45], 'k-', linewidth=1, alpha=0.3)\n",
    "    elif x < 7:\n",
    "        ax.plot([5, x], [7.7, y+0.45], 'k-', linewidth=1, alpha=0.3)\n",
    "    else:\n",
    "        ax.plot([8.5, x], [7.7, y+0.45], 'k-', linewidth=1, alpha=0.3)\n",
    "\n",
    "# 底部: 應用場景\n",
    "scenarios = [\n",
    "    (2, 4.5, '學術研究', 'NLTK + Transformers'),\n",
    "    (5, 4.5, '生產環境', 'spaCy + Docker'),\n",
    "    (8, 4.5, '快速原型', 'Pipeline API')\n",
    "]\n",
    "\n",
    "for x, y, scenario, stack in scenarios:\n",
    "    # 場景框\n",
    "    rect = mpatches.FancyBboxPatch((x-1, y-0.5), 2, 1,\n",
    "                                    boxstyle=\"round,pad=0.1\",\n",
    "                                    linewidth=2, edgecolor='#C73E1D',\n",
    "                                    facecolor='white', alpha=0.8)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(x, y+0.2, scenario, ha='center', va='center',\n",
    "            fontsize=11, fontweight='bold', color='#C73E1D')\n",
    "    ax.text(x, y-0.2, stack, ha='center', va='center',\n",
    "            fontsize=9)\n",
    "\n",
    "# 圖例\n",
    "ax.text(5, 3, '💡 選擇原則: 語言 → 任務需求 → 性能要求 → 部署環境',\n",
    "        ha='center', fontsize=11, style='italic',\n",
    "        bbox=dict(boxstyle='round,pad=0.5', facecolor='lightyellow', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n💡 決策要點:\")\n",
    "print(\"  1. 語言優先: 中文必選 jieba/LTP,英文 spaCy,多語言 Transformers\")\n",
    "print(\"  2. 任務需求: 教學用 NLTK,生產用 spaCy,SOTA 用 Transformers\")\n",
    "print(\"  3. 性能權衡: 速度 vs 準確率 (spaCy 快, Transformers 準)\")\n",
    "print(\"  4. 部署環境: CPU → spaCy, GPU → Transformers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"8\"></a>\n",
    "## 8. 完整技術棧範例\n",
    "\n",
    "### 8.1 場景: 中文情感分析系統\n",
    "\n",
    "**需求:**\n",
    "- 分析電商評論的情感傾向 (正面/負面/中性)\n",
    "- 支援實時推理 (< 100ms)\n",
    "- 準確率要求 > 90%\n",
    "\n",
    "**技術棧選擇:**\n",
    "1. **分詞:** jieba (中文分詞)\n",
    "2. **模型:** BERT-chinese (高準確率)\n",
    "3. **部署:** FastAPI + Docker (生產就緒)\n",
    "4. **優化:** ONNX (推理加速)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 完整技術棧範例: 中文情感分析\n",
    "print(\"=\" * 70)\n",
    "print(\"完整技術棧範例: 中文情感分析系統\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 模擬完整流程 (簡化版)\n",
    "\n",
    "class ChineseSentimentAnalyzer:\n",
    "    \"\"\"\n",
    "    中文情感分析器\n",
    "    \n",
    "    技術棧:\n",
    "    - 分詞: jieba\n",
    "    - 模型: BERT-chinese (Transformers)\n",
    "    - 部署: FastAPI\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        print(\"\\n🔧 初始化情感分析器...\")\n",
    "        \n",
    "        # 1. 載入分詞工具\n",
    "        try:\n",
    "            import jieba\n",
    "            self.jieba = jieba\n",
    "            print(\"  ✅ jieba 分詞器已載入\")\n",
    "        except ImportError:\n",
    "            print(\"  ⚠️ jieba 未安裝\")\n",
    "            self.jieba = None\n",
    "        \n",
    "        # 2. 載入情感分析模型 (模擬)\n",
    "        try:\n",
    "            from transformers import pipeline\n",
    "            # 實際應使用中文 BERT 模型\n",
    "            # self.sentiment = pipeline('sentiment-analysis', model='bert-base-chinese')\n",
    "            self.sentiment = pipeline('sentiment-analysis')\n",
    "            print(\"  ✅ BERT 情感分析模型已載入\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ⚠️ 模型載入失敗: {e}\")\n",
    "            self.sentiment = None\n",
    "        \n",
    "        # 3. 簡單情感詞典 (備用)\n",
    "        self.positive_words = {'好', '棒', '優秀', '喜歡', '推薦', '滿意', '完美'}\n",
    "        self.negative_words = {'差', '糟', '爛', '討厭', '失望', '垃圾', '退貨'}\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        \"\"\"文本預處理\"\"\"\n",
    "        if self.jieba:\n",
    "            words = list(self.jieba.cut(text))\n",
    "            return ' '.join(words)\n",
    "        return text\n",
    "    \n",
    "    def analyze_rule_based(self, text):\n",
    "        \"\"\"基於規則的情感分析 (備用)\"\"\"\n",
    "        if not self.jieba:\n",
    "            return {'label': 'NEUTRAL', 'score': 0.5}\n",
    "        \n",
    "        words = set(self.jieba.cut(text))\n",
    "        pos_count = len(words & self.positive_words)\n",
    "        neg_count = len(words & self.negative_words)\n",
    "        \n",
    "        if pos_count > neg_count:\n",
    "            return {'label': 'POSITIVE', 'score': 0.7 + pos_count * 0.1}\n",
    "        elif neg_count > pos_count:\n",
    "            return {'label': 'NEGATIVE', 'score': 0.7 + neg_count * 0.1}\n",
    "        else:\n",
    "            return {'label': 'NEUTRAL', 'score': 0.5}\n",
    "    \n",
    "    def analyze(self, text):\n",
    "        \"\"\"情感分析主函數\"\"\"\n",
    "        # 預處理\n",
    "        processed_text = self.preprocess(text)\n",
    "        \n",
    "        # 使用模型分析 (如可用)\n",
    "        if self.sentiment:\n",
    "            try:\n",
    "                result = self.sentiment(processed_text)[0]\n",
    "                return result\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # 否則使用規則方法\n",
    "        return self.analyze_rule_based(text)\n",
    "\n",
    "# 測試分析器\n",
    "analyzer = ChineseSentimentAnalyzer()\n",
    "\n",
    "# 測試樣本\n",
    "test_reviews = [\n",
    "    \"這個產品質量非常好,強烈推薦購買!\",\n",
    "    \"收到貨物完全不符合描述,太失望了,申請退貨。\",\n",
    "    \"還可以吧,沒有特別驚艷,也沒有很差。\",\n",
    "    \"物流超快,包裝完美,商品質量優秀,五星好評!\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"情感分析測試\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results = []\n",
    "for i, review in enumerate(test_reviews, 1):\n",
    "    result = analyzer.analyze(review)\n",
    "    \n",
    "    # 轉換標籤為中文\n",
    "    label_map = {\n",
    "        'POSITIVE': '正面',\n",
    "        'NEGATIVE': '負面',\n",
    "        'NEUTRAL': '中性'\n",
    "    }\n",
    "    label_zh = label_map.get(result['label'], result['label'])\n",
    "    \n",
    "    print(f\"\\n[{i}] 評論: {review}\")\n",
    "    print(f\"    情感: {label_zh} (信心度: {result['score']:.2%})\")\n",
    "    \n",
    "    results.append({\n",
    "        '評論': review[:30] + '...' if len(review) > 30 else review,\n",
    "        '情感': label_zh,\n",
    "        '信心度': f\"{result['score']:.2%}\"\n",
    "    })\n",
    "\n",
    "# 視覺化結果\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "table = ax.table(cellText=df_results.values, colLabels=df_results.columns,\n",
    "                cellLoc='left', loc='center',\n",
    "                colWidths=[0.6, 0.2, 0.2])\n",
    "\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1, 2.5)\n",
    "\n",
    "# 設定表頭樣式\n",
    "for i in range(len(df_results.columns)):\n",
    "    table[(0, i)].set_facecolor('#2E86AB')\n",
    "    table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "\n",
    "# 設定行顏色 (根據情感)\n",
    "for i in range(1, len(df_results) + 1):\n",
    "    sentiment = df_results.iloc[i-1]['情感']\n",
    "    if sentiment == '正面':\n",
    "        color = '#95E1D3'\n",
    "    elif sentiment == '負面':\n",
    "        color = '#F18F01'\n",
    "    else:\n",
    "        color = '#FFE66D'\n",
    "    \n",
    "    for j in range(len(df_results.columns)):\n",
    "        table[(i, j)].set_facecolor(color)\n",
    "        table[(i, j)].set_alpha(0.4)\n",
    "\n",
    "plt.title('中文情感分析結果', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✅ 情感分析系統測試完成!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n🚀 生產部署步驟:\")\n",
    "print(\"  1. 使用 BERT-chinese 微調模型\")\n",
    "print(\"  2. 轉換為 ONNX 格式加速推理\")\n",
    "print(\"  3. 使用 FastAPI 包裝為 API\")\n",
    "print(\"  4. Docker 容器化部署\")\n",
    "print(\"  5. Kubernetes 水平擴展\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FastAPI 部署範例 (代碼示範,無需執行)\n",
    "print(\"=\" * 70)\n",
    "print(\"FastAPI 部署範例代碼\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "api_code = '''\n",
    "# main.py - FastAPI 情感分析 API\n",
    "\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "import jieba\n",
    "from transformers import pipeline\n",
    "\n",
    "# 創建 FastAPI 應用\n",
    "app = FastAPI(\n",
    "    title=\"中文情感分析 API\",\n",
    "    description=\"基於 BERT 的中文情感分析服務\",\n",
    "    version=\"1.0.0\"\n",
    ")\n",
    "\n",
    "# 載入模型 (啟動時執行一次)\n",
    "sentiment_model = pipeline(\n",
    "    'sentiment-analysis',\n",
    "    model='bert-base-chinese'\n",
    ")\n",
    "\n",
    "# 請求模型\n",
    "class TextRequest(BaseModel):\n",
    "    text: str\n",
    "\n",
    "# 回應模型\n",
    "class SentimentResponse(BaseModel):\n",
    "    text: str\n",
    "    sentiment: str\n",
    "    confidence: float\n",
    "\n",
    "@app.post(\"/analyze\", response_model=SentimentResponse)\n",
    "async def analyze_sentiment(request: TextRequest):\n",
    "    \"\"\"\n",
    "    情感分析 API\n",
    "    \n",
    "    Args:\n",
    "        request: 包含待分析文本的請求\n",
    "        \n",
    "    Returns:\n",
    "        SentimentResponse: 情感分析結果\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 分詞預處理\n",
    "        words = jieba.cut(request.text)\n",
    "        processed_text = ' '.join(words)\n",
    "        \n",
    "        # 模型推理\n",
    "        result = sentiment_model(processed_text)[0]\n",
    "        \n",
    "        return SentimentResponse(\n",
    "            text=request.text,\n",
    "            sentiment=result['label'],\n",
    "            confidence=result['score']\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health_check():\n",
    "    \"\"\"健康檢查\"\"\"\n",
    "    return {\"status\": \"healthy\"}\n",
    "\n",
    "# 啟動命令:\n",
    "# uvicorn main:app --host 0.0.0.0 --port 8000\n",
    "\n",
    "# 測試 API:\n",
    "# curl -X POST \"http://localhost:8000/analyze\" \\\n",
    "#      -H \"Content-Type: application/json\" \\\n",
    "#      -d '{\"text\": \"這個產品質量非常好!\"}'\n",
    "'''\n",
    "\n",
    "print(api_code)\n",
    "\n",
    "print(\"\\n💡 部署最佳實踐:\")\n",
    "print(\"  1. 模型預載入: 應用啟動時載入模型 (避免每次請求載入)\")\n",
    "print(\"  2. 批次推理: 累積多個請求批次處理 (提升吞吐量)\")\n",
    "print(\"  3. 快取結果: 相同輸入快取結果 (減少計算)\")\n",
    "print(\"  4. 限流保護: 防止過載 (nginx/API Gateway)\")\n",
    "print(\"  5. 監控告警: Prometheus + Grafana (性能監控)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"9\"></a>\n",
    "## 9. 本課總結與延伸閱讀\n",
    "\n",
    "### 9.1 核心要點回顧\n",
    "\n",
    "#### 1. Python NLP 工具生態\n",
    "- **四大類工具:** 傳統 NLP (NLTK/spaCy)、深度學習 (Transformers)、中文專用 (jieba/LTP)、輔助工具 (Gensim)\n",
    "- **工具定位:** NLTK (教學)、spaCy (生產)、Transformers (SOTA)\n",
    "\n",
    "#### 2. 工具對比與選擇\n",
    "- **NLTK:** 功能全面,易於學習,速度慢\n",
    "- **spaCy:** 工業級,高效能 (10-100x 加速),Pipeline 設計\n",
    "- **Transformers:** 預訓練模型庫,統一 API,SOTA 性能\n",
    "- **jieba:** 中文分詞首選,輕量快速\n",
    "\n",
    "#### 3. 效能基準測試\n",
    "- **速度:** spaCy > jieba > NLTK > Transformers\n",
    "- **準確率:** Transformers > spaCy > LTP > NLTK > jieba\n",
    "- **學習曲線:** NLTK < jieba < spaCy < Transformers\n",
    "\n",
    "#### 4. 選擇決策樹\n",
    "- **中文:** jieba (分詞) + LTP/BERT (完整流程/SOTA)\n",
    "- **英文:** NLTK (教學) + spaCy (生產) + Transformers (SOTA)\n",
    "- **多語言:** spaCy (20+) + Transformers (100+)\n",
    "\n",
    "#### 5. 完整技術棧\n",
    "- **預處理:** jieba (中文) / spaCy (英文)\n",
    "- **模型:** BERT/RoBERTa (理解) / GPT (生成) / T5 (轉換)\n",
    "- **部署:** FastAPI + Docker + ONNX\n",
    "\n",
    "---\n",
    "\n",
    "### 9.2 最佳實踐總結\n",
    "\n",
    "| 場景 | 推薦工具組合 | 技術棧 |\n",
    "|:---|:---|:---|\n",
    "| **學術研究** | NLTK + Transformers | Python + Jupyter + PyTorch |\n",
    "| **生產環境** | spaCy + BERT (微調) | FastAPI + Docker + ONNX |\n",
    "| **中文分析** | jieba + BERT-chinese | jieba + Transformers + PyTorch |\n",
    "| **快速原型** | Transformers Pipeline | Python + Streamlit |\n",
    "| **多語言** | spaCy + XLM-R | spaCy + Transformers |\n",
    "\n",
    "---\n",
    "\n",
    "### 9.3 下節預告\n",
    "\n",
    "**CH03-01: 文本預處理基礎**\n",
    "\n",
    "我們將深入探討:\n",
    "- 文本清洗與標準化技術\n",
    "- 分詞算法原理 (規則、統計、深度學習)\n",
    "- 詞形還原與詞幹提取\n",
    "- 停用詞處理策略\n",
    "- 文本標準化與正則化\n",
    "\n",
    "---\n",
    "\n",
    "### 9.4 延伸閱讀\n",
    "\n",
    "#### 官方文檔\n",
    "1. **NLTK Book:** https://www.nltk.org/book/\n",
    "2. **spaCy 101:** https://spacy.io/usage/spacy-101\n",
    "3. **Transformers 文檔:** https://huggingface.co/docs/transformers/\n",
    "4. **jieba GitHub:** https://github.com/fxsjy/jieba\n",
    "\n",
    "#### 工具對比研究\n",
    "- **spaCy vs NLTK:** https://spacy.io/usage/facts-figures\n",
    "- **NLP 工具評測:** https://github.com/explosion/spacy-benchmarks\n",
    "- **中文 NLP 工具對比:** https://github.com/didi/ChineseNLP\n",
    "\n",
    "#### 學習資源\n",
    "- **Hugging Face Course:** https://huggingface.co/course/\n",
    "- **spaCy Projects:** https://github.com/explosion/projects\n",
    "- **NLP 工具實戰:** https://github.com/practical-nlp/practical-nlp\n",
    "\n",
    "#### 論文與書籍\n",
    "- **BERT:** Devlin et al. (2018). *BERT: Pre-training of Deep Bidirectional Transformers*\n",
    "- **GPT-3:** Brown et al. (2020). *Language Models are Few-Shot Learners*\n",
    "- **書籍:** *Natural Language Processing with Python* (NLTK 官方教材)\n",
    "\n",
    "---\n",
    "\n",
    "### 🙋 問題討論\n",
    "\n",
    "有任何問題嗎?歡迎在討論區提問!\n",
    "\n",
    "---\n",
    "\n",
    "**課程資訊:**\n",
    "- **作者:** iSpan NLP Team\n",
    "- **參考講義:** `課程資料/02_自然語言處理入門/講義/04_Python_NLP工具生態完全指南.md`\n",
    "- **版本:** v1.0\n",
    "- **最後更新:** 2025-10-17\n",
    "- **授權:** MIT License (僅供教學使用)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
