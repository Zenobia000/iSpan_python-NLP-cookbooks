{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH02-02: NLP 演變歷程與技術典範\n",
    "\n",
    "**課程目標:**\n",
    "- 理解 NLP 的五大技術典範轉移\n",
    "- 掌握每個時代的核心技術與代表模型\n",
    "- 了解計算能力/數據/演算法三維驅動框架\n",
    "- 認識模型規模的指數級增長\n",
    "\n",
    "**學習時間:** 約 90 分鐘\n",
    "\n",
    "**前置知識:**\n",
    "- 基礎程式設計概念\n",
    "- NLP 基本概念 (完成 CH02-01)\n",
    "- 機率論基礎\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 目錄\n",
    "\n",
    "1. [NLP 發展時間軸 (1950-2024)](#1)\n",
    "2. [第一典範: 規則系統時代 (1950-1990)](#2)\n",
    "3. [第二典範: 統計學習時代 (1990-2010)](#3)\n",
    "4. [第三典範: 淺層神經網路 (2010-2017)](#4)\n",
    "5. [第四典範: 深度學習與 Transformer (2017-2020)](#5)\n",
    "6. [第五典範: 大型語言模型 LLM (2020-至今)](#6)\n",
    "7. [模型規模演進與三維驅動框架](#7)\n",
    "8. [實戰練習](#8)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 環境設定與套件導入\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# 設定中文顯示\n",
    "plt.rcParams['font.sans-serif'] = ['Microsoft JhengHei', 'SimHei', 'Arial Unicode MS']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 設定顯示風格\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"✅ 環境設定完成\")\n",
    "print(f\"NumPy 版本: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "## 1. NLP 發展時間軸 (1950-2024)\n",
    "\n",
    "自然語言處理經歷了五次重大典範轉移,每次轉移都解決了前一代的核心瓶頸,但也引入了新的挑戰。\n",
    "\n",
    "### NLP 五大典範時期\n",
    "\n",
    "| 典範 | 時期 | 核心思想 | 代表技術 | 關鍵突破 |\n",
    "|:---|:---|:---|:---|:---|\n",
    "| **規則系統** | 1950s-1980s | 專家手工編寫規則 | CFG, ELIZA | 可解釋性強 |\n",
    "| **統計學習** | 1990s-2000s | 從數據學習機率模型 | N-gram, HMM, CRF | 自動學習,處理歧義 |\n",
    "| **淺層神經網路** | 2000s-2013 | 詞向量與淺層模型 | Word2Vec, GloVe | 解決稀疏性,捕捉語義 |\n",
    "| **深度學習** | 2013-2017 | RNN/LSTM 序列建模 | Seq2Seq, Attention | 端到端學習,長距離依賴 |\n",
    "| **Transformer & LLM** | 2017-至今 | 純注意力機制+預訓練 | BERT, GPT, ChatGPT | 完全並行,湧現能力 |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化 NLP 發展時間軸\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "# 時間軸數據\n",
    "paradigms = [\n",
    "    {'name': '規則系統', 'start': 1950, 'end': 1990, 'color': '#FF6B6B', 'y': 5},\n",
    "    {'name': '統計學習', 'start': 1990, 'end': 2010, 'color': '#4ECDC4', 'y': 4},\n",
    "    {'name': '淺層神經網路', 'start': 2003, 'end': 2013, 'color': '#45B7D1', 'y': 3},\n",
    "    {'name': '深度學習', 'start': 2013, 'end': 2017, 'color': '#F7B731', 'y': 2},\n",
    "    {'name': 'Transformer & LLM', 'start': 2017, 'end': 2024, 'color': '#5F27CD', 'y': 1}\n",
    "]\n",
    "\n",
    "# 關鍵里程碑\n",
    "milestones = [\n",
    "    {'year': 1954, 'event': 'Georgetown-IBM\\n機器翻譯實驗', 'y': 5.3},\n",
    "    {'year': 1966, 'event': 'ELIZA\\n聊天機器人', 'y': 5.3},\n",
    "    {'year': 1988, 'event': 'HMM 用於\\n語音識別', 'y': 4.3},\n",
    "    {'year': 1997, 'event': 'LSTM\\n長短期記憶網路', 'y': 3.5},\n",
    "    {'year': 2013, 'event': 'Word2Vec\\n詞向量革命', 'y': 3.3},\n",
    "    {'year': 2014, 'event': 'Seq2Seq\\n序列到序列', 'y': 2.3},\n",
    "    {'year': 2017, 'event': 'Transformer\\n\"Attention is All You Need\"', 'y': 1.5},\n",
    "    {'year': 2018, 'event': 'BERT & GPT\\n預訓練革命', 'y': 1.3},\n",
    "    {'year': 2020, 'event': 'GPT-3\\n175B 參數', 'y': 1.3},\n",
    "    {'year': 2022, 'event': 'ChatGPT\\n引爆 AI 熱潮', 'y': 1.3},\n",
    "]\n",
    "\n",
    "# 繪製時期區塊\n",
    "for p in paradigms:\n",
    "    width = p['end'] - p['start']\n",
    "    rect = mpatches.Rectangle(\n",
    "        (p['start'], p['y']-0.35), width, 0.7,\n",
    "        linewidth=2, edgecolor='white', facecolor=p['color'], alpha=0.7\n",
    "    )\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(\n",
    "        p['start'] + width/2, p['y'], p['name'],\n",
    "        ha='center', va='center', fontsize=11, fontweight='bold', color='white'\n",
    "    )\n",
    "\n",
    "# 繪製里程碑\n",
    "for m in milestones:\n",
    "    ax.plot([m['year'], m['year']], [0.5, m['y']-0.4], \n",
    "            'k--', alpha=0.4, linewidth=1)\n",
    "    ax.scatter(m['year'], m['y']-0.4, s=150, c='gold', \n",
    "               edgecolors='black', zorder=5, marker='*')\n",
    "    ax.text(m['year'], 0.3, m['event'], \n",
    "            ha='center', va='top', fontsize=8, rotation=0)\n",
    "\n",
    "# 設定座標軸\n",
    "ax.set_xlim(1945, 2026)\n",
    "ax.set_ylim(0, 6)\n",
    "ax.set_xlabel('年份', fontsize=14, fontweight='bold')\n",
    "ax.set_yticks([])\n",
    "ax.set_title('NLP 技術典範演進時間軸 (1950-2024)', \n",
    "             fontsize=18, fontweight='bold', pad=20)\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n🔍 時間軸觀察:\")\n",
    "print(\"  1. 早期典範持續時間長 (規則系統 40 年)\")\n",
    "print(\"  2. 近期典範更迭加速 (深度學習僅 4 年就被 Transformer 取代)\")\n",
    "print(\"  3. 關鍵突破密集出現在 2013-2020 年間\")\n",
    "print(\"  4. Transformer 架構主導當前 NLP 發展\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 典範轉移的驅動因素\n",
    "\n",
    "每次典範轉移都由**計算能力、數據規模、演算法創新**三股力量共同推動:\n",
    "\n",
    "```\n",
    "規則系統 → 統計學習\n",
    "├─ 數據: 大規模語料庫出現 (Penn Treebank)\n",
    "├─ 演算法: 機率圖模型成熟 (HMM, CRF)\n",
    "└─ 計算: CPU 性能提升,可處理百萬詞級語料\n",
    "\n",
    "統計學習 → 神經網路\n",
    "├─ 數據: 網路爬蟲數據爆炸 (Wikipedia, Common Crawl)\n",
    "├─ 演算法: 反向傳播算法優化,詞向量技術突破\n",
    "└─ 計算: GPU 普及,矩陣運算加速 100 倍\n",
    "\n",
    "RNN → Transformer\n",
    "├─ 數據: 多語言多任務數據集 (GLUE, SuperGLUE)\n",
    "├─ 演算法: 自注意力機制,預訓練-微調範式\n",
    "└─ 計算: TPU/雲端算力,可訓練數十億參數模型\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "## 2. 第一典範: 規則系統時代 (1950-1990)\n",
    "\n",
    "### 核心思想\n",
    "\n",
    "人類專家編寫語法規則,系統按規則執行。\n",
    "\n",
    "### 代表技術\n",
    "\n",
    "- **上下文無關文法 (Context-Free Grammar, CFG)**\n",
    "- **Georgetown-IBM 機器翻譯實驗 (1954)**\n",
    "- **ELIZA 聊天機器人 (1966)**\n",
    "\n",
    "### 優點與侷限\n",
    "\n",
    "| 優點 | 侷限 |\n",
    "|:---|:---|\n",
    "| ✅ 可解釋性強 (每個決策可追溯) | ❌ 規則覆蓋不全 (語言現象無窮) |\n",
    "| ✅ 無需大量數據 | ❌ 維護成本高 (規則衝突) |\n",
    "| ✅ 對簡單任務效果好 | ❌ 無法處理歧義 |\n",
    "\n",
    "---\n",
    "\n",
    "### 實作範例: 正則表達式分詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 規則系統範例: 使用正則表達式進行文本處理\n",
    "import re\n",
    "\n",
    "class RuleBasedTokenizer:\n",
    "    \"\"\"基於規則的分詞器\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # 手工定義規則\n",
    "        self.rules = [\n",
    "            (r\"\\b(don't|won't|can't|isn't)\\b\", self.contraction_rule),\n",
    "            (r\"\\b([A-Z][a-z]+)\\b\", self.proper_noun_rule),\n",
    "            (r\"\\b(\\d+)\\b\", self.number_rule),\n",
    "            (r\"[.,!?;:]\", self.punctuation_rule),\n",
    "        ]\n",
    "    \n",
    "    def contraction_rule(self, match):\n",
    "        return f\"[CONTRACTION: {match.group()}]\"\n",
    "    \n",
    "    def proper_noun_rule(self, match):\n",
    "        return f\"[PROPER_NOUN: {match.group()}]\"\n",
    "    \n",
    "    def number_rule(self, match):\n",
    "        return f\"[NUMBER: {match.group()}]\"\n",
    "    \n",
    "    def punctuation_rule(self, match):\n",
    "        return f\"[PUNCT: {match.group()}]\"\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        \"\"\"應用規則處理文本\"\"\"\n",
    "        result = text\n",
    "        for pattern, rule_func in self.rules:\n",
    "            result = re.sub(pattern, lambda m: rule_func(m), result)\n",
    "        return result\n",
    "\n",
    "# 測試\n",
    "tokenizer = RuleBasedTokenizer()\n",
    "sentences = [\n",
    "    \"John don't like apples.\",\n",
    "    \"Mary has 3 cats and 2 dogs!\",\n",
    "    \"I can't believe it's 2024.\"\n",
    "]\n",
    "\n",
    "print(\"規則系統處理結果:\\n\")\n",
    "for sent in sentences:\n",
    "    print(f\"原文: {sent}\")\n",
    "    print(f\"處理: {tokenizer.tokenize(sent)}\")\n",
    "    print()\n",
    "\n",
    "print(\"\\n💡 規則系統的問題:\")\n",
    "print(\"  1. 規則無法窮盡: 'cannot' 沒被識別為縮寫\")\n",
    "print(\"  2. 維護困難: 新規則可能與舊規則衝突\")\n",
    "print(\"  3. 無法處理歧義: 'John' 可能是動詞 (約翰福音)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 上下文無關文法 (CFG) 示例\n",
    "\n",
    "CFG 是規則系統的代表技術,用於描述句子的語法結構。\n",
    "\n",
    "**文法規則:**\n",
    "```\n",
    "S  → NP VP         (句子 = 名詞短語 + 動詞短語)\n",
    "NP → Det N         (名詞短語 = 限定詞 + 名詞)\n",
    "VP → V NP          (動詞短語 = 動詞 + 名詞短語)\n",
    "Det → \"the\" | \"a\"\n",
    "N → \"cat\" | \"dog\" | \"mat\"\n",
    "V → \"sat\" | \"chased\"\n",
    "```\n",
    "\n",
    "**解析樹範例:**\n",
    "```\n",
    "        S\n",
    "       / \\\n",
    "      NP  VP\n",
    "     / \\  / \\\n",
    "   Det N V  NP\n",
    "    |  | |  / \\\n",
    "  the cat sat Det N\n",
    "              |   |\n",
    "             on  mat\n",
    "```\n",
    "\n",
    "**組合爆炸問題:**\n",
    "- 50 條基本規則 → 2 條組合: C(50,2) = 1,225\n",
    "- 50 條基本規則 → 3 條組合: C(50,3) = 19,600\n",
    "- 實際語言需要數千條規則 → 手工維護不可行\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "## 3. 第二典範: 統計學習時代 (1990-2010)\n",
    "\n",
    "### 核心思想\n",
    "\n",
    "從大規模語料庫中學習機率模型,用統計方法處理歧義。\n",
    "\n",
    "### 代表技術\n",
    "\n",
    "- **N-gram 語言模型**: 預測下一個詞的機率\n",
    "- **隱馬可夫模型 (HMM)**: 詞性標註、語音識別\n",
    "- **統計機器翻譯 (SMT)**: 基於短語的翻譯\n",
    "- **條件隨機場 (CRF)**: 序列標註任務\n",
    "\n",
    "### 核心數學: 最大化條件機率\n",
    "\n",
    "$$\n",
    "\\hat{T} = \\arg\\max_{T} P(T | \\text{sentence})\n",
    "$$\n",
    "\n",
    "將規則問題轉換為優化問題!\n",
    "\n",
    "---\n",
    "\n",
    "### 實作範例: Bigram 語言模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 統計學習範例: Bigram 語言模型\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "class BigramLanguageModel:\n",
    "    \"\"\"Bigram 語言模型: P(w_i | w_{i-1})\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.bigram_counts = defaultdict(Counter)\n",
    "        self.unigram_counts = Counter()\n",
    "    \n",
    "    def train(self, sentences):\n",
    "        \"\"\"從語料庫訓練模型\"\"\"\n",
    "        for sentence in sentences:\n",
    "            words = ['<START>'] + sentence.lower().split() + ['<END>']\n",
    "            for i in range(len(words) - 1):\n",
    "                w1, w2 = words[i], words[i+1]\n",
    "                self.bigram_counts[w1][w2] += 1\n",
    "                self.unigram_counts[w1] += 1\n",
    "    \n",
    "    def probability(self, w1, w2):\n",
    "        \"\"\"計算條件機率 P(w2|w1)\"\"\"\n",
    "        if self.unigram_counts[w1] == 0:\n",
    "            return 0\n",
    "        return self.bigram_counts[w1][w2] / self.unigram_counts[w1]\n",
    "    \n",
    "    def perplexity(self, test_sentence):\n",
    "        \"\"\"計算困惑度 (模型不確定性)\"\"\"\n",
    "        words = ['<START>'] + test_sentence.lower().split() + ['<END>']\n",
    "        log_prob = 0\n",
    "        for i in range(len(words) - 1):\n",
    "            prob = self.probability(words[i], words[i+1])\n",
    "            if prob > 0:\n",
    "                log_prob += np.log2(prob)\n",
    "            else:\n",
    "                return float('inf')  # 未見過的詞組\n",
    "        return 2 ** (-log_prob / (len(words) - 1))\n",
    "    \n",
    "    def generate(self, max_length=10):\n",
    "        \"\"\"生成句子\"\"\"\n",
    "        words = ['<START>']\n",
    "        for _ in range(max_length):\n",
    "            w1 = words[-1]\n",
    "            if w1 == '<END>' or not self.bigram_counts[w1]:\n",
    "                break\n",
    "            # 選擇機率最高的下一個詞\n",
    "            next_word = self.bigram_counts[w1].most_common(1)[0][0]\n",
    "            words.append(next_word)\n",
    "        return ' '.join(words[1:-1]) if '<END>' in words else ' '.join(words[1:])\n",
    "\n",
    "# 訓練語料\n",
    "train_corpus = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the dog sat on the log\",\n",
    "    \"the cat chased the mouse\",\n",
    "    \"the mouse ran away\",\n",
    "    \"the dog chased the cat\",\n",
    "    \"the cat climbed the tree\",\n",
    "]\n",
    "\n",
    "# 訓練模型\n",
    "model = BigramLanguageModel()\n",
    "model.train(train_corpus)\n",
    "\n",
    "print(\"Bigram 語言模型訓練完成\\n\")\n",
    "\n",
    "# 測試條件機率\n",
    "print(\"條件機率測試:\")\n",
    "print(f\"  P(sat | cat) = {model.probability('cat', 'sat'):.3f}\")\n",
    "print(f\"  P(chased | cat) = {model.probability('cat', 'chased'):.3f}\")\n",
    "print(f\"  P(barked | cat) = {model.probability('cat', 'barked'):.3f} (未見過)\")\n",
    "\n",
    "# 生成句子\n",
    "print(\"\\n生成句子範例:\")\n",
    "for i in range(3):\n",
    "    print(f\"  {i+1}. {model.generate()}\")\n",
    "\n",
    "# 計算困惑度\n",
    "test_sentences = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the dog ran away\",\n",
    "    \"the elephant flew high\"  # 未見過的詞組\n",
    "]\n",
    "\n",
    "print(\"\\n困惑度測試 (越低越好):\")\n",
    "for sent in test_sentences:\n",
    "    ppl = model.perplexity(sent)\n",
    "    print(f\"  '{sent}': {ppl:.2f}\")\n",
    "\n",
    "print(\"\\n💡 統計模型的優勢:\")\n",
    "print(\"  1. 自動從數據學習,無需手工規則\")\n",
    "print(\"  2. 可處理歧義 (選擇機率最高的解釋)\")\n",
    "print(\"  3. 魯棒性強,允許小錯誤\")\n",
    "print(\"\\n⚠️ 統計模型的侷限:\")\n",
    "print(\"  1. 稀疏性問題: 未見過的詞組機率為 0\")\n",
    "print(\"  2. 無法捕捉長距離依賴 (Bigram 只看前一個詞)\")\n",
    "print(\"  3. 生成的句子較為機械,缺乏創造性\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化 Bigram 機率矩陣\n",
    "words = ['<START>', 'the', 'cat', 'dog', 'sat', 'chased', 'on', 'mat']\n",
    "prob_matrix = np.zeros((len(words), len(words)))\n",
    "\n",
    "for i, w1 in enumerate(words):\n",
    "    for j, w2 in enumerate(words):\n",
    "        prob_matrix[i, j] = model.probability(w1, w2)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(prob_matrix, annot=True, fmt='.2f', cmap='YlOrRd',\n",
    "            xticklabels=words, yticklabels=words,\n",
    "            cbar_kws={'label': 'Probability P(w_j | w_i)'})\n",
    "plt.xlabel('Next Word (w_j)', fontsize=12)\n",
    "plt.ylabel('Current Word (w_i)', fontsize=12)\n",
    "plt.title('Bigram 轉移機率矩陣', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n📊 機率矩陣解讀:\")\n",
    "print(\"  - 每一行總和為 1 (歸一化)\")\n",
    "print(\"  - 顏色越深表示轉移機率越高\")\n",
    "print(\"  - 'the' 後面最常接名詞 ('cat', 'dog')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 稀疏性問題示例\n",
    "\n",
    "統計模型的核心瓶頸是**稀疏性問題** (Sparsity Problem):\n",
    "\n",
    "```\n",
    "詞彙表大小: 100,000\n",
    "Bigram 組合: 100,000 × 100,000 = 10^10\n",
    "Trigram 組合: 100,000^3 = 10^15\n",
    "\n",
    "實際語料庫: 10^9 詞 (10 億詞)\n",
    "覆蓋率: 10^9 / 10^15 = 0.0001%\n",
    "\n",
    "結論: 絕大多數詞組從未出現,機率為 0!\n",
    "```\n",
    "\n",
    "**解決方案預告:**\n",
    "- 平滑技術 (Smoothing): Add-1, Kneser-Ney\n",
    "- **詞向量 (Word Embeddings)**: 將離散詞彙映射到連續空間 → 下一典範!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "## 4. 第三典範: 淺層神經網路 (2010-2017)\n",
    "\n",
    "### 核心思想\n",
    "\n",
    "將詞彙映射到**連續向量空間**,用神經網路學習分佈式表示。\n",
    "\n",
    "### 代表技術\n",
    "\n",
    "- **Word2Vec (2013)**: CBOW 與 Skip-gram\n",
    "- **GloVe (2014)**: 全域詞向量\n",
    "- **淺層前饋神經網路**: 文本分類、情感分析\n",
    "\n",
    "### 關鍵突破: 解決稀疏性問題\n",
    "\n",
    "```\n",
    "One-Hot 編碼 (離散空間):\n",
    "cat  = [0, 0, 0, 1, 0, ..., 0]  (100,000 維, 99.999% 為 0)\n",
    "dog  = [0, 0, 1, 0, 0, ..., 0]\n",
    "→ 無法捕捉語義相似度!\n",
    "\n",
    "詞向量 (連續空間):\n",
    "cat  = [0.5, -0.3, 0.8, ...]  (300 維, 稠密)\n",
    "dog  = [0.6, -0.2, 0.7, ...]\n",
    "→ 相似詞的向量接近!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 實作範例: Word2Vec 詞向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 詞向量範例: 使用 gensim 訓練 Word2Vec\n",
    "try:\n",
    "    from gensim.models import Word2Vec\n",
    "    gensim_available = True\n",
    "except ImportError:\n",
    "    gensim_available = False\n",
    "    print(\"⚠️ gensim 未安裝,將使用簡化版示例\")\n",
    "\n",
    "if gensim_available:\n",
    "    # 訓練語料 (更大規模)\n",
    "    sentences = [\n",
    "        [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"],\n",
    "        [\"the\", \"dog\", \"sat\", \"on\", \"the\", \"log\"],\n",
    "        [\"the\", \"cat\", \"chased\", \"the\", \"mouse\"],\n",
    "        [\"the\", \"mouse\", \"ran\", \"away\", \"quickly\"],\n",
    "        [\"the\", \"dog\", \"chased\", \"the\", \"cat\"],\n",
    "        [\"the\", \"cat\", \"climbed\", \"the\", \"tree\"],\n",
    "        [\"cats\", \"and\", \"dogs\", \"are\", \"animals\"],\n",
    "        [\"animals\", \"need\", \"food\", \"and\", \"water\"],\n",
    "        [\"the\", \"king\", \"ruled\", \"the\", \"kingdom\"],\n",
    "        [\"the\", \"queen\", \"lived\", \"in\", \"the\", \"palace\"],\n",
    "    ] * 100  # 重複以增加訓練數據\n",
    "    \n",
    "    # 訓練 Word2Vec (Skip-gram)\n",
    "    model = Word2Vec(\n",
    "        sentences=sentences,\n",
    "        vector_size=50,      # 詞向量維度\n",
    "        window=5,            # 上下文窗口大小\n",
    "        min_count=1,         # 最小詞頻\n",
    "        sg=1,                # 1=Skip-gram, 0=CBOW\n",
    "        epochs=50,\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    print(\"Word2Vec 訓練完成\\n\")\n",
    "    \n",
    "    # 查詢詞向量\n",
    "    print(\"詞向量範例 (前 10 維):\")\n",
    "    for word in ['cat', 'dog', 'king']:\n",
    "        vec = model.wv[word][:10]\n",
    "        print(f\"  {word}: {vec}\")\n",
    "    \n",
    "    # 計算相似度\n",
    "    print(\"\\n詞彙相似度測試:\")\n",
    "    pairs = [('cat', 'dog'), ('cat', 'king'), ('king', 'queen')]\n",
    "    for w1, w2 in pairs:\n",
    "        sim = model.wv.similarity(w1, w2)\n",
    "        print(f\"  similarity('{w1}', '{w2}') = {sim:.3f}\")\n",
    "    \n",
    "    # 尋找最相似的詞\n",
    "    print(\"\\n最相似的詞:\")\n",
    "    for word in ['cat', 'king']:\n",
    "        similar = model.wv.most_similar(word, topn=3)\n",
    "        print(f\"  與 '{word}' 最相似: {[w for w, _ in similar]}\")\n",
    "    \n",
    "    # 詞向量運算\n",
    "    print(\"\\n詞向量運算 (類比推理):\")\n",
    "    try:\n",
    "        # king - man + woman ≈ queen (經典範例,需要更大語料)\n",
    "        result = model.wv.most_similar(\n",
    "            positive=['king', 'woman'], \n",
    "            negative=['man'], \n",
    "            topn=1\n",
    "        )\n",
    "        print(f\"  king - man + woman ≈ {result[0][0]}\")\n",
    "    except:\n",
    "        print(\"  (需要更大規模語料才能展現類比推理能力)\")\n",
    "\n",
    "else:\n",
    "    # 簡化版: 手工創建示例詞向量\n",
    "    print(\"\\n使用簡化版詞向量示例:\\n\")\n",
    "    word_vectors = {\n",
    "        'cat':   np.array([0.5, -0.3, 0.8, 0.1]),\n",
    "        'dog':   np.array([0.6, -0.2, 0.7, 0.2]),\n",
    "        'king':  np.array([-0.1, 0.9, -0.2, 0.5]),\n",
    "        'queen': np.array([-0.2, 0.8, -0.1, 0.6]),\n",
    "    }\n",
    "    \n",
    "    def cosine_similarity(v1, v2):\n",
    "        return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "    \n",
    "    print(\"相似度測試:\")\n",
    "    print(f\"  similarity('cat', 'dog') = {cosine_similarity(word_vectors['cat'], word_vectors['dog']):.3f}\")\n",
    "    print(f\"  similarity('king', 'queen') = {cosine_similarity(word_vectors['king'], word_vectors['queen']):.3f}\")\n",
    "\n",
    "print(\"\\n💡 詞向量的優勢:\")\n",
    "print(\"  1. 解決稀疏性: 稠密表示,無 0 值問題\")\n",
    "print(\"  2. 捕捉語義: 相似詞向量接近\")\n",
    "print(\"  3. 遷移學習: 預訓練詞向量可用於下游任務\")\n",
    "print(\"\\n⚠️ 詞向量的侷限:\")\n",
    "print(\"  1. 靜態表示: 一詞多義問題 (bank: 銀行 vs 河岸)\")\n",
    "print(\"  2. 無法捕捉長距離依賴: 前饋網路無記憶\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化詞向量空間 (使用 t-SNE 降維)\n",
    "if gensim_available and len(model.wv) >= 5:\n",
    "    from sklearn.manifold import TSNE\n",
    "    \n",
    "    # 選擇部分詞彙\n",
    "    words = [word for word in model.wv.index_to_key[:20] if len(word) > 2]\n",
    "    word_vecs = np.array([model.wv[word] for word in words])\n",
    "    \n",
    "    # t-SNE 降維到 2D\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=min(5, len(words)-1))\n",
    "    word_vecs_2d = tsne.fit_transform(word_vecs)\n",
    "    \n",
    "    # 繪圖\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.scatter(word_vecs_2d[:, 0], word_vecs_2d[:, 1], s=100, alpha=0.6)\n",
    "    \n",
    "    for i, word in enumerate(words):\n",
    "        plt.annotate(word, xy=(word_vecs_2d[i, 0], word_vecs_2d[i, 1]),\n",
    "                     xytext=(5, 5), textcoords='offset points',\n",
    "                     fontsize=11, fontweight='bold')\n",
    "    \n",
    "    plt.title('詞向量空間視覺化 (t-SNE 降維)', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Dimension 1', fontsize=12)\n",
    "    plt.ylabel('Dimension 2', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n📊 視覺化觀察:\")\n",
    "    print(\"  - 相似詞在空間中距離較近\")\n",
    "    print(\"  - 語義關係被編碼為幾何關係\")\n",
    "else:\n",
    "    print(\"\\n(需要安裝 gensim 和更多訓練數據才能進行視覺化)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec 的數學原理\n",
    "\n",
    "**Skip-gram 目標:** 給定中心詞 $w_c$,預測上下文詞 $w_o$\n",
    "\n",
    "$$\n",
    "\\max \\sum_{t=1}^{T} \\sum_{-c \\leq j \\leq c, j \\neq 0} \\log P(w_{t+j} | w_t)\n",
    "$$\n",
    "\n",
    "其中條件機率:\n",
    "\n",
    "$$\n",
    "P(w_o | w_c) = \\frac{\\exp(u_o \\cdot v_c)}{\\sum_{w=1}^{V} \\exp(u_w \\cdot v_c)}\n",
    "$$\n",
    "\n",
    "- $v_c$: 中心詞向量 (Query)\n",
    "- $u_o$: 上下文詞向量 (Key)\n",
    "- 分母是所有詞的歸一化 (Softmax)\n",
    "\n",
    "**訓練技巧:**\n",
    "- **Negative Sampling**: 避免計算完整 Softmax (太慢)\n",
    "- **Hierarchical Softmax**: 使用二元樹加速\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "## 5. 第四典範: 深度學習與 Transformer (2017-2020)\n",
    "\n",
    "### 核心思想\n",
    "\n",
    "使用**循環神經網路 (RNN/LSTM)** 處理序列,後被 **Transformer** 純注意力架構取代。\n",
    "\n",
    "### 代表技術\n",
    "\n",
    "#### RNN/LSTM 時期 (2013-2017)\n",
    "- **RNN/LSTM**: 序列建模,有記憶能力\n",
    "- **Seq2Seq (2014)**: 編碼器-解碼器架構\n",
    "- **Attention 機制 (2015)**: 動態關注輸入不同部分\n",
    "\n",
    "#### Transformer 革命 (2017-2020)\n",
    "- **Transformer (2017)**: \"Attention is All You Need\"\n",
    "- **BERT (2018)**: 雙向預訓練,刷新 11 項 NLP 紀錄\n",
    "- **GPT-2 (2019)**: 生成式預訓練,15 億參數\n",
    "\n",
    "---\n",
    "\n",
    "### RNN vs Transformer 核心差異"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 對比表格: RNN vs Transformer\n",
    "comparison_data = {\n",
    "    '特性': ['並行化', '長距離依賴', '訓練速度', '計算複雜度', '記憶能力', '可擴展性'],\n",
    "    'RNN/LSTM': ['❌ 串行處理', '⚠️ 梯度消失', '🐌 慢', 'O(n)', '✅ 隱藏狀態', '⚠️ 難擴展'],\n",
    "    'Transformer': ['✅ 完全並行', '✅ 直接連接', '🚀 快 10-100x', 'O(n²)', '✅ Self-Attention', '✅ 可擴展至 100B+']\n",
    "}\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# 顯示表格\n",
    "display(HTML(df.to_html(index=False, escape=False)))\n",
    "\n",
    "print(\"\\n💡 Transformer 的革命性突破:\")\n",
    "print(\"  1. 完全拋棄循環結構,純注意力機制\")\n",
    "print(\"  2. 並行化訓練,GPU 利用率從 20% 提升到 80%+\")\n",
    "print(\"  3. 長距離依賴: 任意兩位置 O(1) 路徑連接\")\n",
    "print(\"  4. 可擴展性: GPT-3 達到 175B 參數\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention 機制可視化\n",
    "\n",
    "**Self-Attention 核心公式:**\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V\n",
    "$$\n",
    "\n",
    "**計算步驟:**\n",
    "1. **Query-Key 相似度**: $QK^T$ (點積計算相關性)\n",
    "2. **縮放**: $\\frac{QK^T}{\\sqrt{d_k}}$ (避免數值過大)\n",
    "3. **Softmax 歸一化**: 轉換為機率分佈\n",
    "4. **加權組合**: 乘以 Value 得到輸出\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 實作簡化版 Self-Attention\n",
    "def simple_self_attention(X, d_k):\n",
    "    \"\"\"\n",
    "    簡化版 Self-Attention 機制\n",
    "    \n",
    "    Args:\n",
    "        X: 輸入矩陣 (seq_len, d_model)\n",
    "        d_k: Key 維度\n",
    "    \n",
    "    Returns:\n",
    "        output: Attention 輸出\n",
    "        attn_weights: 注意力權重矩陣\n",
    "    \"\"\"\n",
    "    Q = K = V = X  # 簡化: 直接使用輸入 (實際需要投影矩陣)\n",
    "    \n",
    "    # Step 1: 計算相似度分數\n",
    "    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n",
    "    \n",
    "    # Step 2: Softmax 歸一化\n",
    "    exp_scores = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n",
    "    attn_weights = exp_scores / np.sum(exp_scores, axis=-1, keepdims=True)\n",
    "    \n",
    "    # Step 3: 加權組合\n",
    "    output = np.dot(attn_weights, V)\n",
    "    \n",
    "    return output, attn_weights\n",
    "\n",
    "# 測試範例\n",
    "np.random.seed(42)\n",
    "seq_len = 6\n",
    "d_model = 8\n",
    "\n",
    "# 模擬輸入序列: \"The cat sat on the mat\"\n",
    "X = np.random.randn(seq_len, d_model)\n",
    "output, attn_weights = simple_self_attention(X, d_model)\n",
    "\n",
    "print(f\"輸入形狀: {X.shape}\")\n",
    "print(f\"輸出形狀: {output.shape}\")\n",
    "print(f\"注意力權重形狀: {attn_weights.shape}\")\n",
    "print(f\"\\n注意力權重矩陣:\\n{attn_weights}\")\n",
    "\n",
    "# 視覺化 Attention 權重\n",
    "words = ['The', 'cat', 'sat', 'on', 'the', 'mat']\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(attn_weights, annot=True, fmt='.2f', cmap='YlOrRd',\n",
    "            xticklabels=words, yticklabels=words,\n",
    "            cbar_kws={'label': 'Attention Score'})\n",
    "plt.xlabel('Key (被關注的詞)', fontsize=12)\n",
    "plt.ylabel('Query (查詢詞)', fontsize=12)\n",
    "plt.title('Self-Attention 權重矩陣視覺化', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n🔍 Attention 權重解讀:\")\n",
    "print(\"  - 每一行總和為 1 (Softmax 歸一化)\")\n",
    "print(\"  - 對角線值較高: 詞關注自己\")\n",
    "print(\"  - 'cat' 也會關注 'sat' (語法關係)\")\n",
    "print(\"  - 'mat' 關注 'the' (限定詞依賴)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Attention 概念\n",
    "\n",
    "**為什麼需要多頭?**\n",
    "\n",
    "一個句子中存在**多種關係**:\n",
    "- **語法關係**: 主詞-動詞-受詞\n",
    "- **語義關係**: 近義詞、上下位詞\n",
    "- **指代關係**: 代名詞指涉\n",
    "\n",
    "**多頭機制讓模型同時學習這些不同關係!**\n",
    "\n",
    "$$\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h) W^O\n",
    "$$\n",
    "\n",
    "其中每個頭:\n",
    "\n",
    "$$\n",
    "\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n",
    "$$\n",
    "\n",
    "**Transformer 原論文設定:** 8 個頭,每個頭維度 64 (總維度 512)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a>\n",
    "## 6. 第五典範: 大型語言模型 LLM (2020-至今)\n",
    "\n",
    "### 核心思想\n",
    "\n",
    "**規模法則 (Scaling Laws)**: 模型越大,性能越強,甚至出現**湧現能力 (Emergent Abilities)**!\n",
    "\n",
    "### 代表模型\n",
    "\n",
    "| 模型 | 發布時間 | 參數量 | 架構類型 | 關鍵特性 |\n",
    "|:---|:---|:---|:---|:---|\n",
    "| **GPT-3** | 2020.06 | 175B | Decoder-Only | Few-shot 學習 |\n",
    "| **T5** | 2020.10 | 11B | Encoder-Decoder | 統一框架 |\n",
    "| **ChatGPT** | 2022.11 | ~175B | Decoder-Only | RLHF 對齊 |\n",
    "| **GPT-4** | 2023.03 | ~1T (估計) | Decoder-Only | 多模態能力 |\n",
    "| **Claude 3** | 2024.03 | 未公開 | Decoder-Only | 長上下文 (200K) |\n",
    "| **LLaMA 3** | 2024.04 | 70B | Decoder-Only | 開源高效 |\n",
    "\n",
    "---\n",
    "\n",
    "### LLM 三大模型家族對比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 三大模型家族對比圖\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 6))\n",
    "\n",
    "families = [\n",
    "    {\n",
    "        'name': 'Encoder-Only\\n(BERT)',\n",
    "        'structure': '雙向編碼',\n",
    "        'tasks': ['文本分類', '命名實體識別', '問答系統', '語義理解'],\n",
    "        'examples': ['BERT', 'RoBERTa', 'ALBERT'],\n",
    "        'color': '#4ECDC4'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Decoder-Only\\n(GPT)',\n",
    "        'structure': '單向生成',\n",
    "        'tasks': ['文本生成', '對話系統', '程式碼生成', '創作寫作'],\n",
    "        'examples': ['GPT-3/4', 'ChatGPT', 'Claude'],\n",
    "        'color': '#5F27CD'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Encoder-Decoder\\n(T5)',\n",
    "        'structure': '轉換架構',\n",
    "        'tasks': ['機器翻譯', '文本摘要', '問答生成', '改寫潤色'],\n",
    "        'examples': ['T5', 'BART', 'mT5'],\n",
    "        'color': '#F7B731'\n",
    "    }\n",
    "]\n",
    "\n",
    "for idx, family in enumerate(families):\n",
    "    ax = axes[idx]\n",
    "    ax.set_xlim(0, 10)\n",
    "    ax.set_ylim(0, 10)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # 標題\n",
    "    ax.text(5, 9, family['name'], ha='center', va='top',\n",
    "            fontsize=14, fontweight='bold', color=family['color'])\n",
    "    \n",
    "    # 架構\n",
    "    ax.text(5, 7.5, f\"架構: {family['structure']}\", ha='center',\n",
    "            fontsize=11, style='italic')\n",
    "    \n",
    "    # 任務列表\n",
    "    y_pos = 6.5\n",
    "    ax.text(5, y_pos, '適用任務:', ha='center', fontsize=10, fontweight='bold')\n",
    "    y_pos -= 0.8\n",
    "    for task in family['tasks']:\n",
    "        ax.text(5, y_pos, f\"• {task}\", ha='center', fontsize=9)\n",
    "        y_pos -= 0.7\n",
    "    \n",
    "    # 代表模型\n",
    "    y_pos -= 0.5\n",
    "    ax.text(5, y_pos, '代表模型:', ha='center', fontsize=10, fontweight='bold')\n",
    "    y_pos -= 0.8\n",
    "    for model in family['examples']:\n",
    "        ax.text(5, y_pos, model, ha='center', fontsize=9,\n",
    "                bbox=dict(boxstyle='round', facecolor=family['color'], alpha=0.3))\n",
    "        y_pos -= 0.7\n",
    "\n",
    "plt.suptitle('Transformer 三大模型家族', fontsize=16, fontweight='bold', y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n💡 如何選擇模型家族?\")\n",
    "print(\"  1. 理解任務 → BERT (雙向上下文)\")\n",
    "print(\"  2. 生成任務 → GPT (自回歸生成)\")\n",
    "print(\"  3. 轉換任務 → T5 (輸入→輸出映射)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM 的湧現能力 (Emergent Abilities)\n",
    "\n",
    "當模型規模超過某個**臨界點**後,會突然出現新能力:\n",
    "\n",
    "```\n",
    "模型規模 < 10B 參數:\n",
    "❌ 無法進行複雜推理\n",
    "❌ Few-shot 學習能力弱\n",
    "❌ 指令遵循能力差\n",
    "\n",
    "模型規模 > 100B 參數:\n",
    "✅ 多步驟推理 (Chain-of-Thought)\n",
    "✅ 少樣本學習 (Few-shot Learning)\n",
    "✅ 指令理解與遵循 (Instruction Following)\n",
    "✅ 程式碼生成與除錯\n",
    "✅ 常識推理能力\n",
    "```\n",
    "\n",
    "**關鍵發現:** 性能提升不是線性的,而是**突現的** (非連續跳躍)!\n",
    "\n",
    "---\n",
    "\n",
    "### 預訓練範式演變\n",
    "\n",
    "| 範式 | 代表模型 | 核心思想 | 下游任務適配 |\n",
    "|:---|:---|:---|:---|\n",
    "| **靜態詞向量** | Word2Vec, GloVe | 預訓練詞向量 | 作為特徵輸入 |\n",
    "| **預訓練 + 微調** | BERT, GPT-2 | 大規模預訓練 | 微調全部參數 |\n",
    "| **提示學習** | GPT-3 | 上下文學習 | 設計提示詞 (零樣本/少樣本) |\n",
    "| **指令微調** | ChatGPT, Claude | RLHF 對齊 | 直接對話,無需微調 |\n",
    "\n",
    "**趨勢:** 從需要大量標註數據 → 零樣本/少樣本 → 自然語言指令\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"7\"></a>\n",
    "## 7. 模型規模演進與三維驅動框架\n",
    "\n",
    "### 模型規模指數級增長\n",
    "\n",
    "從 2013 年至今,NLP 模型規模呈現**指數級增長** (約每 2 年增長 10 倍)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型規模演進數據\n",
    "model_history = [\n",
    "    {'year': 2013, 'model': 'Word2Vec', 'params': 1e6, 'flops': 1e15},\n",
    "    {'year': 2014, 'model': 'GloVe', 'params': 1e6, 'flops': 1e16},\n",
    "    {'year': 2017, 'model': 'Transformer', 'params': 1e8, 'flops': 1e18},\n",
    "    {'year': 2018, 'model': 'BERT-Base', 'params': 1.1e8, 'flops': 1e19},\n",
    "    {'year': 2018, 'model': 'GPT-1', 'params': 1.2e8, 'flops': 1e19},\n",
    "    {'year': 2019, 'model': 'BERT-Large', 'params': 3.4e8, 'flops': 5e19},\n",
    "    {'year': 2019, 'model': 'GPT-2', 'params': 1.5e9, 'flops': 1e20},\n",
    "    {'year': 2020, 'model': 'GPT-3', 'params': 1.75e11, 'flops': 3.14e23},\n",
    "    {'year': 2022, 'model': 'PaLM', 'params': 5.4e11, 'flops': 2.5e24},\n",
    "    {'year': 2023, 'model': 'GPT-4', 'params': 1e12, 'flops': 1e25},\n",
    "]\n",
    "\n",
    "# 視覺化模型規模演進\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# 左圖: 參數量演進\n",
    "years = [m['year'] for m in model_history]\n",
    "params = [m['params'] for m in model_history]\n",
    "models = [m['model'] for m in model_history]\n",
    "\n",
    "ax1.semilogy(years, params, 'o-', linewidth=2, markersize=8, color='#5F27CD')\n",
    "for i, (x, y, label) in enumerate(zip(years, params, models)):\n",
    "    if i % 2 == 0:  # 只標註部分模型以避免重疊\n",
    "        ax1.annotate(label, (x, y), textcoords='offset points',\n",
    "                     xytext=(0, 10), ha='center', fontsize=9)\n",
    "\n",
    "ax1.set_xlabel('年份', fontsize=12)\n",
    "ax1.set_ylabel('參數量 (對數尺度)', fontsize=12)\n",
    "ax1.set_title('NLP 模型參數量演進 (2013-2023)', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3, which='both')\n",
    "ax1.set_xlim(2012, 2024)\n",
    "\n",
    "# 右圖: 訓練算力演進\n",
    "flops = [m['flops'] for m in model_history]\n",
    "\n",
    "ax2.semilogy(years, flops, 's-', linewidth=2, markersize=8, color='#F7B731')\n",
    "for i, (x, y, label) in enumerate(zip(years, flops, models)):\n",
    "    if i % 2 == 1:  # 交替標註\n",
    "        ax2.annotate(label, (x, y), textcoords='offset points',\n",
    "                     xytext=(0, 10), ha='center', fontsize=9)\n",
    "\n",
    "ax2.set_xlabel('年份', fontsize=12)\n",
    "ax2.set_ylabel('訓練算力 FLOPs (對數尺度)', fontsize=12)\n",
    "ax2.set_title('NLP 模型訓練算力演進 (2013-2023)', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3, which='both')\n",
    "ax2.set_xlim(2012, 2024)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n📈 增長趨勢分析:\")\n",
    "print(f\"  - 2013-2023 年參數量增長: {params[-1]/params[0]:.2e} 倍 (100 萬倍!)\")\n",
    "print(f\"  - 2013-2023 年算力增長: {flops[-1]/flops[0]:.2e} 倍 (1000 萬倍!)\")\n",
    "print(f\"  - 摩爾定律: 每 2 年性能翻倍 → 實際增長遠超預期!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 三維驅動框架\n",
    "\n",
    "NLP 典範轉移受到**計算能力、數據規模、演算法創新**三維力量共同驅動。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化三維驅動框架\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# 三維數據 (歸一化到 0-10 範圍)\n",
    "paradigms_3d = [\n",
    "    {'name': '規則系統', 'compute': 1, 'data': 1, 'algo': 1, 'color': '#FF6B6B'},\n",
    "    {'name': '統計學習', 'compute': 2, 'data': 3, 'algo': 3, 'color': '#4ECDC4'},\n",
    "    {'name': '淺層神經網路', 'compute': 4, 'data': 5, 'algo': 5, 'color': '#45B7D1'},\n",
    "    {'name': '深度學習', 'compute': 6, 'data': 7, 'algo': 7, 'color': '#F7B731'},\n",
    "    {'name': 'Transformer', 'compute': 8, 'data': 9, 'algo': 9, 'color': '#5F27CD'},\n",
    "    {'name': 'LLM', 'compute': 10, 'data': 10, 'algo': 10, 'color': '#000000'},\n",
    "]\n",
    "\n",
    "# 繪製點\n",
    "for p in paradigms_3d:\n",
    "    ax.scatter(p['compute'], p['data'], p['algo'], \n",
    "               s=300, c=p['color'], marker='o', edgecolors='black', linewidths=2)\n",
    "    ax.text(p['compute'], p['data'], p['algo'], p['name'],\n",
    "            fontsize=9, ha='center', va='bottom')\n",
    "\n",
    "# 繪製演進路徑\n",
    "compute_path = [p['compute'] for p in paradigms_3d]\n",
    "data_path = [p['data'] for p in paradigms_3d]\n",
    "algo_path = [p['algo'] for p in paradigms_3d]\n",
    "ax.plot(compute_path, data_path, algo_path, 'k--', alpha=0.5, linewidth=1.5)\n",
    "\n",
    "# 設定座標軸\n",
    "ax.set_xlabel('計算能力\\n(Computing Power)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('數據規模\\n(Data Scale)', fontsize=12, fontweight='bold')\n",
    "ax.set_zlabel('演算法創新\\n(Algorithm Innovation)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('NLP 演變的三維驅動框架', fontsize=16, fontweight='bold', pad=20)\n",
    "\n",
    "# 設定視角\n",
    "ax.view_init(elev=20, azim=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n🎯 三維驅動框架解讀:\")\n",
    "print(\"\\n1️⃣ 計算能力 (Computing Power)\")\n",
    "print(\"  - CPU → GPU → TPU → 專用 AI 晶片\")\n",
    "print(\"  - 算力增長: 10^6 倍 (1990-2023)\")\n",
    "print(\"  - 摩爾定律持續驅動硬體進步\")\n",
    "\n",
    "print(\"\\n2️⃣ 數據規模 (Data Scale)\")\n",
    "print(\"  - Penn Treebank (1M 詞) → Common Crawl (1T 詞)\")\n",
    "print(\"  - 語料庫規模增長: 10^6 倍\")\n",
    "print(\"  - 網路爬蟲數據爆炸式增長\")\n",
    "\n",
    "print(\"\\n3️⃣ 演算法創新 (Algorithm Innovation)\")\n",
    "print(\"  - 規則 → 統計 → 神經網路 → Attention → Scaling\")\n",
    "print(\"  - 關鍵突破: Word2Vec (2013), Transformer (2017)\")\n",
    "print(\"  - 預訓練-微調範式成為主流\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 規模法則 (Scaling Laws)\n",
    "\n",
    "**OpenAI 研究發現:** 模型性能與計算量呈冪律關係\n",
    "\n",
    "$$\n",
    "\\text{Performance} \\propto (\\text{Compute})^\\alpha\n",
    "$$\n",
    "\n",
    "其中 $\\alpha \\approx 0.5$ (實驗測得)\n",
    "\n",
    "**啟示:**\n",
    "- 計算量每增加 10 倍 → 性能提升 $\\sqrt{10} \\approx 3.16$ 倍\n",
    "- 計算量每增加 100 倍 → 性能提升 10 倍\n",
    "- **結論:** 只要有足夠算力,模型還有巨大提升空間!\n",
    "\n",
    "**限制因素:**\n",
    "1. 能源消耗 (GPT-3 訓練耗電 ~1,287 MWh)\n",
    "2. 硬體成本 (數千萬美元 GPU 集群)\n",
    "3. 環境影響 (碳排放問題)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"8\"></a>\n",
    "## 8. 實戰練習\n",
    "\n",
    "### 練習 1: 對比不同時代的分詞技術\n",
    "\n",
    "實作並對比規則分詞、統計分詞、神經網路分詞。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 練習 1: 對比三種分詞方法\n",
    "test_sentence = \"I don't believe it's raining cats and dogs!\"\n",
    "\n",
    "print(\"測試句子:\", test_sentence)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# 方法 1: 規則分詞 (正則表達式)\n",
    "def rule_based_tokenize(text):\n",
    "    \"\"\"規則系統: 使用正則表達式\"\"\"\n",
    "    tokens = re.findall(r\"\\b\\w+\\b|[.,!?]\", text)\n",
    "    return tokens\n",
    "\n",
    "tokens_rule = rule_based_tokenize(test_sentence)\n",
    "print(\"\\n1️⃣ 規則分詞 (正則表達式):\")\n",
    "print(f\"  結果: {tokens_rule}\")\n",
    "print(f\"  數量: {len(tokens_rule)} tokens\")\n",
    "print(\"  優點: 快速、可解釋\")\n",
    "print(\"  缺點: 無法處理縮寫 (don't → do n't)\")\n",
    "\n",
    "# 方法 2: 統計分詞 (基於頻率)\n",
    "def statistical_tokenize(text):\n",
    "    \"\"\"統計學習: 基於詞頻的簡單分詞\"\"\"\n",
    "    # 簡化: 使用空格分詞 + 標點分離\n",
    "    tokens = []\n",
    "    for word in text.split():\n",
    "        # 分離標點\n",
    "        if word[-1] in '.,!?':\n",
    "            tokens.extend([word[:-1], word[-1]])\n",
    "        else:\n",
    "            tokens.append(word)\n",
    "    return tokens\n",
    "\n",
    "tokens_stat = statistical_tokenize(test_sentence)\n",
    "print(\"\\n2️⃣ 統計分詞 (基於頻率):\")\n",
    "print(f\"  結果: {tokens_stat}\")\n",
    "print(f\"  數量: {len(tokens_stat)} tokens\")\n",
    "print(\"  優點: 自動學習分詞邊界\")\n",
    "print(\"  缺點: 仍無法完美處理縮寫\")\n",
    "\n",
    "# 方法 3: 神經網路分詞 (子詞分詞 BPE)\n",
    "def neural_tokenize(text):\n",
    "    \"\"\"神經網路: 子詞分詞 (BPE 風格)\"\"\"\n",
    "    # 簡化版: 分離常見縮寫\n",
    "    text = text.replace(\"don't\", \"do n't\")\n",
    "    text = text.replace(\"it's\", \"it 's\")\n",
    "    tokens = re.findall(r\"\\b\\w+\\b|[.,!?]\", text)\n",
    "    return tokens\n",
    "\n",
    "tokens_neural = neural_tokenize(test_sentence)\n",
    "print(\"\\n3️⃣ 神經網路分詞 (子詞分詞):\")\n",
    "print(f\"  結果: {tokens_neural}\")\n",
    "print(f\"  數量: {len(tokens_neural)} tokens\")\n",
    "print(\"  優點: 可處理縮寫、未登錄詞\")\n",
    "print(\"  缺點: 需要大量訓練數據\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\n💡 總結:\")\n",
    "print(\"  - 規則系統: 快但僵化\")\n",
    "print(\"  - 統計學習: 自動但需要特徵工程\")\n",
    "print(\"  - 神經網路: 強大但需要數據與算力\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習 2: 模型規模與性能關係模擬\n",
    "\n",
    "模擬規模法則,觀察模型規模與性能的關係。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 練習 2: 規模法則模擬\n",
    "def scaling_law(compute, alpha=0.5):\n",
    "    \"\"\"規模法則: Performance ∝ Compute^α\"\"\"\n",
    "    base_performance = 50  # 基準性能\n",
    "    return base_performance + 40 * (compute ** alpha) / (1e12 ** alpha)\n",
    "\n",
    "# 模擬不同算力下的性能\n",
    "compute_range = np.logspace(15, 25, 50)  # 10^15 到 10^25 FLOPs\n",
    "performance = [scaling_law(c) for c in compute_range]\n",
    "\n",
    "# 標記真實模型位置\n",
    "real_models = [\n",
    "    {'name': 'Word2Vec', 'compute': 1e15, 'color': '#FF6B6B'},\n",
    "    {'name': 'BERT-Base', 'compute': 1e19, 'color': '#4ECDC4'},\n",
    "    {'name': 'GPT-2', 'compute': 1e20, 'color': '#45B7D1'},\n",
    "    {'name': 'GPT-3', 'compute': 3.14e23, 'color': '#5F27CD'},\n",
    "    {'name': 'GPT-4', 'compute': 1e25, 'color': '#000000'},\n",
    "]\n",
    "\n",
    "# 繪圖\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.plot(compute_range, performance, linewidth=3, color='#5F27CD', alpha=0.7)\n",
    "\n",
    "# 標記真實模型\n",
    "for model in real_models:\n",
    "    perf = scaling_law(model['compute'])\n",
    "    plt.scatter(model['compute'], perf, s=200, c=model['color'], \n",
    "                edgecolors='black', linewidths=2, zorder=5)\n",
    "    plt.annotate(model['name'], xy=(model['compute'], perf),\n",
    "                 xytext=(10, 10), textcoords='offset points',\n",
    "                 fontsize=11, fontweight='bold',\n",
    "                 bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.xlabel('訓練算力 (FLOPs, 對數尺度)', fontsize=13)\n",
    "plt.ylabel('模型性能 (任意單位)', fontsize=13)\n",
    "plt.title('規模法則: 模型性能與計算量的冪律關係', fontsize=16, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3, which='both')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n📈 規模法則分析:\")\n",
    "print(f\"  - 算力從 10^15 到 10^25 (增長 10^10 倍)\")\n",
    "print(f\"  - 性能從 {scaling_law(1e15):.1f} 提升到 {scaling_law(1e25):.1f}\")\n",
    "print(f\"  - 性能增長倍數: {scaling_law(1e25)/scaling_law(1e15):.1f}x\")\n",
    "print(\"\\n💡 啟示:\")\n",
    "print(\"  1. 性能提升不是線性的,而是遵循冪律\")\n",
    "print(\"  2. 早期階段提升快,後期邊際收益遞減\")\n",
    "print(\"  3. 但只要有算力,仍有巨大提升空間!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習 3: 思考題\n",
    "\n",
    "1. **典範轉移的本質:** 每次典範轉移解決了什麼核心問題?引入了什麼新挑戰?\n",
    "\n",
    "2. **未來預測:** 基於歷史趨勢,你認為下一次典範轉移會是什麼?\n",
    "   - 提示: 考慮當前 LLM 的侷限性 (可解釋性、能源消耗、偏見問題)\n",
    "\n",
    "3. **實際應用:** 對於你的應用場景,應該選擇哪個時代的技術?\n",
    "   - 簡單任務: 規則系統?\n",
    "   - 複雜理解: BERT?\n",
    "   - 生成任務: GPT?\n",
    "\n",
    "在下方 Cell 中記錄你的思考:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 練習 3: 記錄你的思考\n",
    "\n",
    "# 問題 1: 典範轉移的核心驅動力是什麼?\n",
    "your_answer_1 = \"\"\"\n",
    "我的觀察:\n",
    "1. 規則 → 統計: \n",
    "2. 統計 → 神經網路:\n",
    "3. RNN → Transformer:\n",
    "\"\"\"\n",
    "\n",
    "# 問題 2: 下一次典範轉移可能是什麼?\n",
    "your_answer_2 = \"\"\"\n",
    "我的預測:\n",
    "- 可能方向:\n",
    "- 技術突破:\n",
    "\"\"\"\n",
    "\n",
    "# 問題 3: 如何選擇合適的技術?\n",
    "your_answer_3 = \"\"\"\n",
    "我的場景:\n",
    "- 應用需求:\n",
    "- 推薦技術:\n",
    "- 理由:\n",
    "\"\"\"\n",
    "\n",
    "print(\"思考記錄已保存,繼續學習!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 📚 本課總結\n",
    "\n",
    "### 核心要點回顧\n",
    "\n",
    "1. **NLP 五大典範轉移:**\n",
    "   - 規則系統 (1950-1990): 專家規則,可解釋但覆蓋不全\n",
    "   - 統計學習 (1990-2010): 機率模型,自動學習但稀疏性問題\n",
    "   - 淺層神經網路 (2010-2017): 詞向量,解決稀疏性但靜態表示\n",
    "   - 深度學習 (2013-2017): RNN/LSTM,序列建模但無法並行\n",
    "   - Transformer & LLM (2017-至今): 純注意力,完全並行且可擴展\n",
    "\n",
    "2. **三維驅動框架:**\n",
    "   - 計算能力: 10^6 倍增長 (1990-2023)\n",
    "   - 數據規模: 10^6 倍增長 (1M → 1T 詞)\n",
    "   - 演算法創新: 規則 → 統計 → 神經網路 → Attention\n",
    "\n",
    "3. **模型規模演進:**\n",
    "   - 指數級增長: 10^6 → 10^12 參數 (100 萬倍)\n",
    "   - 規模法則: Performance ∝ Compute^0.5\n",
    "   - 湧現能力: 規模超過臨界點後出現新能力\n",
    "\n",
    "4. **技術選擇建議:**\n",
    "   - 理解任務 → BERT (Encoder-Only)\n",
    "   - 生成任務 → GPT (Decoder-Only)\n",
    "   - 轉換任務 → T5 (Encoder-Decoder)\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 下節預告\n",
    "\n",
    "**CH02-03: NLP 核心任務與應用**\n",
    "\n",
    "我們將探討:\n",
    "- NLP 核心任務分類 (序列、分類、生成)\n",
    "- 實際應用案例分析\n",
    "- 任務難度梯度與技術選型\n",
    "\n",
    "---\n",
    "\n",
    "## 📖 延伸閱讀\n",
    "\n",
    "### 關鍵論文\n",
    "\n",
    "1. **Word2Vec**: Mikolov et al. (2013). [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781)\n",
    "2. **Seq2Seq**: Sutskever et al. (2014). [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)\n",
    "3. **Attention**: Bahdanau et al. (2015). [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)\n",
    "4. **Transformer**: Vaswani et al. (2017). [Attention is All You Need](https://arxiv.org/abs/1706.03762)\n",
    "5. **BERT**: Devlin et al. (2018). [BERT: Pre-training of Deep Bidirectional Transformers](https://arxiv.org/abs/1810.04805)\n",
    "6. **GPT-3**: Brown et al. (2020). [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)\n",
    "7. **Scaling Laws**: Kaplan et al. (2020). [Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361)\n",
    "\n",
    "### 技術博客\n",
    "\n",
    "- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/) - Jay Alammar\n",
    "- [BERT 論文精讀](https://zhuanlan.zhihu.com/p/46652512) - 知乎專欄\n",
    "- [GPT 系列演進史](https://huggingface.co/blog/gpt-evolution) - Hugging Face\n",
    "\n",
    "### 學習資源\n",
    "\n",
    "- **Stanford CS224N**: [NLP with Deep Learning](http://web.stanford.edu/class/cs224n/)\n",
    "- **Hugging Face Course**: [NLP Course](https://huggingface.co/learn/nlp-course)\n",
    "- **Papers With Code**: [NLP Progress](https://paperswithcode.com/area/natural-language-processing)\n",
    "\n",
    "---\n",
    "\n",
    "**課程資訊:**\n",
    "- **作者:** iSpan NLP Team\n",
    "- **版本:** v1.0\n",
    "- **最後更新:** 2025-10-17\n",
    "- **授權:** MIT License (僅供教學使用)\n",
    "\n",
    "---\n",
    "\n",
    "### 🙋 問題討論\n",
    "\n",
    "有任何問題嗎?歡迎在討論區提問!\n",
    "\n",
    "**常見問題:**\n",
    "1. Q: 為什麼 Transformer 比 RNN 快這麼多?\n",
    "   - A: 並行化!RNN 必須串行計算,Transformer 所有位置同時計算。\n",
    "\n",
    "2. Q: 什麼是湧現能力?\n",
    "   - A: 模型規模超過臨界點後突然出現的新能力,如複雜推理、少樣本學習。\n",
    "\n",
    "3. Q: 我應該選擇 BERT 還是 GPT?\n",
    "   - A: 看任務!理解任務用 BERT (雙向),生成任務用 GPT (單向)。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
