{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH02-02: NLP æ¼”è®Šæ­·ç¨‹èˆ‡æŠ€è¡“å…¸ç¯„\n",
    "\n",
    "**èª²ç¨‹ç›®æ¨™:**\n",
    "- ç†è§£ NLP çš„äº”å¤§æŠ€è¡“å…¸ç¯„è½‰ç§»\n",
    "- æŒæ¡æ¯å€‹æ™‚ä»£çš„æ ¸å¿ƒæŠ€è¡“èˆ‡ä»£è¡¨æ¨¡å‹\n",
    "- äº†è§£è¨ˆç®—èƒ½åŠ›/æ•¸æ“š/æ¼”ç®—æ³•ä¸‰ç¶­é©…å‹•æ¡†æ¶\n",
    "- èªè­˜æ¨¡å‹è¦æ¨¡çš„æŒ‡æ•¸ç´šå¢é•·\n",
    "\n",
    "**å­¸ç¿’æ™‚é–“:** ç´„ 90 åˆ†é˜\n",
    "\n",
    "**å‰ç½®çŸ¥è­˜:**\n",
    "- åŸºç¤ç¨‹å¼è¨­è¨ˆæ¦‚å¿µ\n",
    "- NLP åŸºæœ¬æ¦‚å¿µ (å®Œæˆ CH02-01)\n",
    "- æ©Ÿç‡è«–åŸºç¤\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š ç›®éŒ„\n",
    "\n",
    "1. [NLP ç™¼å±•æ™‚é–“è»¸ (1950-2024)](#1)\n",
    "2. [ç¬¬ä¸€å…¸ç¯„: è¦å‰‡ç³»çµ±æ™‚ä»£ (1950-1990)](#2)\n",
    "3. [ç¬¬äºŒå…¸ç¯„: çµ±è¨ˆå­¸ç¿’æ™‚ä»£ (1990-2010)](#3)\n",
    "4. [ç¬¬ä¸‰å…¸ç¯„: æ·ºå±¤ç¥ç¶“ç¶²è·¯ (2010-2017)](#4)\n",
    "5. [ç¬¬å››å…¸ç¯„: æ·±åº¦å­¸ç¿’èˆ‡ Transformer (2017-2020)](#5)\n",
    "6. [ç¬¬äº”å…¸ç¯„: å¤§å‹èªè¨€æ¨¡å‹ LLM (2020-è‡³ä»Š)](#6)\n",
    "7. [æ¨¡å‹è¦æ¨¡æ¼”é€²èˆ‡ä¸‰ç¶­é©…å‹•æ¡†æ¶](#7)\n",
    "8. [å¯¦æˆ°ç·´ç¿’](#8)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç’°å¢ƒè¨­å®šèˆ‡å¥—ä»¶å°å…¥\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# è¨­å®šä¸­æ–‡é¡¯ç¤º\n",
    "plt.rcParams['font.sans-serif'] = ['Microsoft JhengHei', 'SimHei', 'Arial Unicode MS']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# è¨­å®šé¡¯ç¤ºé¢¨æ ¼\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"âœ… ç’°å¢ƒè¨­å®šå®Œæˆ\")\n",
    "print(f\"NumPy ç‰ˆæœ¬: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "## 1. NLP ç™¼å±•æ™‚é–“è»¸ (1950-2024)\n",
    "\n",
    "è‡ªç„¶èªè¨€è™•ç†ç¶“æ­·äº†äº”æ¬¡é‡å¤§å…¸ç¯„è½‰ç§»,æ¯æ¬¡è½‰ç§»éƒ½è§£æ±ºäº†å‰ä¸€ä»£çš„æ ¸å¿ƒç“¶é ¸,ä½†ä¹Ÿå¼•å…¥äº†æ–°çš„æŒ‘æˆ°ã€‚\n",
    "\n",
    "### NLP äº”å¤§å…¸ç¯„æ™‚æœŸ\n",
    "\n",
    "| å…¸ç¯„ | æ™‚æœŸ | æ ¸å¿ƒæ€æƒ³ | ä»£è¡¨æŠ€è¡“ | é—œéµçªç ´ |\n",
    "|:---|:---|:---|:---|:---|\n",
    "| **è¦å‰‡ç³»çµ±** | 1950s-1980s | å°ˆå®¶æ‰‹å·¥ç·¨å¯«è¦å‰‡ | CFG, ELIZA | å¯è§£é‡‹æ€§å¼· |\n",
    "| **çµ±è¨ˆå­¸ç¿’** | 1990s-2000s | å¾æ•¸æ“šå­¸ç¿’æ©Ÿç‡æ¨¡å‹ | N-gram, HMM, CRF | è‡ªå‹•å­¸ç¿’,è™•ç†æ­§ç¾© |\n",
    "| **æ·ºå±¤ç¥ç¶“ç¶²è·¯** | 2000s-2013 | è©å‘é‡èˆ‡æ·ºå±¤æ¨¡å‹ | Word2Vec, GloVe | è§£æ±ºç¨€ç–æ€§,æ•æ‰èªç¾© |\n",
    "| **æ·±åº¦å­¸ç¿’** | 2013-2017 | RNN/LSTM åºåˆ—å»ºæ¨¡ | Seq2Seq, Attention | ç«¯åˆ°ç«¯å­¸ç¿’,é•·è·é›¢ä¾è³´ |\n",
    "| **Transformer & LLM** | 2017-è‡³ä»Š | ç´”æ³¨æ„åŠ›æ©Ÿåˆ¶+é è¨“ç·´ | BERT, GPT, ChatGPT | å®Œå…¨ä¸¦è¡Œ,æ¹§ç¾èƒ½åŠ› |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¦–è¦ºåŒ– NLP ç™¼å±•æ™‚é–“è»¸\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "# æ™‚é–“è»¸æ•¸æ“š\n",
    "paradigms = [\n",
    "    {'name': 'è¦å‰‡ç³»çµ±', 'start': 1950, 'end': 1990, 'color': '#FF6B6B', 'y': 5},\n",
    "    {'name': 'çµ±è¨ˆå­¸ç¿’', 'start': 1990, 'end': 2010, 'color': '#4ECDC4', 'y': 4},\n",
    "    {'name': 'æ·ºå±¤ç¥ç¶“ç¶²è·¯', 'start': 2003, 'end': 2013, 'color': '#45B7D1', 'y': 3},\n",
    "    {'name': 'æ·±åº¦å­¸ç¿’', 'start': 2013, 'end': 2017, 'color': '#F7B731', 'y': 2},\n",
    "    {'name': 'Transformer & LLM', 'start': 2017, 'end': 2024, 'color': '#5F27CD', 'y': 1}\n",
    "]\n",
    "\n",
    "# é—œéµé‡Œç¨‹ç¢‘\n",
    "milestones = [\n",
    "    {'year': 1954, 'event': 'Georgetown-IBM\\næ©Ÿå™¨ç¿»è­¯å¯¦é©—', 'y': 5.3},\n",
    "    {'year': 1966, 'event': 'ELIZA\\nèŠå¤©æ©Ÿå™¨äºº', 'y': 5.3},\n",
    "    {'year': 1988, 'event': 'HMM ç”¨æ–¼\\nèªéŸ³è­˜åˆ¥', 'y': 4.3},\n",
    "    {'year': 1997, 'event': 'LSTM\\né•·çŸ­æœŸè¨˜æ†¶ç¶²è·¯', 'y': 3.5},\n",
    "    {'year': 2013, 'event': 'Word2Vec\\nè©å‘é‡é©å‘½', 'y': 3.3},\n",
    "    {'year': 2014, 'event': 'Seq2Seq\\nåºåˆ—åˆ°åºåˆ—', 'y': 2.3},\n",
    "    {'year': 2017, 'event': 'Transformer\\n\"Attention is All You Need\"', 'y': 1.5},\n",
    "    {'year': 2018, 'event': 'BERT & GPT\\né è¨“ç·´é©å‘½', 'y': 1.3},\n",
    "    {'year': 2020, 'event': 'GPT-3\\n175B åƒæ•¸', 'y': 1.3},\n",
    "    {'year': 2022, 'event': 'ChatGPT\\nå¼•çˆ† AI ç†±æ½®', 'y': 1.3},\n",
    "]\n",
    "\n",
    "# ç¹ªè£½æ™‚æœŸå€å¡Š\n",
    "for p in paradigms:\n",
    "    width = p['end'] - p['start']\n",
    "    rect = mpatches.Rectangle(\n",
    "        (p['start'], p['y']-0.35), width, 0.7,\n",
    "        linewidth=2, edgecolor='white', facecolor=p['color'], alpha=0.7\n",
    "    )\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(\n",
    "        p['start'] + width/2, p['y'], p['name'],\n",
    "        ha='center', va='center', fontsize=11, fontweight='bold', color='white'\n",
    "    )\n",
    "\n",
    "# ç¹ªè£½é‡Œç¨‹ç¢‘\n",
    "for m in milestones:\n",
    "    ax.plot([m['year'], m['year']], [0.5, m['y']-0.4], \n",
    "            'k--', alpha=0.4, linewidth=1)\n",
    "    ax.scatter(m['year'], m['y']-0.4, s=150, c='gold', \n",
    "               edgecolors='black', zorder=5, marker='*')\n",
    "    ax.text(m['year'], 0.3, m['event'], \n",
    "            ha='center', va='top', fontsize=8, rotation=0)\n",
    "\n",
    "# è¨­å®šåº§æ¨™è»¸\n",
    "ax.set_xlim(1945, 2026)\n",
    "ax.set_ylim(0, 6)\n",
    "ax.set_xlabel('å¹´ä»½', fontsize=14, fontweight='bold')\n",
    "ax.set_yticks([])\n",
    "ax.set_title('NLP æŠ€è¡“å…¸ç¯„æ¼”é€²æ™‚é–“è»¸ (1950-2024)', \n",
    "             fontsize=18, fontweight='bold', pad=20)\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ” æ™‚é–“è»¸è§€å¯Ÿ:\")\n",
    "print(\"  1. æ—©æœŸå…¸ç¯„æŒçºŒæ™‚é–“é•· (è¦å‰‡ç³»çµ± 40 å¹´)\")\n",
    "print(\"  2. è¿‘æœŸå…¸ç¯„æ›´è¿­åŠ é€Ÿ (æ·±åº¦å­¸ç¿’åƒ… 4 å¹´å°±è¢« Transformer å–ä»£)\")\n",
    "print(\"  3. é—œéµçªç ´å¯†é›†å‡ºç¾åœ¨ 2013-2020 å¹´é–“\")\n",
    "print(\"  4. Transformer æ¶æ§‹ä¸»å°ç•¶å‰ NLP ç™¼å±•\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å…¸ç¯„è½‰ç§»çš„é©…å‹•å› ç´ \n",
    "\n",
    "æ¯æ¬¡å…¸ç¯„è½‰ç§»éƒ½ç”±**è¨ˆç®—èƒ½åŠ›ã€æ•¸æ“šè¦æ¨¡ã€æ¼”ç®—æ³•å‰µæ–°**ä¸‰è‚¡åŠ›é‡å…±åŒæ¨å‹•:\n",
    "\n",
    "```\n",
    "è¦å‰‡ç³»çµ± â†’ çµ±è¨ˆå­¸ç¿’\n",
    "â”œâ”€ æ•¸æ“š: å¤§è¦æ¨¡èªæ–™åº«å‡ºç¾ (Penn Treebank)\n",
    "â”œâ”€ æ¼”ç®—æ³•: æ©Ÿç‡åœ–æ¨¡å‹æˆç†Ÿ (HMM, CRF)\n",
    "â””â”€ è¨ˆç®—: CPU æ€§èƒ½æå‡,å¯è™•ç†ç™¾è¬è©ç´šèªæ–™\n",
    "\n",
    "çµ±è¨ˆå­¸ç¿’ â†’ ç¥ç¶“ç¶²è·¯\n",
    "â”œâ”€ æ•¸æ“š: ç¶²è·¯çˆ¬èŸ²æ•¸æ“šçˆ†ç‚¸ (Wikipedia, Common Crawl)\n",
    "â”œâ”€ æ¼”ç®—æ³•: åå‘å‚³æ’­ç®—æ³•å„ªåŒ–,è©å‘é‡æŠ€è¡“çªç ´\n",
    "â””â”€ è¨ˆç®—: GPU æ™®åŠ,çŸ©é™£é‹ç®—åŠ é€Ÿ 100 å€\n",
    "\n",
    "RNN â†’ Transformer\n",
    "â”œâ”€ æ•¸æ“š: å¤šèªè¨€å¤šä»»å‹™æ•¸æ“šé›† (GLUE, SuperGLUE)\n",
    "â”œâ”€ æ¼”ç®—æ³•: è‡ªæ³¨æ„åŠ›æ©Ÿåˆ¶,é è¨“ç·´-å¾®èª¿ç¯„å¼\n",
    "â””â”€ è¨ˆç®—: TPU/é›²ç«¯ç®—åŠ›,å¯è¨“ç·´æ•¸åå„„åƒæ•¸æ¨¡å‹\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "## 2. ç¬¬ä¸€å…¸ç¯„: è¦å‰‡ç³»çµ±æ™‚ä»£ (1950-1990)\n",
    "\n",
    "### æ ¸å¿ƒæ€æƒ³\n",
    "\n",
    "äººé¡å°ˆå®¶ç·¨å¯«èªæ³•è¦å‰‡,ç³»çµ±æŒ‰è¦å‰‡åŸ·è¡Œã€‚\n",
    "\n",
    "### ä»£è¡¨æŠ€è¡“\n",
    "\n",
    "- **ä¸Šä¸‹æ–‡ç„¡é—œæ–‡æ³• (Context-Free Grammar, CFG)**\n",
    "- **Georgetown-IBM æ©Ÿå™¨ç¿»è­¯å¯¦é©— (1954)**\n",
    "- **ELIZA èŠå¤©æ©Ÿå™¨äºº (1966)**\n",
    "\n",
    "### å„ªé»èˆ‡ä¾·é™\n",
    "\n",
    "| å„ªé» | ä¾·é™ |\n",
    "|:---|:---|\n",
    "| âœ… å¯è§£é‡‹æ€§å¼· (æ¯å€‹æ±ºç­–å¯è¿½æº¯) | âŒ è¦å‰‡è¦†è“‹ä¸å…¨ (èªè¨€ç¾è±¡ç„¡çª®) |\n",
    "| âœ… ç„¡éœ€å¤§é‡æ•¸æ“š | âŒ ç¶­è­·æˆæœ¬é«˜ (è¦å‰‡è¡çª) |\n",
    "| âœ… å°ç°¡å–®ä»»å‹™æ•ˆæœå¥½ | âŒ ç„¡æ³•è™•ç†æ­§ç¾© |\n",
    "\n",
    "---\n",
    "\n",
    "### å¯¦ä½œç¯„ä¾‹: æ­£å‰‡è¡¨é”å¼åˆ†è©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¦å‰‡ç³»çµ±ç¯„ä¾‹: ä½¿ç”¨æ­£å‰‡è¡¨é”å¼é€²è¡Œæ–‡æœ¬è™•ç†\n",
    "import re\n",
    "\n",
    "class RuleBasedTokenizer:\n",
    "    \"\"\"åŸºæ–¼è¦å‰‡çš„åˆ†è©å™¨\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # æ‰‹å·¥å®šç¾©è¦å‰‡\n",
    "        self.rules = [\n",
    "            (r\"\\b(don't|won't|can't|isn't)\\b\", self.contraction_rule),\n",
    "            (r\"\\b([A-Z][a-z]+)\\b\", self.proper_noun_rule),\n",
    "            (r\"\\b(\\d+)\\b\", self.number_rule),\n",
    "            (r\"[.,!?;:]\", self.punctuation_rule),\n",
    "        ]\n",
    "    \n",
    "    def contraction_rule(self, match):\n",
    "        return f\"[CONTRACTION: {match.group()}]\"\n",
    "    \n",
    "    def proper_noun_rule(self, match):\n",
    "        return f\"[PROPER_NOUN: {match.group()}]\"\n",
    "    \n",
    "    def number_rule(self, match):\n",
    "        return f\"[NUMBER: {match.group()}]\"\n",
    "    \n",
    "    def punctuation_rule(self, match):\n",
    "        return f\"[PUNCT: {match.group()}]\"\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        \"\"\"æ‡‰ç”¨è¦å‰‡è™•ç†æ–‡æœ¬\"\"\"\n",
    "        result = text\n",
    "        for pattern, rule_func in self.rules:\n",
    "            result = re.sub(pattern, lambda m: rule_func(m), result)\n",
    "        return result\n",
    "\n",
    "# æ¸¬è©¦\n",
    "tokenizer = RuleBasedTokenizer()\n",
    "sentences = [\n",
    "    \"John don't like apples.\",\n",
    "    \"Mary has 3 cats and 2 dogs!\",\n",
    "    \"I can't believe it's 2024.\"\n",
    "]\n",
    "\n",
    "print(\"è¦å‰‡ç³»çµ±è™•ç†çµæœ:\\n\")\n",
    "for sent in sentences:\n",
    "    print(f\"åŸæ–‡: {sent}\")\n",
    "    print(f\"è™•ç†: {tokenizer.tokenize(sent)}\")\n",
    "    print()\n",
    "\n",
    "print(\"\\nğŸ’¡ è¦å‰‡ç³»çµ±çš„å•é¡Œ:\")\n",
    "print(\"  1. è¦å‰‡ç„¡æ³•çª®ç›¡: 'cannot' æ²’è¢«è­˜åˆ¥ç‚ºç¸®å¯«\")\n",
    "print(\"  2. ç¶­è­·å›°é›£: æ–°è¦å‰‡å¯èƒ½èˆ‡èˆŠè¦å‰‡è¡çª\")\n",
    "print(\"  3. ç„¡æ³•è™•ç†æ­§ç¾©: 'John' å¯èƒ½æ˜¯å‹•è© (ç´„ç¿°ç¦éŸ³)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¸Šä¸‹æ–‡ç„¡é—œæ–‡æ³• (CFG) ç¤ºä¾‹\n",
    "\n",
    "CFG æ˜¯è¦å‰‡ç³»çµ±çš„ä»£è¡¨æŠ€è¡“,ç”¨æ–¼æè¿°å¥å­çš„èªæ³•çµæ§‹ã€‚\n",
    "\n",
    "**æ–‡æ³•è¦å‰‡:**\n",
    "```\n",
    "S  â†’ NP VP         (å¥å­ = åè©çŸ­èª + å‹•è©çŸ­èª)\n",
    "NP â†’ Det N         (åè©çŸ­èª = é™å®šè© + åè©)\n",
    "VP â†’ V NP          (å‹•è©çŸ­èª = å‹•è© + åè©çŸ­èª)\n",
    "Det â†’ \"the\" | \"a\"\n",
    "N â†’ \"cat\" | \"dog\" | \"mat\"\n",
    "V â†’ \"sat\" | \"chased\"\n",
    "```\n",
    "\n",
    "**è§£ææ¨¹ç¯„ä¾‹:**\n",
    "```\n",
    "        S\n",
    "       / \\\n",
    "      NP  VP\n",
    "     / \\  / \\\n",
    "   Det N V  NP\n",
    "    |  | |  / \\\n",
    "  the cat sat Det N\n",
    "              |   |\n",
    "             on  mat\n",
    "```\n",
    "\n",
    "**çµ„åˆçˆ†ç‚¸å•é¡Œ:**\n",
    "- 50 æ¢åŸºæœ¬è¦å‰‡ â†’ 2 æ¢çµ„åˆ: C(50,2) = 1,225\n",
    "- 50 æ¢åŸºæœ¬è¦å‰‡ â†’ 3 æ¢çµ„åˆ: C(50,3) = 19,600\n",
    "- å¯¦éš›èªè¨€éœ€è¦æ•¸åƒæ¢è¦å‰‡ â†’ æ‰‹å·¥ç¶­è­·ä¸å¯è¡Œ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "## 3. ç¬¬äºŒå…¸ç¯„: çµ±è¨ˆå­¸ç¿’æ™‚ä»£ (1990-2010)\n",
    "\n",
    "### æ ¸å¿ƒæ€æƒ³\n",
    "\n",
    "å¾å¤§è¦æ¨¡èªæ–™åº«ä¸­å­¸ç¿’æ©Ÿç‡æ¨¡å‹,ç”¨çµ±è¨ˆæ–¹æ³•è™•ç†æ­§ç¾©ã€‚\n",
    "\n",
    "### ä»£è¡¨æŠ€è¡“\n",
    "\n",
    "- **N-gram èªè¨€æ¨¡å‹**: é æ¸¬ä¸‹ä¸€å€‹è©çš„æ©Ÿç‡\n",
    "- **éš±é¦¬å¯å¤«æ¨¡å‹ (HMM)**: è©æ€§æ¨™è¨»ã€èªéŸ³è­˜åˆ¥\n",
    "- **çµ±è¨ˆæ©Ÿå™¨ç¿»è­¯ (SMT)**: åŸºæ–¼çŸ­èªçš„ç¿»è­¯\n",
    "- **æ¢ä»¶éš¨æ©Ÿå ´ (CRF)**: åºåˆ—æ¨™è¨»ä»»å‹™\n",
    "\n",
    "### æ ¸å¿ƒæ•¸å­¸: æœ€å¤§åŒ–æ¢ä»¶æ©Ÿç‡\n",
    "\n",
    "$$\n",
    "\\hat{T} = \\arg\\max_{T} P(T | \\text{sentence})\n",
    "$$\n",
    "\n",
    "å°‡è¦å‰‡å•é¡Œè½‰æ›ç‚ºå„ªåŒ–å•é¡Œ!\n",
    "\n",
    "---\n",
    "\n",
    "### å¯¦ä½œç¯„ä¾‹: Bigram èªè¨€æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# çµ±è¨ˆå­¸ç¿’ç¯„ä¾‹: Bigram èªè¨€æ¨¡å‹\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "class BigramLanguageModel:\n",
    "    \"\"\"Bigram èªè¨€æ¨¡å‹: P(w_i | w_{i-1})\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.bigram_counts = defaultdict(Counter)\n",
    "        self.unigram_counts = Counter()\n",
    "    \n",
    "    def train(self, sentences):\n",
    "        \"\"\"å¾èªæ–™åº«è¨“ç·´æ¨¡å‹\"\"\"\n",
    "        for sentence in sentences:\n",
    "            words = ['<START>'] + sentence.lower().split() + ['<END>']\n",
    "            for i in range(len(words) - 1):\n",
    "                w1, w2 = words[i], words[i+1]\n",
    "                self.bigram_counts[w1][w2] += 1\n",
    "                self.unigram_counts[w1] += 1\n",
    "    \n",
    "    def probability(self, w1, w2):\n",
    "        \"\"\"è¨ˆç®—æ¢ä»¶æ©Ÿç‡ P(w2|w1)\"\"\"\n",
    "        if self.unigram_counts[w1] == 0:\n",
    "            return 0\n",
    "        return self.bigram_counts[w1][w2] / self.unigram_counts[w1]\n",
    "    \n",
    "    def perplexity(self, test_sentence):\n",
    "        \"\"\"è¨ˆç®—å›°æƒ‘åº¦ (æ¨¡å‹ä¸ç¢ºå®šæ€§)\"\"\"\n",
    "        words = ['<START>'] + test_sentence.lower().split() + ['<END>']\n",
    "        log_prob = 0\n",
    "        for i in range(len(words) - 1):\n",
    "            prob = self.probability(words[i], words[i+1])\n",
    "            if prob > 0:\n",
    "                log_prob += np.log2(prob)\n",
    "            else:\n",
    "                return float('inf')  # æœªè¦‹éçš„è©çµ„\n",
    "        return 2 ** (-log_prob / (len(words) - 1))\n",
    "    \n",
    "    def generate(self, max_length=10):\n",
    "        \"\"\"ç”Ÿæˆå¥å­\"\"\"\n",
    "        words = ['<START>']\n",
    "        for _ in range(max_length):\n",
    "            w1 = words[-1]\n",
    "            if w1 == '<END>' or not self.bigram_counts[w1]:\n",
    "                break\n",
    "            # é¸æ“‡æ©Ÿç‡æœ€é«˜çš„ä¸‹ä¸€å€‹è©\n",
    "            next_word = self.bigram_counts[w1].most_common(1)[0][0]\n",
    "            words.append(next_word)\n",
    "        return ' '.join(words[1:-1]) if '<END>' in words else ' '.join(words[1:])\n",
    "\n",
    "# è¨“ç·´èªæ–™\n",
    "train_corpus = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the dog sat on the log\",\n",
    "    \"the cat chased the mouse\",\n",
    "    \"the mouse ran away\",\n",
    "    \"the dog chased the cat\",\n",
    "    \"the cat climbed the tree\",\n",
    "]\n",
    "\n",
    "# è¨“ç·´æ¨¡å‹\n",
    "model = BigramLanguageModel()\n",
    "model.train(train_corpus)\n",
    "\n",
    "print(\"Bigram èªè¨€æ¨¡å‹è¨“ç·´å®Œæˆ\\n\")\n",
    "\n",
    "# æ¸¬è©¦æ¢ä»¶æ©Ÿç‡\n",
    "print(\"æ¢ä»¶æ©Ÿç‡æ¸¬è©¦:\")\n",
    "print(f\"  P(sat | cat) = {model.probability('cat', 'sat'):.3f}\")\n",
    "print(f\"  P(chased | cat) = {model.probability('cat', 'chased'):.3f}\")\n",
    "print(f\"  P(barked | cat) = {model.probability('cat', 'barked'):.3f} (æœªè¦‹é)\")\n",
    "\n",
    "# ç”Ÿæˆå¥å­\n",
    "print(\"\\nç”Ÿæˆå¥å­ç¯„ä¾‹:\")\n",
    "for i in range(3):\n",
    "    print(f\"  {i+1}. {model.generate()}\")\n",
    "\n",
    "# è¨ˆç®—å›°æƒ‘åº¦\n",
    "test_sentences = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the dog ran away\",\n",
    "    \"the elephant flew high\"  # æœªè¦‹éçš„è©çµ„\n",
    "]\n",
    "\n",
    "print(\"\\nå›°æƒ‘åº¦æ¸¬è©¦ (è¶Šä½è¶Šå¥½):\")\n",
    "for sent in test_sentences:\n",
    "    ppl = model.perplexity(sent)\n",
    "    print(f\"  '{sent}': {ppl:.2f}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ çµ±è¨ˆæ¨¡å‹çš„å„ªå‹¢:\")\n",
    "print(\"  1. è‡ªå‹•å¾æ•¸æ“šå­¸ç¿’,ç„¡éœ€æ‰‹å·¥è¦å‰‡\")\n",
    "print(\"  2. å¯è™•ç†æ­§ç¾© (é¸æ“‡æ©Ÿç‡æœ€é«˜çš„è§£é‡‹)\")\n",
    "print(\"  3. é­¯æ£’æ€§å¼·,å…è¨±å°éŒ¯èª¤\")\n",
    "print(\"\\nâš ï¸ çµ±è¨ˆæ¨¡å‹çš„ä¾·é™:\")\n",
    "print(\"  1. ç¨€ç–æ€§å•é¡Œ: æœªè¦‹éçš„è©çµ„æ©Ÿç‡ç‚º 0\")\n",
    "print(\"  2. ç„¡æ³•æ•æ‰é•·è·é›¢ä¾è³´ (Bigram åªçœ‹å‰ä¸€å€‹è©)\")\n",
    "print(\"  3. ç”Ÿæˆçš„å¥å­è¼ƒç‚ºæ©Ÿæ¢°,ç¼ºä¹å‰µé€ æ€§\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¦–è¦ºåŒ– Bigram æ©Ÿç‡çŸ©é™£\n",
    "words = ['<START>', 'the', 'cat', 'dog', 'sat', 'chased', 'on', 'mat']\n",
    "prob_matrix = np.zeros((len(words), len(words)))\n",
    "\n",
    "for i, w1 in enumerate(words):\n",
    "    for j, w2 in enumerate(words):\n",
    "        prob_matrix[i, j] = model.probability(w1, w2)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(prob_matrix, annot=True, fmt='.2f', cmap='YlOrRd',\n",
    "            xticklabels=words, yticklabels=words,\n",
    "            cbar_kws={'label': 'Probability P(w_j | w_i)'})\n",
    "plt.xlabel('Next Word (w_j)', fontsize=12)\n",
    "plt.ylabel('Current Word (w_i)', fontsize=12)\n",
    "plt.title('Bigram è½‰ç§»æ©Ÿç‡çŸ©é™£', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ“Š æ©Ÿç‡çŸ©é™£è§£è®€:\")\n",
    "print(\"  - æ¯ä¸€è¡Œç¸½å’Œç‚º 1 (æ­¸ä¸€åŒ–)\")\n",
    "print(\"  - é¡è‰²è¶Šæ·±è¡¨ç¤ºè½‰ç§»æ©Ÿç‡è¶Šé«˜\")\n",
    "print(\"  - 'the' å¾Œé¢æœ€å¸¸æ¥åè© ('cat', 'dog')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ç¨€ç–æ€§å•é¡Œç¤ºä¾‹\n",
    "\n",
    "çµ±è¨ˆæ¨¡å‹çš„æ ¸å¿ƒç“¶é ¸æ˜¯**ç¨€ç–æ€§å•é¡Œ** (Sparsity Problem):\n",
    "\n",
    "```\n",
    "è©å½™è¡¨å¤§å°: 100,000\n",
    "Bigram çµ„åˆ: 100,000 Ã— 100,000 = 10^10\n",
    "Trigram çµ„åˆ: 100,000^3 = 10^15\n",
    "\n",
    "å¯¦éš›èªæ–™åº«: 10^9 è© (10 å„„è©)\n",
    "è¦†è“‹ç‡: 10^9 / 10^15 = 0.0001%\n",
    "\n",
    "çµè«–: çµ•å¤§å¤šæ•¸è©çµ„å¾æœªå‡ºç¾,æ©Ÿç‡ç‚º 0!\n",
    "```\n",
    "\n",
    "**è§£æ±ºæ–¹æ¡ˆé å‘Š:**\n",
    "- å¹³æ»‘æŠ€è¡“ (Smoothing): Add-1, Kneser-Ney\n",
    "- **è©å‘é‡ (Word Embeddings)**: å°‡é›¢æ•£è©å½™æ˜ å°„åˆ°é€£çºŒç©ºé–“ â†’ ä¸‹ä¸€å…¸ç¯„!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "## 4. ç¬¬ä¸‰å…¸ç¯„: æ·ºå±¤ç¥ç¶“ç¶²è·¯ (2010-2017)\n",
    "\n",
    "### æ ¸å¿ƒæ€æƒ³\n",
    "\n",
    "å°‡è©å½™æ˜ å°„åˆ°**é€£çºŒå‘é‡ç©ºé–“**,ç”¨ç¥ç¶“ç¶²è·¯å­¸ç¿’åˆ†ä½ˆå¼è¡¨ç¤ºã€‚\n",
    "\n",
    "### ä»£è¡¨æŠ€è¡“\n",
    "\n",
    "- **Word2Vec (2013)**: CBOW èˆ‡ Skip-gram\n",
    "- **GloVe (2014)**: å…¨åŸŸè©å‘é‡\n",
    "- **æ·ºå±¤å‰é¥‹ç¥ç¶“ç¶²è·¯**: æ–‡æœ¬åˆ†é¡ã€æƒ…æ„Ÿåˆ†æ\n",
    "\n",
    "### é—œéµçªç ´: è§£æ±ºç¨€ç–æ€§å•é¡Œ\n",
    "\n",
    "```\n",
    "One-Hot ç·¨ç¢¼ (é›¢æ•£ç©ºé–“):\n",
    "cat  = [0, 0, 0, 1, 0, ..., 0]  (100,000 ç¶­, 99.999% ç‚º 0)\n",
    "dog  = [0, 0, 1, 0, 0, ..., 0]\n",
    "â†’ ç„¡æ³•æ•æ‰èªç¾©ç›¸ä¼¼åº¦!\n",
    "\n",
    "è©å‘é‡ (é€£çºŒç©ºé–“):\n",
    "cat  = [0.5, -0.3, 0.8, ...]  (300 ç¶­, ç¨ å¯†)\n",
    "dog  = [0.6, -0.2, 0.7, ...]\n",
    "â†’ ç›¸ä¼¼è©çš„å‘é‡æ¥è¿‘!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### å¯¦ä½œç¯„ä¾‹: Word2Vec è©å‘é‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è©å‘é‡ç¯„ä¾‹: ä½¿ç”¨ gensim è¨“ç·´ Word2Vec\n",
    "try:\n",
    "    from gensim.models import Word2Vec\n",
    "    gensim_available = True\n",
    "except ImportError:\n",
    "    gensim_available = False\n",
    "    print(\"âš ï¸ gensim æœªå®‰è£,å°‡ä½¿ç”¨ç°¡åŒ–ç‰ˆç¤ºä¾‹\")\n",
    "\n",
    "if gensim_available:\n",
    "    # è¨“ç·´èªæ–™ (æ›´å¤§è¦æ¨¡)\n",
    "    sentences = [\n",
    "        [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"],\n",
    "        [\"the\", \"dog\", \"sat\", \"on\", \"the\", \"log\"],\n",
    "        [\"the\", \"cat\", \"chased\", \"the\", \"mouse\"],\n",
    "        [\"the\", \"mouse\", \"ran\", \"away\", \"quickly\"],\n",
    "        [\"the\", \"dog\", \"chased\", \"the\", \"cat\"],\n",
    "        [\"the\", \"cat\", \"climbed\", \"the\", \"tree\"],\n",
    "        [\"cats\", \"and\", \"dogs\", \"are\", \"animals\"],\n",
    "        [\"animals\", \"need\", \"food\", \"and\", \"water\"],\n",
    "        [\"the\", \"king\", \"ruled\", \"the\", \"kingdom\"],\n",
    "        [\"the\", \"queen\", \"lived\", \"in\", \"the\", \"palace\"],\n",
    "    ] * 100  # é‡è¤‡ä»¥å¢åŠ è¨“ç·´æ•¸æ“š\n",
    "    \n",
    "    # è¨“ç·´ Word2Vec (Skip-gram)\n",
    "    model = Word2Vec(\n",
    "        sentences=sentences,\n",
    "        vector_size=50,      # è©å‘é‡ç¶­åº¦\n",
    "        window=5,            # ä¸Šä¸‹æ–‡çª—å£å¤§å°\n",
    "        min_count=1,         # æœ€å°è©é »\n",
    "        sg=1,                # 1=Skip-gram, 0=CBOW\n",
    "        epochs=50,\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    print(\"Word2Vec è¨“ç·´å®Œæˆ\\n\")\n",
    "    \n",
    "    # æŸ¥è©¢è©å‘é‡\n",
    "    print(\"è©å‘é‡ç¯„ä¾‹ (å‰ 10 ç¶­):\")\n",
    "    for word in ['cat', 'dog', 'king']:\n",
    "        vec = model.wv[word][:10]\n",
    "        print(f\"  {word}: {vec}\")\n",
    "    \n",
    "    # è¨ˆç®—ç›¸ä¼¼åº¦\n",
    "    print(\"\\nè©å½™ç›¸ä¼¼åº¦æ¸¬è©¦:\")\n",
    "    pairs = [('cat', 'dog'), ('cat', 'king'), ('king', 'queen')]\n",
    "    for w1, w2 in pairs:\n",
    "        sim = model.wv.similarity(w1, w2)\n",
    "        print(f\"  similarity('{w1}', '{w2}') = {sim:.3f}\")\n",
    "    \n",
    "    # å°‹æ‰¾æœ€ç›¸ä¼¼çš„è©\n",
    "    print(\"\\næœ€ç›¸ä¼¼çš„è©:\")\n",
    "    for word in ['cat', 'king']:\n",
    "        similar = model.wv.most_similar(word, topn=3)\n",
    "        print(f\"  èˆ‡ '{word}' æœ€ç›¸ä¼¼: {[w for w, _ in similar]}\")\n",
    "    \n",
    "    # è©å‘é‡é‹ç®—\n",
    "    print(\"\\nè©å‘é‡é‹ç®— (é¡æ¯”æ¨ç†):\")\n",
    "    try:\n",
    "        # king - man + woman â‰ˆ queen (ç¶“å…¸ç¯„ä¾‹,éœ€è¦æ›´å¤§èªæ–™)\n",
    "        result = model.wv.most_similar(\n",
    "            positive=['king', 'woman'], \n",
    "            negative=['man'], \n",
    "            topn=1\n",
    "        )\n",
    "        print(f\"  king - man + woman â‰ˆ {result[0][0]}\")\n",
    "    except:\n",
    "        print(\"  (éœ€è¦æ›´å¤§è¦æ¨¡èªæ–™æ‰èƒ½å±•ç¾é¡æ¯”æ¨ç†èƒ½åŠ›)\")\n",
    "\n",
    "else:\n",
    "    # ç°¡åŒ–ç‰ˆ: æ‰‹å·¥å‰µå»ºç¤ºä¾‹è©å‘é‡\n",
    "    print(\"\\nä½¿ç”¨ç°¡åŒ–ç‰ˆè©å‘é‡ç¤ºä¾‹:\\n\")\n",
    "    word_vectors = {\n",
    "        'cat':   np.array([0.5, -0.3, 0.8, 0.1]),\n",
    "        'dog':   np.array([0.6, -0.2, 0.7, 0.2]),\n",
    "        'king':  np.array([-0.1, 0.9, -0.2, 0.5]),\n",
    "        'queen': np.array([-0.2, 0.8, -0.1, 0.6]),\n",
    "    }\n",
    "    \n",
    "    def cosine_similarity(v1, v2):\n",
    "        return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "    \n",
    "    print(\"ç›¸ä¼¼åº¦æ¸¬è©¦:\")\n",
    "    print(f\"  similarity('cat', 'dog') = {cosine_similarity(word_vectors['cat'], word_vectors['dog']):.3f}\")\n",
    "    print(f\"  similarity('king', 'queen') = {cosine_similarity(word_vectors['king'], word_vectors['queen']):.3f}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ è©å‘é‡çš„å„ªå‹¢:\")\n",
    "print(\"  1. è§£æ±ºç¨€ç–æ€§: ç¨ å¯†è¡¨ç¤º,ç„¡ 0 å€¼å•é¡Œ\")\n",
    "print(\"  2. æ•æ‰èªç¾©: ç›¸ä¼¼è©å‘é‡æ¥è¿‘\")\n",
    "print(\"  3. é·ç§»å­¸ç¿’: é è¨“ç·´è©å‘é‡å¯ç”¨æ–¼ä¸‹æ¸¸ä»»å‹™\")\n",
    "print(\"\\nâš ï¸ è©å‘é‡çš„ä¾·é™:\")\n",
    "print(\"  1. éœæ…‹è¡¨ç¤º: ä¸€è©å¤šç¾©å•é¡Œ (bank: éŠ€è¡Œ vs æ²³å²¸)\")\n",
    "print(\"  2. ç„¡æ³•æ•æ‰é•·è·é›¢ä¾è³´: å‰é¥‹ç¶²è·¯ç„¡è¨˜æ†¶\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¦–è¦ºåŒ–è©å‘é‡ç©ºé–“ (ä½¿ç”¨ t-SNE é™ç¶­)\n",
    "if gensim_available and len(model.wv) >= 5:\n",
    "    from sklearn.manifold import TSNE\n",
    "    \n",
    "    # é¸æ“‡éƒ¨åˆ†è©å½™\n",
    "    words = [word for word in model.wv.index_to_key[:20] if len(word) > 2]\n",
    "    word_vecs = np.array([model.wv[word] for word in words])\n",
    "    \n",
    "    # t-SNE é™ç¶­åˆ° 2D\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=min(5, len(words)-1))\n",
    "    word_vecs_2d = tsne.fit_transform(word_vecs)\n",
    "    \n",
    "    # ç¹ªåœ–\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.scatter(word_vecs_2d[:, 0], word_vecs_2d[:, 1], s=100, alpha=0.6)\n",
    "    \n",
    "    for i, word in enumerate(words):\n",
    "        plt.annotate(word, xy=(word_vecs_2d[i, 0], word_vecs_2d[i, 1]),\n",
    "                     xytext=(5, 5), textcoords='offset points',\n",
    "                     fontsize=11, fontweight='bold')\n",
    "    \n",
    "    plt.title('è©å‘é‡ç©ºé–“è¦–è¦ºåŒ– (t-SNE é™ç¶­)', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Dimension 1', fontsize=12)\n",
    "    plt.ylabel('Dimension 2', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nğŸ“Š è¦–è¦ºåŒ–è§€å¯Ÿ:\")\n",
    "    print(\"  - ç›¸ä¼¼è©åœ¨ç©ºé–“ä¸­è·é›¢è¼ƒè¿‘\")\n",
    "    print(\"  - èªç¾©é—œä¿‚è¢«ç·¨ç¢¼ç‚ºå¹¾ä½•é—œä¿‚\")\n",
    "else:\n",
    "    print(\"\\n(éœ€è¦å®‰è£ gensim å’Œæ›´å¤šè¨“ç·´æ•¸æ“šæ‰èƒ½é€²è¡Œè¦–è¦ºåŒ–)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec çš„æ•¸å­¸åŸç†\n",
    "\n",
    "**Skip-gram ç›®æ¨™:** çµ¦å®šä¸­å¿ƒè© $w_c$,é æ¸¬ä¸Šä¸‹æ–‡è© $w_o$\n",
    "\n",
    "$$\n",
    "\\max \\sum_{t=1}^{T} \\sum_{-c \\leq j \\leq c, j \\neq 0} \\log P(w_{t+j} | w_t)\n",
    "$$\n",
    "\n",
    "å…¶ä¸­æ¢ä»¶æ©Ÿç‡:\n",
    "\n",
    "$$\n",
    "P(w_o | w_c) = \\frac{\\exp(u_o \\cdot v_c)}{\\sum_{w=1}^{V} \\exp(u_w \\cdot v_c)}\n",
    "$$\n",
    "\n",
    "- $v_c$: ä¸­å¿ƒè©å‘é‡ (Query)\n",
    "- $u_o$: ä¸Šä¸‹æ–‡è©å‘é‡ (Key)\n",
    "- åˆ†æ¯æ˜¯æ‰€æœ‰è©çš„æ­¸ä¸€åŒ– (Softmax)\n",
    "\n",
    "**è¨“ç·´æŠ€å·§:**\n",
    "- **Negative Sampling**: é¿å…è¨ˆç®—å®Œæ•´ Softmax (å¤ªæ…¢)\n",
    "- **Hierarchical Softmax**: ä½¿ç”¨äºŒå…ƒæ¨¹åŠ é€Ÿ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "## 5. ç¬¬å››å…¸ç¯„: æ·±åº¦å­¸ç¿’èˆ‡ Transformer (2017-2020)\n",
    "\n",
    "### æ ¸å¿ƒæ€æƒ³\n",
    "\n",
    "ä½¿ç”¨**å¾ªç’°ç¥ç¶“ç¶²è·¯ (RNN/LSTM)** è™•ç†åºåˆ—,å¾Œè¢« **Transformer** ç´”æ³¨æ„åŠ›æ¶æ§‹å–ä»£ã€‚\n",
    "\n",
    "### ä»£è¡¨æŠ€è¡“\n",
    "\n",
    "#### RNN/LSTM æ™‚æœŸ (2013-2017)\n",
    "- **RNN/LSTM**: åºåˆ—å»ºæ¨¡,æœ‰è¨˜æ†¶èƒ½åŠ›\n",
    "- **Seq2Seq (2014)**: ç·¨ç¢¼å™¨-è§£ç¢¼å™¨æ¶æ§‹\n",
    "- **Attention æ©Ÿåˆ¶ (2015)**: å‹•æ…‹é—œæ³¨è¼¸å…¥ä¸åŒéƒ¨åˆ†\n",
    "\n",
    "#### Transformer é©å‘½ (2017-2020)\n",
    "- **Transformer (2017)**: \"Attention is All You Need\"\n",
    "- **BERT (2018)**: é›™å‘é è¨“ç·´,åˆ·æ–° 11 é … NLP ç´€éŒ„\n",
    "- **GPT-2 (2019)**: ç”Ÿæˆå¼é è¨“ç·´,15 å„„åƒæ•¸\n",
    "\n",
    "---\n",
    "\n",
    "### RNN vs Transformer æ ¸å¿ƒå·®ç•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å°æ¯”è¡¨æ ¼: RNN vs Transformer\n",
    "comparison_data = {\n",
    "    'ç‰¹æ€§': ['ä¸¦è¡ŒåŒ–', 'é•·è·é›¢ä¾è³´', 'è¨“ç·´é€Ÿåº¦', 'è¨ˆç®—è¤‡é›œåº¦', 'è¨˜æ†¶èƒ½åŠ›', 'å¯æ“´å±•æ€§'],\n",
    "    'RNN/LSTM': ['âŒ ä¸²è¡Œè™•ç†', 'âš ï¸ æ¢¯åº¦æ¶ˆå¤±', 'ğŸŒ æ…¢', 'O(n)', 'âœ… éš±è—ç‹€æ…‹', 'âš ï¸ é›£æ“´å±•'],\n",
    "    'Transformer': ['âœ… å®Œå…¨ä¸¦è¡Œ', 'âœ… ç›´æ¥é€£æ¥', 'ğŸš€ å¿« 10-100x', 'O(nÂ²)', 'âœ… Self-Attention', 'âœ… å¯æ“´å±•è‡³ 100B+']\n",
    "}\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# é¡¯ç¤ºè¡¨æ ¼\n",
    "display(HTML(df.to_html(index=False, escape=False)))\n",
    "\n",
    "print(\"\\nğŸ’¡ Transformer çš„é©å‘½æ€§çªç ´:\")\n",
    "print(\"  1. å®Œå…¨æ‹‹æ£„å¾ªç’°çµæ§‹,ç´”æ³¨æ„åŠ›æ©Ÿåˆ¶\")\n",
    "print(\"  2. ä¸¦è¡ŒåŒ–è¨“ç·´,GPU åˆ©ç”¨ç‡å¾ 20% æå‡åˆ° 80%+\")\n",
    "print(\"  3. é•·è·é›¢ä¾è³´: ä»»æ„å…©ä½ç½® O(1) è·¯å¾‘é€£æ¥\")\n",
    "print(\"  4. å¯æ“´å±•æ€§: GPT-3 é”åˆ° 175B åƒæ•¸\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention æ©Ÿåˆ¶å¯è¦–åŒ–\n",
    "\n",
    "**Self-Attention æ ¸å¿ƒå…¬å¼:**\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V\n",
    "$$\n",
    "\n",
    "**è¨ˆç®—æ­¥é©Ÿ:**\n",
    "1. **Query-Key ç›¸ä¼¼åº¦**: $QK^T$ (é»ç©è¨ˆç®—ç›¸é—œæ€§)\n",
    "2. **ç¸®æ”¾**: $\\frac{QK^T}{\\sqrt{d_k}}$ (é¿å…æ•¸å€¼éå¤§)\n",
    "3. **Softmax æ­¸ä¸€åŒ–**: è½‰æ›ç‚ºæ©Ÿç‡åˆ†ä½ˆ\n",
    "4. **åŠ æ¬Šçµ„åˆ**: ä¹˜ä»¥ Value å¾—åˆ°è¼¸å‡º\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¦ä½œç°¡åŒ–ç‰ˆ Self-Attention\n",
    "def simple_self_attention(X, d_k):\n",
    "    \"\"\"\n",
    "    ç°¡åŒ–ç‰ˆ Self-Attention æ©Ÿåˆ¶\n",
    "    \n",
    "    Args:\n",
    "        X: è¼¸å…¥çŸ©é™£ (seq_len, d_model)\n",
    "        d_k: Key ç¶­åº¦\n",
    "    \n",
    "    Returns:\n",
    "        output: Attention è¼¸å‡º\n",
    "        attn_weights: æ³¨æ„åŠ›æ¬Šé‡çŸ©é™£\n",
    "    \"\"\"\n",
    "    Q = K = V = X  # ç°¡åŒ–: ç›´æ¥ä½¿ç”¨è¼¸å…¥ (å¯¦éš›éœ€è¦æŠ•å½±çŸ©é™£)\n",
    "    \n",
    "    # Step 1: è¨ˆç®—ç›¸ä¼¼åº¦åˆ†æ•¸\n",
    "    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n",
    "    \n",
    "    # Step 2: Softmax æ­¸ä¸€åŒ–\n",
    "    exp_scores = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n",
    "    attn_weights = exp_scores / np.sum(exp_scores, axis=-1, keepdims=True)\n",
    "    \n",
    "    # Step 3: åŠ æ¬Šçµ„åˆ\n",
    "    output = np.dot(attn_weights, V)\n",
    "    \n",
    "    return output, attn_weights\n",
    "\n",
    "# æ¸¬è©¦ç¯„ä¾‹\n",
    "np.random.seed(42)\n",
    "seq_len = 6\n",
    "d_model = 8\n",
    "\n",
    "# æ¨¡æ“¬è¼¸å…¥åºåˆ—: \"The cat sat on the mat\"\n",
    "X = np.random.randn(seq_len, d_model)\n",
    "output, attn_weights = simple_self_attention(X, d_model)\n",
    "\n",
    "print(f\"è¼¸å…¥å½¢ç‹€: {X.shape}\")\n",
    "print(f\"è¼¸å‡ºå½¢ç‹€: {output.shape}\")\n",
    "print(f\"æ³¨æ„åŠ›æ¬Šé‡å½¢ç‹€: {attn_weights.shape}\")\n",
    "print(f\"\\næ³¨æ„åŠ›æ¬Šé‡çŸ©é™£:\\n{attn_weights}\")\n",
    "\n",
    "# è¦–è¦ºåŒ– Attention æ¬Šé‡\n",
    "words = ['The', 'cat', 'sat', 'on', 'the', 'mat']\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(attn_weights, annot=True, fmt='.2f', cmap='YlOrRd',\n",
    "            xticklabels=words, yticklabels=words,\n",
    "            cbar_kws={'label': 'Attention Score'})\n",
    "plt.xlabel('Key (è¢«é—œæ³¨çš„è©)', fontsize=12)\n",
    "plt.ylabel('Query (æŸ¥è©¢è©)', fontsize=12)\n",
    "plt.title('Self-Attention æ¬Šé‡çŸ©é™£è¦–è¦ºåŒ–', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ” Attention æ¬Šé‡è§£è®€:\")\n",
    "print(\"  - æ¯ä¸€è¡Œç¸½å’Œç‚º 1 (Softmax æ­¸ä¸€åŒ–)\")\n",
    "print(\"  - å°è§’ç·šå€¼è¼ƒé«˜: è©é—œæ³¨è‡ªå·±\")\n",
    "print(\"  - 'cat' ä¹Ÿæœƒé—œæ³¨ 'sat' (èªæ³•é—œä¿‚)\")\n",
    "print(\"  - 'mat' é—œæ³¨ 'the' (é™å®šè©ä¾è³´)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Attention æ¦‚å¿µ\n",
    "\n",
    "**ç‚ºä»€éº¼éœ€è¦å¤šé ­?**\n",
    "\n",
    "ä¸€å€‹å¥å­ä¸­å­˜åœ¨**å¤šç¨®é—œä¿‚**:\n",
    "- **èªæ³•é—œä¿‚**: ä¸»è©-å‹•è©-å—è©\n",
    "- **èªç¾©é—œä¿‚**: è¿‘ç¾©è©ã€ä¸Šä¸‹ä½è©\n",
    "- **æŒ‡ä»£é—œä¿‚**: ä»£åè©æŒ‡æ¶‰\n",
    "\n",
    "**å¤šé ­æ©Ÿåˆ¶è®“æ¨¡å‹åŒæ™‚å­¸ç¿’é€™äº›ä¸åŒé—œä¿‚!**\n",
    "\n",
    "$$\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h) W^O\n",
    "$$\n",
    "\n",
    "å…¶ä¸­æ¯å€‹é ­:\n",
    "\n",
    "$$\n",
    "\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n",
    "$$\n",
    "\n",
    "**Transformer åŸè«–æ–‡è¨­å®š:** 8 å€‹é ­,æ¯å€‹é ­ç¶­åº¦ 64 (ç¸½ç¶­åº¦ 512)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a>\n",
    "## 6. ç¬¬äº”å…¸ç¯„: å¤§å‹èªè¨€æ¨¡å‹ LLM (2020-è‡³ä»Š)\n",
    "\n",
    "### æ ¸å¿ƒæ€æƒ³\n",
    "\n",
    "**è¦æ¨¡æ³•å‰‡ (Scaling Laws)**: æ¨¡å‹è¶Šå¤§,æ€§èƒ½è¶Šå¼·,ç”šè‡³å‡ºç¾**æ¹§ç¾èƒ½åŠ› (Emergent Abilities)**!\n",
    "\n",
    "### ä»£è¡¨æ¨¡å‹\n",
    "\n",
    "| æ¨¡å‹ | ç™¼å¸ƒæ™‚é–“ | åƒæ•¸é‡ | æ¶æ§‹é¡å‹ | é—œéµç‰¹æ€§ |\n",
    "|:---|:---|:---|:---|:---|\n",
    "| **GPT-3** | 2020.06 | 175B | Decoder-Only | Few-shot å­¸ç¿’ |\n",
    "| **T5** | 2020.10 | 11B | Encoder-Decoder | çµ±ä¸€æ¡†æ¶ |\n",
    "| **ChatGPT** | 2022.11 | ~175B | Decoder-Only | RLHF å°é½Š |\n",
    "| **GPT-4** | 2023.03 | ~1T (ä¼°è¨ˆ) | Decoder-Only | å¤šæ¨¡æ…‹èƒ½åŠ› |\n",
    "| **Claude 3** | 2024.03 | æœªå…¬é–‹ | Decoder-Only | é•·ä¸Šä¸‹æ–‡ (200K) |\n",
    "| **LLaMA 3** | 2024.04 | 70B | Decoder-Only | é–‹æºé«˜æ•ˆ |\n",
    "\n",
    "---\n",
    "\n",
    "### LLM ä¸‰å¤§æ¨¡å‹å®¶æ—å°æ¯”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¸‰å¤§æ¨¡å‹å®¶æ—å°æ¯”åœ–\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 6))\n",
    "\n",
    "families = [\n",
    "    {\n",
    "        'name': 'Encoder-Only\\n(BERT)',\n",
    "        'structure': 'é›™å‘ç·¨ç¢¼',\n",
    "        'tasks': ['æ–‡æœ¬åˆ†é¡', 'å‘½åå¯¦é«”è­˜åˆ¥', 'å•ç­”ç³»çµ±', 'èªç¾©ç†è§£'],\n",
    "        'examples': ['BERT', 'RoBERTa', 'ALBERT'],\n",
    "        'color': '#4ECDC4'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Decoder-Only\\n(GPT)',\n",
    "        'structure': 'å–®å‘ç”Ÿæˆ',\n",
    "        'tasks': ['æ–‡æœ¬ç”Ÿæˆ', 'å°è©±ç³»çµ±', 'ç¨‹å¼ç¢¼ç”Ÿæˆ', 'å‰µä½œå¯«ä½œ'],\n",
    "        'examples': ['GPT-3/4', 'ChatGPT', 'Claude'],\n",
    "        'color': '#5F27CD'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Encoder-Decoder\\n(T5)',\n",
    "        'structure': 'è½‰æ›æ¶æ§‹',\n",
    "        'tasks': ['æ©Ÿå™¨ç¿»è­¯', 'æ–‡æœ¬æ‘˜è¦', 'å•ç­”ç”Ÿæˆ', 'æ”¹å¯«æ½¤è‰²'],\n",
    "        'examples': ['T5', 'BART', 'mT5'],\n",
    "        'color': '#F7B731'\n",
    "    }\n",
    "]\n",
    "\n",
    "for idx, family in enumerate(families):\n",
    "    ax = axes[idx]\n",
    "    ax.set_xlim(0, 10)\n",
    "    ax.set_ylim(0, 10)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # æ¨™é¡Œ\n",
    "    ax.text(5, 9, family['name'], ha='center', va='top',\n",
    "            fontsize=14, fontweight='bold', color=family['color'])\n",
    "    \n",
    "    # æ¶æ§‹\n",
    "    ax.text(5, 7.5, f\"æ¶æ§‹: {family['structure']}\", ha='center',\n",
    "            fontsize=11, style='italic')\n",
    "    \n",
    "    # ä»»å‹™åˆ—è¡¨\n",
    "    y_pos = 6.5\n",
    "    ax.text(5, y_pos, 'é©ç”¨ä»»å‹™:', ha='center', fontsize=10, fontweight='bold')\n",
    "    y_pos -= 0.8\n",
    "    for task in family['tasks']:\n",
    "        ax.text(5, y_pos, f\"â€¢ {task}\", ha='center', fontsize=9)\n",
    "        y_pos -= 0.7\n",
    "    \n",
    "    # ä»£è¡¨æ¨¡å‹\n",
    "    y_pos -= 0.5\n",
    "    ax.text(5, y_pos, 'ä»£è¡¨æ¨¡å‹:', ha='center', fontsize=10, fontweight='bold')\n",
    "    y_pos -= 0.8\n",
    "    for model in family['examples']:\n",
    "        ax.text(5, y_pos, model, ha='center', fontsize=9,\n",
    "                bbox=dict(boxstyle='round', facecolor=family['color'], alpha=0.3))\n",
    "        y_pos -= 0.7\n",
    "\n",
    "plt.suptitle('Transformer ä¸‰å¤§æ¨¡å‹å®¶æ—', fontsize=16, fontweight='bold', y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ’¡ å¦‚ä½•é¸æ“‡æ¨¡å‹å®¶æ—?\")\n",
    "print(\"  1. ç†è§£ä»»å‹™ â†’ BERT (é›™å‘ä¸Šä¸‹æ–‡)\")\n",
    "print(\"  2. ç”Ÿæˆä»»å‹™ â†’ GPT (è‡ªå›æ­¸ç”Ÿæˆ)\")\n",
    "print(\"  3. è½‰æ›ä»»å‹™ â†’ T5 (è¼¸å…¥â†’è¼¸å‡ºæ˜ å°„)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM çš„æ¹§ç¾èƒ½åŠ› (Emergent Abilities)\n",
    "\n",
    "ç•¶æ¨¡å‹è¦æ¨¡è¶…éæŸå€‹**è‡¨ç•Œé»**å¾Œ,æœƒçªç„¶å‡ºç¾æ–°èƒ½åŠ›:\n",
    "\n",
    "```\n",
    "æ¨¡å‹è¦æ¨¡ < 10B åƒæ•¸:\n",
    "âŒ ç„¡æ³•é€²è¡Œè¤‡é›œæ¨ç†\n",
    "âŒ Few-shot å­¸ç¿’èƒ½åŠ›å¼±\n",
    "âŒ æŒ‡ä»¤éµå¾ªèƒ½åŠ›å·®\n",
    "\n",
    "æ¨¡å‹è¦æ¨¡ > 100B åƒæ•¸:\n",
    "âœ… å¤šæ­¥é©Ÿæ¨ç† (Chain-of-Thought)\n",
    "âœ… å°‘æ¨£æœ¬å­¸ç¿’ (Few-shot Learning)\n",
    "âœ… æŒ‡ä»¤ç†è§£èˆ‡éµå¾ª (Instruction Following)\n",
    "âœ… ç¨‹å¼ç¢¼ç”Ÿæˆèˆ‡é™¤éŒ¯\n",
    "âœ… å¸¸è­˜æ¨ç†èƒ½åŠ›\n",
    "```\n",
    "\n",
    "**é—œéµç™¼ç¾:** æ€§èƒ½æå‡ä¸æ˜¯ç·šæ€§çš„,è€Œæ˜¯**çªç¾çš„** (éé€£çºŒè·³èº)!\n",
    "\n",
    "---\n",
    "\n",
    "### é è¨“ç·´ç¯„å¼æ¼”è®Š\n",
    "\n",
    "| ç¯„å¼ | ä»£è¡¨æ¨¡å‹ | æ ¸å¿ƒæ€æƒ³ | ä¸‹æ¸¸ä»»å‹™é©é… |\n",
    "|:---|:---|:---|:---|\n",
    "| **éœæ…‹è©å‘é‡** | Word2Vec, GloVe | é è¨“ç·´è©å‘é‡ | ä½œç‚ºç‰¹å¾µè¼¸å…¥ |\n",
    "| **é è¨“ç·´ + å¾®èª¿** | BERT, GPT-2 | å¤§è¦æ¨¡é è¨“ç·´ | å¾®èª¿å…¨éƒ¨åƒæ•¸ |\n",
    "| **æç¤ºå­¸ç¿’** | GPT-3 | ä¸Šä¸‹æ–‡å­¸ç¿’ | è¨­è¨ˆæç¤ºè© (é›¶æ¨£æœ¬/å°‘æ¨£æœ¬) |\n",
    "| **æŒ‡ä»¤å¾®èª¿** | ChatGPT, Claude | RLHF å°é½Š | ç›´æ¥å°è©±,ç„¡éœ€å¾®èª¿ |\n",
    "\n",
    "**è¶¨å‹¢:** å¾éœ€è¦å¤§é‡æ¨™è¨»æ•¸æ“š â†’ é›¶æ¨£æœ¬/å°‘æ¨£æœ¬ â†’ è‡ªç„¶èªè¨€æŒ‡ä»¤\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"7\"></a>\n",
    "## 7. æ¨¡å‹è¦æ¨¡æ¼”é€²èˆ‡ä¸‰ç¶­é©…å‹•æ¡†æ¶\n",
    "\n",
    "### æ¨¡å‹è¦æ¨¡æŒ‡æ•¸ç´šå¢é•·\n",
    "\n",
    "å¾ 2013 å¹´è‡³ä»Š,NLP æ¨¡å‹è¦æ¨¡å‘ˆç¾**æŒ‡æ•¸ç´šå¢é•·** (ç´„æ¯ 2 å¹´å¢é•· 10 å€)ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¨¡å‹è¦æ¨¡æ¼”é€²æ•¸æ“š\n",
    "model_history = [\n",
    "    {'year': 2013, 'model': 'Word2Vec', 'params': 1e6, 'flops': 1e15},\n",
    "    {'year': 2014, 'model': 'GloVe', 'params': 1e6, 'flops': 1e16},\n",
    "    {'year': 2017, 'model': 'Transformer', 'params': 1e8, 'flops': 1e18},\n",
    "    {'year': 2018, 'model': 'BERT-Base', 'params': 1.1e8, 'flops': 1e19},\n",
    "    {'year': 2018, 'model': 'GPT-1', 'params': 1.2e8, 'flops': 1e19},\n",
    "    {'year': 2019, 'model': 'BERT-Large', 'params': 3.4e8, 'flops': 5e19},\n",
    "    {'year': 2019, 'model': 'GPT-2', 'params': 1.5e9, 'flops': 1e20},\n",
    "    {'year': 2020, 'model': 'GPT-3', 'params': 1.75e11, 'flops': 3.14e23},\n",
    "    {'year': 2022, 'model': 'PaLM', 'params': 5.4e11, 'flops': 2.5e24},\n",
    "    {'year': 2023, 'model': 'GPT-4', 'params': 1e12, 'flops': 1e25},\n",
    "]\n",
    "\n",
    "# è¦–è¦ºåŒ–æ¨¡å‹è¦æ¨¡æ¼”é€²\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# å·¦åœ–: åƒæ•¸é‡æ¼”é€²\n",
    "years = [m['year'] for m in model_history]\n",
    "params = [m['params'] for m in model_history]\n",
    "models = [m['model'] for m in model_history]\n",
    "\n",
    "ax1.semilogy(years, params, 'o-', linewidth=2, markersize=8, color='#5F27CD')\n",
    "for i, (x, y, label) in enumerate(zip(years, params, models)):\n",
    "    if i % 2 == 0:  # åªæ¨™è¨»éƒ¨åˆ†æ¨¡å‹ä»¥é¿å…é‡ç–Š\n",
    "        ax1.annotate(label, (x, y), textcoords='offset points',\n",
    "                     xytext=(0, 10), ha='center', fontsize=9)\n",
    "\n",
    "ax1.set_xlabel('å¹´ä»½', fontsize=12)\n",
    "ax1.set_ylabel('åƒæ•¸é‡ (å°æ•¸å°ºåº¦)', fontsize=12)\n",
    "ax1.set_title('NLP æ¨¡å‹åƒæ•¸é‡æ¼”é€² (2013-2023)', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3, which='both')\n",
    "ax1.set_xlim(2012, 2024)\n",
    "\n",
    "# å³åœ–: è¨“ç·´ç®—åŠ›æ¼”é€²\n",
    "flops = [m['flops'] for m in model_history]\n",
    "\n",
    "ax2.semilogy(years, flops, 's-', linewidth=2, markersize=8, color='#F7B731')\n",
    "for i, (x, y, label) in enumerate(zip(years, flops, models)):\n",
    "    if i % 2 == 1:  # äº¤æ›¿æ¨™è¨»\n",
    "        ax2.annotate(label, (x, y), textcoords='offset points',\n",
    "                     xytext=(0, 10), ha='center', fontsize=9)\n",
    "\n",
    "ax2.set_xlabel('å¹´ä»½', fontsize=12)\n",
    "ax2.set_ylabel('è¨“ç·´ç®—åŠ› FLOPs (å°æ•¸å°ºåº¦)', fontsize=12)\n",
    "ax2.set_title('NLP æ¨¡å‹è¨“ç·´ç®—åŠ›æ¼”é€² (2013-2023)', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3, which='both')\n",
    "ax2.set_xlim(2012, 2024)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ“ˆ å¢é•·è¶¨å‹¢åˆ†æ:\")\n",
    "print(f\"  - 2013-2023 å¹´åƒæ•¸é‡å¢é•·: {params[-1]/params[0]:.2e} å€ (100 è¬å€!)\")\n",
    "print(f\"  - 2013-2023 å¹´ç®—åŠ›å¢é•·: {flops[-1]/flops[0]:.2e} å€ (1000 è¬å€!)\")\n",
    "print(f\"  - æ‘©çˆ¾å®šå¾‹: æ¯ 2 å¹´æ€§èƒ½ç¿»å€ â†’ å¯¦éš›å¢é•·é è¶…é æœŸ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¸‰ç¶­é©…å‹•æ¡†æ¶\n",
    "\n",
    "NLP å…¸ç¯„è½‰ç§»å—åˆ°**è¨ˆç®—èƒ½åŠ›ã€æ•¸æ“šè¦æ¨¡ã€æ¼”ç®—æ³•å‰µæ–°**ä¸‰ç¶­åŠ›é‡å…±åŒé©…å‹•ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¦–è¦ºåŒ–ä¸‰ç¶­é©…å‹•æ¡†æ¶\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# ä¸‰ç¶­æ•¸æ“š (æ­¸ä¸€åŒ–åˆ° 0-10 ç¯„åœ)\n",
    "paradigms_3d = [\n",
    "    {'name': 'è¦å‰‡ç³»çµ±', 'compute': 1, 'data': 1, 'algo': 1, 'color': '#FF6B6B'},\n",
    "    {'name': 'çµ±è¨ˆå­¸ç¿’', 'compute': 2, 'data': 3, 'algo': 3, 'color': '#4ECDC4'},\n",
    "    {'name': 'æ·ºå±¤ç¥ç¶“ç¶²è·¯', 'compute': 4, 'data': 5, 'algo': 5, 'color': '#45B7D1'},\n",
    "    {'name': 'æ·±åº¦å­¸ç¿’', 'compute': 6, 'data': 7, 'algo': 7, 'color': '#F7B731'},\n",
    "    {'name': 'Transformer', 'compute': 8, 'data': 9, 'algo': 9, 'color': '#5F27CD'},\n",
    "    {'name': 'LLM', 'compute': 10, 'data': 10, 'algo': 10, 'color': '#000000'},\n",
    "]\n",
    "\n",
    "# ç¹ªè£½é»\n",
    "for p in paradigms_3d:\n",
    "    ax.scatter(p['compute'], p['data'], p['algo'], \n",
    "               s=300, c=p['color'], marker='o', edgecolors='black', linewidths=2)\n",
    "    ax.text(p['compute'], p['data'], p['algo'], p['name'],\n",
    "            fontsize=9, ha='center', va='bottom')\n",
    "\n",
    "# ç¹ªè£½æ¼”é€²è·¯å¾‘\n",
    "compute_path = [p['compute'] for p in paradigms_3d]\n",
    "data_path = [p['data'] for p in paradigms_3d]\n",
    "algo_path = [p['algo'] for p in paradigms_3d]\n",
    "ax.plot(compute_path, data_path, algo_path, 'k--', alpha=0.5, linewidth=1.5)\n",
    "\n",
    "# è¨­å®šåº§æ¨™è»¸\n",
    "ax.set_xlabel('è¨ˆç®—èƒ½åŠ›\\n(Computing Power)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('æ•¸æ“šè¦æ¨¡\\n(Data Scale)', fontsize=12, fontweight='bold')\n",
    "ax.set_zlabel('æ¼”ç®—æ³•å‰µæ–°\\n(Algorithm Innovation)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('NLP æ¼”è®Šçš„ä¸‰ç¶­é©…å‹•æ¡†æ¶', fontsize=16, fontweight='bold', pad=20)\n",
    "\n",
    "# è¨­å®šè¦–è§’\n",
    "ax.view_init(elev=20, azim=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ¯ ä¸‰ç¶­é©…å‹•æ¡†æ¶è§£è®€:\")\n",
    "print(\"\\n1ï¸âƒ£ è¨ˆç®—èƒ½åŠ› (Computing Power)\")\n",
    "print(\"  - CPU â†’ GPU â†’ TPU â†’ å°ˆç”¨ AI æ™¶ç‰‡\")\n",
    "print(\"  - ç®—åŠ›å¢é•·: 10^6 å€ (1990-2023)\")\n",
    "print(\"  - æ‘©çˆ¾å®šå¾‹æŒçºŒé©…å‹•ç¡¬é«”é€²æ­¥\")\n",
    "\n",
    "print(\"\\n2ï¸âƒ£ æ•¸æ“šè¦æ¨¡ (Data Scale)\")\n",
    "print(\"  - Penn Treebank (1M è©) â†’ Common Crawl (1T è©)\")\n",
    "print(\"  - èªæ–™åº«è¦æ¨¡å¢é•·: 10^6 å€\")\n",
    "print(\"  - ç¶²è·¯çˆ¬èŸ²æ•¸æ“šçˆ†ç‚¸å¼å¢é•·\")\n",
    "\n",
    "print(\"\\n3ï¸âƒ£ æ¼”ç®—æ³•å‰µæ–° (Algorithm Innovation)\")\n",
    "print(\"  - è¦å‰‡ â†’ çµ±è¨ˆ â†’ ç¥ç¶“ç¶²è·¯ â†’ Attention â†’ Scaling\")\n",
    "print(\"  - é—œéµçªç ´: Word2Vec (2013), Transformer (2017)\")\n",
    "print(\"  - é è¨“ç·´-å¾®èª¿ç¯„å¼æˆç‚ºä¸»æµ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è¦æ¨¡æ³•å‰‡ (Scaling Laws)\n",
    "\n",
    "**OpenAI ç ”ç©¶ç™¼ç¾:** æ¨¡å‹æ€§èƒ½èˆ‡è¨ˆç®—é‡å‘ˆå†ªå¾‹é—œä¿‚\n",
    "\n",
    "$$\n",
    "\\text{Performance} \\propto (\\text{Compute})^\\alpha\n",
    "$$\n",
    "\n",
    "å…¶ä¸­ $\\alpha \\approx 0.5$ (å¯¦é©—æ¸¬å¾—)\n",
    "\n",
    "**å•Ÿç¤º:**\n",
    "- è¨ˆç®—é‡æ¯å¢åŠ  10 å€ â†’ æ€§èƒ½æå‡ $\\sqrt{10} \\approx 3.16$ å€\n",
    "- è¨ˆç®—é‡æ¯å¢åŠ  100 å€ â†’ æ€§èƒ½æå‡ 10 å€\n",
    "- **çµè«–:** åªè¦æœ‰è¶³å¤ ç®—åŠ›,æ¨¡å‹é‚„æœ‰å·¨å¤§æå‡ç©ºé–“!\n",
    "\n",
    "**é™åˆ¶å› ç´ :**\n",
    "1. èƒ½æºæ¶ˆè€— (GPT-3 è¨“ç·´è€—é›» ~1,287 MWh)\n",
    "2. ç¡¬é«”æˆæœ¬ (æ•¸åƒè¬ç¾å…ƒ GPU é›†ç¾¤)\n",
    "3. ç’°å¢ƒå½±éŸ¿ (ç¢³æ’æ”¾å•é¡Œ)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"8\"></a>\n",
    "## 8. å¯¦æˆ°ç·´ç¿’\n",
    "\n",
    "### ç·´ç¿’ 1: å°æ¯”ä¸åŒæ™‚ä»£çš„åˆ†è©æŠ€è¡“\n",
    "\n",
    "å¯¦ä½œä¸¦å°æ¯”è¦å‰‡åˆ†è©ã€çµ±è¨ˆåˆ†è©ã€ç¥ç¶“ç¶²è·¯åˆ†è©ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç·´ç¿’ 1: å°æ¯”ä¸‰ç¨®åˆ†è©æ–¹æ³•\n",
    "test_sentence = \"I don't believe it's raining cats and dogs!\"\n",
    "\n",
    "print(\"æ¸¬è©¦å¥å­:\", test_sentence)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# æ–¹æ³• 1: è¦å‰‡åˆ†è© (æ­£å‰‡è¡¨é”å¼)\n",
    "def rule_based_tokenize(text):\n",
    "    \"\"\"è¦å‰‡ç³»çµ±: ä½¿ç”¨æ­£å‰‡è¡¨é”å¼\"\"\"\n",
    "    tokens = re.findall(r\"\\b\\w+\\b|[.,!?]\", text)\n",
    "    return tokens\n",
    "\n",
    "tokens_rule = rule_based_tokenize(test_sentence)\n",
    "print(\"\\n1ï¸âƒ£ è¦å‰‡åˆ†è© (æ­£å‰‡è¡¨é”å¼):\")\n",
    "print(f\"  çµæœ: {tokens_rule}\")\n",
    "print(f\"  æ•¸é‡: {len(tokens_rule)} tokens\")\n",
    "print(\"  å„ªé»: å¿«é€Ÿã€å¯è§£é‡‹\")\n",
    "print(\"  ç¼ºé»: ç„¡æ³•è™•ç†ç¸®å¯« (don't â†’ do n't)\")\n",
    "\n",
    "# æ–¹æ³• 2: çµ±è¨ˆåˆ†è© (åŸºæ–¼é »ç‡)\n",
    "def statistical_tokenize(text):\n",
    "    \"\"\"çµ±è¨ˆå­¸ç¿’: åŸºæ–¼è©é »çš„ç°¡å–®åˆ†è©\"\"\"\n",
    "    # ç°¡åŒ–: ä½¿ç”¨ç©ºæ ¼åˆ†è© + æ¨™é»åˆ†é›¢\n",
    "    tokens = []\n",
    "    for word in text.split():\n",
    "        # åˆ†é›¢æ¨™é»\n",
    "        if word[-1] in '.,!?':\n",
    "            tokens.extend([word[:-1], word[-1]])\n",
    "        else:\n",
    "            tokens.append(word)\n",
    "    return tokens\n",
    "\n",
    "tokens_stat = statistical_tokenize(test_sentence)\n",
    "print(\"\\n2ï¸âƒ£ çµ±è¨ˆåˆ†è© (åŸºæ–¼é »ç‡):\")\n",
    "print(f\"  çµæœ: {tokens_stat}\")\n",
    "print(f\"  æ•¸é‡: {len(tokens_stat)} tokens\")\n",
    "print(\"  å„ªé»: è‡ªå‹•å­¸ç¿’åˆ†è©é‚Šç•Œ\")\n",
    "print(\"  ç¼ºé»: ä»ç„¡æ³•å®Œç¾è™•ç†ç¸®å¯«\")\n",
    "\n",
    "# æ–¹æ³• 3: ç¥ç¶“ç¶²è·¯åˆ†è© (å­è©åˆ†è© BPE)\n",
    "def neural_tokenize(text):\n",
    "    \"\"\"ç¥ç¶“ç¶²è·¯: å­è©åˆ†è© (BPE é¢¨æ ¼)\"\"\"\n",
    "    # ç°¡åŒ–ç‰ˆ: åˆ†é›¢å¸¸è¦‹ç¸®å¯«\n",
    "    text = text.replace(\"don't\", \"do n't\")\n",
    "    text = text.replace(\"it's\", \"it 's\")\n",
    "    tokens = re.findall(r\"\\b\\w+\\b|[.,!?]\", text)\n",
    "    return tokens\n",
    "\n",
    "tokens_neural = neural_tokenize(test_sentence)\n",
    "print(\"\\n3ï¸âƒ£ ç¥ç¶“ç¶²è·¯åˆ†è© (å­è©åˆ†è©):\")\n",
    "print(f\"  çµæœ: {tokens_neural}\")\n",
    "print(f\"  æ•¸é‡: {len(tokens_neural)} tokens\")\n",
    "print(\"  å„ªé»: å¯è™•ç†ç¸®å¯«ã€æœªç™»éŒ„è©\")\n",
    "print(\"  ç¼ºé»: éœ€è¦å¤§é‡è¨“ç·´æ•¸æ“š\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nğŸ’¡ ç¸½çµ:\")\n",
    "print(\"  - è¦å‰‡ç³»çµ±: å¿«ä½†åƒµåŒ–\")\n",
    "print(\"  - çµ±è¨ˆå­¸ç¿’: è‡ªå‹•ä½†éœ€è¦ç‰¹å¾µå·¥ç¨‹\")\n",
    "print(\"  - ç¥ç¶“ç¶²è·¯: å¼·å¤§ä½†éœ€è¦æ•¸æ“šèˆ‡ç®—åŠ›\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ç·´ç¿’ 2: æ¨¡å‹è¦æ¨¡èˆ‡æ€§èƒ½é—œä¿‚æ¨¡æ“¬\n",
    "\n",
    "æ¨¡æ“¬è¦æ¨¡æ³•å‰‡,è§€å¯Ÿæ¨¡å‹è¦æ¨¡èˆ‡æ€§èƒ½çš„é—œä¿‚ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç·´ç¿’ 2: è¦æ¨¡æ³•å‰‡æ¨¡æ“¬\n",
    "def scaling_law(compute, alpha=0.5):\n",
    "    \"\"\"è¦æ¨¡æ³•å‰‡: Performance âˆ Compute^Î±\"\"\"\n",
    "    base_performance = 50  # åŸºæº–æ€§èƒ½\n",
    "    return base_performance + 40 * (compute ** alpha) / (1e12 ** alpha)\n",
    "\n",
    "# æ¨¡æ“¬ä¸åŒç®—åŠ›ä¸‹çš„æ€§èƒ½\n",
    "compute_range = np.logspace(15, 25, 50)  # 10^15 åˆ° 10^25 FLOPs\n",
    "performance = [scaling_law(c) for c in compute_range]\n",
    "\n",
    "# æ¨™è¨˜çœŸå¯¦æ¨¡å‹ä½ç½®\n",
    "real_models = [\n",
    "    {'name': 'Word2Vec', 'compute': 1e15, 'color': '#FF6B6B'},\n",
    "    {'name': 'BERT-Base', 'compute': 1e19, 'color': '#4ECDC4'},\n",
    "    {'name': 'GPT-2', 'compute': 1e20, 'color': '#45B7D1'},\n",
    "    {'name': 'GPT-3', 'compute': 3.14e23, 'color': '#5F27CD'},\n",
    "    {'name': 'GPT-4', 'compute': 1e25, 'color': '#000000'},\n",
    "]\n",
    "\n",
    "# ç¹ªåœ–\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.plot(compute_range, performance, linewidth=3, color='#5F27CD', alpha=0.7)\n",
    "\n",
    "# æ¨™è¨˜çœŸå¯¦æ¨¡å‹\n",
    "for model in real_models:\n",
    "    perf = scaling_law(model['compute'])\n",
    "    plt.scatter(model['compute'], perf, s=200, c=model['color'], \n",
    "                edgecolors='black', linewidths=2, zorder=5)\n",
    "    plt.annotate(model['name'], xy=(model['compute'], perf),\n",
    "                 xytext=(10, 10), textcoords='offset points',\n",
    "                 fontsize=11, fontweight='bold',\n",
    "                 bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.xlabel('è¨“ç·´ç®—åŠ› (FLOPs, å°æ•¸å°ºåº¦)', fontsize=13)\n",
    "plt.ylabel('æ¨¡å‹æ€§èƒ½ (ä»»æ„å–®ä½)', fontsize=13)\n",
    "plt.title('è¦æ¨¡æ³•å‰‡: æ¨¡å‹æ€§èƒ½èˆ‡è¨ˆç®—é‡çš„å†ªå¾‹é—œä¿‚', fontsize=16, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3, which='both')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ“ˆ è¦æ¨¡æ³•å‰‡åˆ†æ:\")\n",
    "print(f\"  - ç®—åŠ›å¾ 10^15 åˆ° 10^25 (å¢é•· 10^10 å€)\")\n",
    "print(f\"  - æ€§èƒ½å¾ {scaling_law(1e15):.1f} æå‡åˆ° {scaling_law(1e25):.1f}\")\n",
    "print(f\"  - æ€§èƒ½å¢é•·å€æ•¸: {scaling_law(1e25)/scaling_law(1e15):.1f}x\")\n",
    "print(\"\\nğŸ’¡ å•Ÿç¤º:\")\n",
    "print(\"  1. æ€§èƒ½æå‡ä¸æ˜¯ç·šæ€§çš„,è€Œæ˜¯éµå¾ªå†ªå¾‹\")\n",
    "print(\"  2. æ—©æœŸéšæ®µæå‡å¿«,å¾ŒæœŸé‚Šéš›æ”¶ç›Šéæ¸›\")\n",
    "print(\"  3. ä½†åªè¦æœ‰ç®—åŠ›,ä»æœ‰å·¨å¤§æå‡ç©ºé–“!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ç·´ç¿’ 3: æ€è€ƒé¡Œ\n",
    "\n",
    "1. **å…¸ç¯„è½‰ç§»çš„æœ¬è³ª:** æ¯æ¬¡å…¸ç¯„è½‰ç§»è§£æ±ºäº†ä»€éº¼æ ¸å¿ƒå•é¡Œ?å¼•å…¥äº†ä»€éº¼æ–°æŒ‘æˆ°?\n",
    "\n",
    "2. **æœªä¾†é æ¸¬:** åŸºæ–¼æ­·å²è¶¨å‹¢,ä½ èªç‚ºä¸‹ä¸€æ¬¡å…¸ç¯„è½‰ç§»æœƒæ˜¯ä»€éº¼?\n",
    "   - æç¤º: è€ƒæ…®ç•¶å‰ LLM çš„ä¾·é™æ€§ (å¯è§£é‡‹æ€§ã€èƒ½æºæ¶ˆè€—ã€åè¦‹å•é¡Œ)\n",
    "\n",
    "3. **å¯¦éš›æ‡‰ç”¨:** å°æ–¼ä½ çš„æ‡‰ç”¨å ´æ™¯,æ‡‰è©²é¸æ“‡å“ªå€‹æ™‚ä»£çš„æŠ€è¡“?\n",
    "   - ç°¡å–®ä»»å‹™: è¦å‰‡ç³»çµ±?\n",
    "   - è¤‡é›œç†è§£: BERT?\n",
    "   - ç”Ÿæˆä»»å‹™: GPT?\n",
    "\n",
    "åœ¨ä¸‹æ–¹ Cell ä¸­è¨˜éŒ„ä½ çš„æ€è€ƒ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç·´ç¿’ 3: è¨˜éŒ„ä½ çš„æ€è€ƒ\n",
    "\n",
    "# å•é¡Œ 1: å…¸ç¯„è½‰ç§»çš„æ ¸å¿ƒé©…å‹•åŠ›æ˜¯ä»€éº¼?\n",
    "your_answer_1 = \"\"\"\n",
    "æˆ‘çš„è§€å¯Ÿ:\n",
    "1. è¦å‰‡ â†’ çµ±è¨ˆ: \n",
    "2. çµ±è¨ˆ â†’ ç¥ç¶“ç¶²è·¯:\n",
    "3. RNN â†’ Transformer:\n",
    "\"\"\"\n",
    "\n",
    "# å•é¡Œ 2: ä¸‹ä¸€æ¬¡å…¸ç¯„è½‰ç§»å¯èƒ½æ˜¯ä»€éº¼?\n",
    "your_answer_2 = \"\"\"\n",
    "æˆ‘çš„é æ¸¬:\n",
    "- å¯èƒ½æ–¹å‘:\n",
    "- æŠ€è¡“çªç ´:\n",
    "\"\"\"\n",
    "\n",
    "# å•é¡Œ 3: å¦‚ä½•é¸æ“‡åˆé©çš„æŠ€è¡“?\n",
    "your_answer_3 = \"\"\"\n",
    "æˆ‘çš„å ´æ™¯:\n",
    "- æ‡‰ç”¨éœ€æ±‚:\n",
    "- æ¨è–¦æŠ€è¡“:\n",
    "- ç†ç”±:\n",
    "\"\"\"\n",
    "\n",
    "print(\"æ€è€ƒè¨˜éŒ„å·²ä¿å­˜,ç¹¼çºŒå­¸ç¿’!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“š æœ¬èª²ç¸½çµ\n",
    "\n",
    "### æ ¸å¿ƒè¦é»å›é¡§\n",
    "\n",
    "1. **NLP äº”å¤§å…¸ç¯„è½‰ç§»:**\n",
    "   - è¦å‰‡ç³»çµ± (1950-1990): å°ˆå®¶è¦å‰‡,å¯è§£é‡‹ä½†è¦†è“‹ä¸å…¨\n",
    "   - çµ±è¨ˆå­¸ç¿’ (1990-2010): æ©Ÿç‡æ¨¡å‹,è‡ªå‹•å­¸ç¿’ä½†ç¨€ç–æ€§å•é¡Œ\n",
    "   - æ·ºå±¤ç¥ç¶“ç¶²è·¯ (2010-2017): è©å‘é‡,è§£æ±ºç¨€ç–æ€§ä½†éœæ…‹è¡¨ç¤º\n",
    "   - æ·±åº¦å­¸ç¿’ (2013-2017): RNN/LSTM,åºåˆ—å»ºæ¨¡ä½†ç„¡æ³•ä¸¦è¡Œ\n",
    "   - Transformer & LLM (2017-è‡³ä»Š): ç´”æ³¨æ„åŠ›,å®Œå…¨ä¸¦è¡Œä¸”å¯æ“´å±•\n",
    "\n",
    "2. **ä¸‰ç¶­é©…å‹•æ¡†æ¶:**\n",
    "   - è¨ˆç®—èƒ½åŠ›: 10^6 å€å¢é•· (1990-2023)\n",
    "   - æ•¸æ“šè¦æ¨¡: 10^6 å€å¢é•· (1M â†’ 1T è©)\n",
    "   - æ¼”ç®—æ³•å‰µæ–°: è¦å‰‡ â†’ çµ±è¨ˆ â†’ ç¥ç¶“ç¶²è·¯ â†’ Attention\n",
    "\n",
    "3. **æ¨¡å‹è¦æ¨¡æ¼”é€²:**\n",
    "   - æŒ‡æ•¸ç´šå¢é•·: 10^6 â†’ 10^12 åƒæ•¸ (100 è¬å€)\n",
    "   - è¦æ¨¡æ³•å‰‡: Performance âˆ Compute^0.5\n",
    "   - æ¹§ç¾èƒ½åŠ›: è¦æ¨¡è¶…éè‡¨ç•Œé»å¾Œå‡ºç¾æ–°èƒ½åŠ›\n",
    "\n",
    "4. **æŠ€è¡“é¸æ“‡å»ºè­°:**\n",
    "   - ç†è§£ä»»å‹™ â†’ BERT (Encoder-Only)\n",
    "   - ç”Ÿæˆä»»å‹™ â†’ GPT (Decoder-Only)\n",
    "   - è½‰æ›ä»»å‹™ â†’ T5 (Encoder-Decoder)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ ä¸‹ç¯€é å‘Š\n",
    "\n",
    "**CH02-03: NLP æ ¸å¿ƒä»»å‹™èˆ‡æ‡‰ç”¨**\n",
    "\n",
    "æˆ‘å€‘å°‡æ¢è¨:\n",
    "- NLP æ ¸å¿ƒä»»å‹™åˆ†é¡ (åºåˆ—ã€åˆ†é¡ã€ç”Ÿæˆ)\n",
    "- å¯¦éš›æ‡‰ç”¨æ¡ˆä¾‹åˆ†æ\n",
    "- ä»»å‹™é›£åº¦æ¢¯åº¦èˆ‡æŠ€è¡“é¸å‹\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“– å»¶ä¼¸é–±è®€\n",
    "\n",
    "### é—œéµè«–æ–‡\n",
    "\n",
    "1. **Word2Vec**: Mikolov et al. (2013). [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781)\n",
    "2. **Seq2Seq**: Sutskever et al. (2014). [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)\n",
    "3. **Attention**: Bahdanau et al. (2015). [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)\n",
    "4. **Transformer**: Vaswani et al. (2017). [Attention is All You Need](https://arxiv.org/abs/1706.03762)\n",
    "5. **BERT**: Devlin et al. (2018). [BERT: Pre-training of Deep Bidirectional Transformers](https://arxiv.org/abs/1810.04805)\n",
    "6. **GPT-3**: Brown et al. (2020). [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)\n",
    "7. **Scaling Laws**: Kaplan et al. (2020). [Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361)\n",
    "\n",
    "### æŠ€è¡“åšå®¢\n",
    "\n",
    "- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/) - Jay Alammar\n",
    "- [BERT è«–æ–‡ç²¾è®€](https://zhuanlan.zhihu.com/p/46652512) - çŸ¥ä¹å°ˆæ¬„\n",
    "- [GPT ç³»åˆ—æ¼”é€²å²](https://huggingface.co/blog/gpt-evolution) - Hugging Face\n",
    "\n",
    "### å­¸ç¿’è³‡æº\n",
    "\n",
    "- **Stanford CS224N**: [NLP with Deep Learning](http://web.stanford.edu/class/cs224n/)\n",
    "- **Hugging Face Course**: [NLP Course](https://huggingface.co/learn/nlp-course)\n",
    "- **Papers With Code**: [NLP Progress](https://paperswithcode.com/area/natural-language-processing)\n",
    "\n",
    "---\n",
    "\n",
    "**èª²ç¨‹è³‡è¨Š:**\n",
    "- **ä½œè€…:** iSpan NLP Team\n",
    "- **ç‰ˆæœ¬:** v1.0\n",
    "- **æœ€å¾Œæ›´æ–°:** 2025-10-17\n",
    "- **æˆæ¬Š:** MIT License (åƒ…ä¾›æ•™å­¸ä½¿ç”¨)\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ™‹ å•é¡Œè¨è«–\n",
    "\n",
    "æœ‰ä»»ä½•å•é¡Œå—?æ­¡è¿åœ¨è¨è«–å€æå•!\n",
    "\n",
    "**å¸¸è¦‹å•é¡Œ:**\n",
    "1. Q: ç‚ºä»€éº¼ Transformer æ¯” RNN å¿«é€™éº¼å¤š?\n",
    "   - A: ä¸¦è¡ŒåŒ–!RNN å¿…é ˆä¸²è¡Œè¨ˆç®—,Transformer æ‰€æœ‰ä½ç½®åŒæ™‚è¨ˆç®—ã€‚\n",
    "\n",
    "2. Q: ä»€éº¼æ˜¯æ¹§ç¾èƒ½åŠ›?\n",
    "   - A: æ¨¡å‹è¦æ¨¡è¶…éè‡¨ç•Œé»å¾Œçªç„¶å‡ºç¾çš„æ–°èƒ½åŠ›,å¦‚è¤‡é›œæ¨ç†ã€å°‘æ¨£æœ¬å­¸ç¿’ã€‚\n",
    "\n",
    "3. Q: æˆ‘æ‡‰è©²é¸æ“‡ BERT é‚„æ˜¯ GPT?\n",
    "   - A: çœ‹ä»»å‹™!ç†è§£ä»»å‹™ç”¨ BERT (é›™å‘),ç”Ÿæˆä»»å‹™ç”¨ GPT (å–®å‘)ã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
