# CH08 講義 04: 優化與生產環境部署最佳實踐

---

**課程**: iSpan Python NLP 速成教案
**章節**: CH08 Hugging Face 函式庫實戰
**講義編號**: 04/04
**預計時間**: 120 分鐘
**對應 Notebook**: `10_進階技巧與優化.ipynb`

---

## 📚 學習目標

完成本講義後,你將能夠:

1. ✅ 掌握模型量化 (Quantization) 技術
2. ✅ 實作推理加速與優化策略
3. ✅ 使用 ONNX 進行跨平台部署
4. ✅ 建立生產級 API 服務
5. ✅ 了解 Docker 容器化部署

---

## 1. 為什麼需要優化?

### 1.1 生產環境的挑戰

| 挑戰 | 說明 | 影響 |
|------|------|------|
| **推理速度慢** | BERT 模型參數量 110M+ | 用戶體驗差 |
| **記憶體占用高** | 單模型需 400MB+ RAM | 成本高昂 |
| **GPU 成本高** | GPU 推理費用 >> CPU | 難以規模化 |
| **擴展性差** | 單機處理能力有限 | 無法支撐高並發 |

### 1.2 優化技術對比

| 技術 | 速度提升 | 大小減少 | 精度損失 | 實作難度 |
|------|---------|---------|----------|----------|
| **量化 (INT8)** | 2-4x | 75% | <1% | ⭐ 低 |
| **知識蒸餾** | 2-10x | 自定義 | 2-5% | ⭐⭐⭐ 高 |
| **ONNX 轉換** | 1.5-2x | - | 0% | ⭐⭐ 中 |
| **批量推理** | 線性擴展 | - | 0% | ⭐ 低 |

---

## 2. 模型量化 (Quantization)

### 2.1 什麼是量化?

**量化**是將模型權重從高精度 (FP32) 轉換為低精度 (INT8)。

```
FP32 (32-bit float)
每個參數: 4 bytes
模型大小: 400 MB

        ↓ 量化

INT8 (8-bit integer)
每個參數: 1 byte
模型大小: 100 MB (減少 75%)
```

#### 量化類型

| 類型 | 說明 | 何時轉換 | 適用場景 |
|------|------|---------|----------|
| **動態量化** | 權重量化,激活動態計算 | 推理時 | CPU 推理 |
| **靜態量化** | 權重和激活都量化 | 提前校準 | 更高性能 |
| **QAT** | 訓練時感知量化 | 訓練中 | 精度要求高 |

---

### 2.2 動態量化實作

```python
import torch
from transformers import AutoModelForSequenceClassification

# 加載模型
model = AutoModelForSequenceClassification.from_pretrained(
    "distilbert-base-uncased-finetuned-sst-2-english"
)
model.eval()

# 動態量化
quantized_model = torch.quantization.quantize_dynamic(
    model,
    {torch.nn.Linear},  # 量化 Linear 層
    dtype=torch.qint8
)

# 比較大小
def get_model_size(model):
    torch.save(model.state_dict(), "temp.p")
    size = os.path.getsize("temp.p") / 1e6  # MB
    os.remove("temp.p")
    return size

original_size = get_model_size(model)
quantized_size = get_model_size(quantized_model)

print(f"原始模型: {original_size:.2f} MB")
print(f"量化模型: {quantized_size:.2f} MB")
print(f"減少: {(1 - quantized_size/original_size)*100:.1f}%")
```

**輸出範例**:
```
原始模型: 268.00 MB
量化模型: 67.50 MB
減少: 74.8%
```

---

### 2.3 性能基準測試

```python
import time
import numpy as np

def benchmark_model(model, tokenizer, texts, iterations=50):
    """測試模型推理速度"""
    # Warmup
    for _ in range(5):
        inputs = tokenizer(texts, return_tensors="pt", padding=True)
        with torch.no_grad():
            _ = model(**inputs)

    # 測試
    times = []
    for _ in range(iterations):
        inputs = tokenizer(texts, return_tensors="pt", padding=True)

        start = time.perf_counter()
        with torch.no_grad():
            _ = model(**inputs)
        end = time.perf_counter()

        times.append((end - start) * 1000)  # ms

    return {
        'mean': np.mean(times),
        'std': np.std(times),
        'min': np.min(times),
        'p95': np.percentile(times, 95)
    }

# 測試
test_texts = ["This is a test sentence."] * 3

print("原始模型:")
original_time = benchmark_model(model, tokenizer, test_texts)
print(f"  平均延遲: {original_time['mean']:.2f} ms")

print("\n量化模型:")
quantized_time = benchmark_model(quantized_model, tokenizer, test_texts)
print(f"  平均延遲: {quantized_time['mean']:.2f} ms")

print(f"\n加速比: {original_time['mean']/quantized_time['mean']:.2f}x")
```

---

## 3. ONNX 轉換與優化

### 3.1 什麼是 ONNX?

**ONNX (Open Neural Network Exchange)** 是跨平台深度學習模型格式。

#### ONNX 優勢

- 🚀 **高效執行**: ONNX Runtime 高度優化
- 🌐 **跨平台**: CPU, GPU, Edge devices
- 📦 **易部署**: 簡化生產環境部署
- ⚡ **加速推理**: 比原生 PyTorch 快 1.5-2x

---

### 3.2 轉換為 ONNX

```python
from optimum.onnxruntime import ORTModelForSequenceClassification
from transformers import AutoTokenizer

# 方法 1: 自動轉換
onnx_model = ORTModelForSequenceClassification.from_pretrained(
    "distilbert-base-uncased-finetuned-sst-2-english",
    export=True
)

# 保存
onnx_save_path = "./onnx_model"
onnx_model.save_pretrained(onnx_save_path)
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
tokenizer.save_pretrained(onnx_save_path)

print(f"✅ ONNX 模型已保存至: {onnx_save_path}")
```

#### 使用 ONNX 模型

```python
from optimum.onnxruntime import ORTModelForSequenceClassification
from transformers import pipeline

# 加載 ONNX 模型
onnx_model = ORTModelForSequenceClassification.from_pretrained(onnx_save_path)
tokenizer = AutoTokenizer.from_pretrained(onnx_save_path)

# 創建 pipeline
classifier = pipeline(
    "sentiment-analysis",
    model=onnx_model,
    tokenizer=tokenizer
)

# 使用
result = classifier("This product is amazing!")
print(result)
```

---

## 4. 批量推理優化

### 4.1 為什麼批量處理更快?

```
單個處理:
[Text1] → Model → [Result1]  (10ms)
[Text2] → Model → [Result2]  (10ms)
[Text3] → Model → [Result3]  (10ms)
總時間: 30ms

批量處理:
[Text1, Text2, Text3] → Model → [Result1, Result2, Result3]
總時間: 15ms (2x faster)
```

---

### 4.2 實作批量推理

```python
def process_single(classifier, texts):
    """逐個處理"""
    results = []
    for text in texts:
        result = classifier(text)
        results.append(result)
    return results

def process_batch(classifier, texts, batch_size=32):
    """批量處理"""
    results = []
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        batch_results = classifier(batch)
        results.extend(batch_results)
    return results

# 測試
test_texts = ["Sample text"] * 100

# 單個處理
start = time.time()
results_single = process_single(classifier, test_texts)
time_single = time.time() - start

# 批量處理
start = time.time()
results_batch = process_batch(classifier, test_texts, batch_size=32)
time_batch = time.time() - start

print(f"單個處理: {time_single:.2f}s")
print(f"批量處理: {time_batch:.2f}s")
print(f"加速比: {time_single/time_batch:.2f}x")
```

---

### 4.3 最佳批次大小尋找

```python
def find_optimal_batch_size(classifier, texts, batch_sizes=[1, 4, 8, 16, 32, 64]):
    """尋找最佳批次大小"""
    results = {}

    for bs in batch_sizes:
        start = time.time()
        process_batch(classifier, texts, batch_size=bs)
        elapsed = time.time() - start

        throughput = len(texts) / elapsed  # samples/sec
        results[bs] = {
            'time': elapsed,
            'throughput': throughput
        }

    # 找出最佳
    best_bs = max(results.keys(), key=lambda k: results[k]['throughput'])

    return results, best_bs

# 測試
test_texts = ["Sample"] * 100
results, best_bs = find_optimal_batch_size(classifier, test_texts)

print("批次大小對比:\n")
for bs, metrics in results.items():
    print(f"Batch Size {bs:2d}: {metrics['throughput']:6.1f} samples/sec")

print(f"\n✅ 最佳批次大小: {best_bs}")
```

---

## 5. FastAPI 生產部署

### 5.1 建立 API 服務

```python
# app.py
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from transformers import pipeline
import torch

# 初始化 FastAPI
app = FastAPI(
    title="Sentiment Analysis API",
    description="Production-ready sentiment analysis service",
    version="1.0.0"
)

# 加載模型 (應用啟動時加載一次)
MODEL_PATH = "./models/sentiment_classifier"

@app.on_event("startup")
async def load_model():
    global classifier
    classifier = pipeline(
        "sentiment-analysis",
        model=MODEL_PATH,
        device=-1  # CPU
    )
    print("✅ Model loaded successfully")

# Request/Response 模型
class TextInput(BaseModel):
    text: str

class BatchTextInput(BaseModel):
    texts: list[str]

class SentimentResponse(BaseModel):
    sentiment: str
    confidence: float

# 單個預測端點
@app.post("/predict", response_model=SentimentResponse)
async def predict(input_data: TextInput):
    """
    Analyze sentiment of a single text
    """
    try:
        result = classifier(input_data.text)[0]
        return SentimentResponse(
            sentiment=result['label'],
            confidence=round(result['score'], 4)
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# 批量預測端點
@app.post("/predict_batch")
async def predict_batch(input_data: BatchTextInput):
    """
    Analyze sentiment of multiple texts
    """
    try:
        results = classifier(input_data.texts)
        return {
            "predictions": [
                {
                    "sentiment": r['label'],
                    "confidence": round(r['score'], 4)
                }
                for r in results
            ]
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# 健康檢查
@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "model_loaded": "classifier" in globals()
    }

# 模型資訊
@app.get("/model_info")
async def model_info():
    return {
        "model_path": MODEL_PATH,
        "device": "CPU",
        "framework": "PyTorch"
    }
```

#### 運行 API

```bash
# 安裝依賴
pip install fastapi uvicorn transformers torch

# 啟動服務
uvicorn app:app --host 0.0.0.0 --port 8000 --workers 4

# 訪問文檔
# http://localhost:8000/docs
```

---

### 5.2 測試 API

```python
# test_api.py
import requests

API_URL = "http://localhost:8000"

# 單個預測
response = requests.post(
    f"{API_URL}/predict",
    json={"text": "This product is amazing!"}
)
print("單個預測:")
print(response.json())

# 批量預測
response = requests.post(
    f"{API_URL}/predict_batch",
    json={
        "texts": [
            "Great product!",
            "Terrible experience.",
            "It's okay."
        ]
    }
)
print("\n批量預測:")
print(response.json())

# 健康檢查
response = requests.get(f"{API_URL}/health")
print("\n健康狀態:")
print(response.json())
```

---

## 6. Docker 容器化部署

### 6.1 Dockerfile

```dockerfile
# Dockerfile
FROM python:3.10-slim

WORKDIR /app

# 安裝系統依賴
RUN apt-get update && apt-get install -y \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

# 複製需求文件
COPY requirements.txt .

# 安裝 Python 依賴
RUN pip install --no-cache-dir -r requirements.txt

# 複製應用代碼
COPY app.py .
COPY models/ ./models/

# 暴露端口
EXPOSE 8000

# 健康檢查
HEALTHCHECK --interval=30s --timeout=3s \
  CMD curl -f http://localhost:8000/health || exit 1

# 啟動命令
CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "4"]
```

### 6.2 requirements.txt

```txt
fastapi==0.104.1
uvicorn[standard]==0.24.0
transformers==4.35.0
torch==2.1.0
pydantic==2.4.2
```

### 6.3 建置與運行

```bash
# 建置 Docker 映像檔
docker build -t sentiment-api:v1.0 .

# 運行容器
docker run -d \
  --name sentiment-api \
  -p 8000:8000 \
  -e MODEL_PATH="/app/models/sentiment_classifier" \
  sentiment-api:v1.0

# 查看日誌
docker logs -f sentiment-api

# 測試
curl http://localhost:8000/health
```

---

### 6.4 Docker Compose (多服務編排)

```yaml
# docker-compose.yml
version: '3.8'

services:
  sentiment-api:
    build: .
    ports:
      - "8000:8000"
    environment:
      - MODEL_PATH=/app/models/sentiment_classifier
      - WORKERS=4
    volumes:
      - ./models:/app/models
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 3s
      retries: 3

  nginx:
    image: nginx:latest
    ports:
      - "80:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
    depends_on:
      - sentiment-api
```

運行:

```bash
docker-compose up -d
```

---

## 7. 性能監控與日誌

### 7.1 添加日誌

```python
import logging
from datetime import datetime

# 配置日誌
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('api.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

@app.post("/predict")
async def predict(input_data: TextInput):
    start_time = datetime.now()

    try:
        result = classifier(input_data.text)[0]

        # 記錄請求
        latency = (datetime.now() - start_time).total_seconds() * 1000
        logger.info(f"Prediction latency: {latency:.2f}ms | Sentiment: {result['label']}")

        return SentimentResponse(
            sentiment=result['label'],
            confidence=round(result['score'], 4)
        )
    except Exception as e:
        logger.error(f"Prediction error: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))
```

---

### 7.2 性能指標監控

```python
from prometheus_client import Counter, Histogram, generate_latest
from fastapi.responses import Response

# Prometheus metrics
REQUEST_COUNT = Counter('api_requests_total', 'Total API requests')
REQUEST_LATENCY = Histogram('api_request_duration_seconds', 'Request latency')
ERROR_COUNT = Counter('api_errors_total', 'Total API errors')

@app.post("/predict")
@REQUEST_LATENCY.time()
async def predict(input_data: TextInput):
    REQUEST_COUNT.inc()

    try:
        # ... prediction code ...
        return result
    except Exception as e:
        ERROR_COUNT.inc()
        raise

# Metrics endpoint
@app.get("/metrics")
async def metrics():
    return Response(generate_latest(), media_type="text/plain")
```

---

## 8. 部署檢查清單

### 8.1 部署前檢查

- [ ] **模型優化完成**
  - [ ] 量化或蒸餾
  - [ ] ONNX 轉換
  - [ ] 性能基準測試

- [ ] **API 服務完善**
  - [ ] 健康檢查端點
  - [ ] 錯誤處理
  - [ ] 輸入驗證
  - [ ] 日誌記錄

- [ ] **容器化**
  - [ ] Dockerfile 編寫
  - [ ] 映像檔建置測試
  - [ ] 多環境配置

- [ ] **安全性**
  - [ ] API 認證 (JWT/API Key)
  - [ ] 請求速率限制
  - [ ] HTTPS 配置

- [ ] **監控與告警**
  - [ ] 性能指標收集
  - [ ] 錯誤監控
  - [ ] 告警設定

---

### 8.2 生產環境最佳實踐

| 項目 | 建議 | 原因 |
|------|------|------|
| **負載均衡** | 使用 Nginx/HAProxy | 分散流量,提升可用性 |
| **自動擴展** | Kubernetes HPA | 應對流量波動 |
| **模型版本管理** | Model Registry | 支持回滾,A/B 測試 |
| **快取** | Redis | 減少重複計算 |
| **非同步處理** | Celery/RQ | 長時間任務 |
| **備份策略** | 定期備份 | 防止數據丟失 |

---

## 9. 總結

### ✅ 核心要點

1. **優化技術**
   - 量化: 減少 75% 大小,加速 2-4x
   - ONNX: 跨平台,加速 1.5-2x
   - 批量處理: 線性加速

2. **部署策略**
   - FastAPI: 快速建立 API
   - Docker: 容器化標準化
   - Kubernetes: 大規模編排

3. **最佳實踐**
   - 完善的日誌與監控
   - 健康檢查與告警
   - 安全性與權限控制

### 🎯 完整學習路徑回顧

```
CH08 Hugging Face 實戰完成! 🎉

第 1 週: 生態系統與 Pipeline API
  ├── 理解 Hugging Face 核心組件
  ├── 掌握 Pipeline 快速應用
  └── 實作 5+ 種 NLP 任務

第 2 週: 模型微調與實戰
  ├── Trainer API 完整流程
  ├── 超參數調優
  └── 端到端商業專案

第 3 週: 優化與部署
  ├── 模型量化與加速
  ├── ONNX 轉換
  ├── FastAPI 服務
  └── Docker 容器化

你現在具備:
✅ 使用預訓練模型解決實際問題
✅ 微調模型適應特定任務
✅ 優化模型提升性能
✅ 部署模型到生產環境
```

---

**講義版本**: v1.0
**最後更新**: 2025-10-17
**作者**: iSpan NLP Team / Claude AI
**授權**: CC BY-NC-SA 4.0

---

## 🔗 延伸學習資源

- 📖 [Hugging Face 官方文檔](https://huggingface.co/docs)
- 🎓 [Hugging Face 課程](https://huggingface.co/course)
- 💻 [ONNX Runtime 文檔](https://onnxruntime.ai/)
- 🐳 [Docker 官方教學](https://docs.docker.com/)
- ⚡ [FastAPI 文檔](https://fastapi.tiangolo.com/)

**恭喜完成 CH08 全部學習內容!** 🎉
