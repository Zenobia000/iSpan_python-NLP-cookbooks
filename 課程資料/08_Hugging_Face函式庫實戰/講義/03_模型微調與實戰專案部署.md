# CH08 講義 03: 模型微調與實戰專案部署

---

**課程**: iSpan Python NLP 速成教案
**章節**: CH08 Hugging Face 函式庫實戰
**講義編號**: 03/04
**預計時間**: 150 分鐘
**對應 Notebooks**: `08_模型微調_Fine_Tuning.ipynb`, `09_專案實戰_客戶意見分析儀.ipynb`

---

## 📚 學習目標

完成本講義後,你將能夠:

1. ✅ 理解遷移學習與模型微調的核心概念
2. ✅ 掌握 Trainer API 完整使用流程
3. ✅ 實作端到端的商業應用專案
4. ✅ 建立生產級的 NLP 分析系統
5. ✅ 掌握超參數調優技巧

---

## Part 1: 模型微調 (Fine-Tuning)

### 1.1 為什麼需要微調?

#### 三種訓練策略對比

| 策略 | 數據需求 | 訓練時間 | 精度 | 適用場景 |
|------|---------|---------|------|----------|
| **從頭訓練** | 百萬級 | 數週 (GPU 集群) | 可自定義 | 特殊領域、充足資源 |
| **預訓練直接用** | 無 | 0 | 中等 | 通用任務、快速原型 |
| **微調** ✅ | 千-萬級 | 小時-天 | 高 | **大多數實際場景** |

#### 微調的核心優勢

```
預訓練模型 (BERT)
    ↓ 已學會語言通用知識
    ↓ (在 Wikipedia 等大規模數據訓練)
微調 (Fine-Tuning)
    ↓ 適應特定任務/領域
    ↓ (用少量標註數據)
任務專用模型
    ↓ 高精度、低成本
```

---

### 1.2 Trainer API 核心概念

**Trainer** 是 Hugging Face 提供的高級訓練接口,自動處理:
- ✅ 訓練循環
- ✅ 梯度計算與更新
- ✅ 驗證與評估
- ✅ 檢查點保存
- ✅ TensorBoard 日誌

#### 基本流程

```python
from transformers import Trainer, TrainingArguments

# 1. 定義訓練參數
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=16,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True
)

# 2. 初始化 Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    compute_metrics=compute_metrics
)

# 3. 開始訓練
trainer.train()

# 4. 評估模型
metrics = trainer.evaluate()

# 5. 保存模型
trainer.save_model("./my_finetuned_model")
```

---

### 1.3 完整微調範例: AG News 新聞分類

#### Step 1: 數據準備

```python
from datasets import load_dataset

# 加載數據集
dataset = load_dataset("ag_news")

# 查看數據結構
print(dataset)
# DatasetDict({
#     train: Dataset({
#         features: ['text', 'label'],
#         num_rows: 120000
#     })
#     test: Dataset({
#         features: ['text', 'label'],
#         num_rows: 7600
#     })
# })

# 類別對應
label_names = ['World', 'Sports', 'Business', 'Sci/Tech']
id2label = {i: name for i, name in enumerate(label_names)}
label2id = {name: i for i, name in enumerate(label_names)}
```

#### Step 2: Tokenization

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")

def tokenize_function(examples):
    return tokenizer(
        examples['text'],
        padding='max_length',
        truncation=True,
        max_length=128
    )

# 應用 tokenization
tokenized_datasets = dataset.map(
    tokenize_function,
    batched=True,
    remove_columns=['text']
)
```

#### Step 3: 模型初始化

```python
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(
    "distilbert-base-uncased",
    num_labels=4,
    id2label=id2label,
    label2id=label2id
)
```

#### Step 4: 評估指標

```python
import evaluate
import numpy as np

accuracy_metric = evaluate.load("accuracy")
f1_metric = evaluate.load("f1")

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)

    accuracy = accuracy_metric.compute(
        predictions=predictions,
        references=labels
    )
    f1 = f1_metric.compute(
        predictions=predictions,
        references=labels,
        average='weighted'
    )

    return {
        'accuracy': accuracy['accuracy'],
        'f1': f1['f1']
    }
```

#### Step 5: 訓練配置

```python
training_args = TrainingArguments(
    output_dir="./ag_news_model",

    # 訓練超參數
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=32,
    learning_rate=2e-5,
    weight_decay=0.01,

    # 評估與保存
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="f1",

    # 日誌
    logging_dir="./logs",
    logging_steps=100,

    # 其他
    seed=42,
)
```

#### Step 6: 訓練

```python
from transformers import Trainer, EarlyStoppingCallback

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets['train'],
    eval_dataset=tokenized_datasets['test'],
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]
)

# 開始訓練
train_result = trainer.train()

# 評估
eval_results = trainer.evaluate()

print(f"Accuracy: {eval_results['eval_accuracy']:.4f}")
print(f"F1-score: {eval_results['eval_f1']:.4f}")
```

---

### 1.4 超參數調優

#### 關鍵超參數說明

| 參數 | 說明 | 推薦範圍 | 影響 |
|------|------|----------|------|
| `learning_rate` | 學習率 | 2e-5 ~ 5e-5 | 太高: 不收斂<br>太低: 訓練慢 |
| `num_train_epochs` | 訓練輪數 | 2-5 | 太多: 過擬合<br>太少: 欠擬合 |
| `per_device_train_batch_size` | 批次大小 | 8-32 | 大: 訓練快但記憶體高<br>小: 穩定但慢 |
| `weight_decay` | 權重衰減 (L2 正則化) | 0.01 | 防止過擬合 |
| `warmup_steps` | 學習率預熱步數 | 500 | 穩定訓練初期 |

#### 自動超參數搜索

```python
def model_init():
    return AutoModelForSequenceClassification.from_pretrained(
        "distilbert-base-uncased",
        num_labels=4
    )

trainer = Trainer(
    model_init=model_init,
    args=training_args,
    train_dataset=tokenized_datasets['train'],
    eval_dataset=tokenized_datasets['test'],
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

# 定義搜索空間
def hp_space(trial):
    return {
        "learning_rate": trial.suggest_float("learning_rate", 1e-5, 5e-5, log=True),
        "num_train_epochs": trial.suggest_int("num_train_epochs", 2, 5),
        "per_device_train_batch_size": trial.suggest_categorical(
            "per_device_train_batch_size", [8, 16, 32]
        ),
    }

# 執行超參數搜索 (需安裝 optuna)
best_run = trainer.hyperparameter_search(
    direction="maximize",
    backend="optuna",
    hp_space=hp_space,
    n_trials=10
)

print(f"Best hyperparameters: {best_run.hyperparameters}")
```

---

## Part 2: 實戰專案 - 客戶意見分析儀

### 2.1 專案需求分析

#### 業務目標

建立自動化客戶意見分析系統,實現:

1. **情感分析**: 判斷評論正負面
2. **主題識別**: 分類問題類型
3. **關鍵字提取**: 找出高頻議題
4. **可視化儀表板**: 管理層決策支持
5. **實時監控**: 異常預警

#### 商業價值

- 💰 **成本節省**: 減少 90% 人工審閱時間
- ⚡ **即時反饋**: 快速發現產品問題
- 📊 **數據驅動**: 客觀量化滿意度
- 🎯 **精準改進**: 定位改善重點

---

### 2.2 系統架構設計

```
客戶評論 (Input)
    ↓
數據預處理 Pipeline
    ├── 去除噪音 (HTML, emoji)
    ├── 文本清理 (標點、空格)
    └── Tokenization
    ↓
NLP 分析引擎
    ├── 情感分析 (Sentiment)
    ├── 主題分類 (Topic)
    └── 關鍵字提取 (Keywords)
    ↓
數據聚合層
    ├── 統計計算
    ├── 趨勢分析
    └── 異常檢測
    ↓
可視化輸出
    ├── 情感分布圖
    ├── 主題熱點圖
    └── 詞雲 (Word Cloud)
```

---

### 2.3 核心代碼實作

#### 情感分析模組

```python
from transformers import pipeline

class SentimentAnalyzer:
    def __init__(self, model_name="distilbert-base-uncased-finetuned-sst-2-english"):
        self.classifier = pipeline(
            "sentiment-analysis",
            model=model_name,
            device=-1  # CPU
        )

    def analyze(self, text):
        """分析單個文本"""
        result = self.classifier(text)[0]
        return {
            'sentiment': result['label'],
            'confidence': round(result['score'], 4)
        }

    def analyze_batch(self, texts, batch_size=32):
        """批量分析"""
        results = []
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i+batch_size]
            batch_results = self.classifier(batch)
            results.extend(batch_results)
        return results

    def get_summary(self, results):
        """生成摘要統計"""
        total = len(results)
        positive = sum(1 for r in results if r['label'] == 'POSITIVE')
        negative = total - positive

        return {
            'total_reviews': total,
            'positive_count': positive,
            'negative_count': negative,
            'positive_percentage': round(positive / total * 100, 2),
            'negative_percentage': round(negative / total * 100, 2)
        }
```

使用範例:

```python
# 初始化分析器
analyzer = SentimentAnalyzer()

# 單個分析
result = analyzer.analyze("This product is amazing!")
print(result)
# {'sentiment': 'POSITIVE', 'confidence': 0.9998}

# 批量分析
reviews = [
    "Great quality!",
    "Terrible experience.",
    "It's okay."
]
results = analyzer.analyze_batch(reviews)

# 摘要統計
summary = analyzer.get_summary(results)
print(summary)
# {'total_reviews': 3,
#  'positive_count': 2,
#  'negative_count': 1,
#  'positive_percentage': 66.67,
#  'negative_percentage': 33.33}
```

---

#### 關鍵字提取模組

```python
import re
from collections import Counter
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

class KeywordExtractor:
    def __init__(self):
        self.stop_words = set(stopwords.words('english'))

    def extract(self, text, top_n=10):
        """提取關鍵字"""
        # 清理文本
        text = re.sub(r'[^a-zA-Z\s]', '', text.lower())

        # Tokenization
        words = word_tokenize(text)

        # 過濾停用詞和短詞
        keywords = [
            w for w in words
            if w not in self.stop_words and len(w) > 3
        ]

        # 統計頻率
        word_freq = Counter(keywords)

        return word_freq.most_common(top_n)

    def extract_by_sentiment(self, reviews, sentiments):
        """按情感分組提取關鍵字"""
        positive_text = ' '.join([
            r for r, s in zip(reviews, sentiments)
            if s['label'] == 'POSITIVE'
        ])

        negative_text = ' '.join([
            r for r, s in zip(reviews, sentiments)
            if s['label'] == 'NEGATIVE'
        ])

        return {
            'positive': self.extract(positive_text),
            'negative': self.extract(negative_text)
        }
```

---

#### 可視化模組

```python
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud

class Visualizer:
    @staticmethod
    def plot_sentiment_distribution(summary):
        """情感分布餅圖"""
        labels = ['Positive', 'Negative']
        sizes = [
            summary['positive_count'],
            summary['negative_count']
        ]
        colors = ['#2ecc71', '#e74c3c']

        plt.figure(figsize=(8, 6))
        plt.pie(sizes, labels=labels, colors=colors,
                autopct='%1.1f%%', startangle=90)
        plt.title('Sentiment Distribution', fontsize=14, fontweight='bold')
        plt.show()

    @staticmethod
    def plot_keyword_wordcloud(text, title="Word Cloud"):
        """詞雲"""
        wordcloud = WordCloud(
            width=800,
            height=400,
            background_color='white',
            colormap='viridis'
        ).generate(text)

        plt.figure(figsize=(12, 6))
        plt.imshow(wordcloud, interpolation='bilinear')
        plt.title(title, fontsize=14, fontweight='bold')
        plt.axis('off')
        plt.show()

    @staticmethod
    def plot_trend(df, date_col='date', sentiment_col='sentiment'):
        """趨勢圖"""
        # 按日期聚合
        daily_sentiment = df.groupby([date_col, sentiment_col]).size().unstack(fill_value=0)

        plt.figure(figsize=(14, 6))
        plt.plot(daily_sentiment.index, daily_sentiment['POSITIVE'],
                marker='o', label='Positive', linewidth=2, color='green')
        plt.plot(daily_sentiment.index, daily_sentiment['NEGATIVE'],
                marker='o', label='Negative', linewidth=2, color='red')

        plt.title('Sentiment Trend Over Time', fontsize=14, fontweight='bold')
        plt.xlabel('Date')
        plt.ylabel('Count')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.show()
```

---

### 2.4 完整系統整合

```python
class CustomerFeedbackAnalyzer:
    """
    完整的客戶意見分析系統
    """
    def __init__(self):
        self.sentiment_analyzer = SentimentAnalyzer()
        self.keyword_extractor = KeywordExtractor()
        self.visualizer = Visualizer()

    def analyze_reviews(self, reviews):
        """
        分析評論並生成完整報告
        """
        # 1. 情感分析
        sentiments = self.sentiment_analyzer.analyze_batch(reviews)

        # 2. 關鍵字提取
        all_text = ' '.join(reviews)
        keywords = self.keyword_extractor.extract(all_text, top_n=20)
        keywords_by_sentiment = self.keyword_extractor.extract_by_sentiment(
            reviews, sentiments
        )

        # 3. 生成摘要
        summary = self.sentiment_analyzer.get_summary(sentiments)

        # 4. 可視化
        self.visualizer.plot_sentiment_distribution(summary)
        self.visualizer.plot_keyword_wordcloud(all_text)

        # 5. 生成報告
        report = {
            'summary': summary,
            'keywords': keywords,
            'positive_keywords': keywords_by_sentiment['positive'],
            'negative_keywords': keywords_by_sentiment['negative'],
            'detailed_results': [
                {'review': r, 'sentiment': s}
                for r, s in zip(reviews, sentiments)
            ]
        }

        return report

# 使用範例
analyzer = CustomerFeedbackAnalyzer()
reviews = load_customer_reviews()  # 假設函數
report = analyzer.analyze_reviews(reviews)

print(f"總評論數: {report['summary']['total_reviews']}")
print(f"正面比例: {report['summary']['positive_percentage']}%")
print(f"\n正面關鍵字:")
for word, freq in report['positive_keywords'][:5]:
    print(f"  - {word}: {freq}")
```

---

## 3. 模型部署準備

### 3.1 保存與加載

```python
# 保存完整模型
model_save_path = "./customer_sentiment_model"
trainer.save_model(model_save_path)
tokenizer.save_pretrained(model_save_path)

# 加載模型
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(model_save_path)
tokenizer = AutoTokenizer.from_pretrained(model_save_path)

# 使用
from transformers import pipeline
classifier = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)
```

---

### 3.2 FastAPI 部署範例

```python
from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI(title="Sentiment Analysis API")

# 加載模型
classifier = pipeline("sentiment-analysis", model="./model")

class TextInput(BaseModel):
    text: str

@app.post("/analyze")
def analyze_sentiment(input_data: TextInput):
    result = classifier(input_data.text)[0]
    return {
        "sentiment": result["label"],
        "confidence": round(result["score"], 4)
    }

# 運行: uvicorn app:app --reload
```

---

## 4. 總結與最佳實踐

### ✅ 微調最佳實踐

1. **數據準備**
   - 至少 1000+ 標註樣本
   - 類別平衡
   - 高質量標註

2. **超參數設置**
   - Learning rate: 2e-5 ~ 5e-5
   - Batch size: 16-32
   - Epochs: 3-5

3. **防止過擬合**
   - Early stopping
   - Weight decay
   - Dropout

4. **評估策略**
   - 使用驗證集
   - 多個指標 (Accuracy, F1, Precision, Recall)
   - 混淆矩陣分析

---

**講義版本**: v1.0
**最後更新**: 2025-10-17
**作者**: iSpan NLP Team / Claude AI
