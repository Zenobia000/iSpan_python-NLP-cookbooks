# CH08 è¬›ç¾© 03: æ¨¡å‹å¾®èª¿èˆ‡å¯¦æˆ°å°ˆæ¡ˆéƒ¨ç½²

---

**èª²ç¨‹**: iSpan Python NLP é€Ÿæˆæ•™æ¡ˆ
**ç« ç¯€**: CH08 Hugging Face å‡½å¼åº«å¯¦æˆ°
**è¬›ç¾©ç·¨è™Ÿ**: 03/04
**é è¨ˆæ™‚é–“**: 150 åˆ†é˜
**å°æ‡‰ Notebooks**: `08_æ¨¡å‹å¾®èª¿_Fine_Tuning.ipynb`, `09_å°ˆæ¡ˆå¯¦æˆ°_å®¢æˆ¶æ„è¦‹åˆ†æå„€.ipynb`

---

## ğŸ“š å­¸ç¿’ç›®æ¨™

å®Œæˆæœ¬è¬›ç¾©å¾Œ,ä½ å°‡èƒ½å¤ :

1. âœ… ç†è§£é·ç§»å­¸ç¿’èˆ‡æ¨¡å‹å¾®èª¿çš„æ ¸å¿ƒæ¦‚å¿µ
2. âœ… æŒæ¡ Trainer API å®Œæ•´ä½¿ç”¨æµç¨‹
3. âœ… å¯¦ä½œç«¯åˆ°ç«¯çš„å•†æ¥­æ‡‰ç”¨å°ˆæ¡ˆ
4. âœ… å»ºç«‹ç”Ÿç”¢ç´šçš„ NLP åˆ†æç³»çµ±
5. âœ… æŒæ¡è¶…åƒæ•¸èª¿å„ªæŠ€å·§

---

## Part 1: æ¨¡å‹å¾®èª¿ (Fine-Tuning)

### 1.1 ç‚ºä»€éº¼éœ€è¦å¾®èª¿?

#### ä¸‰ç¨®è¨“ç·´ç­–ç•¥å°æ¯”

| ç­–ç•¥ | æ•¸æ“šéœ€æ±‚ | è¨“ç·´æ™‚é–“ | ç²¾åº¦ | é©ç”¨å ´æ™¯ |
|------|---------|---------|------|----------|
| **å¾é ­è¨“ç·´** | ç™¾è¬ç´š | æ•¸é€± (GPU é›†ç¾¤) | å¯è‡ªå®šç¾© | ç‰¹æ®Šé ˜åŸŸã€å……è¶³è³‡æº |
| **é è¨“ç·´ç›´æ¥ç”¨** | ç„¡ | 0 | ä¸­ç­‰ | é€šç”¨ä»»å‹™ã€å¿«é€ŸåŸå‹ |
| **å¾®èª¿** âœ… | åƒ-è¬ç´š | å°æ™‚-å¤© | é«˜ | **å¤§å¤šæ•¸å¯¦éš›å ´æ™¯** |

#### å¾®èª¿çš„æ ¸å¿ƒå„ªå‹¢

```
é è¨“ç·´æ¨¡å‹ (BERT)
    â†“ å·²å­¸æœƒèªè¨€é€šç”¨çŸ¥è­˜
    â†“ (åœ¨ Wikipedia ç­‰å¤§è¦æ¨¡æ•¸æ“šè¨“ç·´)
å¾®èª¿ (Fine-Tuning)
    â†“ é©æ‡‰ç‰¹å®šä»»å‹™/é ˜åŸŸ
    â†“ (ç”¨å°‘é‡æ¨™è¨»æ•¸æ“š)
ä»»å‹™å°ˆç”¨æ¨¡å‹
    â†“ é«˜ç²¾åº¦ã€ä½æˆæœ¬
```

---

### 1.2 Trainer API æ ¸å¿ƒæ¦‚å¿µ

**Trainer** æ˜¯ Hugging Face æä¾›çš„é«˜ç´šè¨“ç·´æ¥å£,è‡ªå‹•è™•ç†:
- âœ… è¨“ç·´å¾ªç’°
- âœ… æ¢¯åº¦è¨ˆç®—èˆ‡æ›´æ–°
- âœ… é©—è­‰èˆ‡è©•ä¼°
- âœ… æª¢æŸ¥é»ä¿å­˜
- âœ… TensorBoard æ—¥èªŒ

#### åŸºæœ¬æµç¨‹

```python
from transformers import Trainer, TrainingArguments

# 1. å®šç¾©è¨“ç·´åƒæ•¸
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=16,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True
)

# 2. åˆå§‹åŒ– Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    compute_metrics=compute_metrics
)

# 3. é–‹å§‹è¨“ç·´
trainer.train()

# 4. è©•ä¼°æ¨¡å‹
metrics = trainer.evaluate()

# 5. ä¿å­˜æ¨¡å‹
trainer.save_model("./my_finetuned_model")
```

---

### 1.3 å®Œæ•´å¾®èª¿ç¯„ä¾‹: AG News æ–°èåˆ†é¡

#### Step 1: æ•¸æ“šæº–å‚™

```python
from datasets import load_dataset

# åŠ è¼‰æ•¸æ“šé›†
dataset = load_dataset("ag_news")

# æŸ¥çœ‹æ•¸æ“šçµæ§‹
print(dataset)
# DatasetDict({
#     train: Dataset({
#         features: ['text', 'label'],
#         num_rows: 120000
#     })
#     test: Dataset({
#         features: ['text', 'label'],
#         num_rows: 7600
#     })
# })

# é¡åˆ¥å°æ‡‰
label_names = ['World', 'Sports', 'Business', 'Sci/Tech']
id2label = {i: name for i, name in enumerate(label_names)}
label2id = {name: i for i, name in enumerate(label_names)}
```

#### Step 2: Tokenization

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")

def tokenize_function(examples):
    return tokenizer(
        examples['text'],
        padding='max_length',
        truncation=True,
        max_length=128
    )

# æ‡‰ç”¨ tokenization
tokenized_datasets = dataset.map(
    tokenize_function,
    batched=True,
    remove_columns=['text']
)
```

#### Step 3: æ¨¡å‹åˆå§‹åŒ–

```python
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(
    "distilbert-base-uncased",
    num_labels=4,
    id2label=id2label,
    label2id=label2id
)
```

#### Step 4: è©•ä¼°æŒ‡æ¨™

```python
import evaluate
import numpy as np

accuracy_metric = evaluate.load("accuracy")
f1_metric = evaluate.load("f1")

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)

    accuracy = accuracy_metric.compute(
        predictions=predictions,
        references=labels
    )
    f1 = f1_metric.compute(
        predictions=predictions,
        references=labels,
        average='weighted'
    )

    return {
        'accuracy': accuracy['accuracy'],
        'f1': f1['f1']
    }
```

#### Step 5: è¨“ç·´é…ç½®

```python
training_args = TrainingArguments(
    output_dir="./ag_news_model",

    # è¨“ç·´è¶…åƒæ•¸
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=32,
    learning_rate=2e-5,
    weight_decay=0.01,

    # è©•ä¼°èˆ‡ä¿å­˜
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="f1",

    # æ—¥èªŒ
    logging_dir="./logs",
    logging_steps=100,

    # å…¶ä»–
    seed=42,
)
```

#### Step 6: è¨“ç·´

```python
from transformers import Trainer, EarlyStoppingCallback

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets['train'],
    eval_dataset=tokenized_datasets['test'],
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]
)

# é–‹å§‹è¨“ç·´
train_result = trainer.train()

# è©•ä¼°
eval_results = trainer.evaluate()

print(f"Accuracy: {eval_results['eval_accuracy']:.4f}")
print(f"F1-score: {eval_results['eval_f1']:.4f}")
```

---

### 1.4 è¶…åƒæ•¸èª¿å„ª

#### é—œéµè¶…åƒæ•¸èªªæ˜

| åƒæ•¸ | èªªæ˜ | æ¨è–¦ç¯„åœ | å½±éŸ¿ |
|------|------|----------|------|
| `learning_rate` | å­¸ç¿’ç‡ | 2e-5 ~ 5e-5 | å¤ªé«˜: ä¸æ”¶æ–‚<br>å¤ªä½: è¨“ç·´æ…¢ |
| `num_train_epochs` | è¨“ç·´è¼ªæ•¸ | 2-5 | å¤ªå¤š: éæ“¬åˆ<br>å¤ªå°‘: æ¬ æ“¬åˆ |
| `per_device_train_batch_size` | æ‰¹æ¬¡å¤§å° | 8-32 | å¤§: è¨“ç·´å¿«ä½†è¨˜æ†¶é«”é«˜<br>å°: ç©©å®šä½†æ…¢ |
| `weight_decay` | æ¬Šé‡è¡°æ¸› (L2 æ­£å‰‡åŒ–) | 0.01 | é˜²æ­¢éæ“¬åˆ |
| `warmup_steps` | å­¸ç¿’ç‡é ç†±æ­¥æ•¸ | 500 | ç©©å®šè¨“ç·´åˆæœŸ |

#### è‡ªå‹•è¶…åƒæ•¸æœç´¢

```python
def model_init():
    return AutoModelForSequenceClassification.from_pretrained(
        "distilbert-base-uncased",
        num_labels=4
    )

trainer = Trainer(
    model_init=model_init,
    args=training_args,
    train_dataset=tokenized_datasets['train'],
    eval_dataset=tokenized_datasets['test'],
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

# å®šç¾©æœç´¢ç©ºé–“
def hp_space(trial):
    return {
        "learning_rate": trial.suggest_float("learning_rate", 1e-5, 5e-5, log=True),
        "num_train_epochs": trial.suggest_int("num_train_epochs", 2, 5),
        "per_device_train_batch_size": trial.suggest_categorical(
            "per_device_train_batch_size", [8, 16, 32]
        ),
    }

# åŸ·è¡Œè¶…åƒæ•¸æœç´¢ (éœ€å®‰è£ optuna)
best_run = trainer.hyperparameter_search(
    direction="maximize",
    backend="optuna",
    hp_space=hp_space,
    n_trials=10
)

print(f"Best hyperparameters: {best_run.hyperparameters}")
```

---

## Part 2: å¯¦æˆ°å°ˆæ¡ˆ - å®¢æˆ¶æ„è¦‹åˆ†æå„€

### 2.1 å°ˆæ¡ˆéœ€æ±‚åˆ†æ

#### æ¥­å‹™ç›®æ¨™

å»ºç«‹è‡ªå‹•åŒ–å®¢æˆ¶æ„è¦‹åˆ†æç³»çµ±,å¯¦ç¾:

1. **æƒ…æ„Ÿåˆ†æ**: åˆ¤æ–·è©•è«–æ­£è² é¢
2. **ä¸»é¡Œè­˜åˆ¥**: åˆ†é¡å•é¡Œé¡å‹
3. **é—œéµå­—æå–**: æ‰¾å‡ºé«˜é »è­°é¡Œ
4. **å¯è¦–åŒ–å„€è¡¨æ¿**: ç®¡ç†å±¤æ±ºç­–æ”¯æŒ
5. **å¯¦æ™‚ç›£æ§**: ç•°å¸¸é è­¦

#### å•†æ¥­åƒ¹å€¼

- ğŸ’° **æˆæœ¬ç¯€çœ**: æ¸›å°‘ 90% äººå·¥å¯©é–±æ™‚é–“
- âš¡ **å³æ™‚åé¥‹**: å¿«é€Ÿç™¼ç¾ç”¢å“å•é¡Œ
- ğŸ“Š **æ•¸æ“šé©…å‹•**: å®¢è§€é‡åŒ–æ»¿æ„åº¦
- ğŸ¯ **ç²¾æº–æ”¹é€²**: å®šä½æ”¹å–„é‡é»

---

### 2.2 ç³»çµ±æ¶æ§‹è¨­è¨ˆ

```
å®¢æˆ¶è©•è«– (Input)
    â†“
æ•¸æ“šé è™•ç† Pipeline
    â”œâ”€â”€ å»é™¤å™ªéŸ³ (HTML, emoji)
    â”œâ”€â”€ æ–‡æœ¬æ¸…ç† (æ¨™é»ã€ç©ºæ ¼)
    â””â”€â”€ Tokenization
    â†“
NLP åˆ†æå¼•æ“
    â”œâ”€â”€ æƒ…æ„Ÿåˆ†æ (Sentiment)
    â”œâ”€â”€ ä¸»é¡Œåˆ†é¡ (Topic)
    â””â”€â”€ é—œéµå­—æå– (Keywords)
    â†“
æ•¸æ“šèšåˆå±¤
    â”œâ”€â”€ çµ±è¨ˆè¨ˆç®—
    â”œâ”€â”€ è¶¨å‹¢åˆ†æ
    â””â”€â”€ ç•°å¸¸æª¢æ¸¬
    â†“
å¯è¦–åŒ–è¼¸å‡º
    â”œâ”€â”€ æƒ…æ„Ÿåˆ†å¸ƒåœ–
    â”œâ”€â”€ ä¸»é¡Œç†±é»åœ–
    â””â”€â”€ è©é›² (Word Cloud)
```

---

### 2.3 æ ¸å¿ƒä»£ç¢¼å¯¦ä½œ

#### æƒ…æ„Ÿåˆ†ææ¨¡çµ„

```python
from transformers import pipeline

class SentimentAnalyzer:
    def __init__(self, model_name="distilbert-base-uncased-finetuned-sst-2-english"):
        self.classifier = pipeline(
            "sentiment-analysis",
            model=model_name,
            device=-1  # CPU
        )

    def analyze(self, text):
        """åˆ†æå–®å€‹æ–‡æœ¬"""
        result = self.classifier(text)[0]
        return {
            'sentiment': result['label'],
            'confidence': round(result['score'], 4)
        }

    def analyze_batch(self, texts, batch_size=32):
        """æ‰¹é‡åˆ†æ"""
        results = []
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i+batch_size]
            batch_results = self.classifier(batch)
            results.extend(batch_results)
        return results

    def get_summary(self, results):
        """ç”Ÿæˆæ‘˜è¦çµ±è¨ˆ"""
        total = len(results)
        positive = sum(1 for r in results if r['label'] == 'POSITIVE')
        negative = total - positive

        return {
            'total_reviews': total,
            'positive_count': positive,
            'negative_count': negative,
            'positive_percentage': round(positive / total * 100, 2),
            'negative_percentage': round(negative / total * 100, 2)
        }
```

ä½¿ç”¨ç¯„ä¾‹:

```python
# åˆå§‹åŒ–åˆ†æå™¨
analyzer = SentimentAnalyzer()

# å–®å€‹åˆ†æ
result = analyzer.analyze("This product is amazing!")
print(result)
# {'sentiment': 'POSITIVE', 'confidence': 0.9998}

# æ‰¹é‡åˆ†æ
reviews = [
    "Great quality!",
    "Terrible experience.",
    "It's okay."
]
results = analyzer.analyze_batch(reviews)

# æ‘˜è¦çµ±è¨ˆ
summary = analyzer.get_summary(results)
print(summary)
# {'total_reviews': 3,
#  'positive_count': 2,
#  'negative_count': 1,
#  'positive_percentage': 66.67,
#  'negative_percentage': 33.33}
```

---

#### é—œéµå­—æå–æ¨¡çµ„

```python
import re
from collections import Counter
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

class KeywordExtractor:
    def __init__(self):
        self.stop_words = set(stopwords.words('english'))

    def extract(self, text, top_n=10):
        """æå–é—œéµå­—"""
        # æ¸…ç†æ–‡æœ¬
        text = re.sub(r'[^a-zA-Z\s]', '', text.lower())

        # Tokenization
        words = word_tokenize(text)

        # éæ¿¾åœç”¨è©å’ŒçŸ­è©
        keywords = [
            w for w in words
            if w not in self.stop_words and len(w) > 3
        ]

        # çµ±è¨ˆé »ç‡
        word_freq = Counter(keywords)

        return word_freq.most_common(top_n)

    def extract_by_sentiment(self, reviews, sentiments):
        """æŒ‰æƒ…æ„Ÿåˆ†çµ„æå–é—œéµå­—"""
        positive_text = ' '.join([
            r for r, s in zip(reviews, sentiments)
            if s['label'] == 'POSITIVE'
        ])

        negative_text = ' '.join([
            r for r, s in zip(reviews, sentiments)
            if s['label'] == 'NEGATIVE'
        ])

        return {
            'positive': self.extract(positive_text),
            'negative': self.extract(negative_text)
        }
```

---

#### å¯è¦–åŒ–æ¨¡çµ„

```python
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud

class Visualizer:
    @staticmethod
    def plot_sentiment_distribution(summary):
        """æƒ…æ„Ÿåˆ†å¸ƒé¤…åœ–"""
        labels = ['Positive', 'Negative']
        sizes = [
            summary['positive_count'],
            summary['negative_count']
        ]
        colors = ['#2ecc71', '#e74c3c']

        plt.figure(figsize=(8, 6))
        plt.pie(sizes, labels=labels, colors=colors,
                autopct='%1.1f%%', startangle=90)
        plt.title('Sentiment Distribution', fontsize=14, fontweight='bold')
        plt.show()

    @staticmethod
    def plot_keyword_wordcloud(text, title="Word Cloud"):
        """è©é›²"""
        wordcloud = WordCloud(
            width=800,
            height=400,
            background_color='white',
            colormap='viridis'
        ).generate(text)

        plt.figure(figsize=(12, 6))
        plt.imshow(wordcloud, interpolation='bilinear')
        plt.title(title, fontsize=14, fontweight='bold')
        plt.axis('off')
        plt.show()

    @staticmethod
    def plot_trend(df, date_col='date', sentiment_col='sentiment'):
        """è¶¨å‹¢åœ–"""
        # æŒ‰æ—¥æœŸèšåˆ
        daily_sentiment = df.groupby([date_col, sentiment_col]).size().unstack(fill_value=0)

        plt.figure(figsize=(14, 6))
        plt.plot(daily_sentiment.index, daily_sentiment['POSITIVE'],
                marker='o', label='Positive', linewidth=2, color='green')
        plt.plot(daily_sentiment.index, daily_sentiment['NEGATIVE'],
                marker='o', label='Negative', linewidth=2, color='red')

        plt.title('Sentiment Trend Over Time', fontsize=14, fontweight='bold')
        plt.xlabel('Date')
        plt.ylabel('Count')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.show()
```

---

### 2.4 å®Œæ•´ç³»çµ±æ•´åˆ

```python
class CustomerFeedbackAnalyzer:
    """
    å®Œæ•´çš„å®¢æˆ¶æ„è¦‹åˆ†æç³»çµ±
    """
    def __init__(self):
        self.sentiment_analyzer = SentimentAnalyzer()
        self.keyword_extractor = KeywordExtractor()
        self.visualizer = Visualizer()

    def analyze_reviews(self, reviews):
        """
        åˆ†æè©•è«–ä¸¦ç”Ÿæˆå®Œæ•´å ±å‘Š
        """
        # 1. æƒ…æ„Ÿåˆ†æ
        sentiments = self.sentiment_analyzer.analyze_batch(reviews)

        # 2. é—œéµå­—æå–
        all_text = ' '.join(reviews)
        keywords = self.keyword_extractor.extract(all_text, top_n=20)
        keywords_by_sentiment = self.keyword_extractor.extract_by_sentiment(
            reviews, sentiments
        )

        # 3. ç”Ÿæˆæ‘˜è¦
        summary = self.sentiment_analyzer.get_summary(sentiments)

        # 4. å¯è¦–åŒ–
        self.visualizer.plot_sentiment_distribution(summary)
        self.visualizer.plot_keyword_wordcloud(all_text)

        # 5. ç”Ÿæˆå ±å‘Š
        report = {
            'summary': summary,
            'keywords': keywords,
            'positive_keywords': keywords_by_sentiment['positive'],
            'negative_keywords': keywords_by_sentiment['negative'],
            'detailed_results': [
                {'review': r, 'sentiment': s}
                for r, s in zip(reviews, sentiments)
            ]
        }

        return report

# ä½¿ç”¨ç¯„ä¾‹
analyzer = CustomerFeedbackAnalyzer()
reviews = load_customer_reviews()  # å‡è¨­å‡½æ•¸
report = analyzer.analyze_reviews(reviews)

print(f"ç¸½è©•è«–æ•¸: {report['summary']['total_reviews']}")
print(f"æ­£é¢æ¯”ä¾‹: {report['summary']['positive_percentage']}%")
print(f"\næ­£é¢é—œéµå­—:")
for word, freq in report['positive_keywords'][:5]:
    print(f"  - {word}: {freq}")
```

---

## 3. æ¨¡å‹éƒ¨ç½²æº–å‚™

### 3.1 ä¿å­˜èˆ‡åŠ è¼‰

```python
# ä¿å­˜å®Œæ•´æ¨¡å‹
model_save_path = "./customer_sentiment_model"
trainer.save_model(model_save_path)
tokenizer.save_pretrained(model_save_path)

# åŠ è¼‰æ¨¡å‹
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(model_save_path)
tokenizer = AutoTokenizer.from_pretrained(model_save_path)

# ä½¿ç”¨
from transformers import pipeline
classifier = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)
```

---

### 3.2 FastAPI éƒ¨ç½²ç¯„ä¾‹

```python
from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI(title="Sentiment Analysis API")

# åŠ è¼‰æ¨¡å‹
classifier = pipeline("sentiment-analysis", model="./model")

class TextInput(BaseModel):
    text: str

@app.post("/analyze")
def analyze_sentiment(input_data: TextInput):
    result = classifier(input_data.text)[0]
    return {
        "sentiment": result["label"],
        "confidence": round(result["score"], 4)
    }

# é‹è¡Œ: uvicorn app:app --reload
```

---

## 4. ç¸½çµèˆ‡æœ€ä½³å¯¦è¸

### âœ… å¾®èª¿æœ€ä½³å¯¦è¸

1. **æ•¸æ“šæº–å‚™**
   - è‡³å°‘ 1000+ æ¨™è¨»æ¨£æœ¬
   - é¡åˆ¥å¹³è¡¡
   - é«˜è³ªé‡æ¨™è¨»

2. **è¶…åƒæ•¸è¨­ç½®**
   - Learning rate: 2e-5 ~ 5e-5
   - Batch size: 16-32
   - Epochs: 3-5

3. **é˜²æ­¢éæ“¬åˆ**
   - Early stopping
   - Weight decay
   - Dropout

4. **è©•ä¼°ç­–ç•¥**
   - ä½¿ç”¨é©—è­‰é›†
   - å¤šå€‹æŒ‡æ¨™ (Accuracy, F1, Precision, Recall)
   - æ··æ·†çŸ©é™£åˆ†æ

---

**è¬›ç¾©ç‰ˆæœ¬**: v1.0
**æœ€å¾Œæ›´æ–°**: 2025-10-17
**ä½œè€…**: iSpan NLP Team / Claude AI
