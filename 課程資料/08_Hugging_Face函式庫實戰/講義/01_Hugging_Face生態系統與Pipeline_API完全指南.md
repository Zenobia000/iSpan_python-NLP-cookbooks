# Hugging Face ç”Ÿæ…‹ç³»çµ±èˆ‡ Pipeline API å®Œå…¨æŒ‡å—

**èª²ç¨‹**: iSpan Python NLP Cookbooks v2
**ç« ç¯€**: CH08 Hugging Face å¯¦æˆ°
**ç‰ˆæœ¬**: v1.0
**æœ€å¾Œæ›´æ–°**: 2025-10-17

---

## ğŸ“š å­¸ç¿’ç›®æ¨™

å®Œæˆæœ¬è¬›ç¾©å¾Œ,æ‚¨å°‡èƒ½å¤ :

1. ç†è§£ Hugging Face ç”Ÿæ…‹ç³»çµ±çš„æ ¸å¿ƒåƒ¹å€¼èˆ‡æ¶æ§‹
2. æŒæ¡ Transformersã€Datasetsã€Hub ä¸‰å¤§çµ„ä»¶çš„ä½¿ç”¨
3. ç†Ÿç·´ä½¿ç”¨ Pipeline API å¿«é€Ÿå¯¦ç¾ NLP ä»»å‹™
4. ç†è§£æ¨¡å‹å¡ç‰‡ (Model Card) ä¸¦æ­£ç¢ºé¸æ“‡é è¨“ç·´æ¨¡å‹
5. æŒæ¡è‡ªè¨‚ Pipeline åƒæ•¸èˆ‡æ‰¹æ¬¡è™•ç†æŠ€å·§

---

## 1. Hugging Face ç”Ÿæ…‹ç³»çµ±æ¦‚è¦½

### 1.1 ç‚ºä»€éº¼é¸æ“‡ Hugging Face?

åœ¨ç¾ä»£ NLP é–‹ç™¼ä¸­,Hugging Face å·²æˆç‚ºäº‹å¯¦ä¸Šçš„æ¨™æº–å¹³å°ã€‚å…¶æ ¸å¿ƒå„ªå‹¢åŒ…æ‹¬:

#### **çµ±ä¸€API,é™ä½å­¸ç¿’æˆæœ¬**
```python
# ç„¡è«–ä½¿ç”¨ BERTã€GPTã€T5,API å®Œå…¨ä¸€è‡´
from transformers import AutoModel

bert = AutoModel.from_pretrained("bert-base-uncased")
gpt2 = AutoModel.from_pretrained("gpt2")
t5 = AutoModel.from_pretrained("t5-small")
```

#### **é–‹ç®±å³ç”¨,ç„¡éœ€å¾é›¶è¨“ç·´**
- è¶…é **500,000** å€‹é è¨“ç·´æ¨¡å‹
- æ¶µè“‹ **8,000+** ç¨®èªè¨€èˆ‡æ–¹è¨€
- æ”¯æ´ **100+** ç¨® NLP ä»»å‹™

#### **æ´»èºç¤¾ç¾¤,æŒçºŒæ›´æ–°**
- æ¯å¤©æ–°å¢æ•¸ç™¾å€‹æ¨¡å‹
- å³æ™‚æ•´åˆæœ€æ–°ç ”ç©¶æˆæœ (GPT-4, LLaMA, Mistral...)
- å®Œæ•´çš„æ–‡æª”èˆ‡æ•™å­¸è³‡æº

### 1.2 ç”Ÿæ…‹ç³»çµ±æ ¸å¿ƒçµ„ä»¶

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Hugging Face ç”Ÿæ…‹ç³»çµ±                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Hub (å¹³å°)      â”‚  â”‚ Transformers (API) â”‚   â”‚
â”‚  â”‚  - æ¨¡å‹å…±äº«      â”‚  â”‚ - çµ±ä¸€æ¥å£         â”‚   â”‚
â”‚  â”‚  - æ•¸æ“šé›†å­˜å„²    â”‚  â”‚ - é è¨“ç·´æ¨¡å‹       â”‚   â”‚
â”‚  â”‚  - å”ä½œé–‹ç™¼      â”‚  â”‚ - å¾®èª¿å·¥å…·         â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚           â†“                      â†“               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Datasets         â”‚  â”‚ Tokenizers         â”‚   â”‚
â”‚  â”‚ - 50,000+ æ•¸æ“šé›† â”‚  â”‚ - é«˜æ•ˆåˆ†è©         â”‚   â”‚
â”‚  â”‚ - çµ±ä¸€åŠ è¼‰æ¥å£   â”‚  â”‚ - Rust æ ¸å¿ƒ        â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚           â†“                      â†“               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Accelerate       â”‚  â”‚ Evaluate           â”‚   â”‚
â”‚  â”‚ - åˆ†æ•£å¼è¨“ç·´     â”‚  â”‚ - æ¨¡å‹è©•ä¼°         â”‚   â”‚
â”‚  â”‚ - æ··åˆç²¾åº¦       â”‚  â”‚ - åŸºæº–æ¸¬è©¦         â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### **1. Transformers (æ¨¡å‹API)**
- ğŸ¯ æ ¸å¿ƒåŠŸèƒ½: æä¾›çµ±ä¸€çš„æ¨¡å‹åŠ è¼‰ã€è¨“ç·´ã€æ¨ç†æ¥å£
- ğŸ“¦ æ”¯æ´æ¡†æ¶: PyTorch, TensorFlow, JAX
- ğŸ”§ ä¸»è¦é¡åˆ¥:
  - `AutoModel`: è‡ªå‹•æª¢æ¸¬ä¸¦åŠ è¼‰æ¨¡å‹
  - `AutoTokenizer`: è‡ªå‹•åŠ è¼‰å°æ‡‰çš„åˆ†è©å™¨
  - `AutoConfig`: è‡ªå‹•åŠ è¼‰æ¨¡å‹é…ç½®
  - `Pipeline`: ç«¯åˆ°ç«¯ä»»å‹™å°è£

#### **2. Datasets (æ•¸æ“šé›†)**
- ğŸ—‚ï¸ æ ¸å¿ƒåŠŸèƒ½: çµ±ä¸€çš„æ•¸æ“šé›†åŠ è¼‰èˆ‡è™•ç†æ¥å£
- âš¡ æŠ€è¡“ç‰¹é»: Apache Arrow æ”¯æ’,è¨˜æ†¶é«”æ˜ å°„,é«˜æ•ˆè™•ç†å¤§æ•¸æ“š
- ğŸ“Š æ•¸æ“šä¾†æº:
  - å…§å»ºæ•¸æ“šé›†: IMDB, SQuAD, GLUE, SuperGLUE...
  - ç¤¾ç¾¤æ•¸æ“šé›†: è¶…é 50,000 å€‹å…¬é–‹æ•¸æ“šé›†
  - è‡ªè¨‚æ•¸æ“šé›†: è¼•é¬†ä¸Šå‚³ä¸¦åˆ†äº«

#### **3. Hub (å…±äº«å¹³å°)**
- ğŸ›ï¸ æ ¸å¿ƒåŠŸèƒ½: æ¨¡å‹ã€æ•¸æ“šé›†ã€Space (æ‡‰ç”¨) çš„å…±äº«èˆ‡å”ä½œ
- ğŸ”’ ç‰ˆæœ¬æ§åˆ¶: åŸºæ–¼ Git,æ”¯æ´å®Œæ•´çš„ç‰ˆæœ¬ç®¡ç†
- ğŸŒ è¨ªå•æ–¹å¼:
  - ç¶²é ç€è¦½: https://huggingface.co
  - Python API: `huggingface_hub` å‡½å¼åº«
  - CLI å·¥å…·: `huggingface-cli`

---

## 2. Transformers æ ¸å¿ƒæ¦‚å¿µ

### 2.1 ä¸‰å¤§æ ¸å¿ƒé¡åˆ¥: Model, Tokenizer, Config

#### **Model (æ¨¡å‹)**

æ¨¡å‹æ˜¯ Transformers çš„æ ¸å¿ƒ,åˆ†ç‚ºå…©é¡:

**1. åŸºç¤æ¨¡å‹ (Base Model)**
```python
from transformers import AutoModel

# è¼‰å…¥åŸºç¤æ¨¡å‹ (ç„¡ä»»å‹™é ­)
model = AutoModel.from_pretrained("bert-base-uncased")
# è¼¸å‡º: (batch_size, seq_length, hidden_size) çš„ç‰¹å¾µå‘é‡
```

**ç”¨é€”**:
- æå–æ–‡æœ¬ç‰¹å¾µ (Embeddings)
- ä½œç‚ºä¸‹æ¸¸ä»»å‹™çš„ç‰¹å¾µæå–å™¨
- å¾®èª¿åˆ°ç‰¹å®šä»»å‹™

**2. ä»»å‹™ç‰¹å®šæ¨¡å‹ (Task-Specific Model)**
```python
from transformers import AutoModelForSequenceClassification

# è¼‰å…¥åˆ†é¡æ¨¡å‹ (æœ‰åˆ†é¡é ­)
model = AutoModelForSequenceClassification.from_pretrained(
    "bert-base-uncased",
    num_labels=2  # äºŒåˆ†é¡
)
# è¼¸å‡º: (batch_size, num_labels) çš„ logits
```

**å¸¸ç”¨ä»»å‹™æ¨¡å‹é¡åˆ¥**:

| é¡åˆ¥åç¨± | ä»»å‹™ | æ‡‰ç”¨å ´æ™¯ |
|---------|------|----------|
| `AutoModelForSequenceClassification` | åºåˆ—åˆ†é¡ | æƒ…æ„Ÿåˆ†æã€æ–‡æœ¬åˆ†é¡ |
| `AutoModelForTokenClassification` | æ¨™è¨˜åˆ†é¡ | NERã€è©æ€§æ¨™è¨» |
| `AutoModelForQuestionAnswering` | å•ç­”ç³»çµ± | SQuAD, DRCD |
| `AutoModelForCausalLM` | å› æœèªè¨€æ¨¡å‹ | GPT ç³»åˆ—æ–‡æœ¬ç”Ÿæˆ |
| `AutoModelForSeq2SeqLM` | åºåˆ—åˆ°åºåˆ— | ç¿»è­¯ã€æ‘˜è¦ |
| `AutoModelForMaskedLM` | é®ç½©èªè¨€æ¨¡å‹ | BERT é è¨“ç·´ |
| `AutoModelForMultipleChoice` | å¤šé¸é¡Œ | é–±è®€ç†è§£é¸æ“‡é¡Œ |

#### **Tokenizer (åˆ†è©å™¨)**

Tokenizer è² è²¬å°‡æ–‡æœ¬è½‰æ›ç‚ºæ¨¡å‹å¯ç†è§£çš„æ•¸å­—åºåˆ—ã€‚

**æ ¸å¿ƒæµç¨‹**:
```
åŸå§‹æ–‡æœ¬ â†’ åˆ†è© â†’ è½‰æ›ç‚ºID â†’ æ·»åŠ ç‰¹æ®Šæ¨™è¨˜ â†’ å¡«å……/æˆªæ–· â†’ ç”Ÿæˆæ³¨æ„åŠ›é®ç½©
```

**å¯¦æˆ°ç¯„ä¾‹**:
```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

text = "Hugging Face is amazing!"

# å®Œæ•´ç·¨ç¢¼éç¨‹
encoded = tokenizer(
    text,
    return_tensors="pt",      # è¿”å› PyTorch å¼µé‡
    padding="max_length",      # å¡«å……åˆ°æœ€å¤§é•·åº¦
    truncation=True,           # è¶…é•·æˆªæ–·
    max_length=128,            # æœ€å¤§åºåˆ—é•·åº¦
    add_special_tokens=True,   # æ·»åŠ  [CLS], [SEP]
    return_attention_mask=True # è¿”å›æ³¨æ„åŠ›é®ç½©
)

print("Input IDs:", encoded['input_ids'])
print("Attention Mask:", encoded['attention_mask'])

# è§£ç¢¼å›æ–‡æœ¬
decoded = tokenizer.decode(encoded['input_ids'][0])
print("Decoded:", decoded)
```

**è¼¸å‡º**:
```
Input IDs: tensor([[  101,  17662,  2227,  2003,  6429,   999,   102, ...]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, ...]])
Decoded: [CLS] hugging face is amazing! [SEP] [PAD] [PAD] ...
```

**é‡è¦åƒæ•¸èªªæ˜**:

| åƒæ•¸ | èªªæ˜ | å¸¸ç”¨å€¼ |
|------|------|--------|
| `return_tensors` | è¿”å›å¼µé‡é¡å‹ | `"pt"` (PyTorch), `"tf"` (TensorFlow), `"np"` (NumPy) |
| `padding` | å¡«å……ç­–ç•¥ | `True`, `"max_length"`, `"longest"`, `False` |
| `truncation` | æˆªæ–·ç­–ç•¥ | `True`, `"only_first"`, `"longest_first"`, `False` |
| `max_length` | æœ€å¤§é•·åº¦ | `512` (BERT), `1024` (GPT-2), `2048` (GPT-3) |
| `add_special_tokens` | æ·»åŠ ç‰¹æ®Šæ¨™è¨˜ | `True` (é»˜èª), `False` |
| `return_attention_mask` | è¿”å›æ³¨æ„åŠ›é®ç½© | `True` (é»˜èª), `False` |
| `return_token_type_ids` | è¿”å›å¥å­é¡å‹ID | `True` (BERT), `False` (RoBERTa) |

#### **Config (é…ç½®)**

Config å­˜å„²æ¨¡å‹çš„è¶…åƒæ•¸èˆ‡æ¶æ§‹ç´°ç¯€ã€‚

```python
from transformers import AutoConfig

config = AutoConfig.from_pretrained("bert-base-uncased")

print(f"è©å½™è¡¨å¤§å°: {config.vocab_size}")
print(f"éš±è—å±¤ç¶­åº¦: {config.hidden_size}")
print(f"æ³¨æ„åŠ›é ­æ•¸: {config.num_attention_heads}")
print(f"ç·¨ç¢¼å±¤æ•¸: {config.num_hidden_layers}")
print(f"æœ€å¤§åºåˆ—é•·åº¦: {config.max_position_embeddings}")
```

**è¼¸å‡º**:
```
è©å½™è¡¨å¤§å°: 30522
éš±è—å±¤ç¶­åº¦: 768
æ³¨æ„åŠ›é ­æ•¸: 12
ç·¨ç¢¼å±¤æ•¸: 12
æœ€å¤§åºåˆ—é•·åº¦: 512
```

### 2.2 æ¨¡å‹åŠ è¼‰çš„ä¸‰ç¨®æ¨¡å¼

#### **æ¨¡å¼ 1: å¾ Hub åŠ è¼‰ (æœ€å¸¸ç”¨)**
```python
from transformers import AutoModel

# è‡ªå‹•å¾ Hugging Face Hub ä¸‹è¼‰ä¸¦å¿«å–
model = AutoModel.from_pretrained("bert-base-uncased")
```

#### **æ¨¡å¼ 2: å¾æœ¬åœ°è·¯å¾‘åŠ è¼‰**
```python
# å…ˆä¿å­˜åˆ°æœ¬åœ°
model.save_pretrained("./my_bert_model")

# å¾æœ¬åœ°åŠ è¼‰
local_model = AutoModel.from_pretrained("./my_bert_model")
```

#### **æ¨¡å¼ 3: å¾æª¢æŸ¥é»åŠ è¼‰ (è¨“ç·´çºŒæ¥)**
```python
from transformers import AutoModel, TrainingArguments, Trainer

# è¨“ç·´æ™‚æœƒè‡ªå‹•ä¿å­˜æª¢æŸ¥é»åˆ° output_dir
training_args = TrainingArguments(
    output_dir="./results",
    save_steps=500
)

# å¾æª¢æŸ¥é»æ¢å¾©è¨“ç·´
model = AutoModel.from_pretrained("./results/checkpoint-1000")
```

---

## 3. Pipeline API: æœ€å¿«é€Ÿçš„å¯¦ç¾æ–¹å¼

### 3.1 ä»€éº¼æ˜¯ Pipeline?

Pipeline æ˜¯ Transformers æä¾›çš„é«˜éš API,å°è£äº†:
1. æ¨¡å‹åŠ è¼‰
2. æ–‡æœ¬é è™•ç† (Tokenization)
3. æ¨¡å‹æ¨ç†
4. å¾Œè™•ç† (Post-processing)

**æ ¸å¿ƒå„ªå‹¢**:
- âœ… ä¸€è¡Œä»£ç¢¼è§£æ±º NLP ä»»å‹™
- âœ… è‡ªå‹•é¸æ“‡æœ€ä½³é»˜èªæ¨¡å‹
- âœ… æ”¯æ´æ‰¹æ¬¡è™•ç†
- âœ… è‡ªå‹• GPU åŠ é€Ÿ

### 3.2 å…§å»º Pipeline ä»»å‹™åˆ—è¡¨

| Pipeline åç¨± | ä»»å‹™ | é»˜èªæ¨¡å‹ | æ‡‰ç”¨å ´æ™¯ |
|--------------|------|---------|----------|
| `sentiment-analysis` | æƒ…æ„Ÿåˆ†æ | distilbert-sst-2 | è©•è«–æ­£è² é¢åˆ¤æ–· |
| `ner` | å‘½åå¯¦é«”è­˜åˆ¥ | dbmdz/bert-large-ner | æŠ½å–äººåã€åœ°åã€çµ„ç¹” |
| `question-answering` | å•ç­”ç³»çµ± | distilbert-squad | å¾æ–‡æœ¬ä¸­æ‰¾ç­”æ¡ˆ |
| `text-generation` | æ–‡æœ¬ç”Ÿæˆ | gpt2 | è‡ªå‹•å¯«ä½œã€çºŒå¯« |
| `summarization` | æ–‡æœ¬æ‘˜è¦ | sshleifer/distilbart-cnn-12-6 | æ–°èæ‘˜è¦ã€è«–æ–‡ç¸½çµ |
| `translation` | æ©Ÿå™¨ç¿»è­¯ | t5-small | å¤šèªè¨€ç¿»è­¯ |
| `zero-shot-classification` | é›¶æ¨£æœ¬åˆ†é¡ | facebook/bart-large-mnli | ç„¡éœ€è¨“ç·´çš„åˆ†é¡ |
| `fill-mask` | å®Œå½¢å¡«ç©º | distilroberta-base | BERT å¼å¡«ç©ºä»»å‹™ |
| `text2text-generation` | æ–‡æœ¬åˆ°æ–‡æœ¬ | t5-base | é€šç”¨ç”Ÿæˆä»»å‹™ |
| `feature-extraction` | ç‰¹å¾µæå– | distilbert-base-uncased | ç²å–æ–‡æœ¬åµŒå…¥å‘é‡ |

### 3.3 åŸºç¤ä½¿ç”¨ç¯„ä¾‹

#### **ç¯„ä¾‹ 1: æƒ…æ„Ÿåˆ†æ**
```python
from transformers import pipeline

# è¼‰å…¥ Pipeline (è‡ªå‹•ä¸‹è¼‰æ¨¡å‹)
classifier = pipeline("sentiment-analysis")

# å–®ç­†é æ¸¬
result = classifier("I love this product!")
print(result)
# [{'label': 'POSITIVE', 'score': 0.9998}]

# æ‰¹æ¬¡é æ¸¬
texts = [
    "This is amazing!",
    "I hate waiting in line.",
    "It's okay, nothing special."
]
results = classifier(texts)
for text, result in zip(texts, results):
    print(f"{text} â†’ {result['label']} ({result['score']:.2%})")
```

**è¼¸å‡º**:
```
This is amazing! â†’ POSITIVE (99.98%)
I hate waiting in line. â†’ NEGATIVE (99.95%)
It's okay, nothing special. â†’ NEUTRAL (85.23%)
```

#### **ç¯„ä¾‹ 2: å‘½åå¯¦é«”è­˜åˆ¥ (NER)**
```python
# è¼‰å…¥ NER Pipeline
ner = pipeline("ner", aggregation_strategy="simple")

text = "Hugging Face is based in New York City and was founded by ClÃ©ment Delangue."
entities = ner(text)

for entity in entities:
    print(f"{entity['word']}: {entity['entity_group']} (confidence: {entity['score']:.2%})")
```

**è¼¸å‡º**:
```
Hugging Face: ORG (confidence: 99.89%)
New York City: LOC (confidence: 99.95%)
ClÃ©ment Delangue: PER (confidence: 99.92%)
```

#### **ç¯„ä¾‹ 3: å•ç­”ç³»çµ±**
```python
qa = pipeline("question-answering")

context = """
Hugging Face is a company based in New York City.
It was founded in 2016 and specializes in natural language processing.
The company has over 10,000 models available on its platform.
```

question = "When was Hugging Face founded?"
answer = qa(question=question, context=context)

print(f"å•é¡Œ: {question}")
print(f"ç­”æ¡ˆ: {answer['answer']} (ä¿¡å¿ƒåº¦: {answer['score']:.2%})")
```

**è¼¸å‡º**:
```
å•é¡Œ: When was Hugging Face founded?
ç­”æ¡ˆ: 2016 (ä¿¡å¿ƒåº¦: 98.75%)
```

#### **ç¯„ä¾‹ 4: æ–‡æœ¬ç”Ÿæˆ**
```python
generator = pipeline("text-generation", model="gpt2")

prompt = "Once upon a time in a magical forest,"
generated = generator(
    prompt,
    max_length=50,           # æœ€å¤§ç”Ÿæˆé•·åº¦
    num_return_sequences=2,  # ç”Ÿæˆ 2 å€‹ä¸åŒç‰ˆæœ¬
    temperature=0.7,         # å‰µé€ æ€§åƒæ•¸ (0-1)
    top_p=0.9,              # æ ¸æ¡æ¨£åƒæ•¸
    do_sample=True          # å•Ÿç”¨æ¡æ¨£
)

for i, text in enumerate(generated, 1):
    print(f"\nç‰ˆæœ¬ {i}:")
    print(text['generated_text'])
```

#### **ç¯„ä¾‹ 5: é›¶æ¨£æœ¬åˆ†é¡**
```python
# é›¶æ¨£æœ¬åˆ†é¡: ç„¡éœ€è¨“ç·´,ç›´æ¥æŒ‡å®šé¡åˆ¥
classifier = pipeline("zero-shot-classification")

text = "I have a problem with my phone's battery life."
candidate_labels = ["hardware", "software", "customer service", "billing"]

result = classifier(text, candidate_labels)

print(f"æ–‡æœ¬: {text}\n")
for label, score in zip(result['labels'], result['scores']):
    print(f"{label}: {score:.2%}")
```

**è¼¸å‡º**:
```
æ–‡æœ¬: I have a problem with my phone's battery life.

hardware: 87.23%
software: 8.45%
customer service: 3.12%
billing: 1.20%
```

### 3.4 è‡ªè¨‚ Pipeline åƒæ•¸

#### **æŒ‡å®šæ¨¡å‹**
```python
# ä½¿ç”¨ç‰¹å®šæ¨¡å‹è€Œéé»˜èªæ¨¡å‹
classifier = pipeline(
    "sentiment-analysis",
    model="nlptown/bert-base-multilingual-uncased-sentiment",
    tokenizer="nlptown/bert-base-multilingual-uncased-sentiment"
)
```

#### **è¨­å®šè¨­å‚™ (CPU/GPU)**
```python
import torch

# è‡ªå‹•é¸æ“‡è¨­å‚™
device = 0 if torch.cuda.is_available() else -1  # 0 = GPU 0, -1 = CPU

classifier = pipeline("sentiment-analysis", device=device)
```

#### **æ‰¹æ¬¡è™•ç†åƒæ•¸**
```python
classifier = pipeline("sentiment-analysis", batch_size=32)

# è™•ç†å¤§é‡æ–‡æœ¬
texts = ["Text " + str(i) for i in range(1000)]
results = classifier(texts)  # è‡ªå‹•ä»¥ batch_size=32 è™•ç†
```

#### **æ§åˆ¶ç”Ÿæˆåƒæ•¸**
```python
generator = pipeline("text-generation", model="gpt2")

result = generator(
    "The future of AI is",
    max_new_tokens=100,      # ç”Ÿæˆ 100 å€‹æ–° token
    temperature=0.8,         # å‰µé€ æ€§ (0 = ç¢ºå®šæ€§, 1 = éš¨æ©Ÿæ€§)
    top_k=50,               # Top-K æ¡æ¨£
    top_p=0.95,             # Nucleus æ¡æ¨£
    do_sample=True,         # å•Ÿç”¨æ¡æ¨£
    repetition_penalty=1.2, # é‡è¤‡æ‡²ç½°
    num_return_sequences=3  # ç”Ÿæˆ 3 å€‹ç‰ˆæœ¬
)
```

### 3.5 Pipeline å…§éƒ¨æ©Ÿåˆ¶

ç†è§£ Pipeline å…§éƒ¨æµç¨‹æœ‰åŠ©æ–¼èª¿è©¦èˆ‡å„ªåŒ–:

```python
# Pipeline ç­‰æ•ˆæ–¼ä»¥ä¸‹æ‰‹å‹•æµç¨‹:

from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

# 1. è¼‰å…¥æ¨¡å‹èˆ‡åˆ†è©å™¨
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")
model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")

# 2. é è™•ç†
text = "I love this!"
inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)

# 3. æ¨¡å‹æ¨ç†
with torch.no_grad():
    outputs = model(**inputs)
    logits = outputs.logits

# 4. å¾Œè™•ç†
predictions = torch.softmax(logits, dim=-1)
predicted_class_id = predictions.argmax().item()
label = model.config.id2label[predicted_class_id]
score = predictions[0][predicted_class_id].item()

print(f"Label: {label}, Score: {score:.4f}")
```

---

## 4. Hugging Face Hub æ¨¡å‹é¸æ“‡æŒ‡å—

### 4.1 æ¨¡å‹å‘½åè¦ç¯„

Hugging Face æ¨¡å‹åç¨±é€šå¸¸éµå¾ªä»¥ä¸‹æ ¼å¼:
```
{çµ„ç¹”}/{æ¨¡å‹åç¨±}-{è¦æ¨¡}-{è®Šé«”}-{å¾®èª¿ä»»å‹™}
```

**ç¯„ä¾‹è§£æ**:
```
distilbert-base-uncased-finetuned-sst-2-english
â”‚         â”‚    â”‚        â”‚           â”‚      â”‚
â”‚         â”‚    â”‚        â”‚           â”‚      â””â”€ èªè¨€
â”‚         â”‚    â”‚        â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€ å¾®èª¿ä»»å‹™ (SST-2 æƒ…æ„Ÿåˆ†æ)
â”‚         â”‚    â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ å¾®èª¿æ¨™è¨˜
â”‚         â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ å¤§å°å¯«è™•ç† (uncased)
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ æ¨¡å‹è¦æ¨¡ (base)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ æ¨¡å‹ç³»åˆ— (DistilBERT)
```

### 4.2 æ¨¡å‹é¸æ“‡æ±ºç­–æ¨¹

```
æ­¥é©Ÿ 1: ç¢ºå®šä»»å‹™é¡å‹
â”œâ”€ åˆ†é¡ä»»å‹™ â†’ text-classification
â”œâ”€ åºåˆ—æ¨™è¨» â†’ token-classification (NER, POS)
â”œâ”€ ç”Ÿæˆä»»å‹™ â†’ text-generation, text2text-generation
â”œâ”€ å•ç­”ä»»å‹™ â†’ question-answering
â””â”€ æª¢ç´¢ä»»å‹™ â†’ feature-extraction

æ­¥é©Ÿ 2: é¸æ“‡æ¨¡å‹è¦æ¨¡ (æ ¹æ“šè³‡æºé™åˆ¶)
â”œâ”€ è³‡æºå—é™ (CPU, æ‰‹æ©Ÿ) â†’ distilbert, albert, mobile-bert
â”œâ”€ å¹³è¡¡æ•ˆèƒ½ (å–® GPU)    â†’ bert-base, roberta-base
â””â”€ è¿½æ±‚æ¥µè‡´ (å¤š GPU)    â†’ bert-large, roberta-large, GPT-3

æ­¥é©Ÿ 3: è€ƒæ…®èªè¨€æ”¯æ´
â”œâ”€ åƒ…è‹±æ–‡    â†’ bert, roberta, gpt2
â”œâ”€ åƒ…ä¸­æ–‡    â†’ bert-base-chinese, roberta-wwm-ext
â”œâ”€ å¤šèªè¨€    â†’ mbert, xlm-roberta
â””â”€ ç¹é«”ä¸­æ–‡  â†’ ckiplab/bert-base-chinese, ckiplab/gpt2-base-chinese

æ­¥é©Ÿ 4: æª¢æŸ¥å¾®èª¿ç‹€æ…‹
â”œâ”€ å·²å¾®èª¿ (task-specific) â†’ ç›´æ¥ä½¿ç”¨,ç„¡éœ€è¨“ç·´
â”‚   ä¾‹: distilbert-base-uncased-finetuned-sst-2
â”œâ”€ é è¨“ç·´ (pretrained)    â†’ éœ€è‡ªè¡Œå¾®èª¿åˆ°ä¸‹æ¸¸ä»»å‹™
â”‚   ä¾‹: bert-base-uncased
â””â”€ åŸºç¤æ¨¡å‹ (base)        â†’ éœ€å¾é›¶è¨“ç·´ (ä¸æ¨è–¦)
```

### 4.3 æ¨¡å‹å¡ç‰‡ (Model Card) è§£è®€

æ¯å€‹ Hugging Face æ¨¡å‹éƒ½é™„å¸¶æ¨¡å‹å¡ç‰‡,åŒ…å«é—œéµä¿¡æ¯:

#### **1. Model Description (æ¨¡å‹æè¿°)**
- æ¨¡å‹æ¶æ§‹ (BERT, GPT, T5...)
- è¨“ç·´æ•¸æ“šä¾†æº
- é æœŸç”¨é€”èˆ‡é™åˆ¶

#### **2. Intended Use (é æœŸç”¨é€”)**
```markdown
âœ… é©ç”¨å ´æ™¯:
- è‹±æ–‡é›»å½±è©•è«–æƒ…æ„Ÿåˆ†æ
- æ­£è² é¢åˆ†é¡ (äºŒåˆ†é¡)
- å¥å­ç´šåˆ¥åˆ†é¡

âŒ ä¸é©ç”¨å ´æ™¯:
- ä¸­æ–‡æ–‡æœ¬ (æ¨¡å‹æœªé‡å°ä¸­æ–‡è¨“ç·´)
- å¤šé¡åˆ¥åˆ†é¡ (æ¨¡å‹åƒ…æ”¯æ´äºŒåˆ†é¡)
- é•·æ–‡æª”åˆ†é¡ (æ¨¡å‹æœ€å¤§è¼¸å…¥ 512 token)
```

#### **3. Training Data (è¨“ç·´æ•¸æ“š)**
- æ•¸æ“šä¾†æº: Stanford Sentiment Treebank (SST-2)
- æ•¸æ“šè¦æ¨¡: 67,349 è¨“ç·´æ¨£æœ¬
- æ•¸æ“šç‰¹æ€§: é›»å½±è©•è«–,è‹±æ–‡,å¥å­ç´šåˆ¥

#### **4. Evaluation Results (è©•ä¼°çµæœ)**
```
SST-2 é©—è­‰é›†:
- Accuracy: 91.3%
- F1 Score: 91.0%

èˆ‡åŸºæº–å°æ¯”:
- BERT-base: 92.7%
- RoBERTa-base: 94.8%
- æœ¬æ¨¡å‹ (DistilBERT): 91.3%
```

#### **5. Limitations (é™åˆ¶)**
- å¯èƒ½å°éé›»å½±é ˜åŸŸè©•è«–æ•ˆæœè¼ƒå·®
- å°è«·åˆºã€é›™é—œç­‰è¤‡é›œèªç¾©ç†è§£æœ‰é™
- è¨“ç·´æ•¸æ“šå¯èƒ½å­˜åœ¨åè¦‹ (éœ€è¬¹æ…ä½¿ç”¨)

#### **6. How to Use (ä½¿ç”¨ç¯„ä¾‹)**
```python
from transformers import pipeline

classifier = pipeline("sentiment-analysis", model="distilbert-base-uncased-finetuned-sst-2-english")
result = classifier("This movie was fantastic!")
print(result)
```

### 4.4 ä½¿ç”¨ Hub API æœå°‹æ¨¡å‹

#### **æ–¹æ³• 1: ç¶²é æœå°‹** (æ¨è–¦)
è¨ªå• https://huggingface.co/models,ä½¿ç”¨ç¯©é¸å™¨:
- Task: ä»»å‹™é¡å‹
- Libraries: æ¡†æ¶ (PyTorch, TensorFlow)
- Languages: èªè¨€
- Sort: æ’åºæ–¹å¼ (Most downloaded, Most liked)

#### **æ–¹æ³• 2: Python API æœå°‹**
```python
from huggingface_hub import list_models

# æœå°‹æƒ…æ„Ÿåˆ†ææ¨¡å‹,æŒ‰ä¸‹è¼‰é‡æ’åº
models = list_models(
    filter="text-classification",
    sort="downloads",
    direction=-1,
    limit=5
)

for model in models:
    print(f"æ¨¡å‹: {model.modelId}")
    print(f"  ä¸‹è¼‰é‡: {model.downloads:,}")
    print(f"  æ¨™ç±¤: {model.tags[:5]}\n")
```

#### **æ–¹æ³• 3: CLI æœå°‹**
```bash
# æœå°‹ä¸­æ–‡ BERT æ¨¡å‹
huggingface-cli search --task text-classification --language zh

# æœå°‹ GPT ç³»åˆ—æ¨¡å‹
huggingface-cli search --model-name gpt
```

---

## 5. æœ€ä½³å¯¦è¸èˆ‡å¸¸è¦‹å•é¡Œ

### 5.1 æ•ˆèƒ½å„ªåŒ–æŠ€å·§

#### **1. ä½¿ç”¨é‡åŒ–æ¨¡å‹æ¸›å°‘è¨˜æ†¶é«”**
```python
from transformers import AutoModelForCausalLM

# 8-bit é‡åŒ– (éœ€è¦ bitsandbytes å‡½å¼åº«)
model = AutoModelForCausalLM.from_pretrained(
    "gpt2",
    load_in_8bit=True,
    device_map="auto"
)
```

#### **2. æ‰¹æ¬¡è™•ç†æå‡ååé‡**
```python
# å–®ç­†è™•ç† (æ…¢)
for text in texts:
    result = classifier(text)

# æ‰¹æ¬¡è™•ç† (å¿«)
results = classifier(texts, batch_size=32)
```

#### **3. ä½¿ç”¨ ONNX åŠ é€Ÿæ¨ç†**
```python
from transformers import pipeline

# ä½¿ç”¨ ONNX Runtime å¾Œç«¯
classifier = pipeline(
    "sentiment-analysis",
    model="distilbert-base-uncased-finetuned-sst-2-english",
    framework="onnx"  # éœ€è¦å®‰è£ optimum[onnxruntime]
)
```

#### **4. å•Ÿç”¨æ··åˆç²¾åº¦è¨“ç·´**
```python
from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="./results",
    fp16=True,  # å•Ÿç”¨æ··åˆç²¾åº¦ (éœ€è¦ NVIDIA GPU)
)
```

### 5.2 å¸¸è¦‹å•é¡Œèˆ‡è§£æ±ºæ–¹æ¡ˆ

#### **Q1: æ¨¡å‹ä¸‹è¼‰å¤±æ•— (ä¸­åœ‹å¤§é™¸ç”¨æˆ¶)**
```python
# è§£æ±ºæ–¹æ¡ˆ 1: ä½¿ç”¨é¡åƒç«™
import os
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

# è§£æ±ºæ–¹æ¡ˆ 2: æ‰‹å‹•ä¸‹è¼‰å¾Œæœ¬åœ°åŠ è¼‰
# å¾ https://hf-mirror.com ä¸‹è¼‰æ¨¡å‹åˆ°æœ¬åœ°
model = AutoModel.from_pretrained("./local_model_path")
```

#### **Q2: CUDA Out of Memory (è¨˜æ†¶é«”ä¸è¶³)**
```python
# è§£æ±ºæ–¹æ¡ˆ 1: æ¸›å°‘ batch size
trainer = Trainer(
    per_device_train_batch_size=8,  # é™ä½ batch size
    gradient_accumulation_steps=4   # ä½¿ç”¨æ¢¯åº¦ç´¯ç©è£œå„Ÿ
)

# è§£æ±ºæ–¹æ¡ˆ 2: ä½¿ç”¨æ¢¯åº¦æª¢æŸ¥é»
model = AutoModel.from_pretrained(
    "bert-large-uncased",
    gradient_checkpointing=True  # çŠ§ç‰²é€Ÿåº¦æ›å–è¨˜æ†¶é«”
)

# è§£æ±ºæ–¹æ¡ˆ 3: ä½¿ç”¨é‡åŒ–æ¨¡å‹
model = AutoModel.from_pretrained("bert-base-uncased", load_in_8bit=True)
```

#### **Q3: å¦‚ä½•é›¢ç·šä½¿ç”¨æ¨¡å‹?**
```python
# æ­¥é©Ÿ 1: åœ¨ç·šæ™‚ä¸‹è¼‰ä¸¦ä¿å­˜
from transformers import AutoModel, AutoTokenizer

model = AutoModel.from_pretrained("bert-base-uncased")
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

model.save_pretrained("./offline_model")
tokenizer.save_pretrained("./offline_model")

# æ­¥é©Ÿ 2: é›¢ç·šæ™‚å¾æœ¬åœ°åŠ è¼‰
model = AutoModel.from_pretrained("./offline_model", local_files_only=True)
tokenizer = AutoTokenizer.from_pretrained("./offline_model", local_files_only=True)
```

#### **Q4: Tokenizer è­¦å‘Š: Token indices sequence length is longer than...**
```python
# å•é¡Œ: è¼¸å…¥åºåˆ—è¶…éæ¨¡å‹æœ€å¤§é•·åº¦
# è§£æ±ºæ–¹æ¡ˆ: å•Ÿç”¨æˆªæ–·
tokenizer(
    text,
    truncation=True,        # å•Ÿç”¨æˆªæ–·
    max_length=512,         # æ˜ç¢ºæŒ‡å®šæœ€å¤§é•·åº¦
    return_tensors="pt"
)
```

#### **Q5: å¦‚ä½•è™•ç†å¤šèªè¨€æ–‡æœ¬?**
```python
# ä½¿ç”¨å¤šèªè¨€æ¨¡å‹
from transformers import pipeline

# XLM-RoBERTa æ”¯æ´ 100+ ç¨®èªè¨€
classifier = pipeline(
    "sentiment-analysis",
    model="cardiffnlp/twitter-xlm-roberta-base-sentiment"
)

# æ”¯æ´è‹±æ–‡ã€ä¸­æ–‡ã€æ—¥æ–‡ç­‰
results = classifier([
    "I love this!",           # è‹±æ–‡
    "æˆ‘å–œæ­¡é€™å€‹!",            # ä¸­æ–‡
    "ã“ã‚ŒãŒå¤§å¥½ãã§ã™!"        # æ—¥æ–‡
])
```

### 5.3 èª¿è©¦æŠ€å·§

#### **1. æª¢æŸ¥æ¨¡å‹è¼¸å‡ºå½¢ç‹€**
```python
print(f"æ¨¡å‹è¼¸å‡ºå½¢ç‹€: {outputs.logits.shape}")
# é æœŸ: (batch_size, num_labels)
```

#### **2. æŸ¥çœ‹ Tokenizer ç·¨ç¢¼çµæœ**
```python
encoded = tokenizer("Test text", return_tensors="pt")
print(f"Input IDs: {encoded['input_ids']}")
print(f"Tokens: {tokenizer.convert_ids_to_tokens(encoded['input_ids'][0])}")
```

#### **3. é©—è­‰æ¨¡å‹é…ç½®**
```python
print(model.config)
# æª¢æŸ¥ num_labels, hidden_size ç­‰åƒæ•¸
```

---

## 6. å¯¦æˆ°æ¡ˆä¾‹: å¤šä»»å‹™ NLP æ‡‰ç”¨

### æ¡ˆä¾‹: å®¢æˆ¶è©•è«–æ™ºèƒ½åˆ†æç³»çµ±

**éœ€æ±‚**: åˆ†æå®¢æˆ¶è©•è«–,æå–æƒ…æ„Ÿã€é—œéµå¯¦é«”ã€ç”Ÿæˆæ‘˜è¦ã€‚

**å®Œæ•´å¯¦ç¾**:
```python
from transformers import pipeline

class ReviewAnalyzer:
    def __init__(self):
        self.sentiment = pipeline("sentiment-analysis")
        self.ner = pipeline("ner", aggregation_strategy="simple")
        self.summarizer = pipeline("summarization")

    def analyze(self, review_text):
        # 1. æƒ…æ„Ÿåˆ†æ
        sentiment = self.sentiment(review_text)[0]

        # 2. å¯¦é«”è­˜åˆ¥
        entities = self.ner(review_text)

        # 3. æ‘˜è¦ç”Ÿæˆ (é•·æ–‡æœ¬æ‰éœ€è¦)
        summary = None
        if len(review_text) > 100:
            summary = self.summarizer(
                review_text,
                max_length=50,
                min_length=10,
                do_sample=False
            )[0]['summary_text']

        return {
            "sentiment": sentiment,
            "entities": entities,
            "summary": summary
        }

# ä½¿ç”¨ç¯„ä¾‹
analyzer = ReviewAnalyzer()

review = """
I recently visited the Apple Store in New York to buy an iPhone 15.
The staff was incredibly helpful, especially John, who spent 30 minutes
explaining all the features. However, I was disappointed by the long
wait time. Overall, a mixed experience but the product quality is excellent.
"""

result = analyzer.analyze(review)
print(f"æƒ…æ„Ÿ: {result['sentiment']['label']} ({result['sentiment']['score']:.2%})")
print(f"æåŠå¯¦é«”: {[e['word'] for e in result['entities']]}")
print(f"æ‘˜è¦: {result['summary']}")
```

---

## 7. å»¶ä¼¸å­¸ç¿’è³‡æº

### å®˜æ–¹è³‡æº
- ğŸ“š [Hugging Face æ–‡æª”](https://huggingface.co/docs/transformers)
- ğŸ“ [Hugging Face èª²ç¨‹](https://huggingface.co/course) (å…è²»,å¼·çƒˆæ¨è–¦)
- ğŸ›ï¸ [Model Hub](https://huggingface.co/models)
- ğŸ—‚ï¸ [Datasets Hub](https://huggingface.co/datasets)

### ç¤¾ç¾¤è³‡æº
- ğŸ’¬ [Hugging Face è«–å£‡](https://discuss.huggingface.co)
- ğŸ¦ [Twitter @HuggingFace](https://twitter.com/huggingface)
- ğŸ“º [YouTube é »é“](https://www.youtube.com/@HuggingFace)

### é€²éšä¸»é¡Œ
- æ¨¡å‹å¾®èª¿ (Fine-tuning)
- è‡ªè¨‚æ¨¡å‹æ¶æ§‹
- åˆ†æ•£å¼è¨“ç·´
- æ¨¡å‹é‡åŒ–èˆ‡éƒ¨ç½²
- PEFT (Parameter-Efficient Fine-Tuning)

---

## 8. èª²å¾Œç·´ç¿’

### ç·´ç¿’ 1: Pipeline æ¢ç´¢
å˜—è©¦ä½¿ç”¨ä»¥ä¸‹ Pipeline:
1. `fill-mask`: å®Œå½¢å¡«ç©º
2. `translation_en_to_de`: è‹±å¾·ç¿»è­¯
3. `zero-shot-classification`: é›¶æ¨£æœ¬åˆ†é¡

### ç·´ç¿’ 2: æ¨¡å‹æ¯”è¼ƒ
æ¯”è¼ƒ `distilbert-base-uncased` èˆ‡ `bert-base-uncased` çš„:
- åƒæ•¸é‡
- æ¨ç†é€Ÿåº¦
- æº–ç¢ºåº¦

### ç·´ç¿’ 3: è‡ªè¨‚æ‡‰ç”¨
æ§‹å»ºä¸€å€‹æ–°èåˆ†é¡å™¨,åˆ†é¡æ–°èåˆ°:
- æ”¿æ²»
- ç§‘æŠ€
- é«”è‚²
- å¨›æ¨‚

---

**èª²ç¨‹**: iSpan Python NLP Cookbooks v2
**è¬›å¸«**: Claude AI
**æœ€å¾Œæ›´æ–°**: 2025-10-17
