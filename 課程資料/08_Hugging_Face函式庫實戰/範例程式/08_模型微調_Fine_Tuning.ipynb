{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH08-08: 模型微調 (Fine-Tuning)\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 本節學習目標\n",
    "\n",
    "1. 理解**遷移學習**與**模型微調**的概念\n",
    "2. 掌握 **Hugging Face Trainer API** 的使用\n",
    "3. 學會**超參數調優**技巧\n",
    "4. 實作完整的**模型微調流程** (AG News 新聞分類)\n",
    "5. 了解模型**評估與部署**的最佳實踐\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 什麼是模型微調？\n",
    "\n",
    "### 遷移學習 (Transfer Learning)\n",
    "\n",
    "```\n",
    "預訓練模型 (Pre-trained Model)\n",
    "    ↓\n",
    "    在大規模通用數據上訓練 (如 Wikipedia, BookCorpus)\n",
    "    學習到語言的通用知識\n",
    "    ↓\n",
    "微調 (Fine-Tuning)\n",
    "    ↓\n",
    "    在特定任務數據上微調 (如新聞分類, 情感分析)\n",
    "    適應特定領域或任務\n",
    "    ↓\n",
    "應用模型 (Task-Specific Model)\n",
    "```\n",
    "\n",
    "### 為什麼需要微調？\n",
    "\n",
    "| 方法 | 優點 | 缺點 | 適用場景 |\n",
    "|------|------|------|----------|\n",
    "| **從頭訓練** | 完全客製化 | 需要大量數據 (百萬級)<br>訓練時間長 (數週)<br>需要強大 GPU 資源 | 有大規模數據<br>特殊領域 |\n",
    "| **預訓練模型直接使用** | 零訓練成本<br>立即可用 | 可能不適合特定任務<br>精度受限 | 通用任務<br>快速原型 |\n",
    "| **微調** ✅ | 數據需求少 (千級即可)<br>訓練快 (分鐘到小時)<br>精度高 | 依賴預訓練模型 | **大多數實際場景** |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔧 環境準備\n",
    "\n",
    "### 安裝必要套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install transformers datasets evaluate accelerate -U\n",
    "# !pip install scikit-learn numpy pandas matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "import evaluate\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "import random\n",
    "import torch\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"✅ PyTorch version: {torch.__version__}\")\n",
    "print(f\"✅ CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"✅ GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 📊 數據準備: AG News 新聞分類\n",
    "\n",
    "### 數據集介紹\n",
    "\n",
    "**AG News** 是經典的文本分類數據集:\n",
    "- **類別數**: 4 (世界、體育、商業、科技)\n",
    "- **訓練集**: 120,000 筆新聞\n",
    "- **測試集**: 7,600 筆新聞\n",
    "- **來源**: AG's news corpus\n",
    "\n",
    "### 加載數據"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load AG News dataset from Hugging Face\n",
    "print(\"📥 Loading AG News dataset...\")\n",
    "dataset = load_dataset(\"ag_news\")\n",
    "\n",
    "print(f\"\\n✅ Dataset loaded successfully!\")\n",
    "print(f\"\\nDataset structure:\")\n",
    "print(dataset)\n",
    "\n",
    "# Check sample\n",
    "print(f\"\\n📌 Sample from training set:\")\n",
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create label mapping\n",
    "label_names = ['World', 'Sports', 'Business', 'Sci/Tech']\n",
    "id2label = {i: label for i, label in enumerate(label_names)}\n",
    "label2id = {label: i for i, label in enumerate(label_names)}\n",
    "\n",
    "print(\"📋 Label Mapping:\")\n",
    "for idx, name in id2label.items():\n",
    "    print(f\"  {idx}: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore dataset statistics\n",
    "train_df = pd.DataFrame(dataset['train'])\n",
    "test_df = pd.DataFrame(dataset['test'])\n",
    "\n",
    "print(f\"\\n📊 Dataset Statistics:\")\n",
    "print(f\"  Train samples: {len(train_df):,}\")\n",
    "print(f\"  Test samples: {len(test_df):,}\")\n",
    "print(f\"  Total samples: {len(train_df) + len(test_df):,}\")\n",
    "\n",
    "# Class distribution\n",
    "print(f\"\\n📈 Class Distribution (Training):\")\n",
    "class_counts = train_df['label'].value_counts().sort_index()\n",
    "for idx, count in class_counts.items():\n",
    "    print(f\"  {id2label[idx]}: {count:,} ({count/len(train_df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Training set\n",
    "train_counts = train_df['label'].map(id2label).value_counts()\n",
    "axes[0].bar(train_counts.index, train_counts.values, color='steelblue')\n",
    "axes[0].set_title('Training Set - Class Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Category')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Test set\n",
    "test_counts = test_df['label'].map(id2label).value_counts()\n",
    "axes[1].bar(test_counts.index, test_counts.values, color='coral')\n",
    "axes[1].set_title('Test Set - Class Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Category')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 數據預處理與 Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For demonstration, use a smaller subset to speed up training\n",
    "# Remove this in production for full dataset training\n",
    "SAMPLE_SIZE = 10000  # Use 10k samples for quick training\n",
    "USE_FULL_DATASET = False  # Set to True for production\n",
    "\n",
    "if not USE_FULL_DATASET:\n",
    "    print(f\"⚠️  Using subset of {SAMPLE_SIZE:,} samples for quick training\")\n",
    "    dataset['train'] = dataset['train'].shuffle(seed=SEED).select(range(SAMPLE_SIZE))\n",
    "    dataset['test'] = dataset['test'].shuffle(seed=SEED).select(range(SAMPLE_SIZE // 10))\n",
    "    print(f\"✅ Train: {len(dataset['train']):,} | Test: {len(dataset['test']):,}\")\n",
    "else:\n",
    "    print(f\"✅ Using full dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "MODEL_NAME = \"distilbert-base-uncased\"  # Fast and efficient for classification\n",
    "print(f\"📦 Loading tokenizer: {MODEL_NAME}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "print(f\"✅ Tokenizer loaded: {tokenizer.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tokenization function\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"\n",
    "    Tokenize text data\n",
    "    Args:\n",
    "        examples: batch of text samples\n",
    "    Returns:\n",
    "        tokenized inputs with padding and truncation\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=128  # Adjust based on your data\n",
    "    )\n",
    "\n",
    "# Apply tokenization to dataset\n",
    "print(\"🔄 Tokenizing dataset...\")\n",
    "tokenized_datasets = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=['text']  # Remove original text column\n",
    ")\n",
    "\n",
    "print(\"✅ Tokenization completed!\")\n",
    "print(f\"\\nTokenized dataset structure:\")\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect tokenized sample\n",
    "print(\"\\n📌 Tokenized sample:\")\n",
    "sample = tokenized_datasets['train'][0]\n",
    "print(f\"Input IDs shape: {len(sample['input_ids'])}\")\n",
    "print(f\"Attention mask shape: {len(sample['attention_mask'])}\")\n",
    "print(f\"Label: {sample['label']} ({id2label[sample['label']]})\")\n",
    "\n",
    "# Decode back to text\n",
    "decoded_text = tokenizer.decode(sample['input_ids'], skip_special_tokens=True)\n",
    "print(f\"\\nDecoded text (first 100 chars):\\n{decoded_text[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🤖 模型準備\n",
    "\n",
    "### 加載預訓練模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model for sequence classification\n",
    "print(f\"📦 Loading model: {MODEL_NAME}\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(label_names),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "print(f\"✅ Model loaded successfully!\")\n",
    "print(f\"\\nModel architecture: {model.__class__.__name__}\")\n",
    "print(f\"Number of parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check model structure\n",
    "print(\"\\n🔍 Model Structure:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 📈 評估指標設定\n",
    "\n",
    "### 定義評估函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metrics\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute evaluation metrics during training\n",
    "    Args:\n",
    "        eval_pred: predictions and labels\n",
    "    Returns:\n",
    "        dict of metrics\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels, average='weighted')\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy['accuracy'],\n",
    "        'f1': f1['f1']\n",
    "    }\n",
    "\n",
    "print(\"✅ Metrics configured: Accuracy, F1-score (weighted)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🎯 訓練配置: TrainingArguments\n",
    "\n",
    "### 超參數說明\n",
    "\n",
    "| 參數 | 說明 | 建議值 |\n",
    "|------|------|--------|\n",
    "| `output_dir` | 模型與檢查點保存路徑 | `./results` |\n",
    "| `num_train_epochs` | 訓練輪數 | 3-5 |\n",
    "| `per_device_train_batch_size` | 每個 GPU 的訓練批次大小 | 8-32 |\n",
    "| `per_device_eval_batch_size` | 每個 GPU 的評估批次大小 | 16-64 |\n",
    "| `learning_rate` | 學習率 | 2e-5 ~ 5e-5 |\n",
    "| `weight_decay` | 權重衰減 (L2 正則化) | 0.01 |\n",
    "| `warmup_steps` | 學習率預熱步數 | 500 |\n",
    "| `logging_steps` | 日誌記錄間隔 | 100 |\n",
    "| `evaluation_strategy` | 評估策略 | `epoch` / `steps` |\n",
    "| `save_strategy` | 保存策略 | `epoch` / `steps` |\n",
    "| `load_best_model_at_end` | 訓練結束載入最佳模型 | `True` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results/ag_news_finetuned\",\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=500,\n",
    "    \n",
    "    # Evaluation\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    \n",
    "    # Logging\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    \n",
    "    # Other settings\n",
    "    seed=SEED,\n",
    "    push_to_hub=False,  # Set to True if you want to push to Hugging Face Hub\n",
    ")\n",
    "\n",
    "print(\"✅ Training arguments configured\")\n",
    "print(f\"\\n📋 Key settings:\")\n",
    "print(f\"  - Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  - Batch size (train): {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  - Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  - Evaluation strategy: {training_args.evaluation_strategy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🚀 開始訓練\n",
    "\n",
    "### 初始化 Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['test'],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]  # Stop if no improvement for 2 epochs\n",
    ")\n",
    "\n",
    "print(\"✅ Trainer initialized with early stopping (patience=2)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"\\n🚀 Starting training...\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"✅ Training completed!\\n\")\n",
    "\n",
    "# Print training summary\n",
    "print(\"📊 Training Summary:\")\n",
    "print(f\"  Total training time: {train_result.metrics['train_runtime']:.2f} seconds\")\n",
    "print(f\"  Training samples/second: {train_result.metrics['train_samples_per_second']:.2f}\")\n",
    "print(f\"  Final training loss: {train_result.metrics['train_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 📊 模型評估\n",
    "\n",
    "### 測試集評估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"\\n🔍 Evaluating model on test set...\\n\")\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"\\n📈 Evaluation Results:\")\n",
    "for key, value in eval_results.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 詳細分類報告與混淆矩陣"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "predictions = trainer.predict(tokenized_datasets['test'])\n",
    "pred_labels = np.argmax(predictions.predictions, axis=1)\n",
    "true_labels = predictions.label_ids\n",
    "\n",
    "# Classification report\n",
    "print(\"\\n📋 Classification Report:\\n\")\n",
    "print(classification_report(\n",
    "    true_labels,\n",
    "    pred_labels,\n",
    "    target_names=label_names,\n",
    "    digits=4\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(true_labels, pred_labels)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='Blues',\n",
    "    xticklabels=label_names,\n",
    "    yticklabels=label_names\n",
    ")\n",
    "plt.title('Confusion Matrix - AG News Classification', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 💾 模型保存與加載\n",
    "\n",
    "### 保存微調後的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and tokenizer\n",
    "model_save_path = \"./models/ag_news_classifier\"\n",
    "\n",
    "print(f\"💾 Saving model to {model_save_path}...\")\n",
    "trainer.save_model(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "print(f\"✅ Model and tokenizer saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加載已保存的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved model\n",
    "from transformers import pipeline\n",
    "\n",
    "print(f\"📂 Loading model from {model_save_path}...\")\n",
    "classifier = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=model_save_path,\n",
    "    tokenizer=model_save_path\n",
    ")\n",
    "\n",
    "print(\"✅ Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🎮 實際應用範例\n",
    "\n",
    "### 測試自訂新聞文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with custom news articles\n",
    "test_articles = [\n",
    "    \"Apple releases new iPhone with advanced AI features and improved camera.\",\n",
    "    \"The Lakers defeated the Warriors 112-108 in last night's NBA game.\",\n",
    "    \"Global stock markets surge as inflation data shows signs of cooling.\",\n",
    "    \"Scientists discover potential breakthrough in quantum computing technology.\"\n",
    "]\n",
    "\n",
    "print(\"🎯 Testing on custom news articles:\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i, article in enumerate(test_articles, 1):\n",
    "    result = classifier(article)[0]\n",
    "    \n",
    "    print(f\"\\n📰 Article {i}:\")\n",
    "    print(f\"   Text: {article}\")\n",
    "    print(f\"   Predicted: {result['label']} (Confidence: {result['score']:.2%})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🔧 超參數調優進階技巧\n",
    "\n",
    "### 1. 學習率調整策略\n",
    "\n",
    "```python\n",
    "# Linear warmup + linear decay\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "num_training_steps = len(train_dataloader) * num_epochs\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=500,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "```\n",
    "\n",
    "### 2. 梯度累積 (Gradient Accumulation)\n",
    "\n",
    "當 GPU 記憶體不足時:\n",
    "\n",
    "```python\n",
    "training_args = TrainingArguments(\n",
    "    ...,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=4,  # Effective batch size = 8 * 4 = 32\n",
    ")\n",
    "```\n",
    "\n",
    "### 3. 混合精度訓練 (Mixed Precision)\n",
    "\n",
    "加速訓練並節省記憶體:\n",
    "\n",
    "```python\n",
    "training_args = TrainingArguments(\n",
    "    ...,\n",
    "    fp16=True,  # Enable mixed precision on compatible GPUs\n",
    ")\n",
    "```\n",
    "\n",
    "### 4. 超參數搜索 (Hyperparameter Search)\n",
    "\n",
    "自動尋找最佳超參數:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter search example (commented out - computationally expensive)\n",
    "\"\"\"\n",
    "def model_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=len(label_names),\n",
    "        id2label=id2label,\n",
    "        label2id=label2id\n",
    "    )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['test'],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Search space\n",
    "def hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 5e-5, log=True),\n",
    "        \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 2, 5),\n",
    "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [8, 16, 32]),\n",
    "    }\n",
    "\n",
    "# Run hyperparameter search\n",
    "best_run = trainer.hyperparameter_search(\n",
    "    direction=\"maximize\",\n",
    "    backend=\"optuna\",\n",
    "    hp_space=hp_space,\n",
    "    n_trials=10\n",
    ")\n",
    "\n",
    "print(f\"Best hyperparameters: {best_run.hyperparameters}\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"💡 Hyperparameter search example provided (commented out)\")\n",
    "print(\"   Requires: pip install optuna\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 📚 總結與最佳實踐\n",
    "\n",
    "### ✅ 你學到了什麼\n",
    "\n",
    "1. **遷移學習與微調概念**: 理解為何微調優於從頭訓練\n",
    "2. **完整微調流程**:\n",
    "   - 數據加載與預處理\n",
    "   - Tokenization\n",
    "   - 模型配置\n",
    "   - 訓練與評估\n",
    "   - 模型保存與部署\n",
    "3. **Trainer API 使用**: 簡化訓練流程的強大工具\n",
    "4. **超參數調優**: 學習率、Batch size、Early stopping 等\n",
    "5. **模型評估**: Accuracy, F1-score, Confusion matrix\n",
    "\n",
    "### 🎯 微調最佳實踐\n",
    "\n",
    "| 最佳實踐 | 說明 | 原因 |\n",
    "|---------|------|------|\n",
    "| **使用預訓練模型** | 從 BERT、DistilBERT、RoBERTa 等開始 | 節省訓練時間與資源 |\n",
    "| **較小學習率** | 2e-5 ~ 5e-5 | 避免破壞預訓練權重 |\n",
    "| **少量 Epochs** | 2-5 epochs | 避免過擬合 |\n",
    "| **Early Stopping** | 監控驗證集表現 | 自動停止防止過擬合 |\n",
    "| **數據增強** | 同義詞替換、回譯 | 提升模型泛化能力 |\n",
    "| **類別平衡** | 處理不平衡數據 | 避免偏向多數類 |\n",
    "| **保存檢查點** | 定期保存 | 避免訓練中斷損失 |\n",
    "\n",
    "### 🚀 下一步\n",
    "\n",
    "1. **嘗試不同的預訓練模型**:\n",
    "   - `bert-base-uncased`\n",
    "   - `roberta-base`\n",
    "   - `xlnet-base-cased`\n",
    "\n",
    "2. **應用到你自己的數據**:\n",
    "   - 準備標注數據\n",
    "   - 調整 tokenization 策略\n",
    "   - 實驗不同超參數\n",
    "\n",
    "3. **部署模型**:\n",
    "   - 使用 FastAPI 建立 API\n",
    "   - Docker 容器化\n",
    "   - 部署到雲端 (AWS, GCP, Azure)\n",
    "\n",
    "4. **進階優化**:\n",
    "   - 模型量化 (Quantization)\n",
    "   - 知識蒸餾 (Knowledge Distillation)\n",
    "   - ONNX 轉換加速推理\n",
    "\n",
    "---\n",
    "\n",
    "## 🔗 參考資源\n",
    "\n",
    "- [Hugging Face Transformers 文檔](https://huggingface.co/docs/transformers/)\n",
    "- [Trainer API 指南](https://huggingface.co/docs/transformers/main_classes/trainer)\n",
    "- [AG News Dataset](https://huggingface.co/datasets/ag_news)\n",
    "- [Fine-tuning Best Practices](https://huggingface.co/docs/transformers/training)\n",
    "\n",
    "---\n",
    "\n",
    "**下一節**: `09_專案實戰_客戶意見分析儀.ipynb` - 完整的端到端專案實戰 🎯"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
