{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH08-08: æ¨¡å‹å¾®èª¿ (Fine-Tuning)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š æœ¬ç¯€å­¸ç¿’ç›®æ¨™\n",
    "\n",
    "1. ç†è§£**é·ç§»å­¸ç¿’**èˆ‡**æ¨¡å‹å¾®èª¿**çš„æ¦‚å¿µ\n",
    "2. æŒæ¡ **Hugging Face Trainer API** çš„ä½¿ç”¨\n",
    "3. å­¸æœƒ**è¶…åƒæ•¸èª¿å„ª**æŠ€å·§\n",
    "4. å¯¦ä½œå®Œæ•´çš„**æ¨¡å‹å¾®èª¿æµç¨‹** (AG News æ–°èåˆ†é¡)\n",
    "5. äº†è§£æ¨¡å‹**è©•ä¼°èˆ‡éƒ¨ç½²**çš„æœ€ä½³å¯¦è¸\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ ä»€éº¼æ˜¯æ¨¡å‹å¾®èª¿ï¼Ÿ\n",
    "\n",
    "### é·ç§»å­¸ç¿’ (Transfer Learning)\n",
    "\n",
    "```\n",
    "é è¨“ç·´æ¨¡å‹ (Pre-trained Model)\n",
    "    â†“\n",
    "    åœ¨å¤§è¦æ¨¡é€šç”¨æ•¸æ“šä¸Šè¨“ç·´ (å¦‚ Wikipedia, BookCorpus)\n",
    "    å­¸ç¿’åˆ°èªè¨€çš„é€šç”¨çŸ¥è­˜\n",
    "    â†“\n",
    "å¾®èª¿ (Fine-Tuning)\n",
    "    â†“\n",
    "    åœ¨ç‰¹å®šä»»å‹™æ•¸æ“šä¸Šå¾®èª¿ (å¦‚æ–°èåˆ†é¡, æƒ…æ„Ÿåˆ†æ)\n",
    "    é©æ‡‰ç‰¹å®šé ˜åŸŸæˆ–ä»»å‹™\n",
    "    â†“\n",
    "æ‡‰ç”¨æ¨¡å‹ (Task-Specific Model)\n",
    "```\n",
    "\n",
    "### ç‚ºä»€éº¼éœ€è¦å¾®èª¿ï¼Ÿ\n",
    "\n",
    "| æ–¹æ³• | å„ªé» | ç¼ºé» | é©ç”¨å ´æ™¯ |\n",
    "|------|------|------|----------|\n",
    "| **å¾é ­è¨“ç·´** | å®Œå…¨å®¢è£½åŒ– | éœ€è¦å¤§é‡æ•¸æ“š (ç™¾è¬ç´š)<br>è¨“ç·´æ™‚é–“é•· (æ•¸é€±)<br>éœ€è¦å¼·å¤§ GPU è³‡æº | æœ‰å¤§è¦æ¨¡æ•¸æ“š<br>ç‰¹æ®Šé ˜åŸŸ |\n",
    "| **é è¨“ç·´æ¨¡å‹ç›´æ¥ä½¿ç”¨** | é›¶è¨“ç·´æˆæœ¬<br>ç«‹å³å¯ç”¨ | å¯èƒ½ä¸é©åˆç‰¹å®šä»»å‹™<br>ç²¾åº¦å—é™ | é€šç”¨ä»»å‹™<br>å¿«é€ŸåŸå‹ |\n",
    "| **å¾®èª¿** âœ… | æ•¸æ“šéœ€æ±‚å°‘ (åƒç´šå³å¯)<br>è¨“ç·´å¿« (åˆ†é˜åˆ°å°æ™‚)<br>ç²¾åº¦é«˜ | ä¾è³´é è¨“ç·´æ¨¡å‹ | **å¤§å¤šæ•¸å¯¦éš›å ´æ™¯** |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”§ ç’°å¢ƒæº–å‚™\n",
    "\n",
    "### å®‰è£å¿…è¦å¥—ä»¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install transformers datasets evaluate accelerate -U\n",
    "# !pip install scikit-learn numpy pandas matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "import evaluate\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "import random\n",
    "import torch\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"âœ… PyTorch version: {torch.__version__}\")\n",
    "print(f\"âœ… CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"âœ… GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“Š æ•¸æ“šæº–å‚™: AG News æ–°èåˆ†é¡\n",
    "\n",
    "### æ•¸æ“šé›†ä»‹ç´¹\n",
    "\n",
    "**AG News** æ˜¯ç¶“å…¸çš„æ–‡æœ¬åˆ†é¡æ•¸æ“šé›†:\n",
    "- **é¡åˆ¥æ•¸**: 4 (ä¸–ç•Œã€é«”è‚²ã€å•†æ¥­ã€ç§‘æŠ€)\n",
    "- **è¨“ç·´é›†**: 120,000 ç­†æ–°è\n",
    "- **æ¸¬è©¦é›†**: 7,600 ç­†æ–°è\n",
    "- **ä¾†æº**: AG's news corpus\n",
    "\n",
    "### åŠ è¼‰æ•¸æ“š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load AG News dataset from Hugging Face\n",
    "print(\"ğŸ“¥ Loading AG News dataset...\")\n",
    "dataset = load_dataset(\"ag_news\")\n",
    "\n",
    "print(f\"\\nâœ… Dataset loaded successfully!\")\n",
    "print(f\"\\nDataset structure:\")\n",
    "print(dataset)\n",
    "\n",
    "# Check sample\n",
    "print(f\"\\nğŸ“Œ Sample from training set:\")\n",
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create label mapping\n",
    "label_names = ['World', 'Sports', 'Business', 'Sci/Tech']\n",
    "id2label = {i: label for i, label in enumerate(label_names)}\n",
    "label2id = {label: i for i, label in enumerate(label_names)}\n",
    "\n",
    "print(\"ğŸ“‹ Label Mapping:\")\n",
    "for idx, name in id2label.items():\n",
    "    print(f\"  {idx}: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore dataset statistics\n",
    "train_df = pd.DataFrame(dataset['train'])\n",
    "test_df = pd.DataFrame(dataset['test'])\n",
    "\n",
    "print(f\"\\nğŸ“Š Dataset Statistics:\")\n",
    "print(f\"  Train samples: {len(train_df):,}\")\n",
    "print(f\"  Test samples: {len(test_df):,}\")\n",
    "print(f\"  Total samples: {len(train_df) + len(test_df):,}\")\n",
    "\n",
    "# Class distribution\n",
    "print(f\"\\nğŸ“ˆ Class Distribution (Training):\")\n",
    "class_counts = train_df['label'].value_counts().sort_index()\n",
    "for idx, count in class_counts.items():\n",
    "    print(f\"  {id2label[idx]}: {count:,} ({count/len(train_df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Training set\n",
    "train_counts = train_df['label'].map(id2label).value_counts()\n",
    "axes[0].bar(train_counts.index, train_counts.values, color='steelblue')\n",
    "axes[0].set_title('Training Set - Class Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Category')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Test set\n",
    "test_counts = test_df['label'].map(id2label).value_counts()\n",
    "axes[1].bar(test_counts.index, test_counts.values, color='coral')\n",
    "axes[1].set_title('Test Set - Class Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Category')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ•¸æ“šé è™•ç†èˆ‡ Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For demonstration, use a smaller subset to speed up training\n",
    "# Remove this in production for full dataset training\n",
    "SAMPLE_SIZE = 10000  # Use 10k samples for quick training\n",
    "USE_FULL_DATASET = False  # Set to True for production\n",
    "\n",
    "if not USE_FULL_DATASET:\n",
    "    print(f\"âš ï¸  Using subset of {SAMPLE_SIZE:,} samples for quick training\")\n",
    "    dataset['train'] = dataset['train'].shuffle(seed=SEED).select(range(SAMPLE_SIZE))\n",
    "    dataset['test'] = dataset['test'].shuffle(seed=SEED).select(range(SAMPLE_SIZE // 10))\n",
    "    print(f\"âœ… Train: {len(dataset['train']):,} | Test: {len(dataset['test']):,}\")\n",
    "else:\n",
    "    print(f\"âœ… Using full dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "MODEL_NAME = \"distilbert-base-uncased\"  # Fast and efficient for classification\n",
    "print(f\"ğŸ“¦ Loading tokenizer: {MODEL_NAME}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "print(f\"âœ… Tokenizer loaded: {tokenizer.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tokenization function\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"\n",
    "    Tokenize text data\n",
    "    Args:\n",
    "        examples: batch of text samples\n",
    "    Returns:\n",
    "        tokenized inputs with padding and truncation\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=128  # Adjust based on your data\n",
    "    )\n",
    "\n",
    "# Apply tokenization to dataset\n",
    "print(\"ğŸ”„ Tokenizing dataset...\")\n",
    "tokenized_datasets = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=['text']  # Remove original text column\n",
    ")\n",
    "\n",
    "print(\"âœ… Tokenization completed!\")\n",
    "print(f\"\\nTokenized dataset structure:\")\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect tokenized sample\n",
    "print(\"\\nğŸ“Œ Tokenized sample:\")\n",
    "sample = tokenized_datasets['train'][0]\n",
    "print(f\"Input IDs shape: {len(sample['input_ids'])}\")\n",
    "print(f\"Attention mask shape: {len(sample['attention_mask'])}\")\n",
    "print(f\"Label: {sample['label']} ({id2label[sample['label']]})\")\n",
    "\n",
    "# Decode back to text\n",
    "decoded_text = tokenizer.decode(sample['input_ids'], skip_special_tokens=True)\n",
    "print(f\"\\nDecoded text (first 100 chars):\\n{decoded_text[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¤– æ¨¡å‹æº–å‚™\n",
    "\n",
    "### åŠ è¼‰é è¨“ç·´æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model for sequence classification\n",
    "print(f\"ğŸ“¦ Loading model: {MODEL_NAME}\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(label_names),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "print(f\"âœ… Model loaded successfully!\")\n",
    "print(f\"\\nModel architecture: {model.__class__.__name__}\")\n",
    "print(f\"Number of parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check model structure\n",
    "print(\"\\nğŸ” Model Structure:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ˆ è©•ä¼°æŒ‡æ¨™è¨­å®š\n",
    "\n",
    "### å®šç¾©è©•ä¼°å‡½æ•¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metrics\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute evaluation metrics during training\n",
    "    Args:\n",
    "        eval_pred: predictions and labels\n",
    "    Returns:\n",
    "        dict of metrics\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels, average='weighted')\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy['accuracy'],\n",
    "        'f1': f1['f1']\n",
    "    }\n",
    "\n",
    "print(\"âœ… Metrics configured: Accuracy, F1-score (weighted)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ è¨“ç·´é…ç½®: TrainingArguments\n",
    "\n",
    "### è¶…åƒæ•¸èªªæ˜\n",
    "\n",
    "| åƒæ•¸ | èªªæ˜ | å»ºè­°å€¼ |\n",
    "|------|------|--------|\n",
    "| `output_dir` | æ¨¡å‹èˆ‡æª¢æŸ¥é»ä¿å­˜è·¯å¾‘ | `./results` |\n",
    "| `num_train_epochs` | è¨“ç·´è¼ªæ•¸ | 3-5 |\n",
    "| `per_device_train_batch_size` | æ¯å€‹ GPU çš„è¨“ç·´æ‰¹æ¬¡å¤§å° | 8-32 |\n",
    "| `per_device_eval_batch_size` | æ¯å€‹ GPU çš„è©•ä¼°æ‰¹æ¬¡å¤§å° | 16-64 |\n",
    "| `learning_rate` | å­¸ç¿’ç‡ | 2e-5 ~ 5e-5 |\n",
    "| `weight_decay` | æ¬Šé‡è¡°æ¸› (L2 æ­£å‰‡åŒ–) | 0.01 |\n",
    "| `warmup_steps` | å­¸ç¿’ç‡é ç†±æ­¥æ•¸ | 500 |\n",
    "| `logging_steps` | æ—¥èªŒè¨˜éŒ„é–“éš” | 100 |\n",
    "| `evaluation_strategy` | è©•ä¼°ç­–ç•¥ | `epoch` / `steps` |\n",
    "| `save_strategy` | ä¿å­˜ç­–ç•¥ | `epoch` / `steps` |\n",
    "| `load_best_model_at_end` | è¨“ç·´çµæŸè¼‰å…¥æœ€ä½³æ¨¡å‹ | `True` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results/ag_news_finetuned\",\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=500,\n",
    "    \n",
    "    # Evaluation\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    \n",
    "    # Logging\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    \n",
    "    # Other settings\n",
    "    seed=SEED,\n",
    "    push_to_hub=False,  # Set to True if you want to push to Hugging Face Hub\n",
    ")\n",
    "\n",
    "print(\"âœ… Training arguments configured\")\n",
    "print(f\"\\nğŸ“‹ Key settings:\")\n",
    "print(f\"  - Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  - Batch size (train): {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  - Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  - Evaluation strategy: {training_args.evaluation_strategy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸš€ é–‹å§‹è¨“ç·´\n",
    "\n",
    "### åˆå§‹åŒ– Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['test'],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]  # Stop if no improvement for 2 epochs\n",
    ")\n",
    "\n",
    "print(\"âœ… Trainer initialized with early stopping (patience=2)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"\\nğŸš€ Starting training...\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"âœ… Training completed!\\n\")\n",
    "\n",
    "# Print training summary\n",
    "print(\"ğŸ“Š Training Summary:\")\n",
    "print(f\"  Total training time: {train_result.metrics['train_runtime']:.2f} seconds\")\n",
    "print(f\"  Training samples/second: {train_result.metrics['train_samples_per_second']:.2f}\")\n",
    "print(f\"  Final training loss: {train_result.metrics['train_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“Š æ¨¡å‹è©•ä¼°\n",
    "\n",
    "### æ¸¬è©¦é›†è©•ä¼°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"\\nğŸ” Evaluating model on test set...\\n\")\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"\\nğŸ“ˆ Evaluation Results:\")\n",
    "for key, value in eval_results.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è©³ç´°åˆ†é¡å ±å‘Šèˆ‡æ··æ·†çŸ©é™£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "predictions = trainer.predict(tokenized_datasets['test'])\n",
    "pred_labels = np.argmax(predictions.predictions, axis=1)\n",
    "true_labels = predictions.label_ids\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nğŸ“‹ Classification Report:\\n\")\n",
    "print(classification_report(\n",
    "    true_labels,\n",
    "    pred_labels,\n",
    "    target_names=label_names,\n",
    "    digits=4\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(true_labels, pred_labels)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='Blues',\n",
    "    xticklabels=label_names,\n",
    "    yticklabels=label_names\n",
    ")\n",
    "plt.title('Confusion Matrix - AG News Classification', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ’¾ æ¨¡å‹ä¿å­˜èˆ‡åŠ è¼‰\n",
    "\n",
    "### ä¿å­˜å¾®èª¿å¾Œçš„æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and tokenizer\n",
    "model_save_path = \"./models/ag_news_classifier\"\n",
    "\n",
    "print(f\"ğŸ’¾ Saving model to {model_save_path}...\")\n",
    "trainer.save_model(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "print(f\"âœ… Model and tokenizer saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### åŠ è¼‰å·²ä¿å­˜çš„æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved model\n",
    "from transformers import pipeline\n",
    "\n",
    "print(f\"ğŸ“‚ Loading model from {model_save_path}...\")\n",
    "classifier = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=model_save_path,\n",
    "    tokenizer=model_save_path\n",
    ")\n",
    "\n",
    "print(\"âœ… Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ® å¯¦éš›æ‡‰ç”¨ç¯„ä¾‹\n",
    "\n",
    "### æ¸¬è©¦è‡ªè¨‚æ–°èæ–‡æœ¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with custom news articles\n",
    "test_articles = [\n",
    "    \"Apple releases new iPhone with advanced AI features and improved camera.\",\n",
    "    \"The Lakers defeated the Warriors 112-108 in last night's NBA game.\",\n",
    "    \"Global stock markets surge as inflation data shows signs of cooling.\",\n",
    "    \"Scientists discover potential breakthrough in quantum computing technology.\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ¯ Testing on custom news articles:\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i, article in enumerate(test_articles, 1):\n",
    "    result = classifier(article)[0]\n",
    "    \n",
    "    print(f\"\\nğŸ“° Article {i}:\")\n",
    "    print(f\"   Text: {article}\")\n",
    "    print(f\"   Predicted: {result['label']} (Confidence: {result['score']:.2%})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ”§ è¶…åƒæ•¸èª¿å„ªé€²éšæŠ€å·§\n",
    "\n",
    "### 1. å­¸ç¿’ç‡èª¿æ•´ç­–ç•¥\n",
    "\n",
    "```python\n",
    "# Linear warmup + linear decay\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "num_training_steps = len(train_dataloader) * num_epochs\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=500,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "```\n",
    "\n",
    "### 2. æ¢¯åº¦ç´¯ç© (Gradient Accumulation)\n",
    "\n",
    "ç•¶ GPU è¨˜æ†¶é«”ä¸è¶³æ™‚:\n",
    "\n",
    "```python\n",
    "training_args = TrainingArguments(\n",
    "    ...,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=4,  # Effective batch size = 8 * 4 = 32\n",
    ")\n",
    "```\n",
    "\n",
    "### 3. æ··åˆç²¾åº¦è¨“ç·´ (Mixed Precision)\n",
    "\n",
    "åŠ é€Ÿè¨“ç·´ä¸¦ç¯€çœè¨˜æ†¶é«”:\n",
    "\n",
    "```python\n",
    "training_args = TrainingArguments(\n",
    "    ...,\n",
    "    fp16=True,  # Enable mixed precision on compatible GPUs\n",
    ")\n",
    "```\n",
    "\n",
    "### 4. è¶…åƒæ•¸æœç´¢ (Hyperparameter Search)\n",
    "\n",
    "è‡ªå‹•å°‹æ‰¾æœ€ä½³è¶…åƒæ•¸:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter search example (commented out - computationally expensive)\n",
    "\"\"\"\n",
    "def model_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=len(label_names),\n",
    "        id2label=id2label,\n",
    "        label2id=label2id\n",
    "    )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['test'],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Search space\n",
    "def hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 5e-5, log=True),\n",
    "        \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 2, 5),\n",
    "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [8, 16, 32]),\n",
    "    }\n",
    "\n",
    "# Run hyperparameter search\n",
    "best_run = trainer.hyperparameter_search(\n",
    "    direction=\"maximize\",\n",
    "    backend=\"optuna\",\n",
    "    hp_space=hp_space,\n",
    "    n_trials=10\n",
    ")\n",
    "\n",
    "print(f\"Best hyperparameters: {best_run.hyperparameters}\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"ğŸ’¡ Hyperparameter search example provided (commented out)\")\n",
    "print(\"   Requires: pip install optuna\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“š ç¸½çµèˆ‡æœ€ä½³å¯¦è¸\n",
    "\n",
    "### âœ… ä½ å­¸åˆ°äº†ä»€éº¼\n",
    "\n",
    "1. **é·ç§»å­¸ç¿’èˆ‡å¾®èª¿æ¦‚å¿µ**: ç†è§£ç‚ºä½•å¾®èª¿å„ªæ–¼å¾é ­è¨“ç·´\n",
    "2. **å®Œæ•´å¾®èª¿æµç¨‹**:\n",
    "   - æ•¸æ“šåŠ è¼‰èˆ‡é è™•ç†\n",
    "   - Tokenization\n",
    "   - æ¨¡å‹é…ç½®\n",
    "   - è¨“ç·´èˆ‡è©•ä¼°\n",
    "   - æ¨¡å‹ä¿å­˜èˆ‡éƒ¨ç½²\n",
    "3. **Trainer API ä½¿ç”¨**: ç°¡åŒ–è¨“ç·´æµç¨‹çš„å¼·å¤§å·¥å…·\n",
    "4. **è¶…åƒæ•¸èª¿å„ª**: å­¸ç¿’ç‡ã€Batch sizeã€Early stopping ç­‰\n",
    "5. **æ¨¡å‹è©•ä¼°**: Accuracy, F1-score, Confusion matrix\n",
    "\n",
    "### ğŸ¯ å¾®èª¿æœ€ä½³å¯¦è¸\n",
    "\n",
    "| æœ€ä½³å¯¦è¸ | èªªæ˜ | åŸå›  |\n",
    "|---------|------|------|\n",
    "| **ä½¿ç”¨é è¨“ç·´æ¨¡å‹** | å¾ BERTã€DistilBERTã€RoBERTa ç­‰é–‹å§‹ | ç¯€çœè¨“ç·´æ™‚é–“èˆ‡è³‡æº |\n",
    "| **è¼ƒå°å­¸ç¿’ç‡** | 2e-5 ~ 5e-5 | é¿å…ç ´å£é è¨“ç·´æ¬Šé‡ |\n",
    "| **å°‘é‡ Epochs** | 2-5 epochs | é¿å…éæ“¬åˆ |\n",
    "| **Early Stopping** | ç›£æ§é©—è­‰é›†è¡¨ç¾ | è‡ªå‹•åœæ­¢é˜²æ­¢éæ“¬åˆ |\n",
    "| **æ•¸æ“šå¢å¼·** | åŒç¾©è©æ›¿æ›ã€å›è­¯ | æå‡æ¨¡å‹æ³›åŒ–èƒ½åŠ› |\n",
    "| **é¡åˆ¥å¹³è¡¡** | è™•ç†ä¸å¹³è¡¡æ•¸æ“š | é¿å…åå‘å¤šæ•¸é¡ |\n",
    "| **ä¿å­˜æª¢æŸ¥é»** | å®šæœŸä¿å­˜ | é¿å…è¨“ç·´ä¸­æ–·æå¤± |\n",
    "\n",
    "### ğŸš€ ä¸‹ä¸€æ­¥\n",
    "\n",
    "1. **å˜—è©¦ä¸åŒçš„é è¨“ç·´æ¨¡å‹**:\n",
    "   - `bert-base-uncased`\n",
    "   - `roberta-base`\n",
    "   - `xlnet-base-cased`\n",
    "\n",
    "2. **æ‡‰ç”¨åˆ°ä½ è‡ªå·±çš„æ•¸æ“š**:\n",
    "   - æº–å‚™æ¨™æ³¨æ•¸æ“š\n",
    "   - èª¿æ•´ tokenization ç­–ç•¥\n",
    "   - å¯¦é©—ä¸åŒè¶…åƒæ•¸\n",
    "\n",
    "3. **éƒ¨ç½²æ¨¡å‹**:\n",
    "   - ä½¿ç”¨ FastAPI å»ºç«‹ API\n",
    "   - Docker å®¹å™¨åŒ–\n",
    "   - éƒ¨ç½²åˆ°é›²ç«¯ (AWS, GCP, Azure)\n",
    "\n",
    "4. **é€²éšå„ªåŒ–**:\n",
    "   - æ¨¡å‹é‡åŒ– (Quantization)\n",
    "   - çŸ¥è­˜è’¸é¤¾ (Knowledge Distillation)\n",
    "   - ONNX è½‰æ›åŠ é€Ÿæ¨ç†\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”— åƒè€ƒè³‡æº\n",
    "\n",
    "- [Hugging Face Transformers æ–‡æª”](https://huggingface.co/docs/transformers/)\n",
    "- [Trainer API æŒ‡å—](https://huggingface.co/docs/transformers/main_classes/trainer)\n",
    "- [AG News Dataset](https://huggingface.co/datasets/ag_news)\n",
    "- [Fine-tuning Best Practices](https://huggingface.co/docs/transformers/training)\n",
    "\n",
    "---\n",
    "\n",
    "**ä¸‹ä¸€ç¯€**: `09_å°ˆæ¡ˆå¯¦æˆ°_å®¢æˆ¶æ„è¦‹åˆ†æå„€.ipynb` - å®Œæ•´çš„ç«¯åˆ°ç«¯å°ˆæ¡ˆå¯¦æˆ° ğŸ¯"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
