{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH08-06: 文本摘要 (Summarization)\n",
    "\n",
    "**課程**: iSpan Python NLP Cookbooks v2\n",
    "**章節**: CH08 Hugging Face 函式庫實戰\n",
    "**版本**: v1.0\n",
    "**更新日期**: 2025-10-17\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 本節學習目標\n",
    "\n",
    "1. 理解抽取式 vs 生成式摘要的差異\n",
    "2. 使用 BART/T5/Pegasus 生成摘要\n",
    "3. 掌握摘要參數調整技巧\n",
    "4. 實作多文檔摘要\n",
    "5. 評估摘要質量 (ROUGE 指標)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. 文本摘要基礎\n",
    "\n",
    "### 1.1 摘要類型\n",
    "\n",
    "**抽取式摘要 (Extractive)**:\n",
    "- 從原文中挑選重要句子\n",
    "- 保留原文表達\n",
    "- 不產生新內容\n",
    "\n",
    "**生成式摘要 (Abstractive)**:\n",
    "- 理解原文後重新生成\n",
    "- 可能產生新詞彙\n",
    "- 更接近人類摘要\n",
    "\n",
    "```\n",
    "原文:\n",
    "\"The Transformer architecture has revolutionized NLP. \n",
    "It introduced self-attention mechanisms that allow models \n",
    "to process sequences in parallel.\"\n",
    "\n",
    "抽取式: \"The Transformer architecture has revolutionized NLP.\"\n",
    "\n",
    "生成式: \"Transformers changed NLP with self-attention.\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安裝套件\n",
    "# !pip install transformers torch rouge-score -q\n",
    "\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"✅ 環境準備完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. 使用預訓練模型\n",
    "\n",
    "### 2.1 BART 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入 BART 摘要模型\n",
    "summarizer_bart = pipeline(\n",
    "    \"summarization\",\n",
    "    model=\"facebook/bart-large-cnn\",\n",
    "    device=-1\n",
    ")\n",
    "\n",
    "# 測試文本\n",
    "article = \"\"\"\n",
    "The Transformer architecture, introduced in the paper \"Attention Is All You Need\" \n",
    "by Vaswani et al. in 2017, has revolutionized natural language processing. \n",
    "Unlike previous architectures that relied on recurrent or convolutional layers, \n",
    "Transformers use self-attention mechanisms to process input sequences in parallel. \n",
    "This parallel processing capability makes Transformers significantly faster to train \n",
    "than RNNs. The architecture consists of an encoder and a decoder, each composed of \n",
    "multiple layers of self-attention and feed-forward networks. The self-attention \n",
    "mechanism allows the model to weigh the importance of different words in a sentence \n",
    "when encoding each word. This has proven to be extremely effective for a wide range \n",
    "of NLP tasks, from translation to text generation.\n",
    "\"\"\"\n",
    "\n",
    "# 生成摘要\n",
    "summary = summarizer_bart(\n",
    "    article,\n",
    "    max_length=60,\n",
    "    min_length=30,\n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "print(f\"原文 ({len(article.split())} 詞):\")\n",
    "print(article.strip())\n",
    "print(f\"\\n摘要 ({len(summary[0]['summary_text'].split())} 詞):\")\n",
    "print(summary[0]['summary_text'])\n",
    "print(f\"\\n壓縮比: {len(summary[0]['summary_text'].split())/len(article.split()):.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 T5 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T5 (Text-to-Text Transfer Transformer)\n",
    "summarizer_t5 = pipeline(\n",
    "    \"summarization\",\n",
    "    model=\"t5-small\",\n",
    "    device=-1\n",
    ")\n",
    "\n",
    "# 使用相同文本\n",
    "summary_t5 = summarizer_t5(\n",
    "    article,\n",
    "    max_length=60,\n",
    "    min_length=30\n",
    ")\n",
    "\n",
    "print(\"T5 摘要:\")\n",
    "print(summary_t5[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 模型對比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    (\"BART-large-CNN\", \"facebook/bart-large-cnn\"),\n",
    "    (\"T5-small\", \"t5-small\"),\n",
    "    (\"Pegasus-CNN\", \"google/pegasus-cnn_dailymail\")\n",
    "]\n",
    "\n",
    "print(\"不同模型摘要對比:\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for name, model_name in models:\n",
    "    try:\n",
    "        summarizer = pipeline(\"summarization\", model=model_name, device=-1)\n",
    "        result = summarizer(article, max_length=50, min_length=25)\n",
    "        \n",
    "        print(f\"\\n{name}:\")\n",
    "        print(result[0]['summary_text'])\n",
    "    except Exception as e:\n",
    "        print(f\"\\n{name}: 載入失敗 ({str(e)[:50]}...)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. 參數調整\n",
    "\n",
    "### 3.1 長度控制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 測試不同長度設定\n",
    "length_configs = [\n",
    "    {\"max_length\": 30, \"min_length\": 20, \"name\": \"短摘要\"},\n",
    "    {\"max_length\": 60, \"min_length\": 40, \"name\": \"中摘要\"},\n",
    "    {\"max_length\": 100, \"min_length\": 70, \"name\": \"長摘要\"}\n",
    "]\n",
    "\n",
    "print(\"不同長度摘要對比:\\n\")\n",
    "\n",
    "for config in length_configs:\n",
    "    summary = summarizer_bart(\n",
    "        article,\n",
    "        max_length=config['max_length'],\n",
    "        min_length=config['min_length'],\n",
    "        do_sample=False\n",
    "    )\n",
    "    \n",
    "    text = summary[0]['summary_text']\n",
    "    print(f\"{config['name']} ({len(text.split())} 詞):\")\n",
    "    print(text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 採樣策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beam Search vs Sampling\n",
    "configs = [\n",
    "    {\"do_sample\": False, \"num_beams\": 4, \"name\": \"Beam Search (4)\"},\n",
    "    {\"do_sample\": True, \"top_k\": 50, \"top_p\": 0.95, \"name\": \"Top-K & Top-P Sampling\"},\n",
    "    {\"do_sample\": True, \"temperature\": 0.8, \"name\": \"Temperature Sampling\"}\n",
    "]\n",
    "\n",
    "print(\"不同採樣策略對比:\\n\")\n",
    "\n",
    "for config in configs:\n",
    "    name = config.pop('name')\n",
    "    \n",
    "    summary = summarizer_bart(\n",
    "        article,\n",
    "        max_length=50,\n",
    "        min_length=30,\n",
    "        **config\n",
    "    )\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(summary[0]['summary_text'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. 實戰應用\n",
    "\n",
    "### 4.1 新聞摘要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_article = \"\"\"\n",
    "Apple Inc. announced record-breaking quarterly earnings today, with revenue \n",
    "reaching $120 billion, surpassing analyst expectations. The company's CEO \n",
    "Tim Cook attributed the success to strong iPhone sales and growth in the \n",
    "services sector. Apple's stock price rose 5% in after-hours trading following \n",
    "the announcement. The company also revealed plans to invest $10 billion in \n",
    "new product development over the next year, focusing on augmented reality \n",
    "and artificial intelligence technologies. Analysts predict continued growth \n",
    "for Apple in the coming quarters, citing strong consumer demand and a robust \n",
    "product pipeline.\n",
    "\"\"\"\n",
    "\n",
    "summary = summarizer_bart(\n",
    "    news_article,\n",
    "    max_length=50,\n",
    "    min_length=25,\n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "print(\"📰 新聞摘要生成\\n\")\n",
    "print(\"原文:\")\n",
    "print(news_article.strip())\n",
    "print(f\"\\n摘要:\")\n",
    "print(summary[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 批次處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多篇文章批次摘要\n",
    "articles = [\n",
    "    \"Tesla announced a new electric vehicle model today...\",\n",
    "    \"Scientists discovered a new exoplanet in the habitable zone...\",\n",
    "    \"The stock market reached new highs amid positive economic data...\"\n",
    "]\n",
    "\n",
    "# 批次處理\n",
    "summaries = summarizer_bart(\n",
    "    articles,\n",
    "    max_length=30,\n",
    "    min_length=15,\n",
    "    batch_size=3\n",
    ")\n",
    "\n",
    "print(\"批次摘要結果:\\n\")\n",
    "for i, (article, summary) in enumerate(zip(articles, summaries), 1):\n",
    "    print(f\"{i}. 原文: {article}\")\n",
    "    print(f\"   摘要: {summary['summary_text']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 多文檔摘要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多個相關文檔的統一摘要\n",
    "docs = [\n",
    "    \"Apple released the new iPhone 15 with advanced AI features.\",\n",
    "    \"The iPhone 15 includes a new A17 chip and improved camera system.\",\n",
    "    \"Apple's new phone has been well-received by tech reviewers.\"\n",
    "]\n",
    "\n",
    "# 合併文檔\n",
    "combined_text = \" \".join(docs)\n",
    "\n",
    "# 生成摘要\n",
    "summary = summarizer_bart(\n",
    "    combined_text,\n",
    "    max_length=40,\n",
    "    min_length=20\n",
    ")\n",
    "\n",
    "print(\"多文檔摘要:\\n\")\n",
    "print(\"文檔 1:\", docs[0])\n",
    "print(\"文檔 2:\", docs[1])\n",
    "print(\"文檔 3:\", docs[2])\n",
    "print(f\"\\n統一摘要: {summary[0]['summary_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. 摘要質量評估\n",
    "\n",
    "### 5.1 ROUGE 指標"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# 參考摘要 (人工標註)\n",
    "reference = \"Transformers revolutionized NLP with self-attention mechanisms for parallel processing.\"\n",
    "\n",
    "# 生成摘要\n",
    "generated = summarizer_bart(article, max_length=30, min_length=15)[0]['summary_text']\n",
    "\n",
    "# 計算 ROUGE\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "scores = scorer.score(reference, generated)\n",
    "\n",
    "print(\"ROUGE 評估結果:\\n\")\n",
    "print(f\"參考摘要: {reference}\")\n",
    "print(f\"生成摘要: {generated}\\n\")\n",
    "\n",
    "for metric, score in scores.items():\n",
    "    print(f\"{metric}:\")\n",
    "    print(f\"  Precision: {score.precision:.4f}\")\n",
    "    print(f\"  Recall:    {score.recall:.4f}\")\n",
    "    print(f\"  F1:        {score.fmeasure:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ROUGE 指標說明**:\n",
    "- **ROUGE-1**: Unigram (單詞) 重疊\n",
    "- **ROUGE-2**: Bigram (雙詞) 重疊\n",
    "- **ROUGE-L**: 最長公共子序列 (LCS)\n",
    "\n",
    "### 5.2 長度分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分析不同長度的摘要質量\n",
    "import pandas as pd\n",
    "\n",
    "max_lengths = [20, 30, 40, 50, 60]\n",
    "results = []\n",
    "\n",
    "for max_len in max_lengths:\n",
    "    summary = summarizer_bart(\n",
    "        article, \n",
    "        max_length=max_len, \n",
    "        min_length=max_len-10\n",
    "    )[0]['summary_text']\n",
    "    \n",
    "    scores = scorer.score(reference, summary)\n",
    "    \n",
    "    results.append({\n",
    "        'Max Length': max_len,\n",
    "        'Actual Length': len(summary.split()),\n",
    "        'ROUGE-1': scores['rouge1'].fmeasure,\n",
    "        'ROUGE-2': scores['rouge2'].fmeasure,\n",
    "        'ROUGE-L': scores['rougeL'].fmeasure\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "print(\"長度 vs ROUGE 分數:\\n\")\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# 繪圖\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df['Max Length'], df['ROUGE-1'], marker='o', label='ROUGE-1')\n",
    "plt.plot(df['Max Length'], df['ROUGE-2'], marker='s', label='ROUGE-2')\n",
    "plt.plot(df['Max Length'], df['ROUGE-L'], marker='^', label='ROUGE-L')\n",
    "plt.xlabel('Max Summary Length', fontsize=12)\n",
    "plt.ylabel('ROUGE F1 Score', fontsize=12)\n",
    "plt.title('Summary Length vs ROUGE Scores', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. 進階技巧\n",
    "\n",
    "### 6.1 抽取式摘要 (TextRank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 簡易 TextRank 實作\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "import nltk\n",
    "\n",
    "# nltk.download('punkt')\n",
    "\n",
    "def extractive_summarize(text, num_sentences=3):\n",
    "    # 分句\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    \n",
    "    if len(sentences) <= num_sentences:\n",
    "        return text\n",
    "    \n",
    "    # TF-IDF 向量化\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "    \n",
    "    # 計算相似度\n",
    "    similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "    \n",
    "    # PageRank\n",
    "    nx_graph = nx.from_numpy_array(similarity_matrix)\n",
    "    scores = nx.pagerank(nx_graph)\n",
    "    \n",
    "    # 選擇 top-k 句子\n",
    "    ranked = sorted(((scores[i], s) for i, s in enumerate(sentences)), reverse=True)\n",
    "    top_sentences = [s for _, s in ranked[:num_sentences]]\n",
    "    \n",
    "    # 按原順序排列\n",
    "    summary = [s for s in sentences if s in top_sentences]\n",
    "    \n",
    "    return ' '.join(summary)\n",
    "\n",
    "# 測試\n",
    "extractive = extractive_summarize(article, num_sentences=2)\n",
    "abstractive = summarizer_bart(article, max_length=50, min_length=30)[0]['summary_text']\n",
    "\n",
    "print(\"抽取式 vs 生成式對比:\\n\")\n",
    "print(\"抽取式摘要:\")\n",
    "print(extractive)\n",
    "print(f\"\\n生成式摘要:\")\n",
    "print(abstractive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 自訂摘要風格"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 Prompt 控制摘要風格 (需要支援的模型)\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name = \"t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# T5 需要加前綴\n",
    "input_text = \"summarize: \" + article\n",
    "\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "outputs = model.generate(\n",
    "    inputs.input_ids,\n",
    "    max_length=50,\n",
    "    min_length=25,\n",
    "    num_beams=4,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"T5 生成摘要:\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. 課後練習\n",
    "\n",
    "### 練習 1: 會議紀錄摘要\n",
    "\n",
    "將會議記錄轉換為簡潔的行動項目摘要。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 實作會議紀錄摘要\n",
    "meeting_transcript = \"\"\"\n",
    "[長篇會議記錄]\n",
    "\"\"\"\n",
    "\n",
    "# 提取:\n",
    "# 1. 主要決策\n",
    "# 2. 行動項目\n",
    "# 3. 責任人"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習 2: 多文檔主題摘要\n",
    "\n",
    "從多篇相關新聞中提取共同主題摘要。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 實作多文檔主題摘要\n",
    "# 提示:\n",
    "# 1. 識別共同主題\n",
    "# 2. 合併相關信息\n",
    "# 3. 生成統一摘要"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. 本節總結\n",
    "\n",
    "### ✅ 關鍵要點\n",
    "\n",
    "1. **摘要類型**: 抽取式 (選句) vs 生成式 (重寫)\n",
    "2. **模型選擇**: BART (新聞), T5 (通用), Pegasus (摘要專用)\n",
    "3. **參數調整**: max_length, min_length, num_beams, do_sample\n",
    "4. **評估指標**: ROUGE-1/2/L (與參考摘要比較)\n",
    "\n",
    "### 📊 模型效能對比\n",
    "\n",
    "| 模型 | 參數量 | ROUGE-L | 速度 | 適用場景 |\n",
    "|------|--------|---------|------|----------|\n",
    "| BART-large | 406M | ~44 | 中 | 新聞、長文 |\n",
    "| T5-base | 220M | ~42 | 快 | 通用文本 |\n",
    "| Pegasus | 568M | ~47 | 慢 | 專業摘要 |\n",
    "\n",
    "### 📚 延伸閱讀\n",
    "\n",
    "- [BART 論文](https://arxiv.org/abs/1910.13461)\n",
    "- [T5 論文](https://arxiv.org/abs/1910.10683)\n",
    "- [ROUGE 評估指標](https://aclanthology.org/W04-1013/)\n",
    "\n",
    "### 🚀 下一節預告\n",
    "\n",
    "**CH08-07: 文本生成 (Text Generation)**\n",
    "- GPT-2/GPT-3 文本生成\n",
    "- 生成策略 (Top-K, Top-P, Beam Search)\n",
    "- 控制生成內容\n",
    "\n",
    "---\n",
    "\n",
    "**課程**: iSpan Python NLP Cookbooks v2\n",
    "**講師**: Claude AI\n",
    "**最後更新**: 2025-10-17"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
