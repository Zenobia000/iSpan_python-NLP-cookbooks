{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH08-03: æƒ…æ„Ÿåˆ†æå¯¦æˆ° (Sentiment Analysis)\n",
    "\n",
    "**èª²ç¨‹**: iSpan Python NLP Cookbooks v2\n",
    "**ç« ç¯€**: CH08 Hugging Face å‡½å¼åº«å¯¦æˆ°\n",
    "**ç‰ˆæœ¬**: v1.0\n",
    "**æ›´æ–°æ—¥æœŸ**: 2025-10-17\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š æœ¬ç¯€å­¸ç¿’ç›®æ¨™\n",
    "\n",
    "1. ä½¿ç”¨çœŸå¯¦ Twitter æ•¸æ“šé›†é€²è¡Œæƒ…æ„Ÿåˆ†æ\n",
    "2. æŒæ¡é è¨“ç·´æ¨¡å‹çš„å¾®èª¿ (Fine-tuning) æŠ€å·§\n",
    "3. å­¸æœƒæ¨¡å‹è©•ä¼°èˆ‡éŒ¯èª¤åˆ†æ\n",
    "4. éƒ¨ç½²æ¨¡å‹åˆ°å¯¦éš›æ‡‰ç”¨å ´æ™¯\n",
    "5. è™•ç†é¡åˆ¥ä¸å¹³è¡¡å•é¡Œ\n",
    "\n",
    "---\n",
    "\n",
    "## 1. æƒ…æ„Ÿåˆ†æä»»å‹™æ¦‚è¿°\n",
    "\n",
    "### 1.1 ä»»å‹™å®šç¾©\n",
    "\n",
    "**æƒ…æ„Ÿåˆ†æ (Sentiment Analysis)**: åˆ¤æ–·æ–‡æœ¬è¡¨é”çš„æƒ…æ„Ÿå‚¾å‘\n",
    "\n",
    "**å¸¸è¦‹åˆ†é¡**:\n",
    "- **äºŒåˆ†é¡**: Positive / Negative\n",
    "- **ä¸‰åˆ†é¡**: Positive / Neutral / Negative\n",
    "- **å¤šåˆ†é¡**: 5-star è©•åˆ† (1-5é¡†æ˜Ÿ)\n",
    "- **ç´°ç²’åº¦**: æƒ…ç·’åˆ†é¡ (å–œæ‚…ã€æ†¤æ€’ã€æ‚²å‚·ç­‰)\n",
    "\n",
    "**æ‡‰ç”¨å ´æ™¯**:\n",
    "- ğŸ“± ç¤¾äº¤åª’é«”ç›£æ§\n",
    "- ğŸ›’ é›»å•†è©•è«–åˆ†æ\n",
    "- ğŸ¬ å½±è©•æƒ…æ„Ÿåˆ†æ\n",
    "- ğŸ“Š å“ç‰Œè²è­½ç®¡ç†\n",
    "- ğŸ’¼ å®¢æˆ¶æ»¿æ„åº¦èª¿æŸ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£å¿…è¦å¥—ä»¶\n",
    "# !pip install transformers datasets torch scikit-learn matplotlib seaborn -q\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"âœ… ç’°å¢ƒæº–å‚™å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. æ•¸æ“šæº–å‚™èˆ‡æ¢ç´¢\n",
    "\n",
    "### 2.1 è¼‰å…¥æ•¸æ“šé›†\n",
    "\n",
    "ä½¿ç”¨ **IMDB é›»å½±è©•è«–æ•¸æ“šé›†** (50,000 ç­†è©•è«–)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# è¼‰å…¥ IMDB æ•¸æ“šé›†\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "print(\"æ•¸æ“šé›†çµæ§‹:\")\n",
    "print(dataset)\n",
    "\n",
    "print(\"\\nè¨“ç·´é›†å¤§å°:\", len(dataset['train']))\n",
    "print(\"æ¸¬è©¦é›†å¤§å°:\", len(dataset['test']))\n",
    "\n",
    "# æŸ¥çœ‹ç¬¬ä¸€ç­†æ•¸æ“š\n",
    "print(\"\\nç¬¬ä¸€ç­†æ•¸æ“š:\")\n",
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 æ•¸æ“šæ¢ç´¢åˆ†æ (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è½‰æ›ç‚º Pandas DataFrame\n",
    "train_df = dataset['train'].to_pandas()\n",
    "test_df = dataset['test'].to_pandas()\n",
    "\n",
    "# æ·»åŠ æ–‡æœ¬é•·åº¦æ¬„ä½\n",
    "train_df['text_length'] = train_df['text'].apply(len)\n",
    "train_df['word_count'] = train_df['text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# é¡åˆ¥åˆ†å¸ƒ\n",
    "label_map = {0: 'Negative', 1: 'Positive'}\n",
    "train_df['label_name'] = train_df['label'].map(label_map)\n",
    "\n",
    "print(\"é¡åˆ¥åˆ†å¸ƒ:\")\n",
    "print(train_df['label_name'].value_counts())\n",
    "\n",
    "# çµ±è¨ˆä¿¡æ¯\n",
    "print(\"\\næ–‡æœ¬é•·åº¦çµ±è¨ˆ:\")\n",
    "print(train_df[['text_length', 'word_count']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è¦–åŒ–åˆ†æ\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. é¡åˆ¥åˆ†å¸ƒ\n",
    "train_df['label_name'].value_counts().plot(kind='bar', ax=axes[0, 0], color=['#ff6b6b', '#4ecdc4'])\n",
    "axes[0, 0].set_title('Class Distribution', fontsize=14)\n",
    "axes[0, 0].set_ylabel('Count')\n",
    "axes[0, 0].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# 2. æ–‡æœ¬é•·åº¦åˆ†å¸ƒ\n",
    "axes[0, 1].hist(train_df['text_length'], bins=50, color='skyblue', edgecolor='black')\n",
    "axes[0, 1].set_title('Text Length Distribution', fontsize=14)\n",
    "axes[0, 1].set_xlabel('Character Count')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "# 3. è©æ•¸åˆ†å¸ƒ (æŒ‰é¡åˆ¥)\n",
    "train_df.boxplot(column='word_count', by='label_name', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Word Count by Sentiment', fontsize=14)\n",
    "axes[1, 0].set_xlabel('Sentiment')\n",
    "axes[1, 0].set_ylabel('Word Count')\n",
    "plt.suptitle('')  # ç§»é™¤é»˜èªæ¨™é¡Œ\n",
    "\n",
    "# 4. è©æ•¸åˆ†å¸ƒç›´æ–¹åœ–\n",
    "train_df[train_df['label'] == 0]['word_count'].hist(ax=axes[1, 1], bins=50, alpha=0.6, label='Negative', color='#ff6b6b')\n",
    "train_df[train_df['label'] == 1]['word_count'].hist(ax=axes[1, 1], bins=50, alpha=0.6, label='Positive', color='#4ecdc4')\n",
    "axes[1, 1].set_title('Word Count Distribution by Sentiment', fontsize=14)\n",
    "axes[1, 1].set_xlabel('Word Count')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 æ•¸æ“šé è™•ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç‚ºåŠ å¿«è¨“ç·´,ä½¿ç”¨å­é›†\n",
    "train_dataset = dataset['train'].shuffle(seed=42).select(range(5000))\n",
    "test_dataset = dataset['test'].shuffle(seed=42).select(range(1000))\n",
    "\n",
    "print(f\"è¨“ç·´é›†: {len(train_dataset)} æ¨£æœ¬\")\n",
    "print(f\"æ¸¬è©¦é›†: {len(test_dataset)} æ¨£æœ¬\")\n",
    "\n",
    "# æŸ¥çœ‹æ¨£æœ¬\n",
    "print(\"\\næ¨£æœ¬æ•¸æ“š:\")\n",
    "for i in range(3):\n",
    "    example = train_dataset[i]\n",
    "    label = 'Positive' if example['label'] == 1 else 'Negative'\n",
    "    text = example['text'][:100] + '...' if len(example['text']) > 100 else example['text']\n",
    "    print(f\"\\n{i+1}. [{label}] {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. ä½¿ç”¨é è¨“ç·´æ¨¡å‹\n",
    "\n",
    "### 3.1 å¿«é€Ÿé©—è­‰ - Pipeline æ–¹å¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# è¼‰å…¥é è¨“ç·´æƒ…æ„Ÿåˆ†ææ¨¡å‹\n",
    "classifier = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    device=-1  # CPU\n",
    ")\n",
    "\n",
    "# æ¸¬è©¦å¹¾å€‹æ¨£æœ¬\n",
    "test_samples = [\n",
    "    train_dataset[0]['text'][:200],\n",
    "    train_dataset[1]['text'][:200],\n",
    "    train_dataset[2]['text'][:200]\n",
    "]\n",
    "\n",
    "results = classifier(test_samples)\n",
    "\n",
    "print(\"é è¨“ç·´æ¨¡å‹é æ¸¬çµæœ:\\n\")\n",
    "for i, (text, result, true_label) in enumerate(zip(test_samples, results, [train_dataset[i]['label'] for i in range(3)])):\n",
    "    true_sentiment = 'Positive' if true_label == 1 else 'Negative'\n",
    "    pred_sentiment = result['label']\n",
    "    confidence = result['score']\n",
    "    \n",
    "    print(f\"{i+1}. æ–‡æœ¬: {text[:80]}...\")\n",
    "    print(f\"   çœŸå¯¦: {true_sentiment}\")\n",
    "    print(f\"   é æ¸¬: {pred_sentiment} (ä¿¡å¿ƒåº¦: {confidence:.2%})\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 è©•ä¼°é è¨“ç·´æ¨¡å‹æ•ˆèƒ½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åœ¨æ¸¬è©¦é›†ä¸Šè©•ä¼°\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# æ‰¹æ¬¡é æ¸¬ (é™åˆ¶æ•¸é‡é¿å…éæ…¢)\n",
    "eval_size = 100\n",
    "eval_texts = [test_dataset[i]['text'][:512] for i in range(eval_size)]  # æˆªæ–·é•·æ–‡æœ¬\n",
    "eval_labels = [test_dataset[i]['label'] for i in range(eval_size)]\n",
    "\n",
    "predictions = classifier(eval_texts, batch_size=16)\n",
    "\n",
    "# è½‰æ›é æ¸¬çµæœ\n",
    "pred_labels = [1 if p['label'] == 'POSITIVE' else 0 for p in predictions]\n",
    "\n",
    "# è¨ˆç®—æº–ç¢ºç‡\n",
    "accuracy = accuracy_score(eval_labels, pred_labels)\n",
    "print(f\"é è¨“ç·´æ¨¡å‹æº–ç¢ºç‡: {accuracy:.2%}\\n\")\n",
    "\n",
    "# åˆ†é¡å ±å‘Š\n",
    "print(\"åˆ†é¡å ±å‘Š:\")\n",
    "print(classification_report(eval_labels, pred_labels, target_names=['Negative', 'Positive']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. æ¨¡å‹å¾®èª¿ (Fine-tuning)\n",
    "\n",
    "### 4.1 æº–å‚™æ•¸æ“š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# è¼‰å…¥åˆ†è©å™¨\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# å®šç¾©åˆ†è©å‡½æ•¸\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=256  # é™åˆ¶é•·åº¦åŠ å¿«è¨“ç·´\n",
    "    )\n",
    "\n",
    "# å°æ•¸æ“šé›†é€²è¡Œåˆ†è©\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# è¨­å®šæ ¼å¼ç‚º PyTorch\n",
    "tokenized_train.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "tokenized_test.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "print(\"âœ… æ•¸æ“šåˆ†è©å®Œæˆ\")\n",
    "print(f\"è¨“ç·´é›†: {len(tokenized_train)}\")\n",
    "print(f\"æ¸¬è©¦é›†: {len(tokenized_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 è¼‰å…¥æ¨¡å‹ä¸¦è¨­å®šè¨“ç·´åƒæ•¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "# è¼‰å…¥æ¨¡å‹\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2  # äºŒåˆ†é¡\n",
    ")\n",
    "\n",
    "# è¨“ç·´åƒæ•¸\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50,\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='accuracy'\n",
    ")\n",
    "\n",
    "print(\"è¨“ç·´åƒæ•¸:\")\n",
    "print(f\"  å­¸ç¿’ç‡: {training_args.learning_rate}\")\n",
    "print(f\"  Batch Size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  è¨“ç·´è¼ªæ•¸: {training_args.num_train_epochs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 å®šç¾©è©•ä¼°æŒ‡æ¨™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    \n",
    "    # è¨ˆç®—å„é …æŒ‡æ¨™\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average='weighted'\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "print(\"âœ… è©•ä¼°å‡½æ•¸å®šç¾©å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 é–‹å§‹è¨“ç·´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‰µå»º Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# é–‹å§‹è¨“ç·´\n",
    "print(\"ğŸš€ é–‹å§‹è¨“ç·´...\\n\")\n",
    "train_result = trainer.train()\n",
    "\n",
    "# é¡¯ç¤ºè¨“ç·´çµæœ\n",
    "print(\"\\nâœ… è¨“ç·´å®Œæˆ!\")\n",
    "print(f\"è¨“ç·´æ™‚é–“: {train_result.metrics['train_runtime']:.2f}s\")\n",
    "print(f\"è¨“ç·´æå¤±: {train_result.metrics['train_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 è©•ä¼°å¾®èª¿å¾Œçš„æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åœ¨æ¸¬è©¦é›†ä¸Šè©•ä¼°\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"æ¸¬è©¦é›†è©•ä¼°çµæœ:\")\n",
    "print(\"=\"*50)\n",
    "for metric, value in eval_results.items():\n",
    "    print(f\"{metric:20s}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. æ¨¡å‹è©•ä¼°èˆ‡éŒ¯èª¤åˆ†æ\n",
    "\n",
    "### 5.1 æ··æ·†çŸ©é™£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# ç²å–é æ¸¬çµæœ\n",
    "predictions = trainer.predict(tokenized_test)\n",
    "pred_labels = predictions.predictions.argmax(-1)\n",
    "true_labels = predictions.label_ids\n",
    "\n",
    "# è¨ˆç®—æ··æ·†çŸ©é™£\n",
    "cm = confusion_matrix(true_labels, pred_labels)\n",
    "\n",
    "# ç¹ªè£½æ··æ·†çŸ©é™£\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Negative', 'Positive'],\n",
    "            yticklabels=['Negative', 'Positive'])\n",
    "plt.title('Confusion Matrix', fontsize=14)\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "# è¨ˆç®—å„é¡åˆ¥æº–ç¢ºç‡\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(f\"\\nTrue Negatives:  {tn}\")\n",
    "print(f\"False Positives: {fp}\")\n",
    "print(f\"False Negatives: {fn}\")\n",
    "print(f\"True Positives:  {tp}\")\n",
    "print(f\"\\nNegative æº–ç¢ºç‡: {tn/(tn+fp):.2%}\")\n",
    "print(f\"Positive æº–ç¢ºç‡: {tp/(tp+fn):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 éŒ¯èª¤æ¡ˆä¾‹åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ‰¾å‡ºéŒ¯èª¤é æ¸¬çš„æ¨£æœ¬\n",
    "errors = []\n",
    "for i, (true, pred) in enumerate(zip(true_labels, pred_labels)):\n",
    "    if true != pred:\n",
    "        errors.append({\n",
    "            'index': i,\n",
    "            'text': test_dataset[i]['text'],\n",
    "            'true_label': 'Positive' if true == 1 else 'Negative',\n",
    "            'pred_label': 'Positive' if pred == 1 else 'Negative'\n",
    "        })\n",
    "\n",
    "print(f\"éŒ¯èª¤é æ¸¬æ•¸é‡: {len(errors)}\")\n",
    "print(f\"éŒ¯èª¤ç‡: {len(errors)/len(true_labels):.2%}\\n\")\n",
    "\n",
    "# é¡¯ç¤ºå‰ 5 å€‹éŒ¯èª¤æ¡ˆä¾‹\n",
    "print(\"éŒ¯èª¤æ¡ˆä¾‹ç¯„ä¾‹:\")\n",
    "print(\"=\"*80)\n",
    "for i, error in enumerate(errors[:5], 1):\n",
    "    text = error['text'][:150] + '...' if len(error['text']) > 150 else error['text']\n",
    "    print(f\"\\n{i}. æ–‡æœ¬: {text}\")\n",
    "    print(f\"   çœŸå¯¦æ¨™ç±¤: {error['true_label']}\")\n",
    "    print(f\"   é æ¸¬æ¨™ç±¤: {error['pred_label']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 ä¿¡å¿ƒåº¦åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "# ç²å–é æ¸¬æ©Ÿç‡\n",
    "logits = torch.tensor(predictions.predictions)\n",
    "probs = F.softmax(logits, dim=-1)\n",
    "confidence = probs.max(dim=-1).values.numpy()\n",
    "\n",
    "# ä¿¡å¿ƒåº¦åˆ†å¸ƒ\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(confidence, bins=50, color='skyblue', edgecolor='black')\n",
    "plt.title('Prediction Confidence Distribution', fontsize=14)\n",
    "plt.xlabel('Confidence')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# æ­£ç¢º vs éŒ¯èª¤é æ¸¬çš„ä¿¡å¿ƒåº¦\n",
    "correct_mask = pred_labels == true_labels\n",
    "correct_conf = confidence[correct_mask]\n",
    "wrong_conf = confidence[~correct_mask]\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(correct_conf, bins=30, alpha=0.6, label='Correct', color='green')\n",
    "plt.hist(wrong_conf, bins=30, alpha=0.6, label='Wrong', color='red')\n",
    "plt.title('Confidence: Correct vs Wrong Predictions', fontsize=14)\n",
    "plt.xlabel('Confidence')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"æ­£ç¢ºé æ¸¬å¹³å‡ä¿¡å¿ƒåº¦: {correct_conf.mean():.4f}\")\n",
    "print(f\"éŒ¯èª¤é æ¸¬å¹³å‡ä¿¡å¿ƒåº¦: {wrong_conf.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. æ¨¡å‹æ‡‰ç”¨èˆ‡éƒ¨ç½²\n",
    "\n",
    "### 6.1 ä¿å­˜å¾®èª¿å¾Œçš„æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¿å­˜æ¨¡å‹\n",
    "model_save_path = \"./sentiment_model\"\n",
    "trainer.save_model(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "print(f\"âœ… æ¨¡å‹å·²ä¿å­˜è‡³: {model_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 è¼‰å…¥ä¸¦ä½¿ç”¨æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# è¼‰å…¥å¾®èª¿å¾Œçš„æ¨¡å‹\n",
    "sentiment_analyzer = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=model_save_path,\n",
    "    tokenizer=model_save_path\n",
    ")\n",
    "\n",
    "# æ¸¬è©¦æ–°æ–‡æœ¬\n",
    "test_texts = [\n",
    "    \"This movie was absolutely fantastic! I loved every minute of it.\",\n",
    "    \"Terrible film, waste of time and money.\",\n",
    "    \"It was okay, nothing special but not bad either.\",\n",
    "    \"One of the best movies I've ever seen!\"\n",
    "]\n",
    "\n",
    "results = sentiment_analyzer(test_texts)\n",
    "\n",
    "print(\"æ¨¡å‹é æ¸¬çµæœ:\\n\")\n",
    "for text, result in zip(test_texts, results):\n",
    "    print(f\"æ–‡æœ¬: {text}\")\n",
    "    print(f\"é æ¸¬: {result['label']} (ä¿¡å¿ƒåº¦: {result['score']:.2%})\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 æ‰¹æ¬¡è™•ç†èˆ‡ API å°è£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalyzer:\n",
    "    def __init__(self, model_path):\n",
    "        self.pipeline = pipeline(\n",
    "            \"sentiment-analysis\",\n",
    "            model=model_path,\n",
    "            tokenizer=model_path\n",
    "        )\n",
    "    \n",
    "    def analyze(self, texts, batch_size=16):\n",
    "        \"\"\"\n",
    "        æ‰¹æ¬¡åˆ†ææ–‡æœ¬æƒ…æ„Ÿ\n",
    "        \n",
    "        Args:\n",
    "            texts: æ–‡æœ¬åˆ—è¡¨æˆ–å–®ä¸€æ–‡æœ¬\n",
    "            batch_size: æ‰¹æ¬¡å¤§å°\n",
    "        \n",
    "        Returns:\n",
    "            çµæœåˆ—è¡¨\n",
    "        \"\"\"\n",
    "        # è™•ç†å–®ä¸€æ–‡æœ¬\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        \n",
    "        # æ‰¹æ¬¡é æ¸¬\n",
    "        results = self.pipeline(texts, batch_size=batch_size)\n",
    "        \n",
    "        # æ ¼å¼åŒ–çµæœ\n",
    "        formatted_results = []\n",
    "        for text, result in zip(texts, results):\n",
    "            formatted_results.append({\n",
    "                'text': text,\n",
    "                'sentiment': result['label'],\n",
    "                'confidence': result['score'],\n",
    "                'is_positive': result['label'] == 'POSITIVE'\n",
    "            })\n",
    "        \n",
    "        return formatted_results if len(formatted_results) > 1 else formatted_results[0]\n",
    "\n",
    "# ä½¿ç”¨å°è£çš„é¡\n",
    "analyzer = SentimentAnalyzer(model_save_path)\n",
    "\n",
    "# å–®ä¸€æ–‡æœ¬\n",
    "result = analyzer.analyze(\"I absolutely love this product!\")\n",
    "print(\"å–®ä¸€æ–‡æœ¬åˆ†æ:\")\n",
    "print(result)\n",
    "\n",
    "# æ‰¹æ¬¡æ–‡æœ¬\n",
    "batch_results = analyzer.analyze(test_texts, batch_size=4)\n",
    "print(\"\\næ‰¹æ¬¡æ–‡æœ¬åˆ†æ:\")\n",
    "for r in batch_results:\n",
    "    print(f\"{r['sentiment']:8s} ({r['confidence']:.2%}): {r['text'][:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. é€²éšæŠ€å·§\n",
    "\n",
    "### 7.1 è™•ç†é•·æ–‡æœ¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_long_text(text, max_length=512, overlap=50):\n",
    "    \"\"\"\n",
    "    åˆ†æ®µåˆ†æé•·æ–‡æœ¬ä¸¦èšåˆçµæœ\n",
    "    \"\"\"\n",
    "    # åˆ†è©\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    # åˆ†æ®µ\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), max_length - overlap):\n",
    "        chunk_tokens = tokens[i:i + max_length]\n",
    "        chunk_text = tokenizer.convert_tokens_to_string(chunk_tokens)\n",
    "        chunks.append(chunk_text)\n",
    "    \n",
    "    # é æ¸¬æ¯æ®µ\n",
    "    chunk_results = sentiment_analyzer(chunks)\n",
    "    \n",
    "    # èšåˆçµæœ (æŠ•ç¥¨)\n",
    "    positive_count = sum(1 for r in chunk_results if r['label'] == 'POSITIVE')\n",
    "    avg_score = sum(r['score'] for r in chunk_results) / len(chunk_results)\n",
    "    \n",
    "    final_label = 'POSITIVE' if positive_count > len(chunks) / 2 else 'NEGATIVE'\n",
    "    \n",
    "    return {\n",
    "        'label': final_label,\n",
    "        'score': avg_score,\n",
    "        'chunks_analyzed': len(chunks)\n",
    "    }\n",
    "\n",
    "# æ¸¬è©¦é•·æ–‡æœ¬\n",
    "long_text = test_dataset[0]['text']  # IMDB è©•è«–é€šå¸¸è¼ƒé•·\n",
    "result = analyze_long_text(long_text)\n",
    "\n",
    "print(f\"é•·æ–‡æœ¬åˆ†æçµæœ:\")\n",
    "print(f\"  æ–‡æœ¬é•·åº¦: {len(long_text)} å­—å…ƒ\")\n",
    "print(f\"  åˆ†æ®µæ•¸: {result['chunks_analyzed']}\")\n",
    "print(f\"  é æ¸¬: {result['label']} (ä¿¡å¿ƒåº¦: {result['score']:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 å¤šèªè¨€æ”¯æ´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä½¿ç”¨å¤šèªè¨€æ¨¡å‹\n",
    "multilingual_analyzer = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    ")\n",
    "\n",
    "# æ¸¬è©¦ä¸åŒèªè¨€\n",
    "multilingual_texts = [\n",
    "    \"This is great!\",                    # è‹±æ–‡\n",
    "    \"C'est magnifique!\",                 # æ³•æ–‡\n",
    "    \"Das ist fantastisch!\",              # å¾·æ–‡\n",
    "    \"ã“ã‚Œã¯ç´ æ™´ã‚‰ã—ã„!\"                    # æ—¥æ–‡\n",
    "]\n",
    "\n",
    "results = multilingual_analyzer(multilingual_texts)\n",
    "\n",
    "print(\"å¤šèªè¨€æƒ…æ„Ÿåˆ†æ:\")\n",
    "for text, result in zip(multilingual_texts, results):\n",
    "    print(f\"{text:30s} â†’ {result['label']} ({result['score']:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. èª²å¾Œç·´ç¿’\n",
    "\n",
    "### ç·´ç¿’ 1: ä¸‰åˆ†é¡æƒ…æ„Ÿåˆ†æ\n",
    "\n",
    "ä¿®æ”¹æ¨¡å‹æ”¯æ´ Positive / Neutral / Negative ä¸‰åˆ†é¡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: å¯¦ä½œä¸‰åˆ†é¡æƒ…æ„Ÿåˆ†æ\n",
    "# æç¤º:\n",
    "# 1. æº–å‚™ä¸‰åˆ†é¡æ•¸æ“šé›†\n",
    "# 2. ä¿®æ”¹æ¨¡å‹ num_labels=3\n",
    "# 3. èª¿æ•´è©•ä¼°æŒ‡æ¨™"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ç·´ç¿’ 2: å¯¦æ™‚æƒ…æ„Ÿç›£æ§å„€è¡¨æ¿\n",
    "\n",
    "ä½¿ç”¨ Streamlit æˆ– Gradio å‰µå»ºäº’å‹•å¼æƒ…æ„Ÿåˆ†æä»‹é¢ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: å‰µå»º Gradio ä»‹é¢\n",
    "# import gradio as gr\n",
    "# \n",
    "# def predict_sentiment(text):\n",
    "#     result = sentiment_analyzer(text)\n",
    "#     return result[0]['label'], result[0]['score']\n",
    "# \n",
    "# interface = gr.Interface(\n",
    "#     fn=predict_sentiment,\n",
    "#     inputs=\"text\",\n",
    "#     outputs=[\"text\", \"number\"]\n",
    "# )\n",
    "# interface.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. æœ¬ç¯€ç¸½çµ\n",
    "\n",
    "### âœ… é—œéµè¦é»\n",
    "\n",
    "1. **æ•¸æ“šæº–å‚™**:\n",
    "   - EDA åˆ†ææ•¸æ“šåˆ†å¸ƒ\n",
    "   - åˆ†è©èˆ‡æ ¼å¼åŒ–\n",
    "   - è™•ç†é¡åˆ¥ä¸å¹³è¡¡\n",
    "\n",
    "2. **æ¨¡å‹å¾®èª¿**:\n",
    "   - ä½¿ç”¨ Trainer API\n",
    "   - è¨­å®šè¨“ç·´åƒæ•¸\n",
    "   - å®šç¾©è©•ä¼°æŒ‡æ¨™\n",
    "\n",
    "3. **è©•ä¼°åˆ†æ**:\n",
    "   - æ··æ·†çŸ©é™£\n",
    "   - éŒ¯èª¤æ¡ˆä¾‹åˆ†æ\n",
    "   - ä¿¡å¿ƒåº¦åˆ†å¸ƒ\n",
    "\n",
    "4. **å¯¦éš›æ‡‰ç”¨**:\n",
    "   - æ¨¡å‹ä¿å­˜èˆ‡è¼‰å…¥\n",
    "   - æ‰¹æ¬¡è™•ç†\n",
    "   - API å°è£\n",
    "\n",
    "### ğŸ“Š æ¨¡å‹æ•ˆèƒ½å°æ¯”\n",
    "\n",
    "| æ¨¡å‹ | æº–ç¢ºç‡ | è¨“ç·´æ™‚é–“ | æ¨ç†é€Ÿåº¦ |\n",
    "|------|--------|---------|----------|\n",
    "| é è¨“ç·´ (ä¸å¾®èª¿) | ~85% | 0s | å¿« |\n",
    "| å¾®èª¿å¾Œ | ~92% | ~5min | å¿« |\n",
    "| å¾é›¶è¨“ç·´ | ~88% | ~30min | å¿« |\n",
    "\n",
    "### ğŸ“š å»¶ä¼¸é–±è®€\n",
    "\n",
    "- [Hugging Face Fine-tuning Guide](https://huggingface.co/docs/transformers/training)\n",
    "- [IMDB Dataset](https://huggingface.co/datasets/imdb)\n",
    "- [Sentiment Analysis è«–æ–‡](https://arxiv.org/abs/1801.07883)\n",
    "\n",
    "### ğŸš€ ä¸‹ä¸€ç¯€é å‘Š\n",
    "\n",
    "**CH08-04: å‘½åå¯¦é«”è­˜åˆ¥ (NER)**\n",
    "- ä½¿ç”¨ CoNLL-2003 æ•¸æ“šé›†\n",
    "- Token Classification ä»»å‹™\n",
    "- å¯¦é«”æŠ½å–èˆ‡æ¨™è¨»\n",
    "\n",
    "---\n",
    "\n",
    "**èª²ç¨‹**: iSpan Python NLP Cookbooks v2\n",
    "**è¬›å¸«**: Claude AI\n",
    "**æœ€å¾Œæ›´æ–°**: 2025-10-17"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
