{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH08-07: æ–‡æœ¬ç”Ÿæˆ (Text Generation)\n",
    "\n",
    "**èª²ç¨‹**: iSpan Python NLP Cookbooks v2\n",
    "**ç« ç¯€**: CH08 Hugging Face å‡½å¼åº«å¯¦æˆ°\n",
    "**ç‰ˆæœ¬**: v1.0\n",
    "**æ›´æ–°æ—¥æœŸ**: 2025-10-17\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š æœ¬ç¯€å­¸ç¿’ç›®æ¨™\n",
    "\n",
    "1. ç†è§£è‡ªå›æ­¸æ–‡æœ¬ç”ŸæˆåŸç†\n",
    "2. æŒæ¡ GPT-2 æ–‡æœ¬ç”ŸæˆæŠ€å·§\n",
    "3. å­¸ç¿’ç”Ÿæˆç­–ç•¥ (Greedy, Beam Search, Sampling)\n",
    "4. æ§åˆ¶ç”Ÿæˆè³ªé‡èˆ‡å¤šæ¨£æ€§\n",
    "5. æ‡‰ç”¨æ–¼å¯¦éš›å ´æ™¯ (æ•…äº‹çºŒå¯«ã€å°è©±ç”Ÿæˆ)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. è‡ªå›æ­¸ç”ŸæˆåŸç†\n",
    "\n",
    "### 1.1 ç”Ÿæˆéç¨‹\n",
    "\n",
    "```\n",
    "è‡ªå›æ­¸ç”Ÿæˆ (Autoregressive Generation):\n",
    "\n",
    "çµ¦å®šæç¤ºè© (Prompt): \"Once upon a time\"\n",
    "\n",
    "Step 1: P(w4 | \"Once upon a time\") â†’ \"there\"\n",
    "Step 2: P(w5 | \"Once upon a time there\") â†’ \"was\"\n",
    "Step 3: P(w6 | \"Once upon a time there was\") â†’ \"a\"\n",
    "...\n",
    "\n",
    "é‡è¤‡ç›´åˆ°:\n",
    "- ç”Ÿæˆ <EOS> token\n",
    "- é”åˆ°æœ€å¤§é•·åº¦\n",
    "```\n",
    "\n",
    "### 1.2 ç”Ÿæˆç­–ç•¥å°æ¯”\n",
    "\n",
    "| ç­–ç•¥ | èªªæ˜ | å„ªé» | ç¼ºé» |\n",
    "|------|------|------|------|\n",
    "| **Greedy** | æ¯æ­¥é¸æœ€é«˜æ©Ÿç‡è© | å¿«é€Ÿã€ç¢ºå®š | é‡è¤‡ã€ç„¡å‰µæ„ |\n",
    "| **Beam Search** | ä¿ç•™ k å€‹å€™é¸åºåˆ— | é«˜è³ªé‡ | è¼ƒæ…¢ã€ä»å¯èƒ½é‡è¤‡ |\n",
    "| **Top-K** | å¾å‰ k å€‹è©ä¸­æ¡æ¨£ | å¤šæ¨£æ€§ | å¯èƒ½ä¸é€£è²« |\n",
    "| **Top-P (Nucleus)** | ç´¯ç©æ©Ÿç‡é” p çš„è©ä¸­æ¡æ¨£ | å¹³è¡¡è³ªé‡èˆ‡å¤šæ¨£æ€§ | éœ€èª¿æ•´ p |\n",
    "| **Temperature** | èª¿æ•´æ©Ÿç‡åˆ†å¸ƒå¹³æ»‘åº¦ | æ§åˆ¶å‰µæ„åº¦ | éé«˜æœƒæ··äº‚ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£å¥—ä»¶\n",
    "# !pip install transformers torch -q\n",
    "\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"âœ… ç’°å¢ƒæº–å‚™å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. åŸºç¤æ–‡æœ¬ç”Ÿæˆ\n",
    "\n",
    "### 2.1 ä½¿ç”¨ GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¼‰å…¥ GPT-2 ç”Ÿæˆå™¨\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"gpt2\",\n",
    "    device=-1\n",
    ")\n",
    "\n",
    "# æç¤ºè©\n",
    "prompt = \"Artificial intelligence is\"\n",
    "\n",
    "# ç”Ÿæˆæ–‡æœ¬\n",
    "results = generator(\n",
    "    prompt,\n",
    "    max_length=50,\n",
    "    num_return_sequences=3\n",
    ")\n",
    "\n",
    "print(f\"Prompt: \\\"{prompt}\\\"\\n\")\n",
    "print(\"ç”Ÿæˆçµæœ:\\n\")\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"{i}. {result['generated_text']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 ç”Ÿæˆåƒæ•¸è©³è§£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åƒæ•¸å®Œæ•´ç¤ºç¯„\n",
    "result = generator(\n",
    "    \"In the future, technology will\",\n",
    "    max_length=80,              # æœ€å¤§ç”Ÿæˆé•·åº¦\n",
    "    min_length=30,              # æœ€å°ç”Ÿæˆé•·åº¦\n",
    "    num_return_sequences=1,     # è¿”å›æ•¸é‡\n",
    "    temperature=0.8,            # æº«åº¦ (0.0-2.0)\n",
    "    top_k=50,                   # Top-K æ¡æ¨£\n",
    "    top_p=0.95,                 # Top-P (Nucleus) æ¡æ¨£\n",
    "    repetition_penalty=1.2,     # é‡è¤‡æ‡²ç½°\n",
    "    do_sample=True,             # å•Ÿç”¨æ¡æ¨£\n",
    "    num_beams=1,                # Beam Search (1=ä¸ä½¿ç”¨)\n",
    "    early_stopping=False        # æ—©åœ\n",
    ")\n",
    "\n",
    "print(\"ç”Ÿæˆçµæœ:\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. ç”Ÿæˆç­–ç•¥å¯¦é©—\n",
    "\n",
    "### 3.1 Temperature å½±éŸ¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¸¬è©¦ä¸åŒ temperature\n",
    "prompt = \"The future of AI is\"\n",
    "temperatures = [0.3, 0.7, 1.0, 1.5]\n",
    "\n",
    "print(\"Temperature å°ç”Ÿæˆçš„å½±éŸ¿:\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for temp in temperatures:\n",
    "    result = generator(\n",
    "        prompt,\n",
    "        max_length=40,\n",
    "        temperature=temp,\n",
    "        do_sample=True,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTemperature = {temp}:\")\n",
    "    print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Top-K vs Top-P å°æ¯”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Machine learning is\"\n",
    "\n",
    "configs = [\n",
    "    {\"do_sample\": False, \"name\": \"Greedy\"},\n",
    "    {\"do_sample\": True, \"top_k\": 10, \"name\": \"Top-K (k=10)\"},\n",
    "    {\"do_sample\": True, \"top_k\": 50, \"name\": \"Top-K (k=50)\"},\n",
    "    {\"do_sample\": True, \"top_p\": 0.9, \"name\": \"Top-P (p=0.9)\"},\n",
    "    {\"do_sample\": True, \"top_k\": 50, \"top_p\": 0.95, \"name\": \"Top-K + Top-P\"}\n",
    "]\n",
    "\n",
    "print(\"ç”Ÿæˆç­–ç•¥å°æ¯”:\\n\")\n",
    "\n",
    "for config in configs:\n",
    "    name = config.pop('name')\n",
    "    \n",
    "    result = generator(\n",
    "        prompt,\n",
    "        max_length=40,\n",
    "        num_return_sequences=1,\n",
    "        **config\n",
    "    )\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(result[0]['generated_text'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beam Search ç”Ÿæˆ\n",
    "result = generator(\n",
    "    \"The most important invention in history is\",\n",
    "    max_length=50,\n",
    "    num_beams=5,              # Beam æ•¸é‡\n",
    "    num_return_sequences=3,   # è¿”å›å‰ 3 å€‹åºåˆ—\n",
    "    early_stopping=True,\n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "print(\"Beam Search çµæœ (num_beams=5):\\n\")\n",
    "for i, r in enumerate(result, 1):\n",
    "    print(f\"{i}. {r['generated_text']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. å¯¦æˆ°æ‡‰ç”¨\n",
    "\n",
    "### 4.1 æ•…äº‹çºŒå¯«"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ•…äº‹é–‹é ­\n",
    "story_start = \"\"\"In a small village nestled in the mountains, \n",
    "there lived a young girl who discovered she had magical powers.\"\"\"\n",
    "\n",
    "# ç”ŸæˆçºŒå¯«\n",
    "continuation = generator(\n",
    "    story_start,\n",
    "    max_length=150,\n",
    "    temperature=0.9,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.2,\n",
    "    do_sample=True,\n",
    "    num_return_sequences=2\n",
    ")\n",
    "\n",
    "print(\"æ•…äº‹çºŒå¯«:\\n\")\n",
    "for i, story in enumerate(continuation, 1):\n",
    "    print(f\"ç‰ˆæœ¬ {i}:\")\n",
    "    print(story['generated_text'])\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 ä»£ç¢¼ç”Ÿæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä½¿ç”¨å°ˆé–€çš„ä»£ç¢¼æ¨¡å‹ (å¦‚æœå¯ç”¨)\n",
    "code_prompt = \"\"\"def fibonacci(n):\n",
    "    # Calculate fibonacci sequence\n",
    "\"\"\"\n",
    "\n",
    "result = generator(\n",
    "    code_prompt,\n",
    "    max_length=100,\n",
    "    temperature=0.2,  # ä½æº«åº¦ä¿æŒç¢ºå®šæ€§\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "print(\"ä»£ç¢¼ç”Ÿæˆ:\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 å°è©±ç”Ÿæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å°è©±å¼ç”Ÿæˆ\n",
    "conversation = \"\"\"User: What is machine learning?\n",
    "Assistant:\"\"\"\n",
    "\n",
    "response = generator(\n",
    "    conversation,\n",
    "    max_length=100,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "print(\"å°è©±ç”Ÿæˆ:\")\n",
    "print(response[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. æ§åˆ¶ç”Ÿæˆå…§å®¹\n",
    "\n",
    "### 5.1 é¿å…é‡è¤‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The key to success is\"\n",
    "\n",
    "# ä¸ä½¿ç”¨é‡è¤‡æ‡²ç½°\n",
    "result_no_penalty = generator(\n",
    "    prompt,\n",
    "    max_length=60,\n",
    "    repetition_penalty=1.0,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "# ä½¿ç”¨é‡è¤‡æ‡²ç½°\n",
    "result_with_penalty = generator(\n",
    "    prompt,\n",
    "    max_length=60,\n",
    "    repetition_penalty=1.5,  # å¼·çƒˆæ‡²ç½°é‡è¤‡\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "print(\"ä¸ä½¿ç”¨é‡è¤‡æ‡²ç½°:\")\n",
    "print(result_no_penalty[0]['generated_text'])\n",
    "print(\"\\nä½¿ç”¨é‡è¤‡æ‡²ç½° (1.5):\")\n",
    "print(result_with_penalty[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 é•·åº¦æ§åˆ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”Ÿæˆä¸åŒé•·åº¦\n",
    "lengths = [30, 60, 100]\n",
    "\n",
    "print(\"ä¸åŒé•·åº¦ç”Ÿæˆå°æ¯”:\\n\")\n",
    "\n",
    "for length in lengths:\n",
    "    result = generator(\n",
    "        \"Deep learning models are\",\n",
    "        max_length=length,\n",
    "        temperature=0.8,\n",
    "        do_sample=True\n",
    "    )\n",
    "    \n",
    "    text = result[0]['generated_text']\n",
    "    print(f\"max_length={length} ({len(text.split())} è©):\")\n",
    "    print(text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. é€²éš: æ¢ä»¶ç”Ÿæˆ\n",
    "\n",
    "### 6.1 åŸºæ–¼é—œéµè©ç”Ÿæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä½¿ç”¨é—œéµè©å¼•å°ç”Ÿæˆ\n",
    "keywords = [\"python\", \"data science\", \"tutorial\"]\n",
    "\n",
    "prompt = f\"Write a blog post about {', '.join(keywords)}:\\n\\n\"\n",
    "\n",
    "result = generator(\n",
    "    prompt,\n",
    "    max_length=120,\n",
    "    temperature=0.8,\n",
    "    top_p=0.95,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "print(\"é—œéµè©:\", keywords)\n",
    "print(\"\\nç”Ÿæˆæ–‡ç« :\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 é¢¨æ ¼é·ç§»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¸åŒé¢¨æ ¼çš„ç”Ÿæˆ\n",
    "base_content = \"Machine learning is a powerful technology.\"\n",
    "\n",
    "styles = [\n",
    "    \"Rewrite in simple words for beginners:\",\n",
    "    \"Rewrite in academic style:\",\n",
    "    \"Rewrite as a marketing pitch:\"\n",
    "]\n",
    "\n",
    "print(\"é¢¨æ ¼é·ç§»ç”Ÿæˆ:\\n\")\n",
    "\n",
    "for style in styles:\n",
    "    prompt = f\"{style} {base_content}\"\n",
    "    result = generator(\n",
    "        prompt,\n",
    "        max_length=80,\n",
    "        temperature=0.7,\n",
    "        do_sample=True\n",
    "    )\n",
    "    \n",
    "    print(f\"{style}\")\n",
    "    print(result[0]['generated_text'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. ç”Ÿæˆè³ªé‡è©•ä¼°\n",
    "\n",
    "### 7.1 å›°æƒ‘åº¦ (Perplexity)"
   ]
  },
  {
   "cell_type