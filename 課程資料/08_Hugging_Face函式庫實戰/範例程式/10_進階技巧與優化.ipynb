{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH08-10: é€²éšæŠ€å·§èˆ‡å„ªåŒ– (Advanced Techniques & Optimization)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š æœ¬ç¯€å­¸ç¿’ç›®æ¨™\n",
    "\n",
    "1. æŒæ¡**æ¨¡å‹é‡åŒ–** (Quantization) æŠ€è¡“\n",
    "2. å­¸ç¿’**æ¨ç†åŠ é€Ÿ**ç­–ç•¥\n",
    "3. äº†è§£**æ¨¡å‹å£“ç¸®**æ–¹æ³• (çŸ¥è­˜è’¸é¤¾)\n",
    "4. å¯¦ä½œ**æ‰¹é‡æ¨ç†**å„ªåŒ–\n",
    "5. å­¸ç¿’**ç”Ÿç”¢ç’°å¢ƒéƒ¨ç½²**æœ€ä½³å¯¦è¸\n",
    "6. ä½¿ç”¨ **ONNX** åŠ é€Ÿæ¨ç†\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ ç‚ºä»€éº¼éœ€è¦å„ªåŒ–ï¼Ÿ\n",
    "\n",
    "### ç”Ÿç”¢ç’°å¢ƒçš„æŒ‘æˆ°\n",
    "\n",
    "| æŒ‘æˆ° | èªªæ˜ | å„ªåŒ–ç›®æ¨™ |\n",
    "|------|------|----------|\n",
    "| **æ¨ç†é€Ÿåº¦æ…¢** | BERT ç³»åˆ—æ¨¡å‹åƒæ•¸é‡å¤§ (110M+) | æ¸›å°‘å»¶é²,æå‡ååé‡ |\n",
    "| **è¨˜æ†¶é«”å ç”¨é«˜** | å–®å€‹æ¨¡å‹å¯èƒ½éœ€è¦ 400MB+ RAM | é™ä½è¨˜æ†¶é«”ä½¿ç”¨ |\n",
    "| **æˆæœ¬é«˜æ˜‚** | GPU æ¨ç†æˆæœ¬ >> CPU | å¯¦ç¾ CPU é«˜æ•ˆæ¨ç† |\n",
    "| **æ“´å±•æ€§å·®** | å–® GPU è™•ç†èƒ½åŠ›æœ‰é™ | æå‡ä¸¦ç™¼è™•ç†èƒ½åŠ› |\n",
    "\n",
    "### å„ªåŒ–æŠ€è¡“å°æ¯”\n",
    "\n",
    "| æŠ€è¡“ | é€Ÿåº¦æå‡ | æ¨¡å‹å¤§å°æ¸›å°‘ | ç²¾åº¦æå¤± | é›£åº¦ |\n",
    "|------|---------|-------------|----------|------|\n",
    "| **é‡åŒ– (INT8)** | 2-4x | 4x | å¾®å° (~1%) | â­ ä½ |\n",
    "| **çŸ¥è­˜è’¸é¤¾** | 2-10x | è‡ªå®šç¾© | å° (2-5%) | â­â­â­ é«˜ |\n",
    "| **å‰ªæ (Pruning)** | 1.5-3x | 2-3x | ä¸­ç­‰ | â­â­ ä¸­ |\n",
    "| **ONNX è½‰æ›** | 1.5-2x | - | ç„¡ | â­â­ ä¸­ |\n",
    "| **æ‰¹é‡æ¨ç†** | ç·šæ€§æ“´å±• | - | ç„¡ | â­ ä½ |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”§ ç’°å¢ƒæº–å‚™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install optimization libraries\n",
    "# !pip install transformers torch optimum[onnxruntime]\n",
    "# !pip install onnx onnxruntime\n",
    "# !pip install psutil py-cpuinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import time\n",
    "import psutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    pipeline\n",
    ")\n",
    "from pathlib import Path\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ åŸºæº–æ¸¬è©¦ (Baseline Benchmark)\n",
    "\n",
    "### å»ºç«‹æ€§èƒ½æ¸¬è©¦å·¥å…·"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelBenchmark:\n",
    "    \"\"\"\n",
    "    Benchmark tool for model performance evaluation\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.results = []\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_model_size_mb(model):\n",
    "        \"\"\"\n",
    "        Calculate model size in MB\n",
    "        \"\"\"\n",
    "        param_size = sum(p.nelement() * p.element_size() for p in model.parameters())\n",
    "        buffer_size = sum(b.nelement() * b.element_size() for b in model.buffers())\n",
    "        return (param_size + buffer_size) / 1024 / 1024\n",
    "    \n",
    "    @staticmethod\n",
    "    def measure_inference_time(model, tokenizer, texts, warmup=5, iterations=50):\n",
    "        \"\"\"\n",
    "        Measure average inference time\n",
    "        Args:\n",
    "            model: model to benchmark\n",
    "            tokenizer: tokenizer\n",
    "            texts: list of input texts\n",
    "            warmup: number of warmup runs\n",
    "            iterations: number of measurement iterations\n",
    "        Returns:\n",
    "            dict with timing statistics\n",
    "        \"\"\"\n",
    "        # Warmup\n",
    "        for _ in range(warmup):\n",
    "            inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            with torch.no_grad():\n",
    "                _ = model(**inputs)\n",
    "        \n",
    "        # Measure\n",
    "        times = []\n",
    "        for _ in range(iterations):\n",
    "            inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            \n",
    "            start = time.perf_counter()\n",
    "            with torch.no_grad():\n",
    "                _ = model(**inputs)\n",
    "            end = time.perf_counter()\n",
    "            \n",
    "            times.append((end - start) * 1000)  # Convert to ms\n",
    "        \n",
    "        return {\n",
    "            'mean_ms': np.mean(times),\n",
    "            'std_ms': np.std(times),\n",
    "            'min_ms': np.min(times),\n",
    "            'max_ms': np.max(times),\n",
    "            'p50_ms': np.percentile(times, 50),\n",
    "            'p95_ms': np.percentile(times, 95),\n",
    "            'p99_ms': np.percentile(times, 99)\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def measure_memory_usage():\n",
    "        \"\"\"\n",
    "        Measure current memory usage\n",
    "        \"\"\"\n",
    "        process = psutil.Process()\n",
    "        mem_info = process.memory_info()\n",
    "        return mem_info.rss / 1024 / 1024  # MB\n",
    "    \n",
    "    def add_result(self, name, model, tokenizer, test_texts):\n",
    "        \"\"\"\n",
    "        Add benchmark result\n",
    "        \"\"\"\n",
    "        print(f\"\\nğŸ”„ Benchmarking: {name}...\")\n",
    "        \n",
    "        model_size = self.get_model_size_mb(model)\n",
    "        timing = self.measure_inference_time(model, tokenizer, test_texts)\n",
    "        memory = self.measure_memory_usage()\n",
    "        \n",
    "        result = {\n",
    "            'name': name,\n",
    "            'model_size_mb': model_size,\n",
    "            'memory_mb': memory,\n",
    "            **timing\n",
    "        }\n",
    "        \n",
    "        self.results.append(result)\n",
    "        \n",
    "        print(f\"   Model size: {model_size:.2f} MB\")\n",
    "        print(f\"   Avg latency: {timing['mean_ms']:.2f} ms\")\n",
    "        print(f\"   P95 latency: {timing['p95_ms']:.2f} ms\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def get_summary_df(self):\n",
    "        \"\"\"\n",
    "        Get summary DataFrame\n",
    "        \"\"\"\n",
    "        return pd.DataFrame(self.results)\n",
    "\n",
    "# Initialize benchmark\n",
    "benchmark = ModelBenchmark()\n",
    "print(\"âœ… Benchmark tool initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è¼‰å…¥åŸºæº–æ¨¡å‹ä¸¦æ¸¬è©¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load baseline model\n",
    "MODEL_NAME = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "print(f\"ğŸ“¦ Loading baseline model: {MODEL_NAME}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "print(\"âœ… Baseline model loaded\")\n",
    "\n",
    "# Prepare test texts\n",
    "test_texts = [\n",
    "    \"This product is amazing!\",\n",
    "    \"I'm very disappointed with the quality.\",\n",
    "    \"It's okay, nothing special.\"\n",
    "]\n",
    "\n",
    "# Benchmark baseline model\n",
    "baseline_result = benchmark.add_result(\"Baseline (FP32)\", model, tokenizer, test_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âš¡ å„ªåŒ–æŠ€è¡“ 1: å‹•æ…‹é‡åŒ– (Dynamic Quantization)\n",
    "\n",
    "### ä»€éº¼æ˜¯é‡åŒ–ï¼Ÿ\n",
    "\n",
    "å°‡æ¨¡å‹æ¬Šé‡å¾ **FP32** (32-bit float) è½‰æ›ç‚º **INT8** (8-bit integer):\n",
    "\n",
    "```\n",
    "FP32:  æ¯å€‹åƒæ•¸ 4 bytes\n",
    "INT8:  æ¯å€‹åƒæ•¸ 1 byte\n",
    "\n",
    "å£“ç¸®æ¯”: 4x\n",
    "é€Ÿåº¦æå‡: 2-4x (CPU)\n",
    "ç²¾åº¦æå¤±: ~1% (å¾®å°)\n",
    "```\n",
    "\n",
    "### å‹•æ…‹é‡åŒ–å¯¦ä½œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic quantization\n",
    "print(\"ğŸ”„ Applying dynamic quantization...\\n\")\n",
    "\n",
    "# Reload model for quantization\n",
    "model_quantized = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "model_quantized.eval()\n",
    "\n",
    "# Apply dynamic quantization\n",
    "model_quantized = torch.quantization.quantize_dynamic(\n",
    "    model_quantized,\n",
    "    {torch.nn.Linear},  # Quantize Linear layers\n",
    "    dtype=torch.qint8\n",
    ")\n",
    "\n",
    "print(\"âœ… Quantization applied!\")\n",
    "\n",
    "# Benchmark quantized model\n",
    "quantized_result = benchmark.add_result(\"Quantized (INT8)\", model_quantized, tokenizer, test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare baseline vs quantized\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“Š BASELINE vs QUANTIZED COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nğŸ“ Model Size:\")\n",
    "print(f\"   Baseline:  {baseline_result['model_size_mb']:.2f} MB\")\n",
    "print(f\"   Quantized: {quantized_result['model_size_mb']:.2f} MB\")\n",
    "print(f\"   Reduction: {(1 - quantized_result['model_size_mb']/baseline_result['model_size_mb'])*100:.1f}%\")\n",
    "\n",
    "print(f\"\\nâš¡ Inference Speed:\")\n",
    "print(f\"   Baseline:  {baseline_result['mean_ms']:.2f} ms\")\n",
    "print(f\"   Quantized: {quantized_result['mean_ms']:.2f} ms\")\n",
    "print(f\"   Speedup:   {baseline_result['mean_ms']/quantized_result['mean_ms']:.2f}x\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### é©—è­‰é‡åŒ–æ¨¡å‹ç²¾åº¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test accuracy: baseline vs quantized\n",
    "print(\"ğŸ§ª Testing prediction accuracy...\\n\")\n",
    "\n",
    "test_samples = [\n",
    "    \"Excellent product, highly recommend!\",\n",
    "    \"Terrible experience, very disappointed.\",\n",
    "    \"It's okay, could be better.\",\n",
    "    \"Absolutely fantastic! Best purchase ever!\",\n",
    "    \"Worst product I've ever bought.\"\n",
    "]\n",
    "\n",
    "# Baseline predictions\n",
    "inputs = tokenizer(test_samples, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "with torch.no_grad():\n",
    "    baseline_outputs = model(**inputs)\n",
    "    quantized_outputs = model_quantized(**inputs)\n",
    "\n",
    "baseline_preds = torch.argmax(baseline_outputs.logits, dim=1)\n",
    "quantized_preds = torch.argmax(quantized_outputs.logits, dim=1)\n",
    "\n",
    "# Compare\n",
    "print(\"Prediction Comparison:\")\n",
    "print(\"-\" * 70)\n",
    "for i, text in enumerate(test_samples):\n",
    "    match = \"âœ…\" if baseline_preds[i] == quantized_preds[i] else \"âŒ\"\n",
    "    print(f\"{match} {text[:50]}\")\n",
    "    print(f\"   Baseline:  {baseline_preds[i].item()}\")\n",
    "    print(f\"   Quantized: {quantized_preds[i].item()}\")\n",
    "\n",
    "accuracy = (baseline_preds == quantized_preds).sum().item() / len(test_samples)\n",
    "print(f\"\\nâœ… Prediction match rate: {accuracy*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸš€ å„ªåŒ–æŠ€è¡“ 2: ONNX è½‰æ›\n",
    "\n",
    "### ä»€éº¼æ˜¯ ONNX?\n",
    "\n",
    "**ONNX (Open Neural Network Exchange)** æ˜¯è·¨å¹³å°çš„æ·±åº¦å­¸ç¿’æ¨¡å‹æ ¼å¼:\n",
    "\n",
    "- ğŸ¯ **å„ªåŒ–æ¨ç†**: é‡å°æ¨ç†å„ªåŒ–çš„è¨ˆç®—åœ–\n",
    "- âš¡ **é«˜æ•ˆåŸ·è¡Œ**: ONNX Runtime é«˜åº¦å„ªåŒ–\n",
    "- ğŸŒ **è·¨å¹³å°**: æ”¯æ´å¤šç¨®ç¡¬é«” (CPU, GPU, Edge)\n",
    "- ğŸ“¦ **éƒ¨ç½²å‹å–„**: æ˜“æ–¼æ•´åˆåˆ°ç”Ÿç”¢ç’°å¢ƒ\n",
    "\n",
    "### ONNX è½‰æ›å¯¦ä½œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to ONNX\n",
    "try:\n",
    "    from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "    from optimum.onnxruntime.configuration import AutoQuantizationConfig\n",
    "    \n",
    "    print(\"ğŸ“¦ Converting model to ONNX format...\\n\")\n",
    "    \n",
    "    # Save directory\n",
    "    onnx_save_path = \"./onnx_model\"\n",
    "    \n",
    "    # Convert to ONNX\n",
    "    onnx_model = ORTModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        export=True\n",
    "    )\n",
    "    \n",
    "    # Save ONNX model\n",
    "    onnx_model.save_pretrained(onnx_save_path)\n",
    "    tokenizer.save_pretrained(onnx_save_path)\n",
    "    \n",
    "    print(f\"âœ… ONNX model saved to: {onnx_save_path}\")\n",
    "    \n",
    "    # Benchmark ONNX model\n",
    "    # Note: ORTModel doesn't directly support the same interface\n",
    "    # We'll create a wrapper for benchmarking\n",
    "    \n",
    "    print(\"\\nâš¡ ONNX model loaded and ready!\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"âš ï¸  optimum[onnxruntime] not installed\")\n",
    "    print(\"   Install with: pip install optimum[onnxruntime]\")\n",
    "    onnx_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“¦ å„ªåŒ–æŠ€è¡“ 3: æ‰¹é‡æ¨ç† (Batch Inference)\n",
    "\n",
    "### ç‚ºä»€éº¼æ‰¹é‡è™•ç†æ›´å¿«ï¼Ÿ\n",
    "\n",
    "```\n",
    "å–®å€‹è™•ç†:  [Text1] â†’ Model â†’ [Result1]  (10ms)\n",
    "           [Text2] â†’ Model â†’ [Result2]  (10ms)\n",
    "           [Text3] â†’ Model â†’ [Result3]  (10ms)\n",
    "           Total: 30ms\n",
    "\n",
    "æ‰¹é‡è™•ç†:  [Text1, Text2, Text3] â†’ Model â†’ [Result1, Result2, Result3]\n",
    "           Total: 15ms (2x faster)\n",
    "```\n",
    "\n",
    "### æ‰¹é‡æ¨ç†å¯¦ä½œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare single vs batch inference\n",
    "test_samples_large = [\n",
    "    f\"This is test review number {i} for benchmarking.\"\n",
    "    for i in range(100)\n",
    "]\n",
    "\n",
    "# Single inference\n",
    "print(\"ğŸ”„ Testing single inference...\")\n",
    "start = time.perf_counter()\n",
    "for text in test_samples_large:\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        _ = model(**inputs)\n",
    "end = time.perf_counter()\n",
    "single_time = (end - start) * 1000\n",
    "print(f\"   Single inference: {single_time:.2f} ms\")\n",
    "\n",
    "# Batch inference (batch_size=10)\n",
    "print(\"\\nğŸ”„ Testing batch inference...\")\n",
    "batch_size = 10\n",
    "start = time.perf_counter()\n",
    "for i in range(0, len(test_samples_large), batch_size):\n",
    "    batch = test_samples_large[i:i+batch_size]\n",
    "    inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        _ = model(**inputs)\n",
    "end = time.perf_counter()\n",
    "batch_time = (end - start) * 1000\n",
    "print(f\"   Batch inference (batch_size={batch_size}): {batch_time:.2f} ms\")\n",
    "\n",
    "print(f\"\\nâš¡ Speedup: {single_time/batch_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal batch size\n",
    "print(\"ğŸ” Finding optimal batch size...\\n\")\n",
    "\n",
    "batch_sizes = [1, 4, 8, 16, 32, 64]\n",
    "batch_results = []\n",
    "\n",
    "for bs in batch_sizes:\n",
    "    start = time.perf_counter()\n",
    "    \n",
    "    for i in range(0, len(test_samples_large), bs):\n",
    "        batch = test_samples_large[i:i+bs]\n",
    "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        with torch.no_grad():\n",
    "            _ = model(**inputs)\n",
    "    \n",
    "    elapsed = (time.perf_counter() - start) * 1000\n",
    "    throughput = len(test_samples_large) / elapsed * 1000  # samples/sec\n",
    "    \n",
    "    batch_results.append({\n",
    "        'batch_size': bs,\n",
    "        'time_ms': elapsed,\n",
    "        'throughput': throughput\n",
    "    })\n",
    "    \n",
    "    print(f\"Batch size {bs:2d}: {elapsed:7.2f} ms | {throughput:6.1f} samples/sec\")\n",
    "\n",
    "# Visualize\n",
    "batch_df = pd.DataFrame(batch_results)\n",
    "optimal_idx = batch_df['throughput'].idxmax()\n",
    "optimal_bs = batch_df.loc[optimal_idx, 'batch_size']\n",
    "\n",
    "print(f\"\\nâœ… Optimal batch size: {optimal_bs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot batch size vs throughput\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Throughput\n",
    "axes[0].plot(batch_df['batch_size'], batch_df['throughput'], marker='o', linewidth=2, markersize=8)\n",
    "axes[0].axvline(optimal_bs, color='red', linestyle='--', label=f'Optimal: {optimal_bs}')\n",
    "axes[0].set_title('Throughput vs Batch Size', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Batch Size')\n",
    "axes[0].set_ylabel('Throughput (samples/sec)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Total time\n",
    "axes[1].plot(batch_df['batch_size'], batch_df['time_ms'], marker='s', linewidth=2, markersize=8, color='coral')\n",
    "axes[1].set_title('Total Inference Time vs Batch Size', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Batch Size')\n",
    "axes[1].set_ylabel('Time (ms)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ’¡ å„ªåŒ–æŠ€è¡“ 4: æ¨¡å‹è’¸é¤¾ (Knowledge Distillation)\n",
    "\n",
    "### æ¦‚å¿µèªªæ˜\n",
    "\n",
    "```\n",
    "æ•™å¸«æ¨¡å‹ (Teacher)           å­¸ç”Ÿæ¨¡å‹ (Student)\n",
    "BERT-base (110M params)  â†’  DistilBERT (66M params)\n",
    "RoBERTa-large (355M)     â†’  DistilRoBERTa (82M)\n",
    "\n",
    "æ–¹æ³•: è®“å°æ¨¡å‹å­¸ç¿’å¤§æ¨¡å‹çš„è¼¸å‡ºåˆ†å¸ƒ\n",
    "çµæœ: ä¿ç•™ 95%+ ç²¾åº¦,ä½†æ¨¡å‹å° 40-60%\n",
    "```\n",
    "\n",
    "### ä½¿ç”¨é è’¸é¤¾æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare BERT vs DistilBERT\n",
    "print(\"ğŸ“¦ Loading BERT-base and DistilBERT for comparison...\\n\")\n",
    "\n",
    "# BERT-base\n",
    "bert_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"textattack/bert-base-uncased-SST-2\"\n",
    ")\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"textattack/bert-base-uncased-SST-2\")\n",
    "\n",
    "# DistilBERT (already loaded)\n",
    "distilbert_model = model\n",
    "distilbert_tokenizer = tokenizer\n",
    "\n",
    "# Compare sizes\n",
    "bert_size = benchmark.get_model_size_mb(bert_model)\n",
    "distilbert_size = benchmark.get_model_size_mb(distilbert_model)\n",
    "\n",
    "print(f\"ğŸ“ Model Size Comparison:\")\n",
    "print(f\"   BERT-base:    {bert_size:.2f} MB\")\n",
    "print(f\"   DistilBERT:   {distilbert_size:.2f} MB\")\n",
    "print(f\"   Reduction:    {(1 - distilbert_size/bert_size)*100:.1f}%\")\n",
    "\n",
    "# Benchmark both\n",
    "bert_result = benchmark.add_result(\"BERT-base\", bert_model, bert_tokenizer, test_texts)\n",
    "distilbert_result = benchmark.add_result(\"DistilBERT\", distilbert_model, distilbert_tokenizer, test_texts)\n",
    "\n",
    "print(f\"\\nâš¡ Speed Comparison:\")\n",
    "print(f\"   BERT-base:    {bert_result['mean_ms']:.2f} ms\")\n",
    "print(f\"   DistilBERT:   {distilbert_result['mean_ms']:.2f} ms\")\n",
    "print(f\"   Speedup:      {bert_result['mean_ms']/distilbert_result['mean_ms']:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“Š ç¶œåˆæ€§èƒ½å°æ¯”\n",
    "\n",
    "### è¦–è¦ºåŒ–æ‰€æœ‰å„ªåŒ–çµæœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get summary DataFrame\n",
    "summary_df = benchmark.get_summary_df()\n",
    "\n",
    "print(\"\\nğŸ“Š COMPREHENSIVE BENCHMARK SUMMARY\\n\")\n",
    "print(summary_df[['name', 'model_size_mb', 'mean_ms', 'p95_ms']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Model size and latency comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Model size comparison\n",
    "axes[0].barh(summary_df['name'], summary_df['model_size_mb'], color='steelblue')\n",
    "axes[0].set_title('Model Size Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Size (MB)')\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Latency comparison\n",
    "axes[1].barh(summary_df['name'], summary_df['mean_ms'], color='coral')\n",
    "axes[1].set_title('Average Latency Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Latency (ms)')\n",
    "axes[1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speedup analysis (relative to baseline)\n",
    "baseline_latency = summary_df[summary_df['name'] == 'Baseline (FP32)']['mean_ms'].values[0]\n",
    "baseline_size = summary_df[summary_df['name'] == 'Baseline (FP32)']['model_size_mb'].values[0]\n",
    "\n",
    "summary_df['speedup'] = baseline_latency / summary_df['mean_ms']\n",
    "summary_df['size_reduction'] = (1 - summary_df['model_size_mb'] / baseline_size) * 100\n",
    "\n",
    "print(\"\\nâš¡ Optimization Impact (vs Baseline):\\n\")\n",
    "print(summary_df[['name', 'speedup', 'size_reduction']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸš€ ç”Ÿç”¢ç’°å¢ƒéƒ¨ç½²æœ€ä½³å¯¦è¸\n",
    "\n",
    "### 1. é¸æ“‡åˆé©çš„å„ªåŒ–ç­–ç•¥\n",
    "\n",
    "#### æ±ºç­–æ¨¹\n",
    "\n",
    "```\n",
    "éƒ¨ç½²ç’°å¢ƒ?\n",
    "â”œâ”€ é›²ç«¯æœå‹™å™¨ (GPU å¯ç”¨)\n",
    "â”‚   â””â”€ ä½¿ç”¨ FP16 æ··åˆç²¾åº¦ + æ‰¹é‡æ¨ç†\n",
    "â”‚\n",
    "â”œâ”€ é›²ç«¯æœå‹™å™¨ (CPU only)\n",
    "â”‚   â””â”€ INT8 é‡åŒ– + ONNX + æ‰¹é‡æ¨ç†\n",
    "â”‚\n",
    "â”œâ”€ é‚Šç·£è¨­å‚™ (æ‰‹æ©Ÿ/IoT)\n",
    "â”‚   â””â”€ DistilBERT + INT8 é‡åŒ– + TensorFlow Lite\n",
    "â”‚\n",
    "â””â”€ API æœå‹™ (é«˜ä¸¦ç™¼)\n",
    "    â””â”€ ONNX + æ‰¹é‡æ¨ç† + è² è¼‰å‡è¡¡\n",
    "```\n",
    "\n",
    "### 2. FastAPI éƒ¨ç½²ç¯„ä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FastAPI deployment example (save as app.py)\n",
    "fastapi_code = '''\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# Initialize app\n",
    "app = FastAPI(title=\"Sentiment Analysis API\")\n",
    "\n",
    "# Load optimized model (quantized)\n",
    "model_path = \"./models/quantized_model\"\n",
    "classifier = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=model_path,\n",
    "    device=-1  # CPU\n",
    ")\n",
    "\n",
    "# Request model\n",
    "class TextInput(BaseModel):\n",
    "    text: str\n",
    "\n",
    "class BatchTextInput(BaseModel):\n",
    "    texts: list[str]\n",
    "\n",
    "# Single prediction endpoint\n",
    "@app.post(\"/predict\")\n",
    "def predict(input_data: TextInput):\n",
    "    try:\n",
    "        result = classifier(input_data.text)[0]\n",
    "        return {\n",
    "            \"sentiment\": result[\"label\"],\n",
    "            \"confidence\": round(result[\"score\"], 4)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "# Batch prediction endpoint\n",
    "@app.post(\"/predict_batch\")\n",
    "def predict_batch(input_data: BatchTextInput):\n",
    "    try:\n",
    "        results = classifier(input_data.texts)\n",
    "        return {\n",
    "            \"predictions\": [\n",
    "                {\n",
    "                    \"sentiment\": r[\"label\"],\n",
    "                    \"confidence\": round(r[\"score\"], 4)\n",
    "                }\n",
    "                for r in results\n",
    "            ]\n",
    "        }\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "# Health check\n",
    "@app.get(\"/health\")\n",
    "def health():\n",
    "    return {\"status\": \"healthy\"}\n",
    "\n",
    "# Run with: uvicorn app:app --host 0.0.0.0 --port 8000\n",
    "'''\n",
    "\n",
    "# Save FastAPI code\n",
    "with open(\"fastapi_deployment_example.py\", \"w\") as f:\n",
    "    f.write(fastapi_code)\n",
    "\n",
    "print(\"âœ… FastAPI deployment code saved to: fastapi_deployment_example.py\")\n",
    "print(\"\\nTo run:\")\n",
    "print(\"   1. pip install fastapi uvicorn\")\n",
    "print(\"   2. uvicorn fastapi_deployment_example:app --reload\")\n",
    "print(\"   3. Visit: http://localhost:8000/docs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Docker å®¹å™¨åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dockerfile example\n",
    "dockerfile = '''\n",
    "FROM python:3.10-slim\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "# Install dependencies\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Copy application\n",
    "COPY app.py .\n",
    "COPY models/ ./models/\n",
    "\n",
    "# Expose port\n",
    "EXPOSE 8000\n",
    "\n",
    "# Run application\n",
    "CMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
    "'''\n",
    "\n",
    "# requirements.txt\n",
    "requirements = '''\n",
    "fastapi==0.104.1\n",
    "uvicorn[standard]==0.24.0\n",
    "transformers==4.35.0\n",
    "torch==2.1.0\n",
    "pydantic==2.4.2\n",
    "'''\n",
    "\n",
    "# Save files\n",
    "with open(\"Dockerfile\", \"w\") as f:\n",
    "    f.write(dockerfile)\n",
    "\n",
    "with open(\"requirements.txt\", \"w\") as f:\n",
    "    f.write(requirements)\n",
    "\n",
    "print(\"âœ… Docker configuration files created\")\n",
    "print(\"\\nTo build and run:\")\n",
    "print(\"   docker build -t sentiment-api .\")\n",
    "print(\"   docker run -p 8000:8000 sentiment-api\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“š ç¸½çµèˆ‡æœ€ä½³å¯¦è¸\n",
    "\n",
    "### âœ… é—œéµè¦é»\n",
    "\n",
    "1. **é‡åŒ– (Quantization)**:\n",
    "   - INT8 é‡åŒ–å¯æ¸›å°‘ 75% æ¨¡å‹å¤§å°\n",
    "   - 2-4x æ¨ç†åŠ é€Ÿ (CPU)\n",
    "   - ç²¾åº¦æå¤± < 1%\n",
    "   - **æœ€é©åˆ**: CPU éƒ¨ç½²å ´æ™¯\n",
    "\n",
    "2. **æ¨¡å‹è’¸é¤¾ (Distillation)**:\n",
    "   - ä½¿ç”¨ DistilBERT ç­‰é è’¸é¤¾æ¨¡å‹\n",
    "   - ä¿ç•™ 95%+ ç²¾åº¦\n",
    "   - æ¨¡å‹å° 40%,é€Ÿåº¦å¿« 60%\n",
    "   - **æœ€é©åˆ**: è³‡æºå—é™ç’°å¢ƒ\n",
    "\n",
    "3. **æ‰¹é‡æ¨ç† (Batching)**:\n",
    "   - ç·šæ€§æå‡ååé‡\n",
    "   - æ‰¾åˆ°æœ€ä½³ batch size (é€šå¸¸ 16-32)\n",
    "   - **æœ€é©åˆ**: é«˜ååé‡å ´æ™¯\n",
    "\n",
    "4. **ONNX è½‰æ›**:\n",
    "   - è·¨å¹³å°éƒ¨ç½²\n",
    "   - ONNX Runtime é«˜åº¦å„ªåŒ–\n",
    "   - **æœ€é©åˆ**: ç”Ÿç”¢ç’°å¢ƒéƒ¨ç½²\n",
    "\n",
    "### ğŸ¯ å„ªåŒ–ç­–ç•¥é¸æ“‡æŒ‡å—\n",
    "\n",
    "| å ´æ™¯ | æ¨è–¦ç­–ç•¥ | é æœŸæ•ˆæœ |\n",
    "|------|----------|----------|\n",
    "| **é›²ç«¯ API (CPU)** | INT8 é‡åŒ– + ONNX + Batching | 3-5x åŠ é€Ÿ |\n",
    "| **é‚Šç·£è¨­å‚™** | DistilBERT + INT8 | æ¨¡å‹ç¸®å° 70% |\n",
    "| **é«˜ä¸¦ç™¼æœå‹™** | Batching + è² è¼‰å‡è¡¡ | 10x+ ååé‡ |\n",
    "| **ä½å»¶é²éœ€æ±‚** | GPU + FP16 + Batching | < 10ms å»¶é² |\n",
    "\n",
    "### ğŸ’¡ éƒ¨ç½²æª¢æŸ¥æ¸…å–®\n",
    "\n",
    "- [ ] **æ¨¡å‹å„ªåŒ–**: é¸æ“‡åˆé©çš„é‡åŒ–/è’¸é¤¾ç­–ç•¥\n",
    "- [ ] **æ‰¹é‡è™•ç†**: å¯¦ä½œæ‰¹é‡æ¨ç†é‚è¼¯\n",
    "- [ ] **éŒ¯èª¤è™•ç†**: è™•ç†ç•°å¸¸è¼¸å…¥èˆ‡æ¨¡å‹å¤±æ•—\n",
    "- [ ] **ç›£æ§**: è¨˜éŒ„å»¶é²ã€ååé‡ã€éŒ¯èª¤ç‡\n",
    "- [ ] **ç‰ˆæœ¬ç®¡ç†**: æ¨¡å‹ç‰ˆæœ¬æ§åˆ¶èˆ‡å›æ»¾æ©Ÿåˆ¶\n",
    "- [ ] **è² è¼‰æ¸¬è©¦**: å£“åŠ›æ¸¬è©¦èˆ‡æ•ˆèƒ½åŸºæº–\n",
    "- [ ] **æ–‡æª”**: API æ–‡æª”èˆ‡ä½¿ç”¨ç¯„ä¾‹\n",
    "- [ ] **CI/CD**: è‡ªå‹•åŒ–æ¸¬è©¦èˆ‡éƒ¨ç½²æµç¨‹\n",
    "\n",
    "### ğŸš€ ä¸‹ä¸€æ­¥å­¸ç¿’\n",
    "\n",
    "1. **é€²éšé‡åŒ–**: QAT (Quantization-Aware Training)\n",
    "2. **æ¨¡å‹å‰ªæ**: ç§»é™¤ä¸é‡è¦çš„æ¬Šé‡\n",
    "3. **TensorRT**: NVIDIA GPU æ·±åº¦å„ªåŒ–\n",
    "4. **TensorFlow Lite**: ç§»å‹•ç«¯éƒ¨ç½²\n",
    "5. **å¤šæ¨¡å‹æœå‹™**: TorchServe, TensorFlow Serving\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”— åƒè€ƒè³‡æº\n",
    "\n",
    "- [PyTorch Quantization](https://pytorch.org/docs/stable/quantization.html)\n",
    "- [ONNX Runtime](https://onnxruntime.ai/)\n",
    "- [Hugging Face Optimum](https://huggingface.co/docs/optimum/)\n",
    "- [FastAPI Documentation](https://fastapi.tiangolo.com/)\n",
    "- [Model Optimization Guide](https://huggingface.co/docs/transformers/perf_train_gpu_one)\n",
    "\n",
    "---\n",
    "\n",
    "**æ­å–œå®Œæˆ CH08 Hugging Face å¯¦æˆ°ç³»åˆ—! ğŸ‰**\n",
    "\n",
    "ä½ ç¾åœ¨å·²ç¶“æŒæ¡:\n",
    "- âœ… Hugging Face ç”Ÿæ…‹ç³»çµ±\n",
    "- âœ… Pipeline API èˆ‡å„ç¨® NLP ä»»å‹™\n",
    "- âœ… æ¨¡å‹å¾®èª¿å®Œæ•´æµç¨‹\n",
    "- âœ… ç«¯åˆ°ç«¯å°ˆæ¡ˆå¯¦æˆ°\n",
    "- âœ… ç”Ÿç”¢ç’°å¢ƒå„ªåŒ–èˆ‡éƒ¨ç½²\n",
    "\n",
    "**ä¸‹ä¸€ç« **: CH09 èª²ç¨‹ç¸½çµèˆ‡è·æ¶¯è¦åŠƒ ğŸ“"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
