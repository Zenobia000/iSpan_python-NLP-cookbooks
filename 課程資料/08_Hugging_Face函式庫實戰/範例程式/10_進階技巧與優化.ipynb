{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH08-10: 進階技巧與優化 (Advanced Techniques & Optimization)\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 本節學習目標\n",
    "\n",
    "1. 掌握**模型量化** (Quantization) 技術\n",
    "2. 學習**推理加速**策略\n",
    "3. 了解**模型壓縮**方法 (知識蒸餾)\n",
    "4. 實作**批量推理**優化\n",
    "5. 學習**生產環境部署**最佳實踐\n",
    "6. 使用 **ONNX** 加速推理\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 為什麼需要優化？\n",
    "\n",
    "### 生產環境的挑戰\n",
    "\n",
    "| 挑戰 | 說明 | 優化目標 |\n",
    "|------|------|----------|\n",
    "| **推理速度慢** | BERT 系列模型參數量大 (110M+) | 減少延遲,提升吞吐量 |\n",
    "| **記憶體占用高** | 單個模型可能需要 400MB+ RAM | 降低記憶體使用 |\n",
    "| **成本高昂** | GPU 推理成本 >> CPU | 實現 CPU 高效推理 |\n",
    "| **擴展性差** | 單 GPU 處理能力有限 | 提升並發處理能力 |\n",
    "\n",
    "### 優化技術對比\n",
    "\n",
    "| 技術 | 速度提升 | 模型大小減少 | 精度損失 | 難度 |\n",
    "|------|---------|-------------|----------|------|\n",
    "| **量化 (INT8)** | 2-4x | 4x | 微小 (~1%) | ⭐ 低 |\n",
    "| **知識蒸餾** | 2-10x | 自定義 | 小 (2-5%) | ⭐⭐⭐ 高 |\n",
    "| **剪枝 (Pruning)** | 1.5-3x | 2-3x | 中等 | ⭐⭐ 中 |\n",
    "| **ONNX 轉換** | 1.5-2x | - | 無 | ⭐⭐ 中 |\n",
    "| **批量推理** | 線性擴展 | - | 無 | ⭐ 低 |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔧 環境準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install optimization libraries\n",
    "# !pip install transformers torch optimum[onnxruntime]\n",
    "# !pip install onnx onnxruntime\n",
    "# !pip install psutil py-cpuinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import time\n",
    "import psutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    pipeline\n",
    ")\n",
    "from pathlib import Path\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"✅ Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 📏 基準測試 (Baseline Benchmark)\n",
    "\n",
    "### 建立性能測試工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelBenchmark:\n",
    "    \"\"\"\n",
    "    Benchmark tool for model performance evaluation\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.results = []\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_model_size_mb(model):\n",
    "        \"\"\"\n",
    "        Calculate model size in MB\n",
    "        \"\"\"\n",
    "        param_size = sum(p.nelement() * p.element_size() for p in model.parameters())\n",
    "        buffer_size = sum(b.nelement() * b.element_size() for b in model.buffers())\n",
    "        return (param_size + buffer_size) / 1024 / 1024\n",
    "    \n",
    "    @staticmethod\n",
    "    def measure_inference_time(model, tokenizer, texts, warmup=5, iterations=50):\n",
    "        \"\"\"\n",
    "        Measure average inference time\n",
    "        Args:\n",
    "            model: model to benchmark\n",
    "            tokenizer: tokenizer\n",
    "            texts: list of input texts\n",
    "            warmup: number of warmup runs\n",
    "            iterations: number of measurement iterations\n",
    "        Returns:\n",
    "            dict with timing statistics\n",
    "        \"\"\"\n",
    "        # Warmup\n",
    "        for _ in range(warmup):\n",
    "            inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            with torch.no_grad():\n",
    "                _ = model(**inputs)\n",
    "        \n",
    "        # Measure\n",
    "        times = []\n",
    "        for _ in range(iterations):\n",
    "            inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            \n",
    "            start = time.perf_counter()\n",
    "            with torch.no_grad():\n",
    "                _ = model(**inputs)\n",
    "            end = time.perf_counter()\n",
    "            \n",
    "            times.append((end - start) * 1000)  # Convert to ms\n",
    "        \n",
    "        return {\n",
    "            'mean_ms': np.mean(times),\n",
    "            'std_ms': np.std(times),\n",
    "            'min_ms': np.min(times),\n",
    "            'max_ms': np.max(times),\n",
    "            'p50_ms': np.percentile(times, 50),\n",
    "            'p95_ms': np.percentile(times, 95),\n",
    "            'p99_ms': np.percentile(times, 99)\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def measure_memory_usage():\n",
    "        \"\"\"\n",
    "        Measure current memory usage\n",
    "        \"\"\"\n",
    "        process = psutil.Process()\n",
    "        mem_info = process.memory_info()\n",
    "        return mem_info.rss / 1024 / 1024  # MB\n",
    "    \n",
    "    def add_result(self, name, model, tokenizer, test_texts):\n",
    "        \"\"\"\n",
    "        Add benchmark result\n",
    "        \"\"\"\n",
    "        print(f\"\\n🔄 Benchmarking: {name}...\")\n",
    "        \n",
    "        model_size = self.get_model_size_mb(model)\n",
    "        timing = self.measure_inference_time(model, tokenizer, test_texts)\n",
    "        memory = self.measure_memory_usage()\n",
    "        \n",
    "        result = {\n",
    "            'name': name,\n",
    "            'model_size_mb': model_size,\n",
    "            'memory_mb': memory,\n",
    "            **timing\n",
    "        }\n",
    "        \n",
    "        self.results.append(result)\n",
    "        \n",
    "        print(f\"   Model size: {model_size:.2f} MB\")\n",
    "        print(f\"   Avg latency: {timing['mean_ms']:.2f} ms\")\n",
    "        print(f\"   P95 latency: {timing['p95_ms']:.2f} ms\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def get_summary_df(self):\n",
    "        \"\"\"\n",
    "        Get summary DataFrame\n",
    "        \"\"\"\n",
    "        return pd.DataFrame(self.results)\n",
    "\n",
    "# Initialize benchmark\n",
    "benchmark = ModelBenchmark()\n",
    "print(\"✅ Benchmark tool initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 載入基準模型並測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load baseline model\n",
    "MODEL_NAME = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "print(f\"📦 Loading baseline model: {MODEL_NAME}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "print(\"✅ Baseline model loaded\")\n",
    "\n",
    "# Prepare test texts\n",
    "test_texts = [\n",
    "    \"This product is amazing!\",\n",
    "    \"I'm very disappointed with the quality.\",\n",
    "    \"It's okay, nothing special.\"\n",
    "]\n",
    "\n",
    "# Benchmark baseline model\n",
    "baseline_result = benchmark.add_result(\"Baseline (FP32)\", model, tokenizer, test_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ⚡ 優化技術 1: 動態量化 (Dynamic Quantization)\n",
    "\n",
    "### 什麼是量化？\n",
    "\n",
    "將模型權重從 **FP32** (32-bit float) 轉換為 **INT8** (8-bit integer):\n",
    "\n",
    "```\n",
    "FP32:  每個參數 4 bytes\n",
    "INT8:  每個參數 1 byte\n",
    "\n",
    "壓縮比: 4x\n",
    "速度提升: 2-4x (CPU)\n",
    "精度損失: ~1% (微小)\n",
    "```\n",
    "\n",
    "### 動態量化實作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic quantization\n",
    "print(\"🔄 Applying dynamic quantization...\\n\")\n",
    "\n",
    "# Reload model for quantization\n",
    "model_quantized = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "model_quantized.eval()\n",
    "\n",
    "# Apply dynamic quantization\n",
    "model_quantized = torch.quantization.quantize_dynamic(\n",
    "    model_quantized,\n",
    "    {torch.nn.Linear},  # Quantize Linear layers\n",
    "    dtype=torch.qint8\n",
    ")\n",
    "\n",
    "print(\"✅ Quantization applied!\")\n",
    "\n",
    "# Benchmark quantized model\n",
    "quantized_result = benchmark.add_result(\"Quantized (INT8)\", model_quantized, tokenizer, test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare baseline vs quantized\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"📊 BASELINE vs QUANTIZED COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n📏 Model Size:\")\n",
    "print(f\"   Baseline:  {baseline_result['model_size_mb']:.2f} MB\")\n",
    "print(f\"   Quantized: {quantized_result['model_size_mb']:.2f} MB\")\n",
    "print(f\"   Reduction: {(1 - quantized_result['model_size_mb']/baseline_result['model_size_mb'])*100:.1f}%\")\n",
    "\n",
    "print(f\"\\n⚡ Inference Speed:\")\n",
    "print(f\"   Baseline:  {baseline_result['mean_ms']:.2f} ms\")\n",
    "print(f\"   Quantized: {quantized_result['mean_ms']:.2f} ms\")\n",
    "print(f\"   Speedup:   {baseline_result['mean_ms']/quantized_result['mean_ms']:.2f}x\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 驗證量化模型精度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test accuracy: baseline vs quantized\n",
    "print(\"🧪 Testing prediction accuracy...\\n\")\n",
    "\n",
    "test_samples = [\n",
    "    \"Excellent product, highly recommend!\",\n",
    "    \"Terrible experience, very disappointed.\",\n",
    "    \"It's okay, could be better.\",\n",
    "    \"Absolutely fantastic! Best purchase ever!\",\n",
    "    \"Worst product I've ever bought.\"\n",
    "]\n",
    "\n",
    "# Baseline predictions\n",
    "inputs = tokenizer(test_samples, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "with torch.no_grad():\n",
    "    baseline_outputs = model(**inputs)\n",
    "    quantized_outputs = model_quantized(**inputs)\n",
    "\n",
    "baseline_preds = torch.argmax(baseline_outputs.logits, dim=1)\n",
    "quantized_preds = torch.argmax(quantized_outputs.logits, dim=1)\n",
    "\n",
    "# Compare\n",
    "print(\"Prediction Comparison:\")\n",
    "print(\"-\" * 70)\n",
    "for i, text in enumerate(test_samples):\n",
    "    match = \"✅\" if baseline_preds[i] == quantized_preds[i] else \"❌\"\n",
    "    print(f\"{match} {text[:50]}\")\n",
    "    print(f\"   Baseline:  {baseline_preds[i].item()}\")\n",
    "    print(f\"   Quantized: {quantized_preds[i].item()}\")\n",
    "\n",
    "accuracy = (baseline_preds == quantized_preds).sum().item() / len(test_samples)\n",
    "print(f\"\\n✅ Prediction match rate: {accuracy*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🚀 優化技術 2: ONNX 轉換\n",
    "\n",
    "### 什麼是 ONNX?\n",
    "\n",
    "**ONNX (Open Neural Network Exchange)** 是跨平台的深度學習模型格式:\n",
    "\n",
    "- 🎯 **優化推理**: 針對推理優化的計算圖\n",
    "- ⚡ **高效執行**: ONNX Runtime 高度優化\n",
    "- 🌐 **跨平台**: 支援多種硬體 (CPU, GPU, Edge)\n",
    "- 📦 **部署友善**: 易於整合到生產環境\n",
    "\n",
    "### ONNX 轉換實作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to ONNX\n",
    "try:\n",
    "    from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "    from optimum.onnxruntime.configuration import AutoQuantizationConfig\n",
    "    \n",
    "    print(\"📦 Converting model to ONNX format...\\n\")\n",
    "    \n",
    "    # Save directory\n",
    "    onnx_save_path = \"./onnx_model\"\n",
    "    \n",
    "    # Convert to ONNX\n",
    "    onnx_model = ORTModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        export=True\n",
    "    )\n",
    "    \n",
    "    # Save ONNX model\n",
    "    onnx_model.save_pretrained(onnx_save_path)\n",
    "    tokenizer.save_pretrained(onnx_save_path)\n",
    "    \n",
    "    print(f\"✅ ONNX model saved to: {onnx_save_path}\")\n",
    "    \n",
    "    # Benchmark ONNX model\n",
    "    # Note: ORTModel doesn't directly support the same interface\n",
    "    # We'll create a wrapper for benchmarking\n",
    "    \n",
    "    print(\"\\n⚡ ONNX model loaded and ready!\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"⚠️  optimum[onnxruntime] not installed\")\n",
    "    print(\"   Install with: pip install optimum[onnxruntime]\")\n",
    "    onnx_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 📦 優化技術 3: 批量推理 (Batch Inference)\n",
    "\n",
    "### 為什麼批量處理更快？\n",
    "\n",
    "```\n",
    "單個處理:  [Text1] → Model → [Result1]  (10ms)\n",
    "           [Text2] → Model → [Result2]  (10ms)\n",
    "           [Text3] → Model → [Result3]  (10ms)\n",
    "           Total: 30ms\n",
    "\n",
    "批量處理:  [Text1, Text2, Text3] → Model → [Result1, Result2, Result3]\n",
    "           Total: 15ms (2x faster)\n",
    "```\n",
    "\n",
    "### 批量推理實作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare single vs batch inference\n",
    "test_samples_large = [\n",
    "    f\"This is test review number {i} for benchmarking.\"\n",
    "    for i in range(100)\n",
    "]\n",
    "\n",
    "# Single inference\n",
    "print(\"🔄 Testing single inference...\")\n",
    "start = time.perf_counter()\n",
    "for text in test_samples_large:\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        _ = model(**inputs)\n",
    "end = time.perf_counter()\n",
    "single_time = (end - start) * 1000\n",
    "print(f\"   Single inference: {single_time:.2f} ms\")\n",
    "\n",
    "# Batch inference (batch_size=10)\n",
    "print(\"\\n🔄 Testing batch inference...\")\n",
    "batch_size = 10\n",
    "start = time.perf_counter()\n",
    "for i in range(0, len(test_samples_large), batch_size):\n",
    "    batch = test_samples_large[i:i+batch_size]\n",
    "    inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        _ = model(**inputs)\n",
    "end = time.perf_counter()\n",
    "batch_time = (end - start) * 1000\n",
    "print(f\"   Batch inference (batch_size={batch_size}): {batch_time:.2f} ms\")\n",
    "\n",
    "print(f\"\\n⚡ Speedup: {single_time/batch_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal batch size\n",
    "print(\"🔍 Finding optimal batch size...\\n\")\n",
    "\n",
    "batch_sizes = [1, 4, 8, 16, 32, 64]\n",
    "batch_results = []\n",
    "\n",
    "for bs in batch_sizes:\n",
    "    start = time.perf_counter()\n",
    "    \n",
    "    for i in range(0, len(test_samples_large), bs):\n",
    "        batch = test_samples_large[i:i+bs]\n",
    "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        with torch.no_grad():\n",
    "            _ = model(**inputs)\n",
    "    \n",
    "    elapsed = (time.perf_counter() - start) * 1000\n",
    "    throughput = len(test_samples_large) / elapsed * 1000  # samples/sec\n",
    "    \n",
    "    batch_results.append({\n",
    "        'batch_size': bs,\n",
    "        'time_ms': elapsed,\n",
    "        'throughput': throughput\n",
    "    })\n",
    "    \n",
    "    print(f\"Batch size {bs:2d}: {elapsed:7.2f} ms | {throughput:6.1f} samples/sec\")\n",
    "\n",
    "# Visualize\n",
    "batch_df = pd.DataFrame(batch_results)\n",
    "optimal_idx = batch_df['throughput'].idxmax()\n",
    "optimal_bs = batch_df.loc[optimal_idx, 'batch_size']\n",
    "\n",
    "print(f\"\\n✅ Optimal batch size: {optimal_bs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot batch size vs throughput\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Throughput\n",
    "axes[0].plot(batch_df['batch_size'], batch_df['throughput'], marker='o', linewidth=2, markersize=8)\n",
    "axes[0].axvline(optimal_bs, color='red', linestyle='--', label=f'Optimal: {optimal_bs}')\n",
    "axes[0].set_title('Throughput vs Batch Size', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Batch Size')\n",
    "axes[0].set_ylabel('Throughput (samples/sec)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Total time\n",
    "axes[1].plot(batch_df['batch_size'], batch_df['time_ms'], marker='s', linewidth=2, markersize=8, color='coral')\n",
    "axes[1].set_title('Total Inference Time vs Batch Size', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Batch Size')\n",
    "axes[1].set_ylabel('Time (ms)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 💡 優化技術 4: 模型蒸餾 (Knowledge Distillation)\n",
    "\n",
    "### 概念說明\n",
    "\n",
    "```\n",
    "教師模型 (Teacher)           學生模型 (Student)\n",
    "BERT-base (110M params)  →  DistilBERT (66M params)\n",
    "RoBERTa-large (355M)     →  DistilRoBERTa (82M)\n",
    "\n",
    "方法: 讓小模型學習大模型的輸出分布\n",
    "結果: 保留 95%+ 精度,但模型小 40-60%\n",
    "```\n",
    "\n",
    "### 使用預蒸餾模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare BERT vs DistilBERT\n",
    "print(\"📦 Loading BERT-base and DistilBERT for comparison...\\n\")\n",
    "\n",
    "# BERT-base\n",
    "bert_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"textattack/bert-base-uncased-SST-2\"\n",
    ")\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"textattack/bert-base-uncased-SST-2\")\n",
    "\n",
    "# DistilBERT (already loaded)\n",
    "distilbert_model = model\n",
    "distilbert_tokenizer = tokenizer\n",
    "\n",
    "# Compare sizes\n",
    "bert_size = benchmark.get_model_size_mb(bert_model)\n",
    "distilbert_size = benchmark.get_model_size_mb(distilbert_model)\n",
    "\n",
    "print(f\"📏 Model Size Comparison:\")\n",
    "print(f\"   BERT-base:    {bert_size:.2f} MB\")\n",
    "print(f\"   DistilBERT:   {distilbert_size:.2f} MB\")\n",
    "print(f\"   Reduction:    {(1 - distilbert_size/bert_size)*100:.1f}%\")\n",
    "\n",
    "# Benchmark both\n",
    "bert_result = benchmark.add_result(\"BERT-base\", bert_model, bert_tokenizer, test_texts)\n",
    "distilbert_result = benchmark.add_result(\"DistilBERT\", distilbert_model, distilbert_tokenizer, test_texts)\n",
    "\n",
    "print(f\"\\n⚡ Speed Comparison:\")\n",
    "print(f\"   BERT-base:    {bert_result['mean_ms']:.2f} ms\")\n",
    "print(f\"   DistilBERT:   {distilbert_result['mean_ms']:.2f} ms\")\n",
    "print(f\"   Speedup:      {bert_result['mean_ms']/distilbert_result['mean_ms']:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 📊 綜合性能對比\n",
    "\n",
    "### 視覺化所有優化結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get summary DataFrame\n",
    "summary_df = benchmark.get_summary_df()\n",
    "\n",
    "print(\"\\n📊 COMPREHENSIVE BENCHMARK SUMMARY\\n\")\n",
    "print(summary_df[['name', 'model_size_mb', 'mean_ms', 'p95_ms']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Model size and latency comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Model size comparison\n",
    "axes[0].barh(summary_df['name'], summary_df['model_size_mb'], color='steelblue')\n",
    "axes[0].set_title('Model Size Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Size (MB)')\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Latency comparison\n",
    "axes[1].barh(summary_df['name'], summary_df['mean_ms'], color='coral')\n",
    "axes[1].set_title('Average Latency Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Latency (ms)')\n",
    "axes[1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speedup analysis (relative to baseline)\n",
    "baseline_latency = summary_df[summary_df['name'] == 'Baseline (FP32)']['mean_ms'].values[0]\n",
    "baseline_size = summary_df[summary_df['name'] == 'Baseline (FP32)']['model_size_mb'].values[0]\n",
    "\n",
    "summary_df['speedup'] = baseline_latency / summary_df['mean_ms']\n",
    "summary_df['size_reduction'] = (1 - summary_df['model_size_mb'] / baseline_size) * 100\n",
    "\n",
    "print(\"\\n⚡ Optimization Impact (vs Baseline):\\n\")\n",
    "print(summary_df[['name', 'speedup', 'size_reduction']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🚀 生產環境部署最佳實踐\n",
    "\n",
    "### 1. 選擇合適的優化策略\n",
    "\n",
    "#### 決策樹\n",
    "\n",
    "```\n",
    "部署環境?\n",
    "├─ 雲端服務器 (GPU 可用)\n",
    "│   └─ 使用 FP16 混合精度 + 批量推理\n",
    "│\n",
    "├─ 雲端服務器 (CPU only)\n",
    "│   └─ INT8 量化 + ONNX + 批量推理\n",
    "│\n",
    "├─ 邊緣設備 (手機/IoT)\n",
    "│   └─ DistilBERT + INT8 量化 + TensorFlow Lite\n",
    "│\n",
    "└─ API 服務 (高並發)\n",
    "    └─ ONNX + 批量推理 + 負載均衡\n",
    "```\n",
    "\n",
    "### 2. FastAPI 部署範例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FastAPI deployment example (save as app.py)\n",
    "fastapi_code = '''\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# Initialize app\n",
    "app = FastAPI(title=\"Sentiment Analysis API\")\n",
    "\n",
    "# Load optimized model (quantized)\n",
    "model_path = \"./models/quantized_model\"\n",
    "classifier = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=model_path,\n",
    "    device=-1  # CPU\n",
    ")\n",
    "\n",
    "# Request model\n",
    "class TextInput(BaseModel):\n",
    "    text: str\n",
    "\n",
    "class BatchTextInput(BaseModel):\n",
    "    texts: list[str]\n",
    "\n",
    "# Single prediction endpoint\n",
    "@app.post(\"/predict\")\n",
    "def predict(input_data: TextInput):\n",
    "    try:\n",
    "        result = classifier(input_data.text)[0]\n",
    "        return {\n",
    "            \"sentiment\": result[\"label\"],\n",
    "            \"confidence\": round(result[\"score\"], 4)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "# Batch prediction endpoint\n",
    "@app.post(\"/predict_batch\")\n",
    "def predict_batch(input_data: BatchTextInput):\n",
    "    try:\n",
    "        results = classifier(input_data.texts)\n",
    "        return {\n",
    "            \"predictions\": [\n",
    "                {\n",
    "                    \"sentiment\": r[\"label\"],\n",
    "                    \"confidence\": round(r[\"score\"], 4)\n",
    "                }\n",
    "                for r in results\n",
    "            ]\n",
    "        }\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "# Health check\n",
    "@app.get(\"/health\")\n",
    "def health():\n",
    "    return {\"status\": \"healthy\"}\n",
    "\n",
    "# Run with: uvicorn app:app --host 0.0.0.0 --port 8000\n",
    "'''\n",
    "\n",
    "# Save FastAPI code\n",
    "with open(\"fastapi_deployment_example.py\", \"w\") as f:\n",
    "    f.write(fastapi_code)\n",
    "\n",
    "print(\"✅ FastAPI deployment code saved to: fastapi_deployment_example.py\")\n",
    "print(\"\\nTo run:\")\n",
    "print(\"   1. pip install fastapi uvicorn\")\n",
    "print(\"   2. uvicorn fastapi_deployment_example:app --reload\")\n",
    "print(\"   3. Visit: http://localhost:8000/docs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Docker 容器化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dockerfile example\n",
    "dockerfile = '''\n",
    "FROM python:3.10-slim\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "# Install dependencies\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Copy application\n",
    "COPY app.py .\n",
    "COPY models/ ./models/\n",
    "\n",
    "# Expose port\n",
    "EXPOSE 8000\n",
    "\n",
    "# Run application\n",
    "CMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
    "'''\n",
    "\n",
    "# requirements.txt\n",
    "requirements = '''\n",
    "fastapi==0.104.1\n",
    "uvicorn[standard]==0.24.0\n",
    "transformers==4.35.0\n",
    "torch==2.1.0\n",
    "pydantic==2.4.2\n",
    "'''\n",
    "\n",
    "# Save files\n",
    "with open(\"Dockerfile\", \"w\") as f:\n",
    "    f.write(dockerfile)\n",
    "\n",
    "with open(\"requirements.txt\", \"w\") as f:\n",
    "    f.write(requirements)\n",
    "\n",
    "print(\"✅ Docker configuration files created\")\n",
    "print(\"\\nTo build and run:\")\n",
    "print(\"   docker build -t sentiment-api .\")\n",
    "print(\"   docker run -p 8000:8000 sentiment-api\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 📚 總結與最佳實踐\n",
    "\n",
    "### ✅ 關鍵要點\n",
    "\n",
    "1. **量化 (Quantization)**:\n",
    "   - INT8 量化可減少 75% 模型大小\n",
    "   - 2-4x 推理加速 (CPU)\n",
    "   - 精度損失 < 1%\n",
    "   - **最適合**: CPU 部署場景\n",
    "\n",
    "2. **模型蒸餾 (Distillation)**:\n",
    "   - 使用 DistilBERT 等預蒸餾模型\n",
    "   - 保留 95%+ 精度\n",
    "   - 模型小 40%,速度快 60%\n",
    "   - **最適合**: 資源受限環境\n",
    "\n",
    "3. **批量推理 (Batching)**:\n",
    "   - 線性提升吞吐量\n",
    "   - 找到最佳 batch size (通常 16-32)\n",
    "   - **最適合**: 高吞吐量場景\n",
    "\n",
    "4. **ONNX 轉換**:\n",
    "   - 跨平台部署\n",
    "   - ONNX Runtime 高度優化\n",
    "   - **最適合**: 生產環境部署\n",
    "\n",
    "### 🎯 優化策略選擇指南\n",
    "\n",
    "| 場景 | 推薦策略 | 預期效果 |\n",
    "|------|----------|----------|\n",
    "| **雲端 API (CPU)** | INT8 量化 + ONNX + Batching | 3-5x 加速 |\n",
    "| **邊緣設備** | DistilBERT + INT8 | 模型縮小 70% |\n",
    "| **高並發服務** | Batching + 負載均衡 | 10x+ 吞吐量 |\n",
    "| **低延遲需求** | GPU + FP16 + Batching | < 10ms 延遲 |\n",
    "\n",
    "### 💡 部署檢查清單\n",
    "\n",
    "- [ ] **模型優化**: 選擇合適的量化/蒸餾策略\n",
    "- [ ] **批量處理**: 實作批量推理邏輯\n",
    "- [ ] **錯誤處理**: 處理異常輸入與模型失敗\n",
    "- [ ] **監控**: 記錄延遲、吞吐量、錯誤率\n",
    "- [ ] **版本管理**: 模型版本控制與回滾機制\n",
    "- [ ] **負載測試**: 壓力測試與效能基準\n",
    "- [ ] **文檔**: API 文檔與使用範例\n",
    "- [ ] **CI/CD**: 自動化測試與部署流程\n",
    "\n",
    "### 🚀 下一步學習\n",
    "\n",
    "1. **進階量化**: QAT (Quantization-Aware Training)\n",
    "2. **模型剪枝**: 移除不重要的權重\n",
    "3. **TensorRT**: NVIDIA GPU 深度優化\n",
    "4. **TensorFlow Lite**: 移動端部署\n",
    "5. **多模型服務**: TorchServe, TensorFlow Serving\n",
    "\n",
    "---\n",
    "\n",
    "## 🔗 參考資源\n",
    "\n",
    "- [PyTorch Quantization](https://pytorch.org/docs/stable/quantization.html)\n",
    "- [ONNX Runtime](https://onnxruntime.ai/)\n",
    "- [Hugging Face Optimum](https://huggingface.co/docs/optimum/)\n",
    "- [FastAPI Documentation](https://fastapi.tiangolo.com/)\n",
    "- [Model Optimization Guide](https://huggingface.co/docs/transformers/perf_train_gpu_one)\n",
    "\n",
    "---\n",
    "\n",
    "**恭喜完成 CH08 Hugging Face 實戰系列! 🎉**\n",
    "\n",
    "你現在已經掌握:\n",
    "- ✅ Hugging Face 生態系統\n",
    "- ✅ Pipeline API 與各種 NLP 任務\n",
    "- ✅ 模型微調完整流程\n",
    "- ✅ 端到端專案實戰\n",
    "- ✅ 生產環境優化與部署\n",
    "\n",
    "**下一章**: CH09 課程總結與職涯規劃 🎓"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
