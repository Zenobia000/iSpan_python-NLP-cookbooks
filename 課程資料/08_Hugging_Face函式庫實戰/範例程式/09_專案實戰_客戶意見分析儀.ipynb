{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH08-09: å°ˆæ¡ˆå¯¦æˆ° - å®¢æˆ¶æ„è¦‹åˆ†æå„€ (Customer Feedback Analyzer)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š å°ˆæ¡ˆç›®æ¨™\n",
    "\n",
    "å»ºç«‹ä¸€å¥—**å®Œæ•´çš„å®¢æˆ¶æ„è¦‹åˆ†æç³»çµ±**,èƒ½å¤ :\n",
    "\n",
    "1. ğŸ¯ **æƒ…æ„Ÿåˆ†æ**: è‡ªå‹•åˆ¤æ–·å®¢æˆ¶è©•è«–æ˜¯æ­£é¢/è² é¢/ä¸­æ€§\n",
    "2. ğŸ·ï¸ **ä¸»é¡Œåˆ†é¡**: è­˜åˆ¥è©•è«–æ¶‰åŠçš„ç”¢å“é¡åˆ¥æˆ–å•é¡Œé¡å‹\n",
    "3. ğŸ” **é—œéµå­—æå–**: æ‰¾å‡ºå®¢æˆ¶æœ€é—œæ³¨çš„è­°é¡Œ\n",
    "4. ğŸ“Š **å¯è¦–åŒ–å„€è¡¨æ¿**: å‘ˆç¾åˆ†æçµæœèˆ‡è¶¨å‹¢\n",
    "5. ğŸš€ **å¯¦éš›éƒ¨ç½²**: å»ºç«‹ API ä¾›æ¥­å‹™ç³»çµ±èª¿ç”¨\n",
    "\n",
    "### å•†æ¥­åƒ¹å€¼\n",
    "\n",
    "- **è‡ªå‹•åŒ–è™•ç†**: æ¯å¤©æ•¸åƒç­†è©•è«–è‡ªå‹•åˆ†æ\n",
    "- **å³æ™‚åé¥‹**: å¿«é€Ÿç™¼ç¾å®¢æˆ¶ä¸æ»¿èˆ‡ç”¢å“å•é¡Œ\n",
    "- **æ•¸æ“šé©…å‹•**: å®¢è§€é‡åŒ–å®¢æˆ¶æ»¿æ„åº¦\n",
    "- **æˆæœ¬ç¯€çœ**: æ¸›å°‘äººå·¥å¯©é–±æ™‚é–“ 90%\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ å°ˆæ¡ˆæ¶æ§‹\n",
    "\n",
    "```\n",
    "å®¢æˆ¶è©•è«– (Raw Reviews)\n",
    "    â†“\n",
    "é è™•ç† (Preprocessing)\n",
    "    â”œâ”€â”€ å»é™¤å™ªéŸ³\n",
    "    â”œâ”€â”€ æ–‡æœ¬æ¸…ç†\n",
    "    â””â”€â”€ Tokenization\n",
    "    â†“\n",
    "NLP åˆ†æ (NLP Pipeline)\n",
    "    â”œâ”€â”€ æƒ…æ„Ÿåˆ†æ (Sentiment)\n",
    "    â”œâ”€â”€ ä¸»é¡Œåˆ†é¡ (Topic)\n",
    "    â””â”€â”€ é—œéµå­—æå– (Keywords)\n",
    "    â†“\n",
    "çµæœæ•´åˆ (Aggregation)\n",
    "    â”œâ”€â”€ çµ±è¨ˆåˆ†æ\n",
    "    â”œâ”€â”€ è¶¨å‹¢åˆ†æ\n",
    "    â””â”€â”€ ç•°å¸¸æª¢æ¸¬\n",
    "    â†“\n",
    "å¯è¦–åŒ–è¼¸å‡º (Visualization)\n",
    "    â”œâ”€â”€ æƒ…æ„Ÿåˆ†å¸ƒåœ–\n",
    "    â”œâ”€â”€ ä¸»é¡Œç†±é»åœ–\n",
    "    â””â”€â”€ è©é›² (Word Cloud)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”§ ç’°å¢ƒæº–å‚™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install transformers datasets torch\n",
    "# !pip install pandas numpy matplotlib seaborn plotly\n",
    "# !pip install wordcloud scikit-learn nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from transformers import pipeline\n",
    "from collections import Counter\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“Š æ•¸æ“šæº–å‚™\n",
    "\n",
    "### 1. ç”Ÿæˆæ¨¡æ“¬å®¢æˆ¶è©•è«–æ•¸æ“š\n",
    "\n",
    "åœ¨å¯¦éš›æ‡‰ç”¨ä¸­,ä½ æœƒå¾ä»¥ä¸‹ä¾†æºç²å–æ•¸æ“š:\n",
    "- é›»å•†å¹³å° (Amazon, PChome)\n",
    "- Google Reviews\n",
    "- å®¢æœç³»çµ±\n",
    "- ç¤¾äº¤åª’é«” (Facebook, Twitter)\n",
    "\n",
    "é€™è£¡æˆ‘å€‘ç”Ÿæˆæ¨¡æ“¬æ•¸æ“šé€²è¡Œæ¼”ç¤º:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic customer reviews\n",
    "np.random.seed(42)\n",
    "\n",
    "# Sample reviews with different sentiments and topics\n",
    "positive_reviews = [\n",
    "    \"Excellent product! Fast shipping and great quality. Highly recommend!\",\n",
    "    \"Love this item! Exactly as described. Will buy again.\",\n",
    "    \"Amazing customer service. They resolved my issue quickly.\",\n",
    "    \"Best purchase I've made this year. Super satisfied!\",\n",
    "    \"Great value for money. The product exceeded my expectations.\",\n",
    "    \"Fast delivery and product is in perfect condition. Very happy!\",\n",
    "    \"Outstanding quality! This is exactly what I was looking for.\",\n",
    "    \"Fantastic experience from order to delivery. Five stars!\",\n",
    "]\n",
    "\n",
    "negative_reviews = [\n",
    "    \"Terrible quality. Product broke after two days. Very disappointed.\",\n",
    "    \"Shipping took forever. Item arrived damaged. Not happy.\",\n",
    "    \"Poor customer service. They didn't respond to my complaints.\",\n",
    "    \"Complete waste of money. Product doesn't work as advertised.\",\n",
    "    \"Do not buy! Cheap materials and terrible build quality.\",\n",
    "    \"Worst purchase ever. Requesting a full refund immediately.\",\n",
    "    \"Product is defective. Customer support was unhelpful.\",\n",
    "    \"Very disappointed with the quality. Not worth the price.\",\n",
    "]\n",
    "\n",
    "neutral_reviews = [\n",
    "    \"Product is okay. Nothing special but does the job.\",\n",
    "    \"Average quality. Shipping was standard. No complaints.\",\n",
    "    \"It's fine. Meets basic expectations but nothing more.\",\n",
    "    \"Decent product for the price. Not amazing, not terrible.\",\n",
    "    \"Standard item. Delivery was on time. No issues.\",\n",
    "    \"Acceptable quality. Could be better but works as intended.\",\n",
    "]\n",
    "\n",
    "# Topics\n",
    "topics = ['Product Quality', 'Shipping', 'Customer Service', 'Pricing', 'Features']\n",
    "\n",
    "# Generate dataset\n",
    "num_reviews = 300\n",
    "reviews_data = []\n",
    "\n",
    "for i in range(num_reviews):\n",
    "    # Random sentiment distribution: 50% positive, 30% negative, 20% neutral\n",
    "    sentiment_choice = np.random.choice(['positive', 'negative', 'neutral'], p=[0.5, 0.3, 0.2])\n",
    "    \n",
    "    if sentiment_choice == 'positive':\n",
    "        review_text = np.random.choice(positive_reviews)\n",
    "    elif sentiment_choice == 'negative':\n",
    "        review_text = np.random.choice(negative_reviews)\n",
    "    else:\n",
    "        review_text = np.random.choice(neutral_reviews)\n",
    "    \n",
    "    # Generate random date (last 30 days)\n",
    "    review_date = datetime.now() - timedelta(days=np.random.randint(0, 30))\n",
    "    \n",
    "    # Random rating (1-5 stars)\n",
    "    if sentiment_choice == 'positive':\n",
    "        rating = np.random.choice([4, 5], p=[0.3, 0.7])\n",
    "    elif sentiment_choice == 'negative':\n",
    "        rating = np.random.choice([1, 2], p=[0.6, 0.4])\n",
    "    else:\n",
    "        rating = 3\n",
    "    \n",
    "    reviews_data.append({\n",
    "        'id': f'REV{i+1:04d}',\n",
    "        'date': review_date.strftime('%Y-%m-%d'),\n",
    "        'rating': rating,\n",
    "        'review': review_text,\n",
    "        'topic': np.random.choice(topics)\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(reviews_data)\n",
    "\n",
    "print(f\"âœ… Generated {len(df):,} synthetic customer reviews\")\n",
    "print(f\"\\nğŸ“Š Dataset shape: {df.shape}\")\n",
    "print(f\"\\nğŸ” Sample data:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset overview\n",
    "print(\"ğŸ“‹ Dataset Information:\\n\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\nğŸ“ˆ Rating Distribution:\")\n",
    "print(df['rating'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. æ•¸æ“šæ¢ç´¢æ€§åˆ†æ (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize rating distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Rating counts\n",
    "rating_counts = df['rating'].value_counts().sort_index()\n",
    "axes[0].bar(rating_counts.index, rating_counts.values, color='steelblue')\n",
    "axes[0].set_title('Rating Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Rating (Stars)')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_xticks([1, 2, 3, 4, 5])\n",
    "\n",
    "# Topic distribution\n",
    "topic_counts = df['topic'].value_counts()\n",
    "axes[1].barh(topic_counts.index, topic_counts.values, color='coral')\n",
    "axes[1].set_title('Topic Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Count')\n",
    "axes[1].set_ylabel('Topic')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series: reviews over time\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "daily_reviews = df.groupby('date').size().reset_index(name='count')\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(daily_reviews['date'], daily_reviews['count'], marker='o', linewidth=2, color='steelblue')\n",
    "plt.title('Daily Review Volume', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Reviews')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¤– NLP æ¨¡å‹è¼‰å…¥\n",
    "\n",
    "### è¼‰å…¥æƒ…æ„Ÿåˆ†ææ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sentiment analysis pipeline\n",
    "print(\"ğŸ“¦ Loading sentiment analysis model...\")\n",
    "\n",
    "sentiment_analyzer = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    device=0 if __import__('torch').cuda.is_available() else -1  # Use GPU if available\n",
    ")\n",
    "\n",
    "print(\"âœ… Sentiment analyzer loaded!\")\n",
    "\n",
    "# Test the model\n",
    "test_text = \"This product is amazing! I love it!\"\n",
    "result = sentiment_analyzer(test_text)[0]\n",
    "print(f\"\\nğŸ§ª Test prediction:\")\n",
    "print(f\"   Text: {test_text}\")\n",
    "print(f\"   Sentiment: {result['label']} (Score: {result['score']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ” æƒ…æ„Ÿåˆ†æ\n",
    "\n",
    "### æ‰¹é‡è™•ç†æ‰€æœ‰è©•è«–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze sentiment for all reviews\n",
    "print(\"ğŸ”„ Analyzing sentiment for all reviews...\\n\")\n",
    "\n",
    "# Process in batches for efficiency\n",
    "batch_size = 32\n",
    "sentiments = []\n",
    "sentiment_scores = []\n",
    "\n",
    "for i in range(0, len(df), batch_size):\n",
    "    batch = df['review'].iloc[i:i+batch_size].tolist()\n",
    "    results = sentiment_analyzer(batch)\n",
    "    \n",
    "    for result in results:\n",
    "        sentiments.append(result['label'])\n",
    "        sentiment_scores.append(result['score'])\n",
    "    \n",
    "    # Progress indicator\n",
    "    progress = min((i + batch_size) / len(df) * 100, 100)\n",
    "    print(f\"\\rProgress: {progress:.1f}% ({min(i+batch_size, len(df))}/{len(df)})\", end='')\n",
    "\n",
    "print(\"\\n\\nâœ… Sentiment analysis completed!\")\n",
    "\n",
    "# Add results to DataFrame\n",
    "df['predicted_sentiment'] = sentiments\n",
    "df['sentiment_confidence'] = sentiment_scores\n",
    "\n",
    "print(f\"\\nğŸ“Š Sentiment Distribution:\")\n",
    "print(df['predicted_sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample predictions\n",
    "print(\"\\nğŸ” Sample Predictions:\\n\")\n",
    "sample_df = df[['review', 'predicted_sentiment', 'sentiment_confidence']].head(10)\n",
    "for idx, row in sample_df.iterrows():\n",
    "    print(f\"Review: {row['review'][:60]}...\")\n",
    "    print(f\"Sentiment: {row['predicted_sentiment']} (Confidence: {row['sentiment_confidence']:.2%})\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æƒ…æ„Ÿåˆ†æçµæœå¯è¦–åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sentiment distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Sentiment pie chart\n",
    "sentiment_counts = df['predicted_sentiment'].value_counts()\n",
    "colors = ['#2ecc71', '#e74c3c']  # Green for positive, red for negative\n",
    "axes[0, 0].pie(\n",
    "    sentiment_counts.values,\n",
    "    labels=sentiment_counts.index,\n",
    "    autopct='%1.1f%%',\n",
    "    startangle=90,\n",
    "    colors=colors\n",
    ")\n",
    "axes[0, 0].set_title('Overall Sentiment Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 2. Confidence distribution\n",
    "axes[0, 1].hist(df['sentiment_confidence'], bins=30, color='steelblue', edgecolor='black')\n",
    "axes[0, 1].set_title('Sentiment Confidence Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Confidence Score')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].axvline(df['sentiment_confidence'].mean(), color='red', linestyle='--', label='Mean')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# 3. Sentiment by topic\n",
    "sentiment_by_topic = pd.crosstab(df['topic'], df['predicted_sentiment'])\n",
    "sentiment_by_topic.plot(kind='bar', ax=axes[1, 0], color=colors, width=0.7)\n",
    "axes[1, 0].set_title('Sentiment by Topic', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Topic')\n",
    "axes[1, 0].set_ylabel('Count')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "axes[1, 0].legend(title='Sentiment')\n",
    "\n",
    "# 4. Rating vs Sentiment\n",
    "rating_sentiment = pd.crosstab(df['rating'], df['predicted_sentiment'], normalize='index') * 100\n",
    "rating_sentiment.plot(kind='bar', ax=axes[1, 1], color=colors, width=0.7)\n",
    "axes[1, 1].set_title('Sentiment Distribution by Rating', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Rating (Stars)')\n",
    "axes[1, 1].set_ylabel('Percentage (%)')\n",
    "axes[1, 1].tick_params(axis='x', rotation=0)\n",
    "axes[1, 1].legend(title='Sentiment')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ·ï¸ é—œéµå­—æå–\n",
    "\n",
    "### æå–é«˜é »è©å½™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing for keyword extraction\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download NLTK data (run once)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def extract_keywords(text, top_n=10):\n",
    "    \"\"\"\n",
    "    Extract top keywords from text\n",
    "    Args:\n",
    "        text: input text\n",
    "        top_n: number of top keywords to return\n",
    "    Returns:\n",
    "        list of (keyword, frequency) tuples\n",
    "    \"\"\"\n",
    "    # Lowercase and remove special characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text.lower())\n",
    "    \n",
    "    # Tokenize\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords and short words\n",
    "    keywords = [w for w in words if w not in stop_words and len(w) > 3]\n",
    "    \n",
    "    # Count frequencies\n",
    "    word_freq = Counter(keywords)\n",
    "    \n",
    "    return word_freq.most_common(top_n)\n",
    "\n",
    "print(\"âœ… Keyword extraction function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract keywords from positive and negative reviews separately\n",
    "positive_text = ' '.join(df[df['predicted_sentiment'] == 'POSITIVE']['review'])\n",
    "negative_text = ' '.join(df[df['predicted_sentiment'] == 'NEGATIVE']['review'])\n",
    "\n",
    "positive_keywords = extract_keywords(positive_text, top_n=15)\n",
    "negative_keywords = extract_keywords(negative_text, top_n=15)\n",
    "\n",
    "print(\"âœ… Top Keywords in Positive Reviews:\")\n",
    "for word, freq in positive_keywords:\n",
    "    print(f\"   {word}: {freq}\")\n",
    "\n",
    "print(\"\\nâŒ Top Keywords in Negative Reviews:\")\n",
    "for word, freq in negative_keywords:\n",
    "    print(f\"   {word}: {freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è©é›²å¯è¦–åŒ– (Word Cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate word clouds\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Positive word cloud\n",
    "positive_wordcloud = WordCloud(\n",
    "    width=800,\n",
    "    height=400,\n",
    "    background_color='white',\n",
    "    colormap='Greens',\n",
    "    stopwords=stop_words\n",
    ").generate(positive_text)\n",
    "\n",
    "axes[0].imshow(positive_wordcloud, interpolation='bilinear')\n",
    "axes[0].set_title('Positive Reviews - Word Cloud', fontsize=14, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Negative word cloud\n",
    "negative_wordcloud = WordCloud(\n",
    "    width=800,\n",
    "    height=400,\n",
    "    background_color='white',\n",
    "    colormap='Reds',\n",
    "    stopwords=stop_words\n",
    ").generate(negative_text)\n",
    "\n",
    "axes[1].imshow(negative_wordcloud, interpolation='bilinear')\n",
    "axes[1].set_title('Negative Reviews - Word Cloud', fontsize=14, fontweight='bold')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“Š ç¶œåˆåˆ†æå ±å‘Š\n",
    "\n",
    "### ç”Ÿæˆæ¥­å‹™æ´å¯Ÿå ±å‘Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive analysis report\n",
    "def generate_analysis_report(df):\n",
    "    \"\"\"\n",
    "    Generate business insights report\n",
    "    \"\"\"\n",
    "    report = {}\n",
    "    \n",
    "    # Overall metrics\n",
    "    total_reviews = len(df)\n",
    "    avg_rating = df['rating'].mean()\n",
    "    positive_pct = (df['predicted_sentiment'] == 'POSITIVE').sum() / total_reviews * 100\n",
    "    negative_pct = (df['predicted_sentiment'] == 'NEGATIVE').sum() / total_reviews * 100\n",
    "    avg_confidence = df['sentiment_confidence'].mean()\n",
    "    \n",
    "    report['overview'] = {\n",
    "        'total_reviews': total_reviews,\n",
    "        'avg_rating': round(avg_rating, 2),\n",
    "        'positive_percentage': round(positive_pct, 1),\n",
    "        'negative_percentage': round(negative_pct, 1),\n",
    "        'avg_confidence': round(avg_confidence, 3)\n",
    "    }\n",
    "    \n",
    "    # Topic-level insights\n",
    "    topic_analysis = df.groupby('topic').agg({\n",
    "        'rating': 'mean',\n",
    "        'predicted_sentiment': lambda x: (x == 'POSITIVE').sum() / len(x) * 100\n",
    "    }).round(2)\n",
    "    topic_analysis.columns = ['avg_rating', 'positive_pct']\n",
    "    report['topic_insights'] = topic_analysis.to_dict('index')\n",
    "    \n",
    "    # Identify problem areas (topics with low ratings)\n",
    "    problem_topics = topic_analysis[topic_analysis['avg_rating'] < 3.5].index.tolist()\n",
    "    report['problem_areas'] = problem_topics\n",
    "    \n",
    "    # Identify strengths (topics with high ratings)\n",
    "    strength_topics = topic_analysis[topic_analysis['avg_rating'] >= 4.5].index.tolist()\n",
    "    report['strengths'] = strength_topics\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Generate report\n",
    "report = generate_analysis_report(df)\n",
    "\n",
    "# Print formatted report\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ“Š CUSTOMER FEEDBACK ANALYSIS REPORT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nğŸ“Œ OVERVIEW\")\n",
    "print(f\"   Total Reviews Analyzed: {report['overview']['total_reviews']:,}\")\n",
    "print(f\"   Average Rating: {report['overview']['avg_rating']} / 5.0\")\n",
    "print(f\"   Positive Sentiment: {report['overview']['positive_percentage']}%\")\n",
    "print(f\"   Negative Sentiment: {report['overview']['negative_percentage']}%\")\n",
    "print(f\"   Average Confidence: {report['overview']['avg_confidence']:.1%}\")\n",
    "\n",
    "print(\"\\nğŸ“ˆ TOPIC-LEVEL INSIGHTS\")\n",
    "for topic, metrics in report['topic_insights'].items():\n",
    "    print(f\"   {topic}:\")\n",
    "    print(f\"      - Avg Rating: {metrics['avg_rating']}\")\n",
    "    print(f\"      - Positive %: {metrics['positive_pct']}%\")\n",
    "\n",
    "if report['problem_areas']:\n",
    "    print(\"\\nâš ï¸  PROBLEM AREAS (Require Attention)\")\n",
    "    for topic in report['problem_areas']:\n",
    "        print(f\"   - {topic}\")\n",
    "else:\n",
    "    print(\"\\nâœ… No major problem areas identified!\")\n",
    "\n",
    "if report['strengths']:\n",
    "    print(\"\\nğŸ’ª STRENGTHS (Performing Well)\")\n",
    "    for topic in report['strengths']:\n",
    "        print(f\"   - {topic}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ˆ æ™‚é–“è¶¨å‹¢åˆ†æ\n",
    "\n",
    "### æƒ…æ„Ÿéš¨æ™‚é–“çš„è®ŠåŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment trend over time\n",
    "df_sorted = df.sort_values('date')\n",
    "daily_sentiment = df_sorted.groupby(['date', 'predicted_sentiment']).size().unstack(fill_value=0)\n",
    "\n",
    "# Plot sentiment trend\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(daily_sentiment.index, daily_sentiment['POSITIVE'], marker='o', label='Positive', linewidth=2, color='green')\n",
    "plt.plot(daily_sentiment.index, daily_sentiment['NEGATIVE'], marker='o', label='Negative', linewidth=2, color='red')\n",
    "plt.fill_between(daily_sentiment.index, daily_sentiment['POSITIVE'], alpha=0.3, color='green')\n",
    "plt.fill_between(daily_sentiment.index, daily_sentiment['NEGATIVE'], alpha=0.3, color='red')\n",
    "\n",
    "plt.title('Sentiment Trend Over Time', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Reviews')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸš€ éƒ¨ç½²æº–å‚™\n",
    "\n",
    "### 1. å»ºç«‹åˆ†æå‡½æ•¸ API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reusable analysis function\n",
    "class CustomerFeedbackAnalyzer:\n",
    "    \"\"\"\n",
    "    Customer Feedback Analyzer - Production-ready class\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        print(\"ğŸ”„ Initializing Customer Feedback Analyzer...\")\n",
    "        self.sentiment_analyzer = pipeline(\n",
    "            \"sentiment-analysis\",\n",
    "            model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "            device=-1  # CPU for production\n",
    "        )\n",
    "        print(\"âœ… Analyzer ready!\")\n",
    "    \n",
    "    def analyze_sentiment(self, text):\n",
    "        \"\"\"\n",
    "        Analyze sentiment of a single review\n",
    "        Args:\n",
    "            text: review text\n",
    "        Returns:\n",
    "            dict with sentiment and confidence\n",
    "        \"\"\"\n",
    "        result = self.sentiment_analyzer(text)[0]\n",
    "        return {\n",
    "            'sentiment': result['label'],\n",
    "            'confidence': round(result['score'], 4)\n",
    "        }\n",
    "    \n",
    "    def batch_analyze(self, reviews):\n",
    "        \"\"\"\n",
    "        Batch analyze multiple reviews\n",
    "        Args:\n",
    "            reviews: list of review texts\n",
    "        Returns:\n",
    "            list of analysis results\n",
    "        \"\"\"\n",
    "        results = self.sentiment_analyzer(reviews)\n",
    "        return [\n",
    "            {\n",
    "                'sentiment': r['label'],\n",
    "                'confidence': round(r['score'], 4)\n",
    "            }\n",
    "            for r in results\n",
    "        ]\n",
    "    \n",
    "    def get_summary_stats(self, results):\n",
    "        \"\"\"\n",
    "        Calculate summary statistics\n",
    "        Args:\n",
    "            results: list of analysis results\n",
    "        Returns:\n",
    "            dict with summary statistics\n",
    "        \"\"\"\n",
    "        total = len(results)\n",
    "        positive = sum(1 for r in results if r['sentiment'] == 'POSITIVE')\n",
    "        negative = total - positive\n",
    "        \n",
    "        return {\n",
    "            'total_reviews': total,\n",
    "            'positive_count': positive,\n",
    "            'negative_count': negative,\n",
    "            'positive_percentage': round(positive / total * 100, 2),\n",
    "            'negative_percentage': round(negative / total * 100, 2)\n",
    "        }\n",
    "\n",
    "# Initialize analyzer\n",
    "analyzer = CustomerFeedbackAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the analyzer\n",
    "test_reviews = [\n",
    "    \"This product is absolutely fantastic!\",\n",
    "    \"Terrible experience, will never buy again.\",\n",
    "    \"Good quality but shipping was slow.\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ§ª Testing analyzer with sample reviews:\\n\")\n",
    "results = analyzer.batch_analyze(test_reviews)\n",
    "\n",
    "for review, result in zip(test_reviews, results):\n",
    "    print(f\"Review: {review}\")\n",
    "    print(f\"Sentiment: {result['sentiment']} (Confidence: {result['confidence']:.2%})\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "# Get summary\n",
    "summary = analyzer.get_summary_stats(results)\n",
    "print(\"\\nğŸ“Š Summary Statistics:\")\n",
    "print(json.dumps(summary, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. å°å‡ºåˆ†æçµæœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results to CSV\n",
    "output_path = \"./customer_feedback_analysis_results.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"âœ… Results exported to: {output_path}\")\n",
    "\n",
    "# Export summary report to JSON\n",
    "report_path = \"./analysis_report.json\"\n",
    "with open(report_path, 'w') as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "print(f\"âœ… Report exported to: {report_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“š ç¸½çµèˆ‡å»¶ä¼¸æ‡‰ç”¨\n",
    "\n",
    "### âœ… ä½ å­¸åˆ°äº†ä»€éº¼\n",
    "\n",
    "1. **ç«¯åˆ°ç«¯å°ˆæ¡ˆæµç¨‹**:\n",
    "   - æ•¸æ“šæ”¶é›†èˆ‡æº–å‚™\n",
    "   - NLP æ¨¡å‹æ‡‰ç”¨\n",
    "   - çµæœåˆ†æèˆ‡å¯è¦–åŒ–\n",
    "   - æ¥­å‹™æ´å¯Ÿç”Ÿæˆ\n",
    "   - æ¨¡å‹éƒ¨ç½²æº–å‚™\n",
    "\n",
    "2. **å¯¦æˆ°æŠ€èƒ½**:\n",
    "   - Hugging Face Pipeline åœ¨ç”Ÿç”¢ç’°å¢ƒçš„æ‡‰ç”¨\n",
    "   - æ‰¹é‡è™•ç†å¤§é‡æ–‡æœ¬æ•¸æ“š\n",
    "   - é—œéµå­—æå–èˆ‡è©é›²ç”Ÿæˆ\n",
    "   - å¤šç¶­åº¦æ•¸æ“šå¯è¦–åŒ–\n",
    "   - å¯é‡ç”¨çš„åˆ†æé¡åˆ¥è¨­è¨ˆ\n",
    "\n",
    "3. **å•†æ¥­åƒ¹å€¼**:\n",
    "   - è‡ªå‹•åŒ–å®¢æˆ¶åé¥‹åˆ†æ\n",
    "   - å¿«é€Ÿè­˜åˆ¥å•é¡Œé ˜åŸŸ\n",
    "   - æ•¸æ“šé©…å‹•æ±ºç­–æ”¯æŒ\n",
    "\n",
    "### ğŸš€ å»¶ä¼¸æ‡‰ç”¨æ–¹å‘\n",
    "\n",
    "1. **æ•´åˆæ›´å¤š NLP åŠŸèƒ½**:\n",
    "   - å‘½åå¯¦é«”è­˜åˆ¥ (æå–ç”¢å“åç¨±ã€å“ç‰Œ)\n",
    "   - ä¸»é¡Œå»ºæ¨¡ (LDA, BERTopic)\n",
    "   - é›¶æ¨£æœ¬åˆ†é¡ (è‡ªå‹•åˆ†é¡å•é¡Œé¡å‹)\n",
    "\n",
    "2. **å»ºç«‹ Web API**:\n",
    "   ```python\n",
    "   # FastAPI example\n",
    "   from fastapi import FastAPI\n",
    "   \n",
    "   app = FastAPI()\n",
    "   analyzer = CustomerFeedbackAnalyzer()\n",
    "   \n",
    "   @app.post(\"/analyze\")\n",
    "   def analyze_review(text: str):\n",
    "       return analyzer.analyze_sentiment(text)\n",
    "   ```\n",
    "\n",
    "3. **å³æ™‚ç›£æ§å„€è¡¨æ¿**:\n",
    "   - ä½¿ç”¨ Streamlit æˆ– Dash å»ºç«‹äº’å‹•å¼å„€è¡¨æ¿\n",
    "   - æ•´åˆ Plotly è£½ä½œå‹•æ…‹åœ–è¡¨\n",
    "   - è¨­å®šè­¦å ±ç³»çµ± (è² è©•è¶…éé–¾å€¼æ™‚é€šçŸ¥)\n",
    "\n",
    "4. **å¤šèªè¨€æ”¯æŒ**:\n",
    "   - ä½¿ç”¨å¤šèªè¨€æ¨¡å‹ (XLM-RoBERTa)\n",
    "   - æ•´åˆç¿»è­¯ API\n",
    "   - æ”¯æ´ä¸­æ–‡ã€æ—¥æ–‡ç­‰äºæ´²èªè¨€\n",
    "\n",
    "5. **é€²éšåˆ†æ**:\n",
    "   - æƒ…æ„Ÿå¼·åº¦åˆ†æ (1-5 æ˜Ÿç´°ç·»åº¦)\n",
    "   - æƒ…ç·’è­˜åˆ¥ (å–œæ‚…ã€æ†¤æ€’ã€å¤±æœ›ç­‰)\n",
    "   - ç•°å¸¸æª¢æ¸¬ (çªç™¼è² è©•è­¦å ±)\n",
    "\n",
    "### ğŸ’¼ å•†æ¥­æ‡‰ç”¨å ´æ™¯\n",
    "\n",
    "| ç”¢æ¥­ | æ‡‰ç”¨å ´æ™¯ | åƒ¹å€¼ |\n",
    "|------|----------|------|\n",
    "| **é›»å•†** | ç”¢å“è©•è«–åˆ†æ | å¿«é€Ÿè­˜åˆ¥å•é¡Œç”¢å“,æ”¹å–„å®¢æˆ¶é«”é©— |\n",
    "| **é¤é£²** | Google/Yelp è©•è«–ç›£æ§ | å³æ™‚å›æ‡‰è² è©•,ç¶­è­·å“ç‰Œè²è­½ |\n",
    "| **æ—…éŠ** | é…’åº—/æ™¯é»è©•åƒ¹åˆ†æ | å„ªåŒ–æœå‹™å“è³ª,æå‡ç«¶çˆ­åŠ› |\n",
    "| **é‡‘è** | å®¢æœå°è©±åˆ†æ | è©•ä¼°æœå‹™å“è³ª,è¨“ç·´å®¢æœäººå“¡ |\n",
    "| **SaaS** | ç”¨æˆ¶åé¥‹åˆ†æ | ç”¢å“è¿­ä»£å„ªå…ˆç´šæ’åº |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”— åƒè€ƒè³‡æº\n",
    "\n",
    "- [Hugging Face Transformers](https://huggingface.co/docs/transformers/)\n",
    "- [DistilBERT Model Card](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n",
    "- [WordCloud Documentation](https://github.com/amueller/word_cloud)\n",
    "- [Plotly Python](https://plotly.com/python/)\n",
    "\n",
    "---\n",
    "\n",
    "**ä¸‹ä¸€ç¯€**: `10_é€²éšæŠ€å·§èˆ‡å„ªåŒ–.ipynb` - æ¨¡å‹é‡åŒ–ã€æ¨ç†åŠ é€Ÿã€éƒ¨ç½²å„ªåŒ– âš¡"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
