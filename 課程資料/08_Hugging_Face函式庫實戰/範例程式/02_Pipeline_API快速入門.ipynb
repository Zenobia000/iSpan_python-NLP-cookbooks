{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH08-02: Pipeline API å¿«é€Ÿå…¥é–€\n",
    "\n",
    "**èª²ç¨‹**: iSpan Python NLP Cookbooks v2\n",
    "**ç« ç¯€**: CH08 Hugging Face å‡½å¼åº«å¯¦æˆ°\n",
    "**ç‰ˆæœ¬**: v1.0\n",
    "**æ›´æ–°æ—¥æœŸ**: 2025-10-17\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š æœ¬ç¯€å­¸ç¿’ç›®æ¨™\n",
    "\n",
    "1. æ·±å…¥ç†è§£ Pipeline çš„å…§éƒ¨å·¥ä½œæ©Ÿåˆ¶\n",
    "2. æŒæ¡ Pipeline çš„é€²éšåƒæ•¸è¨­å®š\n",
    "3. å­¸æœƒæ‰¹æ¬¡è™•ç†èˆ‡æ•ˆèƒ½å„ªåŒ–æŠ€å·§\n",
    "4. è‡ªè¨‚ Pipeline åƒæ•¸èˆ‡å¾Œè™•ç†é‚è¼¯\n",
    "5. æ•´åˆ Pipeline åˆ°å¯¦éš›æ‡‰ç”¨ä¸­\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Pipeline å…§éƒ¨æ©Ÿåˆ¶æ·±å…¥è§£æ\n",
    "\n",
    "### 1.1 Pipeline çš„ä¸‰éšæ®µè™•ç†æµç¨‹\n",
    "\n",
    "```\n",
    "Pipeline åŸ·è¡Œæµç¨‹:\n",
    "\n",
    "Input Text\n",
    "    â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Preprocessing  â”‚  â† Tokenizer ç·¨ç¢¼\n",
    "â”‚  (tokenization) â”‚     - åˆ†è©\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     - æ·»åŠ ç‰¹æ®Šæ¨™è¨˜\n",
    "    â†“                    - å¡«å……/æˆªæ–·\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   Inference     â”‚  â† Model æ¨ç†\n",
    "â”‚   (forward)     â”‚     - å‰å‘å‚³æ’­\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     - è¨ˆç®— logits\n",
    "    â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Postprocessing  â”‚  â† çµæœè§£æ\n",
    "â”‚  (decode)       â”‚     - Softmax\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     - Top-k é¸æ“‡\n",
    "    â†“                    - æ ¼å¼åŒ–è¼¸å‡º\n",
    "Output Result\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å…ˆå®‰è£å¿…è¦å¥—ä»¶\n",
    "# !pip install transformers torch -q\n",
    "\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# å‰µå»ºæƒ…æ„Ÿåˆ†æ Pipeline\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# æŸ¥çœ‹ Pipeline å…§éƒ¨çµ„ä»¶\n",
    "print(\"Pipeline çµ„ä»¶:\")\n",
    "print(f\"1. Model: {classifier.model.__class__.__name__}\")\n",
    "print(f\"2. Tokenizer: {classifier.tokenizer.__class__.__name__}\")\n",
    "print(f\"3. Device: {classifier.device}\")\n",
    "print(f\"4. Framework: {classifier.framework}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 æ‰‹å‹•åˆ†è§£ Pipeline æµç¨‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä½¿ç”¨ Pipeline çš„å…§éƒ¨çµ„ä»¶æ‰‹å‹•åŸ·è¡Œ\n",
    "text = \"This movie is absolutely fantastic!\"\n",
    "\n",
    "# Step 1: Preprocessing (Tokenization)\n",
    "inputs = classifier.tokenizer(\n",
    "    text, \n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True\n",
    ")\n",
    "print(\"Step 1 - Tokenization:\")\n",
    "print(f\"input_ids: {inputs['input_ids']}\")\n",
    "print(f\"attention_mask: {inputs['attention_mask']}\")\n",
    "\n",
    "# Step 2: Inference (Forward Pass)\n",
    "with torch.no_grad():\n",
    "    outputs = classifier.model(**inputs)\n",
    "    logits = outputs.logits\n",
    "\n",
    "print(f\"\\nStep 2 - Model Inference:\")\n",
    "print(f\"logits: {logits}\")\n",
    "\n",
    "# Step 3: Postprocessing (Decode)\n",
    "predictions = torch.softmax(logits, dim=-1)\n",
    "predicted_class = torch.argmax(predictions, dim=-1).item()\n",
    "confidence = predictions[0][predicted_class].item()\n",
    "\n",
    "# ç²å–æ¨™ç±¤æ˜ å°„\n",
    "id2label = classifier.model.config.id2label\n",
    "label = id2label[predicted_class]\n",
    "\n",
    "print(f\"\\nStep 3 - Postprocessing:\")\n",
    "print(f\"Predicted Label: {label}\")\n",
    "print(f\"Confidence: {confidence:.4f}\")\n",
    "\n",
    "# å°æ¯” Pipeline ç›´æ¥è¼¸å‡º\n",
    "print(f\"\\nPipeline ç›´æ¥è¼¸å‡º:\")\n",
    "print(classifier(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Pipeline é€²éšåƒæ•¸è¨­å®š\n",
    "\n",
    "### 2.1 æ ¸å¿ƒåƒæ•¸å®Œæ•´åˆ—è¡¨\n",
    "\n",
    "| åƒæ•¸ | èªªæ˜ | ç¯„ä¾‹å€¼ | é©ç”¨å ´æ™¯ |\n",
    "|------|------|--------|----------|\n",
    "| `model` | æŒ‡å®šæ¨¡å‹ | `\"bert-base-uncased\"` | ä½¿ç”¨ç‰¹å®šæ¨¡å‹ |\n",
    "| `tokenizer` | æŒ‡å®šåˆ†è©å™¨ | `\"bert-base-uncased\"` | è‡ªè¨‚åˆ†è©é‚è¼¯ |\n",
    "| `device` | é‹ç®—è¨­å‚™ | `0` (GPU), `-1` (CPU) | GPU åŠ é€Ÿ |\n",
    "| `batch_size` | æ‰¹æ¬¡å¤§å° | `8`, `16`, `32` | æ‰¹æ¬¡è™•ç† |\n",
    "| `return_all_scores` | è¿”å›æ‰€æœ‰é¡åˆ¥åˆ†æ•¸ | `True`, `False` | æŸ¥çœ‹æ‰€æœ‰æ©Ÿç‡ |\n",
    "| `top_k` | è¿”å›å‰ k å€‹çµæœ | `3`, `5` | å¤šå€™é¸çµæœ |\n",
    "| `max_length` | æœ€å¤§åºåˆ—é•·åº¦ | `512`, `128` | é™åˆ¶è¼¸å…¥é•·åº¦ |\n",
    "| `truncation` | æˆªæ–·ç­–ç•¥ | `True`, `\"only_first\"` | è™•ç†é•·æ–‡æœ¬ |\n",
    "\n",
    "### 2.2 æŒ‡å®šæ¨¡å‹èˆ‡è¨­å‚™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ–¹å¼ 1: ä½¿ç”¨é è¨­æ¨¡å‹\n",
    "pipe_default = pipeline(\"sentiment-analysis\")\n",
    "print(f\"é è¨­æ¨¡å‹: {pipe_default.model.config._name_or_path}\")\n",
    "\n",
    "# æ–¹å¼ 2: æŒ‡å®šç‰¹å®šæ¨¡å‹\n",
    "pipe_custom = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    ")\n",
    "print(f\"è‡ªè¨‚æ¨¡å‹: {pipe_custom.model.config._name_or_path}\")\n",
    "\n",
    "# æ–¹å¼ 3: GPU åŠ é€Ÿ (å¦‚æœå¯ç”¨)\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "pipe_gpu = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    device=device\n",
    ")\n",
    "print(f\"é‹ç®—è¨­å‚™: {'GPU' if device == 0 else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 è¿”å›æ‰€æœ‰åˆ†æ•¸èˆ‡ Top-K çµæœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‰µå»ºé›¶æ¨£æœ¬åˆ†é¡ Pipeline\n",
    "classifier = pipeline(\n",
    "    \"zero-shot-classification\",\n",
    "    model=\"facebook/bart-large-mnli\"\n",
    ")\n",
    "\n",
    "# æ¸¬è©¦æ–‡æœ¬\n",
    "text = \"I love programming in Python!\"\n",
    "candidate_labels = [\"technology\", \"sports\", \"politics\", \"entertainment\"]\n",
    "\n",
    "# é è¨­: åªè¿”å›æœ€é«˜åˆ†é¡\n",
    "result = classifier(text, candidate_labels)\n",
    "print(\"é è¨­è¼¸å‡º (Top-1):\")\n",
    "print(f\"æ¨™ç±¤: {result['labels'][0]}\")\n",
    "print(f\"åˆ†æ•¸: {result['scores'][0]:.4f}\")\n",
    "\n",
    "# è¿”å›æ‰€æœ‰é¡åˆ¥çš„åˆ†æ•¸\n",
    "result_all = classifier(\n",
    "    text, \n",
    "    candidate_labels,\n",
    "    multi_label=False  # å–®æ¨™ç±¤åˆ†é¡\n",
    ")\n",
    "print(\"\\næ‰€æœ‰é¡åˆ¥åˆ†æ•¸:\")\n",
    "for label, score in zip(result_all['labels'], result_all['scores']):\n",
    "    print(f\"{label:15s}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. æ‰¹æ¬¡è™•ç†èˆ‡æ•ˆèƒ½å„ªåŒ–\n",
    "\n",
    "### 3.1 æ‰¹æ¬¡è™•ç†åŸºç¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# æº–å‚™æ¸¬è©¦æ•¸æ“š\n",
    "texts = [\n",
    "    \"This is great!\",\n",
    "    \"I hate this product.\",\n",
    "    \"Not bad, could be better.\",\n",
    "    \"Absolutely amazing experience!\",\n",
    "    \"Terrible service, very disappointed.\"\n",
    "] * 20  # 100 ç­†æ•¸æ“š\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\", device=-1)\n",
    "\n",
    "# æ–¹å¼ 1: é€ç­†è™•ç† (æ…¢)\n",
    "start = time.time()\n",
    "results_single = [classifier(text)[0] for text in texts]\n",
    "time_single = time.time() - start\n",
    "\n",
    "print(f\"é€ç­†è™•ç†æ™‚é–“: {time_single:.2f}s\")\n",
    "\n",
    "# æ–¹å¼ 2: æ‰¹æ¬¡è™•ç† (å¿«)\n",
    "start = time.time()\n",
    "results_batch = classifier(texts, batch_size=16)\n",
    "time_batch = time.time() - start\n",
    "\n",
    "print(f\"æ‰¹æ¬¡è™•ç†æ™‚é–“: {time_batch:.2f}s\")\n",
    "print(f\"åŠ é€Ÿæ¯”: {time_single/time_batch:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 æœ€ä½³ Batch Size é¸æ“‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# æ¸¬è©¦ä¸åŒ batch size\n",
    "batch_sizes = [1, 2, 4, 8, 16, 32]\n",
    "processing_times = []\n",
    "\n",
    "test_texts = texts[:50]  # ä½¿ç”¨ 50 ç­†æ¸¬è©¦\n",
    "\n",
    "for bs in batch_sizes:\n",
    "    start = time.time()\n",
    "    _ = classifier(test_texts, batch_size=bs)\n",
    "    elapsed = time.time() - start\n",
    "    processing_times.append(elapsed)\n",
    "    print(f\"Batch Size {bs:2d}: {elapsed:.3f}s\")\n",
    "\n",
    "# ç¹ªè£½æ•ˆèƒ½æ›²ç·š\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(batch_sizes, processing_times, marker='o', linewidth=2)\n",
    "plt.xlabel('Batch Size', fontsize=12)\n",
    "plt.ylabel('Processing Time (s)', fontsize=12)\n",
    "plt.title('Batch Size vs Processing Time', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(batch_sizes)\n",
    "plt.show()\n",
    "\n",
    "# æ‰¾å‡ºæœ€ä½³ batch size\n",
    "best_idx = processing_times.index(min(processing_times))\n",
    "print(f\"\\næœ€ä½³ Batch Size: {batch_sizes[best_idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 é•·æ–‡æœ¬è™•ç†ç­–ç•¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”Ÿæˆé•·æ–‡æœ¬\n",
    "long_text = \"This is a great product. \" * 200  # è¶…é 512 tokens\n",
    "\n",
    "# ç­–ç•¥ 1: æˆªæ–· (é è¨­)\n",
    "pipe_truncate = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    truncation=True,\n",
    "    max_length=512\n",
    ")\n",
    "result1 = pipe_truncate(long_text)\n",
    "print(\"ç­–ç•¥ 1 - æˆªæ–·:\")\n",
    "print(result1)\n",
    "\n",
    "# ç­–ç•¥ 2: åˆ†æ®µè™•ç† + æŠ•ç¥¨\n",
    "def chunk_text(text, max_length=400, overlap=50):\n",
    "    \"\"\"å°‡é•·æ–‡æœ¬åˆ†æ®µ\"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    \n",
    "    for i in range(0, len(words), max_length - overlap):\n",
    "        chunk = ' '.join(words[i:i + max_length])\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "chunks = chunk_text(long_text)\n",
    "print(f\"\\nç­–ç•¥ 2 - åˆ†æ®µè™•ç† ({len(chunks)} æ®µ):\")\n",
    "\n",
    "# å°æ¯æ®µé€²è¡Œé æ¸¬\n",
    "chunk_results = pipe_truncate(chunks)\n",
    "\n",
    "# æŠ•ç¥¨æ±ºå®šæœ€çµ‚çµæœ\n",
    "from collections import Counter\n",
    "labels = [r['label'] for r in chunk_results]\n",
    "final_label = Counter(labels).most_common(1)[0][0]\n",
    "avg_score = sum(r['score'] for r in chunk_results) / len(chunk_results)\n",
    "\n",
    "print(f\"æœ€çµ‚é æ¸¬: {final_label} (å¹³å‡ä¿¡å¿ƒåº¦: {avg_score:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. è‡ªè¨‚ Pipeline åƒæ•¸\n",
    "\n",
    "### 4.1 è‡ªè¨‚åˆ†è©å™¨åƒæ•¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "\n",
    "# è¼‰å…¥æ¨¡å‹èˆ‡åˆ†è©å™¨\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# è‡ªè¨‚åˆ†è©åƒæ•¸\n",
    "custom_pipeline = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    padding=\"max_length\",     # å¡«å……ç­–ç•¥\n",
    "    truncation=True,           # æˆªæ–·\n",
    "    max_length=128,            # æœ€å¤§é•·åº¦\n",
    "    return_tensors=\"pt\"        # PyTorch å¼µé‡\n",
    ")\n",
    "\n",
    "# æ¸¬è©¦\n",
    "test_text = \"I absolutely love this!\"\n",
    "result = custom_pipeline(test_text)\n",
    "print(f\"çµæœ: {result}\")\n",
    "\n",
    "# æŸ¥çœ‹å¯¦éš› token æ•¸é‡\n",
    "tokens = tokenizer(test_text, return_tensors=\"pt\")\n",
    "print(f\"Token æ•¸é‡: {tokens['input_ids'].shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 è‡ªè¨‚å¾Œè™•ç†é‚è¼¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CustomSentimentPipeline:\n",
    "    def __init__(self, model_name):\n",
    "        self.pipe = pipeline(\"sentiment-analysis\", model=model_name)\n",
    "    \n",
    "    def __call__(self, texts, threshold=0.6):\n",
    "        \"\"\"è‡ªè¨‚å¾Œè™•ç†: ä½ä¿¡å¿ƒåº¦æ¨™è¨˜ç‚º NEUTRAL\"\"\"\n",
    "        results = self.pipe(texts)\n",
    "        \n",
    "        # å¦‚æœæ˜¯å–®ä¸€æ–‡æœ¬,è½‰ç‚ºåˆ—è¡¨\n",
    "        if not isinstance(results, list):\n",
    "            results = [results]\n",
    "        \n",
    "        # è‡ªè¨‚å¾Œè™•ç†é‚è¼¯\n",
    "        processed_results = []\n",
    "        for result in results:\n",
    "            if result['score'] < threshold:\n",
    "                result = {\n",
    "                    'label': 'NEUTRAL',\n",
    "                    'score': 1 - result['score']\n",
    "                }\n",
    "            processed_results.append(result)\n",
    "        \n",
    "        return processed_results\n",
    "\n",
    "# æ¸¬è©¦è‡ªè¨‚ Pipeline\n",
    "custom_pipe = CustomSentimentPipeline(\n",
    "    \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    ")\n",
    "\n",
    "test_cases = [\n",
    "    \"This is absolutely fantastic!\",  # é«˜ä¿¡å¿ƒåº¦ POSITIVE\n",
    "    \"It's okay, I guess.\",             # ä½ä¿¡å¿ƒåº¦ â†’ NEUTRAL\n",
    "    \"Terrible experience!\"             # é«˜ä¿¡å¿ƒåº¦ NEGATIVE\n",
    "]\n",
    "\n",
    "results = custom_pipe(test_cases, threshold=0.7)\n",
    "\n",
    "for text, result in zip(test_cases, results):\n",
    "    print(f\"æ–‡æœ¬: {text}\")\n",
    "    print(f\"çµæœ: {result['label']} ({result['score']:.4f})\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Pipeline ä»»å‹™æ·±å…¥æ¢ç´¢\n",
    "\n",
    "### 5.1 Fill-Mask (å®Œå½¢å¡«ç©º)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill-Mask Pipeline\n",
    "unmasker = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
    "\n",
    "# æ¸¬è©¦æ–‡æœ¬ (ä½¿ç”¨ [MASK] æ¨™è¨˜)\n",
    "text = \"Hugging Face is [MASK] for NLP tasks.\"\n",
    "results = unmasker(text, top_k=5)\n",
    "\n",
    "print(f\"åŸå§‹å¥å­: {text}\\n\")\n",
    "print(\"Top 5 é æ¸¬:\")\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"{i}. {result['sequence']}\")\n",
    "    print(f\"   Token: {result['token_str']}, Score: {result['score']:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Text Generation (æ–‡æœ¬ç”Ÿæˆ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Generation Pipeline\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"gpt2\",\n",
    "    device=-1\n",
    ")\n",
    "\n",
    "# åŸºç¤ç”Ÿæˆ\n",
    "prompt = \"Artificial intelligence is\"\n",
    "result = generator(\n",
    "    prompt,\n",
    "    max_length=50,\n",
    "    num_return_sequences=3,\n",
    "    temperature=0.8,          # å‰µæ„åº¦ (0.0-1.0)\n",
    "    top_k=50,                 # Top-K æ¡æ¨£\n",
    "    top_p=0.95,               # Nucleus æ¡æ¨£\n",
    "    do_sample=True            # å•Ÿç”¨æ¡æ¨£\n",
    ")\n",
    "\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "for i, gen in enumerate(result, 1):\n",
    "    print(f\"ç”Ÿæˆ {i}:\")\n",
    "    print(gen['generated_text'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ç”Ÿæˆåƒæ•¸èªªæ˜**:\n",
    "\n",
    "| åƒæ•¸ | ç¯„åœ | èªªæ˜ | æ•ˆæœ |\n",
    "|------|------|------|------|\n",
    "| `temperature` | 0.0-2.0 | æ§åˆ¶éš¨æ©Ÿæ€§ | è¶Šé«˜è¶Šå‰µæ„,è¶Šä½è¶Šç¢ºå®š |\n",
    "| `top_k` | 1-100 | Top-K æ¡æ¨£ | å¾å‰ k å€‹æœ€é«˜æ©Ÿç‡ä¸­é¸æ“‡ |\n",
    "| `top_p` | 0.0-1.0 | Nucleus æ¡æ¨£ | ç´¯ç©æ©Ÿç‡é” p æ™‚åœæ­¢ |\n",
    "| `repetition_penalty` | 1.0-2.0 | é‡è¤‡æ‡²ç½° | é¿å…é‡è¤‡è©å½™ |\n",
    "| `num_beams` | 1-10 | Beam Search | æ›´å¥½ä½†æ›´æ…¢çš„ç”Ÿæˆ |\n",
    "\n",
    "### 5.3 Question Answering (å•ç­”ç³»çµ±)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question Answering Pipeline\n",
    "qa_pipeline = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=\"distilbert-base-cased-distilled-squad\"\n",
    ")\n",
    "\n",
    "# æº–å‚™ä¸Šä¸‹æ–‡èˆ‡å•é¡Œ\n",
    "context = \"\"\"\n",
    "Hugging Face is a company founded in 2016 that specializes in natural language processing.\n",
    "The company is headquartered in New York City and Paris.\n",
    "Their Transformers library has over 100,000 stars on GitHub and is used by thousands of companies.\n",
    "\"\"\"\n",
    "\n",
    "questions = [\n",
    "    \"When was Hugging Face founded?\",\n",
    "    \"Where is Hugging Face headquartered?\",\n",
    "    \"How many stars does the Transformers library have?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    result = qa_pipeline(\n",
    "        question=question,\n",
    "        context=context,\n",
    "        top_k=1\n",
    "    )\n",
    "    \n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"A: {result['answer']}\")\n",
    "    print(f\"   ä¿¡å¿ƒåº¦: {result['score']:.4f}\")\n",
    "    print(f\"   ä½ç½®: {result['start']}-{result['end']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Summarization (æ–‡æœ¬æ‘˜è¦)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarization Pipeline\n",
    "summarizer = pipeline(\n",
    "    \"summarization\",\n",
    "    model=\"facebook/bart-large-cnn\"\n",
    ")\n",
    "\n",
    "# é•·æ–‡æœ¬ç¯„ä¾‹\n",
    "article = \"\"\"\n",
    "The Transformer architecture, introduced in the paper \"Attention Is All You Need\" by Vaswani et al. in 2017,\n",
    "revolutionized natural language processing. Unlike previous architectures that relied on recurrent or convolutional layers,\n",
    "Transformers use self-attention mechanisms to process input sequences in parallel. This parallel processing capability\n",
    "makes Transformers significantly faster to train than RNNs. The architecture consists of an encoder and a decoder,\n",
    "each composed of multiple layers of self-attention and feed-forward networks. The self-attention mechanism allows\n",
    "the model to weigh the importance of different words in a sentence when encoding each word. This has proven to be\n",
    "extremely effective for a wide range of NLP tasks, from translation to text generation.\n",
    "\"\"\"\n",
    "\n",
    "# ç”Ÿæˆæ‘˜è¦\n",
    "summary = summarizer(\n",
    "    article,\n",
    "    max_length=60,\n",
    "    min_length=30,\n",
    "    do_sample=False  # ä½¿ç”¨ Beam Search (æ›´ç©©å®š)\n",
    ")\n",
    "\n",
    "print(\"åŸæ–‡é•·åº¦:\", len(article.split()))\n",
    "print(\"\\nåŸæ–‡:\")\n",
    "print(article.strip())\n",
    "print(\"\\næ‘˜è¦:\")\n",
    "print(summary[0]['summary_text'])\n",
    "print(\"\\næ‘˜è¦é•·åº¦:\", len(summary[0]['summary_text'].split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. å¯¦æˆ°æ¡ˆä¾‹: å¤šä»»å‹™ NLP æ‡‰ç”¨\n",
    "\n",
    "### 6.1 æ•´åˆå¤šå€‹ Pipeline çš„æ™ºèƒ½åŠ©æ‰‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskNLPAssistant:\n",
    "    def __init__(self):\n",
    "        # åˆå§‹åŒ–å¤šå€‹ Pipeline\n",
    "        self.sentiment = pipeline(\"sentiment-analysis\")\n",
    "        self.ner = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
    "        self.qa = pipeline(\"question-answering\")\n",
    "        self.summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "    \n",
    "    def analyze(self, text):\n",
    "        \"\"\"ç¶œåˆåˆ†ææ–‡æœ¬\"\"\"\n",
    "        print(\"=\"*60)\n",
    "        print(\"NLP æ™ºèƒ½åŠ©æ‰‹åˆ†æå ±å‘Š\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # 1. æƒ…æ„Ÿåˆ†æ\n",
    "        sentiment = self.sentiment(text)[0]\n",
    "        print(f\"\\nğŸ“Š æƒ…æ„Ÿåˆ†æ:\")\n",
    "        print(f\"   {sentiment['label']} (ä¿¡å¿ƒåº¦: {sentiment['score']:.2%})\")\n",
    "        \n",
    "        # 2. å‘½åå¯¦é«”è­˜åˆ¥\n",
    "        entities = self.ner(text)\n",
    "        print(f\"\\nğŸ·ï¸  å¯¦é«”è­˜åˆ¥:\")\n",
    "        if entities:\n",
    "            for ent in entities:\n",
    "                print(f\"   {ent['word']:20s} â†’ {ent['entity_group']} ({ent['score']:.2%})\")\n",
    "        else:\n",
    "            print(\"   (æœªç™¼ç¾å¯¦é«”)\")\n",
    "        \n",
    "        # 3. æ–‡æœ¬æ‘˜è¦ (å¦‚æœæ–‡æœ¬å¤ é•·)\n",
    "        if len(text.split()) > 50:\n",
    "            summary = self.summarizer(text, max_length=50, min_length=20)[0]\n",
    "            print(f\"\\nğŸ“ æ–‡æœ¬æ‘˜è¦:\")\n",
    "            print(f\"   {summary['summary_text']}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# æ¸¬è©¦æ™ºèƒ½åŠ©æ‰‹\n",
    "assistant = MultiTaskNLPAssistant()\n",
    "\n",
    "test_text = \"\"\"\n",
    "Apple Inc. announced today that Tim Cook, the company's CEO, will speak at a conference in San Francisco next week.\n",
    "The event is expected to unveil new products including the latest iPhone model.\n",
    "Investors are excited about the announcement, and Apple's stock price rose by 3% in after-hours trading.\n",
    "\"\"\"\n",
    "\n",
    "assistant.analyze(test_text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 ä¸²æ¥ Pipeline çš„å°è©±ç³»çµ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationalAssistant:\n",
    "    def __init__(self):\n",
    "        self.sentiment = pipeline(\"sentiment-analysis\")\n",
    "        self.qa = pipeline(\"question-answering\")\n",
    "        self.generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "        \n",
    "        # çŸ¥è­˜åº«\n",
    "        self.knowledge_base = \"\"\"\n",
    "        Hugging Face is a company specializing in NLP. It was founded in 2016.\n",
    "        The company provides the Transformers library, which supports over 50,000 pretrained models.\n",
    "        \"\"\"\n",
    "    \n",
    "    def respond(self, user_input):\n",
    "        # 1. æƒ…æ„Ÿæª¢æ¸¬\n",
    "        sentiment = self.sentiment(user_input)[0]\n",
    "        \n",
    "        # 2. åˆ¤æ–·æ˜¯å¦ç‚ºå•é¡Œ\n",
    "        if \"?\" in user_input or user_input.lower().startswith((\"what\", \"when\", \"who\", \"where\", \"how\")):\n",
    "            # ä½¿ç”¨ QA Pipeline\n",
    "            try:\n",
    "                answer = self.qa(\n",
    "                    question=user_input,\n",
    "                    context=self.knowledge_base\n",
    "                )\n",
    "                return f\"æ ¹æ“šæˆ‘çš„çŸ¥è­˜: {answer['answer']}\"\n",
    "            except:\n",
    "                return \"æŠ±æ­‰,æˆ‘ç„¡æ³•å›ç­”é€™å€‹å•é¡Œã€‚\"\n",
    "        \n",
    "        # 3. æ ¹æ“šæƒ…æ„Ÿç”Ÿæˆå›æ‡‰\n",
    "        if sentiment['label'] == 'NEGATIVE':\n",
    "            return \"è½èµ·ä¾†æ‚¨ä¼¼ä¹ä¸å¤ªé–‹å¿ƒ,æˆ‘èƒ½ç‚ºæ‚¨åšäº›ä»€éº¼å—?\"\n",
    "        else:\n",
    "            return \"å¾ˆé«˜èˆˆè½åˆ°é€™å€‹!é‚„æœ‰å…¶ä»–æˆ‘å¯ä»¥å”åŠ©çš„å—?\"\n",
    "\n",
    "# æ¸¬è©¦å°è©±ç³»çµ±\n",
    "chatbot = ConversationalAssistant()\n",
    "\n",
    "conversations = [\n",
    "    \"When was Hugging Face founded?\",\n",
    "    \"I'm really frustrated with this!\",\n",
    "    \"This is working great, thank you!\"\n",
    "]\n",
    "\n",
    "for user_msg in conversations:\n",
    "    bot_reply = chatbot.respond(user_msg)\n",
    "    print(f\"ç”¨æˆ¶: {user_msg}\")\n",
    "    print(f\"åŠ©æ‰‹: {bot_reply}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. æ•ˆèƒ½å„ªåŒ–é€²éšæŠ€å·§\n",
    "\n",
    "### 7.1 æ¨¡å‹é‡åŒ– (Quantization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "# åŸå§‹æ¨¡å‹\n",
    "model_fp32 = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# å‹•æ…‹é‡åŒ– (PyTorch)\n",
    "model_int8 = torch.quantization.quantize_dynamic(\n",
    "    model_fp32,\n",
    "    {torch.nn.Linear},  # é‡åŒ–çš„å±¤é¡å‹\n",
    "    dtype=torch.qint8\n",
    ")\n",
    "\n",
    "# æ¯”è¼ƒæ¨¡å‹å¤§å°\n",
    "def get_model_size(model):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    size = os.path.getsize(\"temp.p\") / 1e6  # MB\n",
    "    os.remove(\"temp.p\")\n",
    "    return size\n",
    "\n",
    "size_fp32 = get_model_size(model_fp32)\n",
    "size_int8 = get_model_size(model_int8)\n",
    "\n",
    "print(f\"FP32 æ¨¡å‹å¤§å°: {size_fp32:.2f} MB\")\n",
    "print(f\"INT8 æ¨¡å‹å¤§å°: {size_int8:.2f} MB\")\n",
    "print(f\"å£“ç¸®æ¯”: {size_fp32/size_int8:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 æ¨¡å‹å¿«å–ç­–ç•¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨­å®šå¿«å–ç›®éŒ„\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# è‡ªè¨‚å¿«å–ä½ç½®\n",
    "cache_dir = Path(\"./model_cache\")\n",
    "cache_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# ä¸‹è¼‰ä¸¦å¿«å–æ¨¡å‹\n",
    "pipe = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    model_kwargs={\"cache_dir\": str(cache_dir)}\n",
    ")\n",
    "\n",
    "print(f\"æ¨¡å‹å·²å¿«å–è‡³: {cache_dir}\")\n",
    "print(f\"å¿«å–æª”æ¡ˆ:\")\n",
    "for file in cache_dir.rglob(\"*\"):\n",
    "    if file.is_file():\n",
    "        print(f\"  {file.name} ({file.stat().st_size / 1e6:.2f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. èª²å¾Œç·´ç¿’\n",
    "\n",
    "### ç·´ç¿’ 1: æ‰¹æ¬¡æ•ˆèƒ½å„ªåŒ–\n",
    "\n",
    "æ¯”è¼ƒä¸åŒ batch size å°æ¨ç†æ™‚é–“çš„å½±éŸ¿,æ‰¾å‡ºæœ€ä½³è¨­å®šã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: å¯¦ä½œæ‰¹æ¬¡æ•ˆèƒ½æ¸¬è©¦\n",
    "# 1. æº–å‚™ 500 ç­†æ¸¬è©¦æ•¸æ“š\n",
    "# 2. æ¸¬è©¦ batch_size = [1, 4, 8, 16, 32, 64]\n",
    "# 3. è¨˜éŒ„æ¯å€‹è¨­å®šçš„è™•ç†æ™‚é–“\n",
    "# 4. ç¹ªè£½æ•ˆèƒ½æ›²ç·š\n",
    "# 5. åˆ†ææœ€ä½³ batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ç·´ç¿’ 2: è‡ªè¨‚ Pipeline å¾Œè™•ç†\n",
    "\n",
    "å‰µå»ºä¸€å€‹æƒ…æ„Ÿåˆ†æ Pipeline,ç•¶ä¿¡å¿ƒåº¦ä½æ–¼é–¾å€¼æ™‚,æ¨™è¨˜ç‚º \"UNCERTAIN\"ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: å¯¦ä½œè‡ªè¨‚å¾Œè™•ç†é‚è¼¯\n",
    "# 1. ç¹¼æ‰¿ pipeline æˆ–å‰µå»ºåŒ…è£é¡\n",
    "# 2. æ·»åŠ  threshold åƒæ•¸\n",
    "# 3. ä½ä¿¡å¿ƒåº¦æ¨£æœ¬æ¨™è¨˜ç‚º UNCERTAIN\n",
    "# 4. æ¸¬è©¦ä¸åŒé–¾å€¼çš„æ•ˆæœ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ç·´ç¿’ 3: å¤šèªè¨€æ”¯æ´\n",
    "\n",
    "ä½¿ç”¨å¤šèªè¨€æ¨¡å‹ (å¦‚ xlm-roberta) å‰µå»ºæ”¯æ´ä¸­è‹±æ–‡çš„æƒ…æ„Ÿåˆ†æ Pipelineã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: å¯¦ä½œå¤šèªè¨€æƒ…æ„Ÿåˆ†æ\n",
    "# 1. è¼‰å…¥ xlm-roberta-base\n",
    "# 2. æ¸¬è©¦è‹±æ–‡å’Œä¸­æ–‡æ–‡æœ¬\n",
    "# 3. æ¯”è¼ƒä¸åŒèªè¨€çš„é æ¸¬çµæœ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. æœ¬ç¯€ç¸½çµ\n",
    "\n",
    "### âœ… é—œéµè¦é»\n",
    "\n",
    "1. **Pipeline å…§éƒ¨æ©Ÿåˆ¶**:\n",
    "   - Preprocessing (Tokenizer) â†’ Inference (Model) â†’ Postprocessing\n",
    "   - ç†è§£ä¸‰éšæ®µæœ‰åŠ©æ–¼ Debug å’Œå„ªåŒ–\n",
    "\n",
    "2. **æ•ˆèƒ½å„ªåŒ–ç­–ç•¥**:\n",
    "   - æ‰¹æ¬¡è™•ç† (Batch Processing)\n",
    "   - é¸æ“‡åˆé©çš„ Batch Size\n",
    "   - æ¨¡å‹é‡åŒ– (Quantization)\n",
    "   - å¿«å–æ©Ÿåˆ¶ (Caching)\n",
    "\n",
    "3. **é€²éšåƒæ•¸è¨­å®š**:\n",
    "   - `device`: CPU/GPU é¸æ“‡\n",
    "   - `batch_size`: æ‰¹æ¬¡å¤§å°\n",
    "   - `top_k`, `return_all_scores`: æ§åˆ¶è¼¸å‡º\n",
    "   - `max_length`, `truncation`: è™•ç†é•·æ–‡æœ¬\n",
    "\n",
    "4. **å¯¦æˆ°æ‡‰ç”¨**:\n",
    "   - å¤šä»»å‹™ NLP åŠ©æ‰‹\n",
    "   - å°è©±ç³»çµ±æ•´åˆ\n",
    "   - è‡ªè¨‚å¾Œè™•ç†é‚è¼¯\n",
    "\n",
    "### ğŸ“Š æ•ˆèƒ½å°æ¯”ç¸½çµ\n",
    "\n",
    "| å„ªåŒ–æ–¹æ³• | é€Ÿåº¦æå‡ | è¨˜æ†¶é«”ç¯€çœ | ç²¾åº¦æå¤± |\n",
    "|---------|---------|-----------|----------|\n",
    "| æ‰¹æ¬¡è™•ç† | 2-5x | - | ç„¡ |\n",
    "| INT8 é‡åŒ– | 1.5-2x | 4x | å¾®å° (<1%) |\n",
    "| æ¨¡å‹è’¸é¤¾ | 2-3x | 2-4x | å° (1-3%) |\n",
    "| GPU åŠ é€Ÿ | 5-10x | - | ç„¡ |\n",
    "\n",
    "### ğŸ“š å»¶ä¼¸é–±è®€\n",
    "\n",
    "- [Pipeline å®˜æ–¹æ–‡æª”](https://huggingface.co/docs/transformers/main_classes/pipelines)\n",
    "- [æ¨¡å‹é‡åŒ–æŒ‡å—](https://huggingface.co/docs/optimum/concept_guides/quantization)\n",
    "- [æ•ˆèƒ½å„ªåŒ–æŠ€å·§](https://huggingface.co/docs/transformers/performance)\n",
    "\n",
    "### ğŸš€ ä¸‹ä¸€ç¯€é å‘Š\n",
    "\n",
    "**CH08-03: æƒ…æ„Ÿåˆ†æå¯¦æˆ° (Sentiment Analysis)**\n",
    "- ä½¿ç”¨çœŸå¯¦ Twitter æ•¸æ“šé›†\n",
    "- æ¨¡å‹å¾®èª¿èˆ‡è©•ä¼°\n",
    "- éƒ¨ç½²åˆ°ç”Ÿç”¢ç’°å¢ƒ\n",
    "\n",
    "---\n",
    "\n",
    "**èª²ç¨‹**: iSpan Python NLP Cookbooks v2\n",
    "**è¬›å¸«**: Claude AI\n",
    "**æœ€å¾Œæ›´æ–°**: 2025-10-17"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
