{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH08-02: Pipeline API 快速入門\n",
    "\n",
    "**課程**: iSpan Python NLP Cookbooks v2\n",
    "**章節**: CH08 Hugging Face 函式庫實戰\n",
    "**版本**: v1.0\n",
    "**更新日期**: 2025-10-17\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 本節學習目標\n",
    "\n",
    "1. 深入理解 Pipeline 的內部工作機制\n",
    "2. 掌握 Pipeline 的進階參數設定\n",
    "3. 學會批次處理與效能優化技巧\n",
    "4. 自訂 Pipeline 參數與後處理邏輯\n",
    "5. 整合 Pipeline 到實際應用中\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Pipeline 內部機制深入解析\n",
    "\n",
    "### 1.1 Pipeline 的三階段處理流程\n",
    "\n",
    "```\n",
    "Pipeline 執行流程:\n",
    "\n",
    "Input Text\n",
    "    ↓\n",
    "┌─────────────────┐\n",
    "│  Preprocessing  │  ← Tokenizer 編碼\n",
    "│  (tokenization) │     - 分詞\n",
    "└─────────────────┘     - 添加特殊標記\n",
    "    ↓                    - 填充/截斷\n",
    "┌─────────────────┐\n",
    "│   Inference     │  ← Model 推理\n",
    "│   (forward)     │     - 前向傳播\n",
    "└─────────────────┘     - 計算 logits\n",
    "    ↓\n",
    "┌─────────────────┐\n",
    "│ Postprocessing  │  ← 結果解析\n",
    "│  (decode)       │     - Softmax\n",
    "└─────────────────┘     - Top-k 選擇\n",
    "    ↓                    - 格式化輸出\n",
    "Output Result\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先安裝必要套件\n",
    "# !pip install transformers torch -q\n",
    "\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# 創建情感分析 Pipeline\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# 查看 Pipeline 內部組件\n",
    "print(\"Pipeline 組件:\")\n",
    "print(f\"1. Model: {classifier.model.__class__.__name__}\")\n",
    "print(f\"2. Tokenizer: {classifier.tokenizer.__class__.__name__}\")\n",
    "print(f\"3. Device: {classifier.device}\")\n",
    "print(f\"4. Framework: {classifier.framework}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 手動分解 Pipeline 流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 Pipeline 的內部組件手動執行\n",
    "text = \"This movie is absolutely fantastic!\"\n",
    "\n",
    "# Step 1: Preprocessing (Tokenization)\n",
    "inputs = classifier.tokenizer(\n",
    "    text, \n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True\n",
    ")\n",
    "print(\"Step 1 - Tokenization:\")\n",
    "print(f\"input_ids: {inputs['input_ids']}\")\n",
    "print(f\"attention_mask: {inputs['attention_mask']}\")\n",
    "\n",
    "# Step 2: Inference (Forward Pass)\n",
    "with torch.no_grad():\n",
    "    outputs = classifier.model(**inputs)\n",
    "    logits = outputs.logits\n",
    "\n",
    "print(f\"\\nStep 2 - Model Inference:\")\n",
    "print(f\"logits: {logits}\")\n",
    "\n",
    "# Step 3: Postprocessing (Decode)\n",
    "predictions = torch.softmax(logits, dim=-1)\n",
    "predicted_class = torch.argmax(predictions, dim=-1).item()\n",
    "confidence = predictions[0][predicted_class].item()\n",
    "\n",
    "# 獲取標籤映射\n",
    "id2label = classifier.model.config.id2label\n",
    "label = id2label[predicted_class]\n",
    "\n",
    "print(f\"\\nStep 3 - Postprocessing:\")\n",
    "print(f\"Predicted Label: {label}\")\n",
    "print(f\"Confidence: {confidence:.4f}\")\n",
    "\n",
    "# 對比 Pipeline 直接輸出\n",
    "print(f\"\\nPipeline 直接輸出:\")\n",
    "print(classifier(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Pipeline 進階參數設定\n",
    "\n",
    "### 2.1 核心參數完整列表\n",
    "\n",
    "| 參數 | 說明 | 範例值 | 適用場景 |\n",
    "|------|------|--------|----------|\n",
    "| `model` | 指定模型 | `\"bert-base-uncased\"` | 使用特定模型 |\n",
    "| `tokenizer` | 指定分詞器 | `\"bert-base-uncased\"` | 自訂分詞邏輯 |\n",
    "| `device` | 運算設備 | `0` (GPU), `-1` (CPU) | GPU 加速 |\n",
    "| `batch_size` | 批次大小 | `8`, `16`, `32` | 批次處理 |\n",
    "| `return_all_scores` | 返回所有類別分數 | `True`, `False` | 查看所有機率 |\n",
    "| `top_k` | 返回前 k 個結果 | `3`, `5` | 多候選結果 |\n",
    "| `max_length` | 最大序列長度 | `512`, `128` | 限制輸入長度 |\n",
    "| `truncation` | 截斷策略 | `True`, `\"only_first\"` | 處理長文本 |\n",
    "\n",
    "### 2.2 指定模型與設備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方式 1: 使用預設模型\n",
    "pipe_default = pipeline(\"sentiment-analysis\")\n",
    "print(f\"預設模型: {pipe_default.model.config._name_or_path}\")\n",
    "\n",
    "# 方式 2: 指定特定模型\n",
    "pipe_custom = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    ")\n",
    "print(f\"自訂模型: {pipe_custom.model.config._name_or_path}\")\n",
    "\n",
    "# 方式 3: GPU 加速 (如果可用)\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "pipe_gpu = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    device=device\n",
    ")\n",
    "print(f\"運算設備: {'GPU' if device == 0 else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 返回所有分數與 Top-K 結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 創建零樣本分類 Pipeline\n",
    "classifier = pipeline(\n",
    "    \"zero-shot-classification\",\n",
    "    model=\"facebook/bart-large-mnli\"\n",
    ")\n",
    "\n",
    "# 測試文本\n",
    "text = \"I love programming in Python!\"\n",
    "candidate_labels = [\"technology\", \"sports\", \"politics\", \"entertainment\"]\n",
    "\n",
    "# 預設: 只返回最高分類\n",
    "result = classifier(text, candidate_labels)\n",
    "print(\"預設輸出 (Top-1):\")\n",
    "print(f\"標籤: {result['labels'][0]}\")\n",
    "print(f\"分數: {result['scores'][0]:.4f}\")\n",
    "\n",
    "# 返回所有類別的分數\n",
    "result_all = classifier(\n",
    "    text, \n",
    "    candidate_labels,\n",
    "    multi_label=False  # 單標籤分類\n",
    ")\n",
    "print(\"\\n所有類別分數:\")\n",
    "for label, score in zip(result_all['labels'], result_all['scores']):\n",
    "    print(f\"{label:15s}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. 批次處理與效能優化\n",
    "\n",
    "### 3.1 批次處理基礎"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# 準備測試數據\n",
    "texts = [\n",
    "    \"This is great!\",\n",
    "    \"I hate this product.\",\n",
    "    \"Not bad, could be better.\",\n",
    "    \"Absolutely amazing experience!\",\n",
    "    \"Terrible service, very disappointed.\"\n",
    "] * 20  # 100 筆數據\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\", device=-1)\n",
    "\n",
    "# 方式 1: 逐筆處理 (慢)\n",
    "start = time.time()\n",
    "results_single = [classifier(text)[0] for text in texts]\n",
    "time_single = time.time() - start\n",
    "\n",
    "print(f\"逐筆處理時間: {time_single:.2f}s\")\n",
    "\n",
    "# 方式 2: 批次處理 (快)\n",
    "start = time.time()\n",
    "results_batch = classifier(texts, batch_size=16)\n",
    "time_batch = time.time() - start\n",
    "\n",
    "print(f\"批次處理時間: {time_batch:.2f}s\")\n",
    "print(f\"加速比: {time_single/time_batch:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 最佳 Batch Size 選擇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 測試不同 batch size\n",
    "batch_sizes = [1, 2, 4, 8, 16, 32]\n",
    "processing_times = []\n",
    "\n",
    "test_texts = texts[:50]  # 使用 50 筆測試\n",
    "\n",
    "for bs in batch_sizes:\n",
    "    start = time.time()\n",
    "    _ = classifier(test_texts, batch_size=bs)\n",
    "    elapsed = time.time() - start\n",
    "    processing_times.append(elapsed)\n",
    "    print(f\"Batch Size {bs:2d}: {elapsed:.3f}s\")\n",
    "\n",
    "# 繪製效能曲線\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(batch_sizes, processing_times, marker='o', linewidth=2)\n",
    "plt.xlabel('Batch Size', fontsize=12)\n",
    "plt.ylabel('Processing Time (s)', fontsize=12)\n",
    "plt.title('Batch Size vs Processing Time', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(batch_sizes)\n",
    "plt.show()\n",
    "\n",
    "# 找出最佳 batch size\n",
    "best_idx = processing_times.index(min(processing_times))\n",
    "print(f\"\\n最佳 Batch Size: {batch_sizes[best_idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 長文本處理策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成長文本\n",
    "long_text = \"This is a great product. \" * 200  # 超過 512 tokens\n",
    "\n",
    "# 策略 1: 截斷 (預設)\n",
    "pipe_truncate = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    truncation=True,\n",
    "    max_length=512\n",
    ")\n",
    "result1 = pipe_truncate(long_text)\n",
    "print(\"策略 1 - 截斷:\")\n",
    "print(result1)\n",
    "\n",
    "# 策略 2: 分段處理 + 投票\n",
    "def chunk_text(text, max_length=400, overlap=50):\n",
    "    \"\"\"將長文本分段\"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    \n",
    "    for i in range(0, len(words), max_length - overlap):\n",
    "        chunk = ' '.join(words[i:i + max_length])\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "chunks = chunk_text(long_text)\n",
    "print(f\"\\n策略 2 - 分段處理 ({len(chunks)} 段):\")\n",
    "\n",
    "# 對每段進行預測\n",
    "chunk_results = pipe_truncate(chunks)\n",
    "\n",
    "# 投票決定最終結果\n",
    "from collections import Counter\n",
    "labels = [r['label'] for r in chunk_results]\n",
    "final_label = Counter(labels).most_common(1)[0][0]\n",
    "avg_score = sum(r['score'] for r in chunk_results) / len(chunk_results)\n",
    "\n",
    "print(f\"最終預測: {final_label} (平均信心度: {avg_score:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. 自訂 Pipeline 參數\n",
    "\n",
    "### 4.1 自訂分詞器參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "\n",
    "# 載入模型與分詞器\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# 自訂分詞參數\n",
    "custom_pipeline = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    padding=\"max_length\",     # 填充策略\n",
    "    truncation=True,           # 截斷\n",
    "    max_length=128,            # 最大長度\n",
    "    return_tensors=\"pt\"        # PyTorch 張量\n",
    ")\n",
    "\n",
    "# 測試\n",
    "test_text = \"I absolutely love this!\"\n",
    "result = custom_pipeline(test_text)\n",
    "print(f\"結果: {result}\")\n",
    "\n",
    "# 查看實際 token 數量\n",
    "tokens = tokenizer(test_text, return_tensors=\"pt\")\n",
    "print(f\"Token 數量: {tokens['input_ids'].shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 自訂後處理邏輯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CustomSentimentPipeline:\n",
    "    def __init__(self, model_name):\n",
    "        self.pipe = pipeline(\"sentiment-analysis\", model=model_name)\n",
    "    \n",
    "    def __call__(self, texts, threshold=0.6):\n",
    "        \"\"\"自訂後處理: 低信心度標記為 NEUTRAL\"\"\"\n",
    "        results = self.pipe(texts)\n",
    "        \n",
    "        # 如果是單一文本,轉為列表\n",
    "        if not isinstance(results, list):\n",
    "            results = [results]\n",
    "        \n",
    "        # 自訂後處理邏輯\n",
    "        processed_results = []\n",
    "        for result in results:\n",
    "            if result['score'] < threshold:\n",
    "                result = {\n",
    "                    'label': 'NEUTRAL',\n",
    "                    'score': 1 - result['score']\n",
    "                }\n",
    "            processed_results.append(result)\n",
    "        \n",
    "        return processed_results\n",
    "\n",
    "# 測試自訂 Pipeline\n",
    "custom_pipe = CustomSentimentPipeline(\n",
    "    \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    ")\n",
    "\n",
    "test_cases = [\n",
    "    \"This is absolutely fantastic!\",  # 高信心度 POSITIVE\n",
    "    \"It's okay, I guess.\",             # 低信心度 → NEUTRAL\n",
    "    \"Terrible experience!\"             # 高信心度 NEGATIVE\n",
    "]\n",
    "\n",
    "results = custom_pipe(test_cases, threshold=0.7)\n",
    "\n",
    "for text, result in zip(test_cases, results):\n",
    "    print(f\"文本: {text}\")\n",
    "    print(f\"結果: {result['label']} ({result['score']:.4f})\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Pipeline 任務深入探索\n",
    "\n",
    "### 5.1 Fill-Mask (完形填空)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill-Mask Pipeline\n",
    "unmasker = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
    "\n",
    "# 測試文本 (使用 [MASK] 標記)\n",
    "text = \"Hugging Face is [MASK] for NLP tasks.\"\n",
    "results = unmasker(text, top_k=5)\n",
    "\n",
    "print(f\"原始句子: {text}\\n\")\n",
    "print(\"Top 5 預測:\")\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"{i}. {result['sequence']}\")\n",
    "    print(f\"   Token: {result['token_str']}, Score: {result['score']:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Text Generation (文本生成)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Generation Pipeline\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"gpt2\",\n",
    "    device=-1\n",
    ")\n",
    "\n",
    "# 基礎生成\n",
    "prompt = \"Artificial intelligence is\"\n",
    "result = generator(\n",
    "    prompt,\n",
    "    max_length=50,\n",
    "    num_return_sequences=3,\n",
    "    temperature=0.8,          # 創意度 (0.0-1.0)\n",
    "    top_k=50,                 # Top-K 採樣\n",
    "    top_p=0.95,               # Nucleus 採樣\n",
    "    do_sample=True            # 啟用採樣\n",
    ")\n",
    "\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "for i, gen in enumerate(result, 1):\n",
    "    print(f\"生成 {i}:\")\n",
    "    print(gen['generated_text'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**生成參數說明**:\n",
    "\n",
    "| 參數 | 範圍 | 說明 | 效果 |\n",
    "|------|------|------|------|\n",
    "| `temperature` | 0.0-2.0 | 控制隨機性 | 越高越創意,越低越確定 |\n",
    "| `top_k` | 1-100 | Top-K 採樣 | 從前 k 個最高機率中選擇 |\n",
    "| `top_p` | 0.0-1.0 | Nucleus 採樣 | 累積機率達 p 時停止 |\n",
    "| `repetition_penalty` | 1.0-2.0 | 重複懲罰 | 避免重複詞彙 |\n",
    "| `num_beams` | 1-10 | Beam Search | 更好但更慢的生成 |\n",
    "\n",
    "### 5.3 Question Answering (問答系統)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question Answering Pipeline\n",
    "qa_pipeline = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=\"distilbert-base-cased-distilled-squad\"\n",
    ")\n",
    "\n",
    "# 準備上下文與問題\n",
    "context = \"\"\"\n",
    "Hugging Face is a company founded in 2016 that specializes in natural language processing.\n",
    "The company is headquartered in New York City and Paris.\n",
    "Their Transformers library has over 100,000 stars on GitHub and is used by thousands of companies.\n",
    "\"\"\"\n",
    "\n",
    "questions = [\n",
    "    \"When was Hugging Face founded?\",\n",
    "    \"Where is Hugging Face headquartered?\",\n",
    "    \"How many stars does the Transformers library have?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    result = qa_pipeline(\n",
    "        question=question,\n",
    "        context=context,\n",
    "        top_k=1\n",
    "    )\n",
    "    \n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"A: {result['answer']}\")\n",
    "    print(f\"   信心度: {result['score']:.4f}\")\n",
    "    print(f\"   位置: {result['start']}-{result['end']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Summarization (文本摘要)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarization Pipeline\n",
    "summarizer = pipeline(\n",
    "    \"summarization\",\n",
    "    model=\"facebook/bart-large-cnn\"\n",
    ")\n",
    "\n",
    "# 長文本範例\n",
    "article = \"\"\"\n",
    "The Transformer architecture, introduced in the paper \"Attention Is All You Need\" by Vaswani et al. in 2017,\n",
    "revolutionized natural language processing. Unlike previous architectures that relied on recurrent or convolutional layers,\n",
    "Transformers use self-attention mechanisms to process input sequences in parallel. This parallel processing capability\n",
    "makes Transformers significantly faster to train than RNNs. The architecture consists of an encoder and a decoder,\n",
    "each composed of multiple layers of self-attention and feed-forward networks. The self-attention mechanism allows\n",
    "the model to weigh the importance of different words in a sentence when encoding each word. This has proven to be\n",
    "extremely effective for a wide range of NLP tasks, from translation to text generation.\n",
    "\"\"\"\n",
    "\n",
    "# 生成摘要\n",
    "summary = summarizer(\n",
    "    article,\n",
    "    max_length=60,\n",
    "    min_length=30,\n",
    "    do_sample=False  # 使用 Beam Search (更穩定)\n",
    ")\n",
    "\n",
    "print(\"原文長度:\", len(article.split()))\n",
    "print(\"\\n原文:\")\n",
    "print(article.strip())\n",
    "print(\"\\n摘要:\")\n",
    "print(summary[0]['summary_text'])\n",
    "print(\"\\n摘要長度:\", len(summary[0]['summary_text'].split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. 實戰案例: 多任務 NLP 應用\n",
    "\n",
    "### 6.1 整合多個 Pipeline 的智能助手"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskNLPAssistant:\n",
    "    def __init__(self):\n",
    "        # 初始化多個 Pipeline\n",
    "        self.sentiment = pipeline(\"sentiment-analysis\")\n",
    "        self.ner = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
    "        self.qa = pipeline(\"question-answering\")\n",
    "        self.summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "    \n",
    "    def analyze(self, text):\n",
    "        \"\"\"綜合分析文本\"\"\"\n",
    "        print(\"=\"*60)\n",
    "        print(\"NLP 智能助手分析報告\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # 1. 情感分析\n",
    "        sentiment = self.sentiment(text)[0]\n",
    "        print(f\"\\n📊 情感分析:\")\n",
    "        print(f\"   {sentiment['label']} (信心度: {sentiment['score']:.2%})\")\n",
    "        \n",
    "        # 2. 命名實體識別\n",
    "        entities = self.ner(text)\n",
    "        print(f\"\\n🏷️  實體識別:\")\n",
    "        if entities:\n",
    "            for ent in entities:\n",
    "                print(f\"   {ent['word']:20s} → {ent['entity_group']} ({ent['score']:.2%})\")\n",
    "        else:\n",
    "            print(\"   (未發現實體)\")\n",
    "        \n",
    "        # 3. 文本摘要 (如果文本夠長)\n",
    "        if len(text.split()) > 50:\n",
    "            summary = self.summarizer(text, max_length=50, min_length=20)[0]\n",
    "            print(f\"\\n📝 文本摘要:\")\n",
    "            print(f\"   {summary['summary_text']}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# 測試智能助手\n",
    "assistant = MultiTaskNLPAssistant()\n",
    "\n",
    "test_text = \"\"\"\n",
    "Apple Inc. announced today that Tim Cook, the company's CEO, will speak at a conference in San Francisco next week.\n",
    "The event is expected to unveil new products including the latest iPhone model.\n",
    "Investors are excited about the announcement, and Apple's stock price rose by 3% in after-hours trading.\n",
    "\"\"\"\n",
    "\n",
    "assistant.analyze(test_text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 串接 Pipeline 的對話系統"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationalAssistant:\n",
    "    def __init__(self):\n",
    "        self.sentiment = pipeline(\"sentiment-analysis\")\n",
    "        self.qa = pipeline(\"question-answering\")\n",
    "        self.generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "        \n",
    "        # 知識庫\n",
    "        self.knowledge_base = \"\"\"\n",
    "        Hugging Face is a company specializing in NLP. It was founded in 2016.\n",
    "        The company provides the Transformers library, which supports over 50,000 pretrained models.\n",
    "        \"\"\"\n",
    "    \n",
    "    def respond(self, user_input):\n",
    "        # 1. 情感檢測\n",
    "        sentiment = self.sentiment(user_input)[0]\n",
    "        \n",
    "        # 2. 判斷是否為問題\n",
    "        if \"?\" in user_input or user_input.lower().startswith((\"what\", \"when\", \"who\", \"where\", \"how\")):\n",
    "            # 使用 QA Pipeline\n",
    "            try:\n",
    "                answer = self.qa(\n",
    "                    question=user_input,\n",
    "                    context=self.knowledge_base\n",
    "                )\n",
    "                return f\"根據我的知識: {answer['answer']}\"\n",
    "            except:\n",
    "                return \"抱歉,我無法回答這個問題。\"\n",
    "        \n",
    "        # 3. 根據情感生成回應\n",
    "        if sentiment['label'] == 'NEGATIVE':\n",
    "            return \"聽起來您似乎不太開心,我能為您做些什麼嗎?\"\n",
    "        else:\n",
    "            return \"很高興聽到這個!還有其他我可以協助的嗎?\"\n",
    "\n",
    "# 測試對話系統\n",
    "chatbot = ConversationalAssistant()\n",
    "\n",
    "conversations = [\n",
    "    \"When was Hugging Face founded?\",\n",
    "    \"I'm really frustrated with this!\",\n",
    "    \"This is working great, thank you!\"\n",
    "]\n",
    "\n",
    "for user_msg in conversations:\n",
    "    bot_reply = chatbot.respond(user_msg)\n",
    "    print(f\"用戶: {user_msg}\")\n",
    "    print(f\"助手: {bot_reply}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. 效能優化進階技巧\n",
    "\n",
    "### 7.1 模型量化 (Quantization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "# 原始模型\n",
    "model_fp32 = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 動態量化 (PyTorch)\n",
    "model_int8 = torch.quantization.quantize_dynamic(\n",
    "    model_fp32,\n",
    "    {torch.nn.Linear},  # 量化的層類型\n",
    "    dtype=torch.qint8\n",
    ")\n",
    "\n",
    "# 比較模型大小\n",
    "def get_model_size(model):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    size = os.path.getsize(\"temp.p\") / 1e6  # MB\n",
    "    os.remove(\"temp.p\")\n",
    "    return size\n",
    "\n",
    "size_fp32 = get_model_size(model_fp32)\n",
    "size_int8 = get_model_size(model_int8)\n",
    "\n",
    "print(f\"FP32 模型大小: {size_fp32:.2f} MB\")\n",
    "print(f\"INT8 模型大小: {size_int8:.2f} MB\")\n",
    "print(f\"壓縮比: {size_fp32/size_int8:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 模型快取策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定快取目錄\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# 自訂快取位置\n",
    "cache_dir = Path(\"./model_cache\")\n",
    "cache_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# 下載並快取模型\n",
    "pipe = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    model_kwargs={\"cache_dir\": str(cache_dir)}\n",
    ")\n",
    "\n",
    "print(f\"模型已快取至: {cache_dir}\")\n",
    "print(f\"快取檔案:\")\n",
    "for file in cache_dir.rglob(\"*\"):\n",
    "    if file.is_file():\n",
    "        print(f\"  {file.name} ({file.stat().st_size / 1e6:.2f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. 課後練習\n",
    "\n",
    "### 練習 1: 批次效能優化\n",
    "\n",
    "比較不同 batch size 對推理時間的影響,找出最佳設定。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 實作批次效能測試\n",
    "# 1. 準備 500 筆測試數據\n",
    "# 2. 測試 batch_size = [1, 4, 8, 16, 32, 64]\n",
    "# 3. 記錄每個設定的處理時間\n",
    "# 4. 繪製效能曲線\n",
    "# 5. 分析最佳 batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習 2: 自訂 Pipeline 後處理\n",
    "\n",
    "創建一個情感分析 Pipeline,當信心度低於閾值時,標記為 \"UNCERTAIN\"。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 實作自訂後處理邏輯\n",
    "# 1. 繼承 pipeline 或創建包裝類\n",
    "# 2. 添加 threshold 參數\n",
    "# 3. 低信心度樣本標記為 UNCERTAIN\n",
    "# 4. 測試不同閾值的效果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習 3: 多語言支援\n",
    "\n",
    "使用多語言模型 (如 xlm-roberta) 創建支援中英文的情感分析 Pipeline。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 實作多語言情感分析\n",
    "# 1. 載入 xlm-roberta-base\n",
    "# 2. 測試英文和中文文本\n",
    "# 3. 比較不同語言的預測結果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. 本節總結\n",
    "\n",
    "### ✅ 關鍵要點\n",
    "\n",
    "1. **Pipeline 內部機制**:\n",
    "   - Preprocessing (Tokenizer) → Inference (Model) → Postprocessing\n",
    "   - 理解三階段有助於 Debug 和優化\n",
    "\n",
    "2. **效能優化策略**:\n",
    "   - 批次處理 (Batch Processing)\n",
    "   - 選擇合適的 Batch Size\n",
    "   - 模型量化 (Quantization)\n",
    "   - 快取機制 (Caching)\n",
    "\n",
    "3. **進階參數設定**:\n",
    "   - `device`: CPU/GPU 選擇\n",
    "   - `batch_size`: 批次大小\n",
    "   - `top_k`, `return_all_scores`: 控制輸出\n",
    "   - `max_length`, `truncation`: 處理長文本\n",
    "\n",
    "4. **實戰應用**:\n",
    "   - 多任務 NLP 助手\n",
    "   - 對話系統整合\n",
    "   - 自訂後處理邏輯\n",
    "\n",
    "### 📊 效能對比總結\n",
    "\n",
    "| 優化方法 | 速度提升 | 記憶體節省 | 精度損失 |\n",
    "|---------|---------|-----------|----------|\n",
    "| 批次處理 | 2-5x | - | 無 |\n",
    "| INT8 量化 | 1.5-2x | 4x | 微小 (<1%) |\n",
    "| 模型蒸餾 | 2-3x | 2-4x | 小 (1-3%) |\n",
    "| GPU 加速 | 5-10x | - | 無 |\n",
    "\n",
    "### 📚 延伸閱讀\n",
    "\n",
    "- [Pipeline 官方文檔](https://huggingface.co/docs/transformers/main_classes/pipelines)\n",
    "- [模型量化指南](https://huggingface.co/docs/optimum/concept_guides/quantization)\n",
    "- [效能優化技巧](https://huggingface.co/docs/transformers/performance)\n",
    "\n",
    "### 🚀 下一節預告\n",
    "\n",
    "**CH08-03: 情感分析實戰 (Sentiment Analysis)**\n",
    "- 使用真實 Twitter 數據集\n",
    "- 模型微調與評估\n",
    "- 部署到生產環境\n",
    "\n",
    "---\n",
    "\n",
    "**課程**: iSpan Python NLP Cookbooks v2\n",
    "**講師**: Claude AI\n",
    "**最後更新**: 2025-10-17"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
