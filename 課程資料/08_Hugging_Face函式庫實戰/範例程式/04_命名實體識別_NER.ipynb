{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH08-04: å‘½åå¯¦é«”è­˜åˆ¥ (Named Entity Recognition)\n",
    "\n",
    "**èª²ç¨‹**: iSpan Python NLP Cookbooks v2\n",
    "**ç« ç¯€**: CH08 Hugging Face å‡½å¼åº«å¯¦æˆ°\n",
    "**ç‰ˆæœ¬**: v1.0\n",
    "**æ›´æ–°æ—¥æœŸ**: 2025-10-17\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š æœ¬ç¯€å­¸ç¿’ç›®æ¨™\n",
    "\n",
    "1. ç†è§£ NER ä»»å‹™çš„å®šç¾©èˆ‡æ‡‰ç”¨å ´æ™¯\n",
    "2. æŒæ¡ Token Classification çš„è¨“ç·´æµç¨‹\n",
    "3. ä½¿ç”¨ CoNLL-2003 æ¨™æº–æ•¸æ“šé›†\n",
    "4. å­¸æœƒå¯¦é«”æ¨™è¨»èˆ‡ BIO æ¨™è¨˜æ–¹æ¡ˆ\n",
    "5. å¯¦ä½œä¸­æ–‡ NER ç³»çµ±\n",
    "\n",
    "---\n",
    "\n",
    "## 1. NER ä»»å‹™æ¦‚è¿°\n",
    "\n",
    "### 1.1 ä»€éº¼æ˜¯å‘½åå¯¦é«”è­˜åˆ¥?\n",
    "\n",
    "**å®šç¾©**: å¾æ–‡æœ¬ä¸­è­˜åˆ¥ä¸¦åˆ†é¡å‘½åå¯¦é«” (äººåã€åœ°åã€çµ„ç¹”åç­‰)\n",
    "\n",
    "**å¸¸è¦‹å¯¦é«”é¡å‹**:\n",
    "- **PER** (Person): äººå\n",
    "- **LOC** (Location): åœ°å\n",
    "- **ORG** (Organization): çµ„ç¹”å\n",
    "- **MISC** (Miscellaneous): å…¶ä»– (ç”¢å“ã€äº‹ä»¶ç­‰)\n",
    "\n",
    "**ç¯„ä¾‹**:\n",
    "```\n",
    "è¼¸å…¥: \"Apple CEO Tim Cook announced new products in San Francisco.\"\n",
    "\n",
    "è¼¸å‡º:\n",
    "- Apple â†’ ORG\n",
    "- Tim Cook â†’ PER\n",
    "- San Francisco â†’ LOC\n",
    "```\n",
    "\n",
    "### 1.2 BIO æ¨™è¨˜æ–¹æ¡ˆ\n",
    "\n",
    "**B-I-O æ¨™è¨˜**:\n",
    "- **B** (Begin): å¯¦é«”é–‹å§‹\n",
    "- **I** (Inside): å¯¦é«”å…§éƒ¨\n",
    "- **O** (Outside): éå¯¦é«”\n",
    "\n",
    "```\n",
    "Tim    Cook   announced   new   products   in   San   Francisco\n",
    "B-PER  I-PER  O           O     O          O    B-LOC I-LOC\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£å¿…è¦å¥—ä»¶\n",
    "# !pip install transformers datasets seqeval torch -q\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"âœ… ç’°å¢ƒæº–å‚™å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. è¼‰å…¥ CoNLL-2003 æ•¸æ“šé›†\n",
    "\n",
    "### 2.1 æ•¸æ“šé›†ç°¡ä»‹\n",
    "\n",
    "**CoNLL-2003**: NER æ¨™æº–è©•æ¸¬æ•¸æ“šé›†\n",
    "- èªè¨€: è‹±æ–‡\n",
    "- å¯¦é«”é¡å‹: PER, LOC, ORG, MISC\n",
    "- è¨“ç·´é›†: 14,041 å¥\n",
    "- æ¸¬è©¦é›†: 3,453 å¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# è¼‰å…¥æ•¸æ“šé›†\n",
    "dataset = load_dataset(\"conll2003\")\n",
    "\n",
    "print(\"æ•¸æ“šé›†çµæ§‹:\")\n",
    "print(dataset)\n",
    "\n",
    "print(\"\\nè¨“ç·´é›†å¤§å°:\", len(dataset['train']))\n",
    "print(\"é©—è­‰é›†å¤§å°:\", len(dataset['validation']))\n",
    "print(\"æ¸¬è©¦é›†å¤§å°:\", len(dataset['test']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 æ•¸æ“šæ ¼å¼æ¢ç´¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æŸ¥çœ‹ç¬¬ä¸€ç­†æ•¸æ“š\n",
    "example = dataset['train'][0]\n",
    "\n",
    "print(\"æ•¸æ“šæ¬„ä½:\")\n",
    "print(example.keys())\n",
    "\n",
    "print(\"\\nè©åºåˆ—:\")\n",
    "print(example['tokens'])\n",
    "\n",
    "print(\"\\nNER æ¨™ç±¤ (æ•¸å­—):\")\n",
    "print(example['ner_tags'])\n",
    "\n",
    "# ç²å–æ¨™ç±¤åç¨±\n",
    "label_names = dataset['train'].features['ner_tags'].feature.names\n",
    "print(\"\\næ¨™ç±¤æ˜ å°„:\")\n",
    "for i, name in enumerate(label_names):\n",
    "    print(f\"{i}: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è¦–åŒ–æ¨™è¨»ç¯„ä¾‹\n",
    "def display_ner_example(example, label_names):\n",
    "    tokens = example['tokens']\n",
    "    ner_tags = example['ner_tags']\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"NER æ¨™è¨»ç¯„ä¾‹\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for token, tag_id in zip(tokens, ner_tags):\n",
    "        tag = label_names[tag_id]\n",
    "        \n",
    "        # é¡è‰²æ¨™è¨˜\n",
    "        if tag.startswith('B-'):\n",
    "            color = '\\033[92m'  # ç¶ è‰²\n",
    "        elif tag.startswith('I-'):\n",
    "            color = '\\033[94m'  # è—è‰²\n",
    "        else:\n",
    "            color = '\\033[0m'   # é»˜èª\n",
    "        \n",
    "        print(f\"{color}{token:15s} â†’ {tag}\\033[0m\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "\n",
    "# é¡¯ç¤ºå‰ 3 å€‹ç¯„ä¾‹\n",
    "for i in range(3):\n",
    "    display_ner_example(dataset['train'][i], label_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 æ•¸æ“šçµ±è¨ˆåˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# çµ±è¨ˆæ¨™ç±¤åˆ†å¸ƒ\n",
    "all_tags = []\n",
    "for example in dataset['train']:\n",
    "    all_tags.extend(example['ner_tags'])\n",
    "\n",
    "tag_counts = Counter(all_tags)\n",
    "\n",
    "# è½‰æ›ç‚ºæ¨™ç±¤åç¨±\n",
    "tag_dist = {label_names[tag_id]: count for tag_id, count in tag_counts.items()}\n",
    "tag_dist = dict(sorted(tag_dist.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "print(\"æ¨™ç±¤åˆ†å¸ƒ:\")\n",
    "for tag, count in tag_dist.items():\n",
    "    print(f\"{tag:10s}: {count:6d} ({count/len(all_tags)*100:.1f}%)\")\n",
    "\n",
    "# ç¹ªè£½åˆ†å¸ƒåœ–\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(tag_dist.keys(), tag_dist.values(), color='skyblue')\n",
    "plt.title('NER Tag Distribution (CoNLL-2003 Training Set)', fontsize=14)\n",
    "plt.xlabel('Tag')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. æ•¸æ“šé è™•ç†\n",
    "\n",
    "### 3.1 Tokenization å°é½Šå•é¡Œ\n",
    "\n",
    "**æŒ‘æˆ°**: WordPiece/BPE åˆ†è©æœƒå°‡è©æ‹†åˆ†æˆå­è©,éœ€è¦å°é½Šæ¨™ç±¤\n",
    "\n",
    "```\n",
    "åŸå§‹: \"Washington\" â†’ B-LOC\n",
    "åˆ†è©: [\"Wash\", \"##ing\", \"##ton\"] â†’ [B-LOC, I-LOC, I-LOC]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# è¼‰å…¥åˆ†è©å™¨\n",
    "model_name = \"bert-base-cased\"  # NER é€šå¸¸éœ€è¦å€åˆ†å¤§å°å¯«\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# æ¸¬è©¦åˆ†è©å°é½Š\n",
    "test_tokens = [\"Apple\", \"CEO\", \"Tim\", \"Cook\"]\n",
    "test_labels = [3, 0, 1, 2]  # B-ORG, O, B-PER, I-PER\n",
    "\n",
    "# åˆ†è©\n",
    "tokenized = tokenizer(\n",
    "    test_tokens,\n",
    "    is_split_into_words=True,  # é‡è¦: å‘Šè¨´åˆ†è©å™¨è¼¸å…¥å·²åˆ†è©\n",
    "    return_offsets_mapping=True\n",
    ")\n",
    "\n",
    "print(\"åŸå§‹ tokens:\", test_tokens)\n",
    "print(\"åŸå§‹ labels:\", test_labels)\n",
    "print(\"\\nåˆ†è©å¾Œ tokens:\", tokenizer.convert_ids_to_tokens(tokenized['input_ids']))\n",
    "print(\"Word IDs:\", tokenized.word_ids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®šç¾©æ¨™ç±¤å°é½Šå‡½æ•¸\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples['tokens'],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        padding='max_length',\n",
    "        max_length=128\n",
    "    )\n",
    "    \n",
    "    labels = []\n",
    "    for i, label in enumerate(examples['ner_tags']):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        label_ids = []\n",
    "        previous_word_idx = None\n",
    "        \n",
    "        for word_idx in word_ids:\n",
    "            # ç‰¹æ®Š token (CLS, SEP, PAD) æ¨™è¨˜ç‚º -100 (å¿½ç•¥)\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # è©çš„ç¬¬ä¸€å€‹å­è©ä¿ç•™åŸå§‹æ¨™ç±¤\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # è©çš„å¾ŒçºŒå­è©æ¨™è¨˜ç‚º -100 (æˆ–ä¿ç•™åŸæ¨™ç±¤)\n",
    "            else:\n",
    "                label_ids.append(-100)  # å¯æ”¹ç‚º label[word_idx] ä¿ç•™æ¨™ç±¤\n",
    "            \n",
    "            previous_word_idx = word_idx\n",
    "        \n",
    "        labels.append(label_ids)\n",
    "    \n",
    "    tokenized_inputs['labels'] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# æ¸¬è©¦å°é½Šå‡½æ•¸\n",
    "test_example = {'tokens': [test_tokens], 'ner_tags': [test_labels]}\n",
    "aligned = tokenize_and_align_labels(test_example)\n",
    "\n",
    "print(\"å°é½Šå¾Œçš„æ¨™ç±¤:\")\n",
    "print(aligned['labels'][0][:15])  # é¡¯ç¤ºå‰ 15 å€‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 è™•ç†æ•¸æ“šé›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å°æ•´å€‹æ•¸æ“šé›†é€²è¡Œåˆ†è©èˆ‡å°é½Š\n",
    "tokenized_datasets = dataset.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=dataset['train'].column_names\n",
    ")\n",
    "\n",
    "# è¨­å®šæ ¼å¼ç‚º PyTorch\n",
    "tokenized_datasets.set_format(type='torch')\n",
    "\n",
    "print(\"âœ… æ•¸æ“šé è™•ç†å®Œæˆ\")\n",
    "print(f\"è¨“ç·´é›†: {len(tokenized_datasets['train'])}\")\n",
    "print(f\"é©—è­‰é›†: {len(tokenized_datasets['validation'])}\")\n",
    "print(f\"æ¸¬è©¦é›†: {len(tokenized_datasets['test'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. æ¨¡å‹è¨“ç·´\n",
    "\n",
    "### 4.1 è¼‰å…¥æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "# è¼‰å…¥æ¨¡å‹\n",
    "num_labels = len(label_names)\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels,\n",
    "    id2label={i: label for i, label in enumerate(label_names)},\n",
    "    label2id={label: i for i, label in enumerate(label_names)}\n",
    ")\n",
    "\n",
    "print(f\"æ¨¡å‹: {model_name}\")\n",
    "print(f\"æ¨™ç±¤æ•¸é‡: {num_labels}\")\n",
    "print(f\"åƒæ•¸é‡: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 å®šç¾©è©•ä¼°æŒ‡æ¨™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import classification_report, f1_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    \n",
    "    # ç§»é™¤ -100 æ¨™ç±¤\n",
    "    true_labels = [\n",
    "        [label_names[l] for l in label if l != -100]\n",
    "        for label in labels\n",
    "    ]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        'precision': precision_score(true_labels, true_predictions),\n",
    "        'recall': recall_score(true_labels, true_predictions),\n",
    "        'f1': f1_score(true_labels, true_predictions)\n",
    "    }\n",
    "\n",
    "print(\"âœ… è©•ä¼°å‡½æ•¸å®šç¾©å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 è¨“ç·´é…ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
    "\n",
    "# Data Collator (è™•ç†å‹•æ…‹ padding)\n",
    "data_collator = DataCollatorForTokenClassification(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=True\n",
    ")\n",
    "\n",
    "# è¨“ç·´åƒæ•¸\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./ner_results',\n",
    "    evaluation_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=100,\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1'\n",
    ")\n",
    "\n",
    "# å‰µå»º Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print(\"âœ… Trainer å‰µå»ºå®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 é–‹å§‹è¨“ç·´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨“ç·´æ¨¡å‹\n",
    "print(\"ğŸš€ é–‹å§‹è¨“ç·´...\\n\")\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\nâœ… è¨“ç·´å®Œæˆ!\")\n",
    "print(f\"è¨“ç·´æ™‚é–“: {train_result.metrics['train_runtime']:.2f}s\")\n",
    "print(f\"è¨“ç·´æå¤±: {train_result.metrics['train_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 è©•ä¼°æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è©•ä¼°\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"é©—è­‰é›†è©•ä¼°çµæœ:\")\n",
    "print(\"=\"*50)\n",
    "for metric, value in eval_results.items():\n",
    "    print(f\"{metric:20s}: {value:.4f}\")\n",
    "\n",
    "# åœ¨æ¸¬è©¦é›†ä¸Šè©•ä¼°\n",
    "test_results = trainer.evaluate(tokenized_datasets['test'])\n",
    "\n",
    "print(\"\\næ¸¬è©¦é›†è©•ä¼°çµæœ:\")\n",
    "print(\"=\"*50)\n",
    "for metric, value in test_results.items():\n",
    "    print(f\"{metric:20s}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. æ¨¡å‹æ¨ç†èˆ‡æ‡‰ç”¨\n",
    "\n",
    "### 5.1 ä½¿ç”¨ Pipeline é€²è¡Œé æ¸¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# å‰µå»º NER Pipeline\n",
    "ner_pipeline = pipeline(\n",
    "    \"ner\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    aggregation_strategy=\"simple\"  # èšåˆå­è©\n",
    ")\n",
    "\n",
    "# æ¸¬è©¦æ–‡æœ¬\n",
    "test_texts = [\n",
    "    \"Apple CEO Tim Cook announced new products in San Francisco.\",\n",
    "    \"Microsoft was founded by Bill Gates in Seattle.\",\n",
    "    \"The Eiffel Tower is located in Paris, France.\"\n",
    "]\n",
    "\n",
    "print(\"NER é æ¸¬çµæœ:\\n\")\n",
    "for text in test_texts:\n",
    "    results = ner_pipeline(text)\n",
    "    \n",
    "    print(f\"æ–‡æœ¬: {text}\")\n",
    "    print(\"å¯¦é«”:\")\n",
    "    for entity in results:\n",
    "        print(f\"  - {entity['word']:20s} â†’ {entity['entity_group']:5s} (score: {entity['score']:.2f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 å¯è¦–åŒ–å¯¦é«”æ¨™è¨»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "def visualize_ner(text, entities):\n",
    "    \"\"\"\n",
    "    å°‡ NER çµæœä»¥ HTML æ ¼å¼å¯è¦–åŒ–\n",
    "    \"\"\"\n",
    "    # é¡è‰²æ˜ å°„\n",
    "    colors = {\n",
    "        'PER': '#8ef',\n",
    "        'LOC': '#faa',\n",
    "        'ORG': '#afa',\n",
    "        'MISC': '#fea'\n",
    "    }\n",
    "    \n",
    "    html = f'<p style=\"font-size: 16px; line-height: 2.5;\">'\n",
    "    \n",
    "    last_end = 0\n",
    "    for entity in entities:\n",
    "        # æ·»åŠ å¯¦é«”å‰çš„æ–‡æœ¬\n",
    "        html += text[last_end:entity['start']]\n",
    "        \n",
    "        # æ·»åŠ æ¨™è¨»çš„å¯¦é«”\n",
    "        color = colors.get(entity['entity_group'], '#ddd')\n",
    "        html += f'<mark style=\"background-color: {color}; padding: 2px 4px; border-radius: 3px;\"'\n",
    "        html += f'title=\"{entity[\"entity_group\"]} ({entity[\"score\"]:.2f})\">'\n",
    "        html += entity['word']\n",
    "        html += f'</mark>'\n",
    "        \n",
    "        last_end = entity['end']\n",
    "    \n",
    "    # æ·»åŠ å‰©é¤˜æ–‡æœ¬\n",
    "    html += text[last_end:]\n",
    "    html += '</p>'\n",
    "    \n",
    "    # æ·»åŠ åœ–ä¾‹\n",
    "    legend = '<div style=\"margin-top: 20px;\">'\n",
    "    for entity_type, color in colors.items():\n",
    "        legend += f'<span style=\"background-color: {color}; padding: 2px 8px; margin-right: 10px; border-radius: 3px;\">{entity_type}</span>'\n",
    "    legend += '</div>'\n",
    "    \n",
    "    return HTML(html + legend)\n",
    "\n",
    "# å¯è¦–åŒ–ç¬¬ä¸€å€‹ç¯„ä¾‹\n",
    "text = test_texts[0]\n",
    "entities = ner_pipeline(text)\n",
    "visualize_ner(text, entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 æ‰¹æ¬¡è™•ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ‰¹æ¬¡é æ¸¬\n",
    "batch_texts = [\n",
    "    \"Google was founded in California.\",\n",
    "    \"Barack Obama was born in Hawaii.\",\n",
    "    \"The Amazon River flows through Brazil.\",\n",
    "    \"Tesla CEO Elon Musk announced new plans.\"\n",
    "]\n",
    "\n",
    "batch_results = ner_pipeline(batch_texts)\n",
    "\n",
    "# åŒ¯ç¸½çµ±è¨ˆ\n",
    "entity_stats = {'PER': 0, 'LOC': 0, 'ORG': 0, 'MISC': 0}\n",
    "\n",
    "for text, entities in zip(batch_texts, batch_results):\n",
    "    for entity in entities:\n",
    "        entity_type = entity['entity_group']\n",
    "        if entity_type in entity_stats:\n",
    "            entity_stats[entity_type] += 1\n",
    "\n",
    "print(\"å¯¦é«”çµ±è¨ˆ:\")\n",
    "for entity_type, count in entity_stats.items():\n",
    "    print(f\"{entity_type}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. é€²éš: ä¸­æ–‡ NER\n",
    "\n",
    "### 6.1 ä¸­æ–‡ NER æŒ‘æˆ°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä½¿ç”¨ä¸­æ–‡ NER æ¨¡å‹\n",
    "chinese_ner = pipeline(\n",
    "    \"ner\",\n",
    "    model=\"ckiplab/bert-base-chinese-ner\",\n",
    "    aggregation_strategy=\"simple\"\n",
    ")\n",
    "\n",
    "# æ¸¬è©¦ä¸­æ–‡æ–‡æœ¬\n",
    "chinese_texts = [\n",
    "    \"è˜‹æœå…¬å¸çš„åŸ·è¡Œé•·æå§†Â·åº«å…‹åœ¨èˆŠé‡‘å±±ç™¼è¡¨æ–°ç”¢å“ã€‚\",\n",
    "    \"é˜¿é‡Œå·´å·´å‰µè¾¦äººé¦¬é›²å‡ºç”Ÿæ–¼æ­å·ã€‚\"\n",
    "]\n",
    "\n",
    "print(\"ä¸­æ–‡ NER çµæœ:\\n\")\n",
    "for text in chinese_texts:\n",
    "    results = chinese_ner(text)\n",
    "    print(f\"æ–‡æœ¬: {text}\")\n",
    "    print(\"å¯¦é«”:\")\n",
    "    for entity in results:\n",
    "        print(f\"  - {entity['word']:15s} â†’ {entity['entity_group']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. éŒ¯èª¤åˆ†æ\n",
    "\n",
    "### 7.1 å¸¸è¦‹éŒ¯èª¤é¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ†æé æ¸¬éŒ¯èª¤\n",
    "predictions = trainer.predict(tokenized_datasets['test'])\n",
    "pred_labels = np.argmax(predictions.predictions, axis=2)\n",
    "true_labels = predictions.label_ids\n",
    "\n",
    "# æ‰¾å‡ºéŒ¯èª¤æ¡ˆä¾‹\n",
    "errors = []\n",
    "for i, (pred_seq, true_seq) in enumerate(zip(pred_labels, true_labels)):\n",
    "    for j, (pred, true) in enumerate(zip(pred_seq, true_seq)):\n",
    "        if true != -100 and pred != true:\n",
    "            errors.append({\n",
    "                'example_id': i,\n",
    "                'position': j,\n",
    "                'true_label': label_names[true],\n",
    "                'pred_label': label_names[pred]\n",
    "            })\n",
    "\n",
    "# éŒ¯èª¤é¡å‹çµ±è¨ˆ\n",
    "from collections import Counter\n",
    "\n",
    "error_types = Counter([(e['true_label'], e['pred_label']) for e in errors])\n",
    "\n",
    "print(\"\\næœ€å¸¸è¦‹çš„éŒ¯èª¤é¡å‹ (Top 10):\")\n",
    "print(\"=\"*60)\n",
    "for (true_label, pred_label), count in error_types.most_common(10):\n",
    "    print(f\"{true_label:10s} â†’ {pred_label:10s}: {count:4d} æ¬¡\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. èª²å¾Œç·´ç¿’\n",
    "\n",
    "### ç·´ç¿’ 1: è‡ªè¨‚å¯¦é«”é¡å‹\n",
    "\n",
    "æ·»åŠ æ–°çš„å¯¦é«”é¡å‹ (å¦‚ç”¢å“åã€æ—¥æœŸã€é‡‘é¡)ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: å‰µå»ºè‡ªè¨‚ NER æ•¸æ“šé›†\n",
    "# æç¤º:\n",
    "# 1. æº–å‚™æ¨™è¨»æ•¸æ“š (å¯ä½¿ç”¨ Doccano ç­‰å·¥å…·)\n",
    "# 2. å®šç¾©æ–°çš„æ¨™ç±¤é›†\n",
    "# 3. è¨“ç·´æ¨¡å‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ç·´ç¿’ 2: NER + é—œä¿‚æŠ½å–\n",
    "\n",
    "çµåˆ NER èˆ‡é—œä¿‚æŠ½å–,æ§‹å»ºçŸ¥è­˜åœ–è­œã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: å¯¦ä½œé—œä¿‚æŠ½å–\n",
    "# ç¯„ä¾‹: å¾ \"Tim Cook is the CEO of Apple\" æŠ½å– (Tim Cook, CEO_OF, Apple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. æœ¬ç¯€ç¸½çµ\n",
    "\n",
    "### âœ… é—œéµè¦é»\n",
    "\n",
    "1. **NER ä»»å‹™**:\n",
    "   - Token Classification\n",
    "   - BIO æ¨™è¨˜æ–¹æ¡ˆ\n",
    "   - å¯¦é«”é¡å‹: PER, LOC, ORG, MISC\n",
    "\n",
    "2. **æ•¸æ“šé è™•ç†**:\n",
    "   - Tokenization å°é½Š\n",
    "   - å­è©æ¨™ç±¤è™•ç†\n",
    "   - ç‰¹æ®Š token è™•ç† (-100)\n",
    "\n",
    "3. **æ¨¡å‹è¨“ç·´**:\n",
    "   - AutoModelForTokenClassification\n",
    "   - seqeval è©•ä¼°æŒ‡æ¨™\n",
    "   - DataCollatorForTokenClassification\n",
    "\n",
    "4. **å¯¦éš›æ‡‰ç”¨**:\n",
    "   - Pipeline å¿«é€Ÿæ¨ç†\n",
    "   - å¯¦é«”å¯è¦–åŒ–\n",
    "   - æ‰¹æ¬¡è™•ç†\n",
    "\n",
    "### ğŸ“Š æ¨¡å‹æ•ˆèƒ½\n",
    "\n",
    "| æŒ‡æ¨™ | è¨“ç·´é›† | é©—è­‰é›† | æ¸¬è©¦é›† |\n",
    "|------|--------|--------|--------|\n",
    "| Precision | ~98% | ~95% | ~94% |\n",
    "| Recall | ~98% | ~94% | ~93% |\n",
    "| F1 Score | ~98% | ~94% | ~93% |\n",
    "\n",
    "### ğŸ“š å»¶ä¼¸é–±è®€\n",
    "\n",
    "- [CoNLL-2003 æ•¸æ“šé›†](https://huggingface.co/datasets/conll2003)\n",
    "- [Token Classification Guide](https://huggingface.co/docs/transformers/tasks/token_classification)\n",
    "- [seqeval æ–‡æª”](https://github.com/chakki-works/seqeval)\n",
    "\n",
    "### ğŸš€ ä¸‹ä¸€ç¯€é å‘Š\n",
    "\n",
    "**åº•å±¤å¯¦ä½œ02: å¾é›¶æ‰“é€  MLP (å¤šå±¤æ„ŸçŸ¥å™¨)**\n",
    "- å‰å‘å‚³æ’­å¯¦ä½œ\n",
    "- åå‘å‚³æ’­æ¨å°\n",
    "- æ¬Šé‡åˆå§‹åŒ–ç­–ç•¥\n",
    "\n",
    "---\n",
    "\n",
    "**èª²ç¨‹**: iSpan Python NLP Cookbooks v2\n",
    "**è¬›å¸«**: Claude AI\n",
    "**æœ€å¾Œæ›´æ–°**: 2025-10-17"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
