{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH08-04: 命名實體識別 (Named Entity Recognition)\n",
    "\n",
    "**課程**: iSpan Python NLP Cookbooks v2\n",
    "**章節**: CH08 Hugging Face 函式庫實戰\n",
    "**版本**: v1.0\n",
    "**更新日期**: 2025-10-17\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 本節學習目標\n",
    "\n",
    "1. 理解 NER 任務的定義與應用場景\n",
    "2. 掌握 Token Classification 的訓練流程\n",
    "3. 使用 CoNLL-2003 標準數據集\n",
    "4. 學會實體標註與 BIO 標記方案\n",
    "5. 實作中文 NER 系統\n",
    "\n",
    "---\n",
    "\n",
    "## 1. NER 任務概述\n",
    "\n",
    "### 1.1 什麼是命名實體識別?\n",
    "\n",
    "**定義**: 從文本中識別並分類命名實體 (人名、地名、組織名等)\n",
    "\n",
    "**常見實體類型**:\n",
    "- **PER** (Person): 人名\n",
    "- **LOC** (Location): 地名\n",
    "- **ORG** (Organization): 組織名\n",
    "- **MISC** (Miscellaneous): 其他 (產品、事件等)\n",
    "\n",
    "**範例**:\n",
    "```\n",
    "輸入: \"Apple CEO Tim Cook announced new products in San Francisco.\"\n",
    "\n",
    "輸出:\n",
    "- Apple → ORG\n",
    "- Tim Cook → PER\n",
    "- San Francisco → LOC\n",
    "```\n",
    "\n",
    "### 1.2 BIO 標記方案\n",
    "\n",
    "**B-I-O 標記**:\n",
    "- **B** (Begin): 實體開始\n",
    "- **I** (Inside): 實體內部\n",
    "- **O** (Outside): 非實體\n",
    "\n",
    "```\n",
    "Tim    Cook   announced   new   products   in   San   Francisco\n",
    "B-PER  I-PER  O           O     O          O    B-LOC I-LOC\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安裝必要套件\n",
    "# !pip install transformers datasets seqeval torch -q\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"✅ 環境準備完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. 載入 CoNLL-2003 數據集\n",
    "\n",
    "### 2.1 數據集簡介\n",
    "\n",
    "**CoNLL-2003**: NER 標準評測數據集\n",
    "- 語言: 英文\n",
    "- 實體類型: PER, LOC, ORG, MISC\n",
    "- 訓練集: 14,041 句\n",
    "- 測試集: 3,453 句"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# 載入數據集\n",
    "dataset = load_dataset(\"conll2003\")\n",
    "\n",
    "print(\"數據集結構:\")\n",
    "print(dataset)\n",
    "\n",
    "print(\"\\n訓練集大小:\", len(dataset['train']))\n",
    "print(\"驗證集大小:\", len(dataset['validation']))\n",
    "print(\"測試集大小:\", len(dataset['test']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 數據格式探索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看第一筆數據\n",
    "example = dataset['train'][0]\n",
    "\n",
    "print(\"數據欄位:\")\n",
    "print(example.keys())\n",
    "\n",
    "print(\"\\n詞序列:\")\n",
    "print(example['tokens'])\n",
    "\n",
    "print(\"\\nNER 標籤 (數字):\")\n",
    "print(example['ner_tags'])\n",
    "\n",
    "# 獲取標籤名稱\n",
    "label_names = dataset['train'].features['ner_tags'].feature.names\n",
    "print(\"\\n標籤映射:\")\n",
    "for i, name in enumerate(label_names):\n",
    "    print(f\"{i}: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可視化標註範例\n",
    "def display_ner_example(example, label_names):\n",
    "    tokens = example['tokens']\n",
    "    ner_tags = example['ner_tags']\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"NER 標註範例\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for token, tag_id in zip(tokens, ner_tags):\n",
    "        tag = label_names[tag_id]\n",
    "        \n",
    "        # 顏色標記\n",
    "        if tag.startswith('B-'):\n",
    "            color = '\\033[92m'  # 綠色\n",
    "        elif tag.startswith('I-'):\n",
    "            color = '\\033[94m'  # 藍色\n",
    "        else:\n",
    "            color = '\\033[0m'   # 默認\n",
    "        \n",
    "        print(f\"{color}{token:15s} → {tag}\\033[0m\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "\n",
    "# 顯示前 3 個範例\n",
    "for i in range(3):\n",
    "    display_ner_example(dataset['train'][i], label_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 數據統計分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# 統計標籤分布\n",
    "all_tags = []\n",
    "for example in dataset['train']:\n",
    "    all_tags.extend(example['ner_tags'])\n",
    "\n",
    "tag_counts = Counter(all_tags)\n",
    "\n",
    "# 轉換為標籤名稱\n",
    "tag_dist = {label_names[tag_id]: count for tag_id, count in tag_counts.items()}\n",
    "tag_dist = dict(sorted(tag_dist.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "print(\"標籤分布:\")\n",
    "for tag, count in tag_dist.items():\n",
    "    print(f\"{tag:10s}: {count:6d} ({count/len(all_tags)*100:.1f}%)\")\n",
    "\n",
    "# 繪製分布圖\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(tag_dist.keys(), tag_dist.values(), color='skyblue')\n",
    "plt.title('NER Tag Distribution (CoNLL-2003 Training Set)', fontsize=14)\n",
    "plt.xlabel('Tag')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. 數據預處理\n",
    "\n",
    "### 3.1 Tokenization 對齊問題\n",
    "\n",
    "**挑戰**: WordPiece/BPE 分詞會將詞拆分成子詞,需要對齊標籤\n",
    "\n",
    "```\n",
    "原始: \"Washington\" → B-LOC\n",
    "分詞: [\"Wash\", \"##ing\", \"##ton\"] → [B-LOC, I-LOC, I-LOC]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 載入分詞器\n",
    "model_name = \"bert-base-cased\"  # NER 通常需要區分大小寫\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 測試分詞對齊\n",
    "test_tokens = [\"Apple\", \"CEO\", \"Tim\", \"Cook\"]\n",
    "test_labels = [3, 0, 1, 2]  # B-ORG, O, B-PER, I-PER\n",
    "\n",
    "# 分詞\n",
    "tokenized = tokenizer(\n",
    "    test_tokens,\n",
    "    is_split_into_words=True,  # 重要: 告訴分詞器輸入已分詞\n",
    "    return_offsets_mapping=True\n",
    ")\n",
    "\n",
    "print(\"原始 tokens:\", test_tokens)\n",
    "print(\"原始 labels:\", test_labels)\n",
    "print(\"\\n分詞後 tokens:\", tokenizer.convert_ids_to_tokens(tokenized['input_ids']))\n",
    "print(\"Word IDs:\", tokenized.word_ids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義標籤對齊函數\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples['tokens'],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        padding='max_length',\n",
    "        max_length=128\n",
    "    )\n",
    "    \n",
    "    labels = []\n",
    "    for i, label in enumerate(examples['ner_tags']):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        label_ids = []\n",
    "        previous_word_idx = None\n",
    "        \n",
    "        for word_idx in word_ids:\n",
    "            # 特殊 token (CLS, SEP, PAD) 標記為 -100 (忽略)\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # 詞的第一個子詞保留原始標籤\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # 詞的後續子詞標記為 -100 (或保留原標籤)\n",
    "            else:\n",
    "                label_ids.append(-100)  # 可改為 label[word_idx] 保留標籤\n",
    "            \n",
    "            previous_word_idx = word_idx\n",
    "        \n",
    "        labels.append(label_ids)\n",
    "    \n",
    "    tokenized_inputs['labels'] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# 測試對齊函數\n",
    "test_example = {'tokens': [test_tokens], 'ner_tags': [test_labels]}\n",
    "aligned = tokenize_and_align_labels(test_example)\n",
    "\n",
    "print(\"對齊後的標籤:\")\n",
    "print(aligned['labels'][0][:15])  # 顯示前 15 個"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 處理數據集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 對整個數據集進行分詞與對齊\n",
    "tokenized_datasets = dataset.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=dataset['train'].column_names\n",
    ")\n",
    "\n",
    "# 設定格式為 PyTorch\n",
    "tokenized_datasets.set_format(type='torch')\n",
    "\n",
    "print(\"✅ 數據預處理完成\")\n",
    "print(f\"訓練集: {len(tokenized_datasets['train'])}\")\n",
    "print(f\"驗證集: {len(tokenized_datasets['validation'])}\")\n",
    "print(f\"測試集: {len(tokenized_datasets['test'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. 模型訓練\n",
    "\n",
    "### 4.1 載入模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "# 載入模型\n",
    "num_labels = len(label_names)\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels,\n",
    "    id2label={i: label for i, label in enumerate(label_names)},\n",
    "    label2id={label: i for i, label in enumerate(label_names)}\n",
    ")\n",
    "\n",
    "print(f\"模型: {model_name}\")\n",
    "print(f\"標籤數量: {num_labels}\")\n",
    "print(f\"參數量: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 定義評估指標"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import classification_report, f1_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    \n",
    "    # 移除 -100 標籤\n",
    "    true_labels = [\n",
    "        [label_names[l] for l in label if l != -100]\n",
    "        for label in labels\n",
    "    ]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        'precision': precision_score(true_labels, true_predictions),\n",
    "        'recall': recall_score(true_labels, true_predictions),\n",
    "        'f1': f1_score(true_labels, true_predictions)\n",
    "    }\n",
    "\n",
    "print(\"✅ 評估函數定義完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 訓練配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
    "\n",
    "# Data Collator (處理動態 padding)\n",
    "data_collator = DataCollatorForTokenClassification(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=True\n",
    ")\n",
    "\n",
    "# 訓練參數\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./ner_results',\n",
    "    evaluation_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=100,\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1'\n",
    ")\n",
    "\n",
    "# 創建 Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print(\"✅ Trainer 創建完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 開始訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練模型\n",
    "print(\"🚀 開始訓練...\\n\")\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\n✅ 訓練完成!\")\n",
    "print(f\"訓練時間: {train_result.metrics['train_runtime']:.2f}s\")\n",
    "print(f\"訓練損失: {train_result.metrics['train_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 評估模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 評估\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"驗證集評估結果:\")\n",
    "print(\"=\"*50)\n",
    "for metric, value in eval_results.items():\n",
    "    print(f\"{metric:20s}: {value:.4f}\")\n",
    "\n",
    "# 在測試集上評估\n",
    "test_results = trainer.evaluate(tokenized_datasets['test'])\n",
    "\n",
    "print(\"\\n測試集評估結果:\")\n",
    "print(\"=\"*50)\n",
    "for metric, value in test_results.items():\n",
    "    print(f\"{metric:20s}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. 模型推理與應用\n",
    "\n",
    "### 5.1 使用 Pipeline 進行預測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# 創建 NER Pipeline\n",
    "ner_pipeline = pipeline(\n",
    "    \"ner\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    aggregation_strategy=\"simple\"  # 聚合子詞\n",
    ")\n",
    "\n",
    "# 測試文本\n",
    "test_texts = [\n",
    "    \"Apple CEO Tim Cook announced new products in San Francisco.\",\n",
    "    \"Microsoft was founded by Bill Gates in Seattle.\",\n",
    "    \"The Eiffel Tower is located in Paris, France.\"\n",
    "]\n",
    "\n",
    "print(\"NER 預測結果:\\n\")\n",
    "for text in test_texts:\n",
    "    results = ner_pipeline(text)\n",
    "    \n",
    "    print(f\"文本: {text}\")\n",
    "    print(\"實體:\")\n",
    "    for entity in results:\n",
    "        print(f\"  - {entity['word']:20s} → {entity['entity_group']:5s} (score: {entity['score']:.2f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 可視化實體標註"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "def visualize_ner(text, entities):\n",
    "    \"\"\"\n",
    "    將 NER 結果以 HTML 格式可視化\n",
    "    \"\"\"\n",
    "    # 顏色映射\n",
    "    colors = {\n",
    "        'PER': '#8ef',\n",
    "        'LOC': '#faa',\n",
    "        'ORG': '#afa',\n",
    "        'MISC': '#fea'\n",
    "    }\n",
    "    \n",
    "    html = f'<p style=\"font-size: 16px; line-height: 2.5;\">'\n",
    "    \n",
    "    last_end = 0\n",
    "    for entity in entities:\n",
    "        # 添加實體前的文本\n",
    "        html += text[last_end:entity['start']]\n",
    "        \n",
    "        # 添加標註的實體\n",
    "        color = colors.get(entity['entity_group'], '#ddd')\n",
    "        html += f'<mark style=\"background-color: {color}; padding: 2px 4px; border-radius: 3px;\"'\n",
    "        html += f'title=\"{entity[\"entity_group\"]} ({entity[\"score\"]:.2f})\">'\n",
    "        html += entity['word']\n",
    "        html += f'</mark>'\n",
    "        \n",
    "        last_end = entity['end']\n",
    "    \n",
    "    # 添加剩餘文本\n",
    "    html += text[last_end:]\n",
    "    html += '</p>'\n",
    "    \n",
    "    # 添加圖例\n",
    "    legend = '<div style=\"margin-top: 20px;\">'\n",
    "    for entity_type, color in colors.items():\n",
    "        legend += f'<span style=\"background-color: {color}; padding: 2px 8px; margin-right: 10px; border-radius: 3px;\">{entity_type}</span>'\n",
    "    legend += '</div>'\n",
    "    \n",
    "    return HTML(html + legend)\n",
    "\n",
    "# 可視化第一個範例\n",
    "text = test_texts[0]\n",
    "entities = ner_pipeline(text)\n",
    "visualize_ner(text, entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 批次處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 批次預測\n",
    "batch_texts = [\n",
    "    \"Google was founded in California.\",\n",
    "    \"Barack Obama was born in Hawaii.\",\n",
    "    \"The Amazon River flows through Brazil.\",\n",
    "    \"Tesla CEO Elon Musk announced new plans.\"\n",
    "]\n",
    "\n",
    "batch_results = ner_pipeline(batch_texts)\n",
    "\n",
    "# 匯總統計\n",
    "entity_stats = {'PER': 0, 'LOC': 0, 'ORG': 0, 'MISC': 0}\n",
    "\n",
    "for text, entities in zip(batch_texts, batch_results):\n",
    "    for entity in entities:\n",
    "        entity_type = entity['entity_group']\n",
    "        if entity_type in entity_stats:\n",
    "            entity_stats[entity_type] += 1\n",
    "\n",
    "print(\"實體統計:\")\n",
    "for entity_type, count in entity_stats.items():\n",
    "    print(f\"{entity_type}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. 進階: 中文 NER\n",
    "\n",
    "### 6.1 中文 NER 挑戰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用中文 NER 模型\n",
    "chinese_ner = pipeline(\n",
    "    \"ner\",\n",
    "    model=\"ckiplab/bert-base-chinese-ner\",\n",
    "    aggregation_strategy=\"simple\"\n",
    ")\n",
    "\n",
    "# 測試中文文本\n",
    "chinese_texts = [\n",
    "    \"蘋果公司的執行長提姆·庫克在舊金山發表新產品。\",\n",
    "    \"阿里巴巴創辦人馬雲出生於杭州。\"\n",
    "]\n",
    "\n",
    "print(\"中文 NER 結果:\\n\")\n",
    "for text in chinese_texts:\n",
    "    results = chinese_ner(text)\n",
    "    print(f\"文本: {text}\")\n",
    "    print(\"實體:\")\n",
    "    for entity in results:\n",
    "        print(f\"  - {entity['word']:15s} → {entity['entity_group']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. 錯誤分析\n",
    "\n",
    "### 7.1 常見錯誤類型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分析預測錯誤\n",
    "predictions = trainer.predict(tokenized_datasets['test'])\n",
    "pred_labels = np.argmax(predictions.predictions, axis=2)\n",
    "true_labels = predictions.label_ids\n",
    "\n",
    "# 找出錯誤案例\n",
    "errors = []\n",
    "for i, (pred_seq, true_seq) in enumerate(zip(pred_labels, true_labels)):\n",
    "    for j, (pred, true) in enumerate(zip(pred_seq, true_seq)):\n",
    "        if true != -100 and pred != true:\n",
    "            errors.append({\n",
    "                'example_id': i,\n",
    "                'position': j,\n",
    "                'true_label': label_names[true],\n",
    "                'pred_label': label_names[pred]\n",
    "            })\n",
    "\n",
    "# 錯誤類型統計\n",
    "from collections import Counter\n",
    "\n",
    "error_types = Counter([(e['true_label'], e['pred_label']) for e in errors])\n",
    "\n",
    "print(\"\\n最常見的錯誤類型 (Top 10):\")\n",
    "print(\"=\"*60)\n",
    "for (true_label, pred_label), count in error_types.most_common(10):\n",
    "    print(f\"{true_label:10s} → {pred_label:10s}: {count:4d} 次\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. 課後練習\n",
    "\n",
    "### 練習 1: 自訂實體類型\n",
    "\n",
    "添加新的實體類型 (如產品名、日期、金額)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 創建自訂 NER 數據集\n",
    "# 提示:\n",
    "# 1. 準備標註數據 (可使用 Doccano 等工具)\n",
    "# 2. 定義新的標籤集\n",
    "# 3. 訓練模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習 2: NER + 關係抽取\n",
    "\n",
    "結合 NER 與關係抽取,構建知識圖譜。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 實作關係抽取\n",
    "# 範例: 從 \"Tim Cook is the CEO of Apple\" 抽取 (Tim Cook, CEO_OF, Apple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. 本節總結\n",
    "\n",
    "### ✅ 關鍵要點\n",
    "\n",
    "1. **NER 任務**:\n",
    "   - Token Classification\n",
    "   - BIO 標記方案\n",
    "   - 實體類型: PER, LOC, ORG, MISC\n",
    "\n",
    "2. **數據預處理**:\n",
    "   - Tokenization 對齊\n",
    "   - 子詞標籤處理\n",
    "   - 特殊 token 處理 (-100)\n",
    "\n",
    "3. **模型訓練**:\n",
    "   - AutoModelForTokenClassification\n",
    "   - seqeval 評估指標\n",
    "   - DataCollatorForTokenClassification\n",
    "\n",
    "4. **實際應用**:\n",
    "   - Pipeline 快速推理\n",
    "   - 實體可視化\n",
    "   - 批次處理\n",
    "\n",
    "### 📊 模型效能\n",
    "\n",
    "| 指標 | 訓練集 | 驗證集 | 測試集 |\n",
    "|------|--------|--------|--------|\n",
    "| Precision | ~98% | ~95% | ~94% |\n",
    "| Recall | ~98% | ~94% | ~93% |\n",
    "| F1 Score | ~98% | ~94% | ~93% |\n",
    "\n",
    "### 📚 延伸閱讀\n",
    "\n",
    "- [CoNLL-2003 數據集](https://huggingface.co/datasets/conll2003)\n",
    "- [Token Classification Guide](https://huggingface.co/docs/transformers/tasks/token_classification)\n",
    "- [seqeval 文檔](https://github.com/chakki-works/seqeval)\n",
    "\n",
    "### 🚀 下一節預告\n",
    "\n",
    "**底層實作02: 從零打造 MLP (多層感知器)**\n",
    "- 前向傳播實作\n",
    "- 反向傳播推導\n",
    "- 權重初始化策略\n",
    "\n",
    "---\n",
    "\n",
    "**課程**: iSpan Python NLP Cookbooks v2\n",
    "**講師**: Claude AI\n",
    "**最後更新**: 2025-10-17"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
