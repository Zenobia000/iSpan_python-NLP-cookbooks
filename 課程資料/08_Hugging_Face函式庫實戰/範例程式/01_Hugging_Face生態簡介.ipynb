{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH08-01: Hugging Face ç”Ÿæ…‹ç°¡ä»‹\n",
    "\n",
    "**èª²ç¨‹**: iSpan Python NLP Cookbooks v2\n",
    "**ç« ç¯€**: CH08 Hugging Face å¯¦æˆ°\n",
    "**ç‰ˆæœ¬**: v1.0\n",
    "**æ›´æ–°æ—¥æœŸ**: 2025-10-17\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š æœ¬ç¯€å­¸ç¿’ç›®æ¨™\n",
    "\n",
    "1. ç†è§£ Hugging Face ç”Ÿæ…‹ç³»çµ±çš„æ ¸å¿ƒçµ„ä»¶\n",
    "2. æŒæ¡ Transformers å‡½å¼åº«çš„åŸºæœ¬æ¶æ§‹\n",
    "3. ç†Ÿæ‚‰ Hugging Face Hub æ¨¡å‹èˆ‡æ•¸æ“šé›†è³‡æº\n",
    "4. äº†è§£æ¨¡å‹å¡ç‰‡ (Model Card) çš„é‡è¦æ€§\n",
    "5. å­¸æœƒå¿«é€Ÿæœå°‹èˆ‡é¸æ“‡é è¨“ç·´æ¨¡å‹\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Hugging Face ç”Ÿæ…‹ç³»çµ±æ¦‚è¦½\n",
    "\n",
    "### 1.1 ä»€éº¼æ˜¯ Hugging Face?\n",
    "\n",
    "**Hugging Face** æ˜¯ç›®å‰æœ€æµè¡Œçš„ NLP é–‹æºç¤¾ç¾¤å¹³å°,æä¾›:\n",
    "\n",
    "- ğŸ¤— **Transformers**: é è¨“ç·´æ¨¡å‹å‡½å¼åº« (PyTorch, TensorFlow, JAX)\n",
    "- ğŸ—‚ï¸ **Datasets**: æ•¸æ“šé›†å‡½å¼åº« (è¶…é 50,000 å€‹æ•¸æ“šé›†)\n",
    "- ğŸ›ï¸ **Hub**: æ¨¡å‹èˆ‡æ•¸æ“šé›†å…±äº«å¹³å° (è¶…é 500,000 å€‹æ¨¡å‹)\n",
    "- âš¡ **Accelerate**: åˆ†æ•£å¼è¨“ç·´åŠ é€Ÿå·¥å…·\n",
    "- ğŸ” **Tokenizers**: é«˜æ•ˆèƒ½åˆ†è©å™¨\n",
    "\n",
    "**æ ¸å¿ƒå„ªå‹¢**:\n",
    "- âœ… çµ±ä¸€ API,æ”¯æ´æ‰€æœ‰ä¸»æµæ¨¡å‹ (BERT, GPT, T5...)\n",
    "- âœ… é–‹ç®±å³ç”¨,ç„¡éœ€å¾é›¶è¨“ç·´\n",
    "- âœ… æ´»èºç¤¾ç¾¤,æŒçºŒæ›´æ–°æœ€æ–°æ¨¡å‹\n",
    "- âœ… å®Œæ•´æ–‡æª”èˆ‡æ•™å­¸è³‡æº"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£ Hugging Face æ ¸å¿ƒå¥—ä»¶\n",
    "# !pip install transformers datasets accelerate tokenizers -q\n",
    "\n",
    "# é©—è­‰å®‰è£\n",
    "import transformers\n",
    "import datasets\n",
    "\n",
    "print(f\"âœ… Transformers ç‰ˆæœ¬: {transformers.__version__}\")\n",
    "print(f\"âœ… Datasets ç‰ˆæœ¬: {datasets.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 ç”Ÿæ…‹ç³»çµ±æ¶æ§‹åœ–\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚           Hugging Face ç”Ÿæ…‹ç³»çµ±              â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                             â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\n",
    "â”‚  â”‚   Hub    â”‚  â”‚Transformersâ”‚ â”‚ Datasets â”‚ â”‚\n",
    "â”‚  â”‚ (æ¨¡å‹åº«) â”‚  â”‚ (æ¨¡å‹API)  â”‚ â”‚ (æ•¸æ“šé›†) â”‚ â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\n",
    "â”‚       â†“              â†“              â†“      â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\n",
    "â”‚  â”‚Tokenizersâ”‚  â”‚Accelerateâ”‚  â”‚ Evaluate â”‚ â”‚\n",
    "â”‚  â”‚ (åˆ†è©å™¨) â”‚  â”‚ (åŠ é€Ÿå™¨) â”‚  â”‚ (è©•ä¼°)   â”‚ â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Transformers å‡½å¼åº«æ ¸å¿ƒæ¦‚å¿µ\n",
    "\n",
    "### 2.1 ä¸‰å¤§æ ¸å¿ƒçµ„ä»¶\n",
    "\n",
    "Transformers å‡½å¼åº«åœç¹ä¸‰å€‹æ ¸å¿ƒæ¦‚å¿µ:\n",
    "\n",
    "1. **Model (æ¨¡å‹)**: é è¨“ç·´çš„ç¥ç¶“ç¶²è·¯\n",
    "2. **Tokenizer (åˆ†è©å™¨)**: æ–‡æœ¬ â†’ æ•¸å­—çš„è½‰æ›å™¨\n",
    "3. **Pipeline (ç®¡é“)**: ç«¯åˆ°ç«¯çš„å¿«é€Ÿæ‡‰ç”¨ API\n",
    "\n",
    "#### 2.1.1 Model (æ¨¡å‹)\n",
    "\n",
    "**æ¨¡å‹æ¶æ§‹åˆ†é¡**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoModelForSequenceClassification\n",
    "\n",
    "# è¼‰å…¥é è¨“ç·´æ¨¡å‹\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "\n",
    "# 1. åŸºç¤æ¨¡å‹ (ç„¡ä»»å‹™é ­)\n",
    "base_model = AutoModel.from_pretrained(model_name)\n",
    "print(f\"åŸºç¤æ¨¡å‹: {base_model.__class__.__name__}\")\n",
    "print(f\"åƒæ•¸é‡: {base_model.num_parameters():,}\")\n",
    "\n",
    "# 2. ä»»å‹™ç‰¹å®šæ¨¡å‹ (æœ‰åˆ†é¡é ­)\n",
    "classifier_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=2  # äºŒåˆ†é¡ä»»å‹™\n",
    ")\n",
    "print(f\"\\nåˆ†é¡æ¨¡å‹: {classifier_model.__class__.__name__}\")\n",
    "print(f\"åƒæ•¸é‡: {classifier_model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**æ¨¡å‹å‘½åè¦ç¯„**:\n",
    "\n",
    "| å‰ç¶´ | èªªæ˜ | ç¯„ä¾‹ |\n",
    "|------|------|------|\n",
    "| `AutoModel` | åŸºç¤æ¨¡å‹ (ç„¡ä»»å‹™é ­) | `AutoModel.from_pretrained('bert-base')` |\n",
    "| `AutoModelForSequenceClassification` | åºåˆ—åˆ†é¡ | æƒ…æ„Ÿåˆ†æã€æ–‡æœ¬åˆ†é¡ |\n",
    "| `AutoModelForTokenClassification` | æ¨™è¨˜åˆ†é¡ | å‘½åå¯¦é«”è­˜åˆ¥ (NER) |\n",
    "| `AutoModelForQuestionAnswering` | å•ç­”ç³»çµ± | SQuAD, DRCD |\n",
    "| `AutoModelForCausalLM` | å› æœèªè¨€æ¨¡å‹ | GPT, LLaMA (æ–‡æœ¬ç”Ÿæˆ) |\n",
    "| `AutoModelForSeq2SeqLM` | åºåˆ—åˆ°åºåˆ— | T5, BART (ç¿»è­¯ã€æ‘˜è¦) |\n",
    "\n",
    "#### 2.1.2 Tokenizer (åˆ†è©å™¨)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# è¼‰å…¥åˆ†è©å™¨ (å¿…é ˆèˆ‡æ¨¡å‹åŒ¹é…)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# æ–‡æœ¬ç·¨ç¢¼\n",
    "text = \"Hugging Face is amazing!\"\n",
    "encoded = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "print(\"åŸå§‹æ–‡æœ¬:\", text)\n",
    "print(\"\\nç·¨ç¢¼çµæœ:\")\n",
    "print(f\"input_ids: {encoded['input_ids']}\")\n",
    "print(f\"attention_mask: {encoded['attention_mask']}\")\n",
    "\n",
    "# è§£ç¢¼å›æ–‡æœ¬\n",
    "decoded = tokenizer.decode(encoded['input_ids'][0])\n",
    "print(f\"\\nè§£ç¢¼æ–‡æœ¬: {decoded}\")\n",
    "\n",
    "# æŸ¥çœ‹è©å½™è¡¨å¤§å°\n",
    "print(f\"\\nè©å½™è¡¨å¤§å°: {tokenizer.vocab_size:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenizer é‡è¦åƒæ•¸**:\n",
    "\n",
    "| åƒæ•¸ | èªªæ˜ | ç¯„ä¾‹ |\n",
    "|------|------|------|\n",
    "| `return_tensors` | è¿”å›å¼µé‡é¡å‹ | `\"pt\"` (PyTorch), `\"tf\"` (TensorFlow) |\n",
    "| `padding` | å¡«å……ç­–ç•¥ | `True`, `\"max_length\"`, `\"longest\"` |\n",
    "| `truncation` | æˆªæ–·ç­–ç•¥ | `True`, `\"only_first\"`, `\"longest_first\"` |\n",
    "| `max_length` | æœ€å¤§åºåˆ—é•·åº¦ | `512`, `128` |\n",
    "| `add_special_tokens` | æ˜¯å¦æ·»åŠ ç‰¹æ®Šæ¨™è¨˜ | `True` (é»˜èª) |\n",
    "\n",
    "#### 2.1.3 Pipeline (ç®¡é“)\n",
    "\n",
    "**Pipeline æ˜¯æœ€å¿«é€Ÿçš„ä½¿ç”¨æ–¹å¼**,å°è£äº†æ¨¡å‹ã€åˆ†è©å™¨ã€å¾Œè™•ç†é‚è¼¯:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# 1. æƒ…æ„Ÿåˆ†æ Pipeline\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "result = sentiment_pipeline(\"I love this tutorial!\")\n",
    "print(\"æƒ…æ„Ÿåˆ†æ:\", result)\n",
    "\n",
    "# 2. å‘½åå¯¦é«”è­˜åˆ¥ Pipeline\n",
    "ner_pipeline = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
    "result = ner_pipeline(\"Hugging Face is based in New York City.\")\n",
    "print(\"\\nå‘½åå¯¦é«”è­˜åˆ¥:\", result)\n",
    "\n",
    "# 3. æ–‡æœ¬ç”Ÿæˆ Pipeline\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "result = generator(\n",
    "    \"Once upon a time\",\n",
    "    max_length=50,\n",
    "    num_return_sequences=1\n",
    ")\n",
    "print(\"\\næ–‡æœ¬ç”Ÿæˆ:\", result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**å…§å»º Pipeline ä»»å‹™åˆ—è¡¨**:\n",
    "\n",
    "| Pipeline åç¨± | ä»»å‹™ | ç¯„ä¾‹æ‡‰ç”¨ |\n",
    "|--------------|------|----------|\n",
    "| `sentiment-analysis` | æƒ…æ„Ÿåˆ†æ | è©•è«–æ­£è² é¢åˆ¤æ–· |\n",
    "| `ner` | å‘½åå¯¦é«”è­˜åˆ¥ | æŠ½å–äººåã€åœ°åã€çµ„ç¹” |\n",
    "| `question-answering` | å•ç­”ç³»çµ± | å¾æ–‡æœ¬ä¸­æ‰¾ç­”æ¡ˆ |\n",
    "| `text-generation` | æ–‡æœ¬ç”Ÿæˆ | è‡ªå‹•å¯«ä½œã€çºŒå¯« |\n",
    "| `summarization` | æ–‡æœ¬æ‘˜è¦ | æ–°èæ‘˜è¦ã€è«–æ–‡ç¸½çµ |\n",
    "| `translation` | æ©Ÿå™¨ç¿»è­¯ | å¤šèªè¨€ç¿»è­¯ |\n",
    "| `zero-shot-classification` | é›¶æ¨£æœ¬åˆ†é¡ | ç„¡éœ€è¨“ç·´çš„åˆ†é¡ |\n",
    "| `fill-mask` | å®Œå½¢å¡«ç©º | BERT å¼å¡«ç©ºä»»å‹™ |\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Hugging Face Hub æ¨¡å‹è³‡æº\n",
    "\n",
    "### 3.1 ç€è¦½èˆ‡æœå°‹æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import list_models\n",
    "\n",
    "# æœå°‹æƒ…æ„Ÿåˆ†ææ¨¡å‹ (é™åˆ¶å‰ 5 å€‹)\n",
    "models = list_models(\n",
    "    filter=\"text-classification\",\n",
    "    sort=\"downloads\",\n",
    "    direction=-1,\n",
    "    limit=5\n",
    ")\n",
    "\n",
    "print(\"ğŸ” æœ€å—æ­¡è¿çš„æƒ…æ„Ÿåˆ†ææ¨¡å‹:\\n\")\n",
    "for i, model in enumerate(models, 1):\n",
    "    print(f\"{i}. {model.modelId}\")\n",
    "    print(f\"   ä¸‹è¼‰æ¬¡æ•¸: {model.downloads:,}\")\n",
    "    print(f\"   æ¨™ç±¤: {model.tags[:5]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 æ¨¡å‹å¡ç‰‡ (Model Card) è§£è®€\n",
    "\n",
    "**æ¨¡å‹å¡ç‰‡åŒ…å«çš„é—œéµä¿¡æ¯**:\n",
    "\n",
    "1. **Model Description**: æ¨¡å‹ç°¡ä»‹èˆ‡æ¶æ§‹\n",
    "2. **Intended Use**: é æœŸç”¨é€”èˆ‡é™åˆ¶\n",
    "3. **Training Data**: è¨“ç·´æ•¸æ“šä¾†æº\n",
    "4. **Training Procedure**: è¨“ç·´ç´°ç¯€ (è¶…åƒæ•¸ã€ç¡¬é«”)\n",
    "5. **Evaluation Results**: è©•ä¼°æŒ‡æ¨™èˆ‡åŸºæº–å°æ¯”\n",
    "6. **Limitations**: å·²çŸ¥é™åˆ¶èˆ‡åå·®\n",
    "7. **How to Use**: ä½¿ç”¨ç¯„ä¾‹ä»£ç¢¼\n",
    "\n",
    "**ç¯„ä¾‹: æŸ¥çœ‹æ¨¡å‹å¡ç‰‡**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import ModelCard\n",
    "\n",
    "# è¼‰å…¥æ¨¡å‹å¡ç‰‡\n",
    "model_id = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "card = ModelCard.load(model_id)\n",
    "\n",
    "# é¡¯ç¤ºæ¨¡å‹å¡ç‰‡éƒ¨åˆ†å…§å®¹\n",
    "print(f\"ğŸ“„ æ¨¡å‹: {model_id}\")\n",
    "print(f\"\\næ¨¡å‹ç°¡ä»‹:\\n{card.text[:500]}...\")  # é¡¯ç¤ºå‰ 500 å­—å…ƒ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 æ¨¡å‹é¸æ“‡æ±ºç­–æ¨¹\n",
    "\n",
    "```\n",
    "é¸æ“‡ Hugging Face æ¨¡å‹çš„æµç¨‹:\n",
    "\n",
    "1. ç¢ºå®šä»»å‹™é¡å‹\n",
    "   â”œâ”€ åˆ†é¡ â†’ text-classification\n",
    "   â”œâ”€ NER â†’ token-classification\n",
    "   â”œâ”€ ç”Ÿæˆ â†’ text-generation\n",
    "   â””â”€ å•ç­” â†’ question-answering\n",
    "\n",
    "2. é¸æ“‡æ¨¡å‹å¤§å°\n",
    "   â”œâ”€ è³‡æºå—é™ â†’ distilbert, albert, mobile\n",
    "   â”œâ”€ å¹³è¡¡æ•ˆèƒ½ â†’ bert-base, roberta-base\n",
    "   â””â”€ æ¥µè‡´æ•ˆèƒ½ â†’ bert-large, roberta-large\n",
    "\n",
    "3. è€ƒæ…®èªè¨€\n",
    "   â”œâ”€ è‹±æ–‡ â†’ bert, roberta, gpt2\n",
    "   â”œâ”€ ä¸­æ–‡ â†’ bert-base-chinese, roberta-wwm-ext\n",
    "   â””â”€ å¤šèªè¨€ â†’ mbert, xlm-roberta\n",
    "\n",
    "4. æª¢æŸ¥ Fine-tune ç‹€æ…‹\n",
    "   â”œâ”€ å·²å¾®èª¿ (task-specific) â†’ ç›´æ¥ä½¿ç”¨\n",
    "   â””â”€ é è¨“ç·´ (pretrained) â†’ éœ€è‡ªè¡Œå¾®èª¿\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Datasets å‡½å¼åº«\n",
    "\n",
    "### 4.1 è¼‰å…¥å…§å»ºæ•¸æ“šé›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# è¼‰å…¥ IMDB é›»å½±è©•è«–æ•¸æ“šé›†\n",
    "dataset = load_dataset(\"imdb\", split=\"train[:100]\")  # å…ˆè¼‰å…¥ 100 ç­†æ¸¬è©¦\n",
    "\n",
    "print(f\"æ•¸æ“šé›†å¤§å°: {len(dataset)}\")\n",
    "print(f\"\\næ¬„ä½: {dataset.column_names}\")\n",
    "print(f\"\\nç¬¬ä¸€ç­†è³‡æ–™:\")\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 æ•¸æ“šé›†åŸºæœ¬æ“ä½œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# éæ¿¾æ•¸æ“š\n",
    "positive_reviews = dataset.filter(lambda x: x['label'] == 1)\n",
    "print(f\"æ­£é¢è©•è«–æ•¸é‡: {len(positive_reviews)}\")\n",
    "\n",
    "# æ˜ å°„å‡½æ•¸ (æ·»åŠ æ–‡æœ¬é•·åº¦æ¬„ä½)\n",
    "dataset_with_length = dataset.map(\n",
    "    lambda x: {\"text_length\": len(x['text'])}\n",
    ")\n",
    "print(f\"\\næ–°å¢æ¬„ä½: {dataset_with_length.column_names}\")\n",
    "\n",
    "# æŸ¥çœ‹çµ±è¨ˆä¿¡æ¯\n",
    "import numpy as np\n",
    "lengths = dataset_with_length['text_length']\n",
    "print(f\"\\næ–‡æœ¬é•·åº¦çµ±è¨ˆ:\")\n",
    "print(f\"  å¹³å‡: {np.mean(lengths):.0f} å­—å…ƒ\")\n",
    "print(f\"  æœ€å¤§: {max(lengths)} å­—å…ƒ\")\n",
    "print(f\"  æœ€å°: {min(lengths)} å­—å…ƒ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 æ•¸æ“šé›†æ ¼å¼è½‰æ›"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è½‰æ›ç‚º Pandas DataFrame\n",
    "df = dataset.to_pandas()\n",
    "print(\"Pandas DataFrame:\")\n",
    "print(df.head(3))\n",
    "\n",
    "# è½‰æ›ç‚º PyTorch Dataset\n",
    "dataset.set_format(type=\"torch\", columns=[\"label\"])\n",
    "print(f\"\\nPyTorch æ ¼å¼: {dataset[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. å¯¦æˆ°æ¡ˆä¾‹: å®Œæ•´æµç¨‹ç¤ºç¯„\n",
    "\n",
    "### 5.1 æƒ…æ„Ÿåˆ†æå®Œæ•´æµç¨‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Step 1: è¼‰å…¥é è¨“ç·´æ¨¡å‹ (Pipeline å°è£)\n",
    "classifier = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    ")\n",
    "\n",
    "# Step 2: æº–å‚™æ¸¬è©¦æ•¸æ“š\n",
    "test_texts = [\n",
    "    \"This movie is absolutely fantastic!\",\n",
    "    \"I hated every minute of it.\",\n",
    "    \"It was okay, nothing special.\",\n",
    "    \"Best film I've seen this year!\"\n",
    "]\n",
    "\n",
    "# Step 3: æ‰¹æ¬¡é æ¸¬\n",
    "results = classifier(test_texts)\n",
    "\n",
    "# Step 4: é¡¯ç¤ºçµæœ\n",
    "for text, result in zip(test_texts, results):\n",
    "    label = result['label']\n",
    "    score = result['score']\n",
    "    print(f\"æ–‡æœ¬: {text}\")\n",
    "    print(f\"é æ¸¬: {label} (ä¿¡å¿ƒåº¦: {score:.2%})\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 æ‰‹å‹•æµç¨‹ (ä¸ä½¿ç”¨ Pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Step 1: è¼‰å…¥æ¨¡å‹èˆ‡åˆ†è©å™¨\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Step 2: ç·¨ç¢¼æ–‡æœ¬\n",
    "text = \"Hugging Face makes NLP so easy!\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Step 3: æ¨¡å‹æ¨ç†\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.softmax(logits, dim=-1)\n",
    "\n",
    "# Step 4: è§£æçµæœ\n",
    "predicted_class = torch.argmax(predictions, dim=-1).item()\n",
    "confidence = predictions[0][predicted_class].item()\n",
    "\n",
    "label_map = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "print(f\"æ–‡æœ¬: {text}\")\n",
    "print(f\"é æ¸¬: {label_map[predicted_class]}\")\n",
    "print(f\"ä¿¡å¿ƒåº¦: {confidence:.2%}\")\n",
    "print(f\"\\nåŸå§‹ logits: {logits}\")\n",
    "print(f\"softmax æ©Ÿç‡: {predictions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. æœ€ä½³å¯¦è¸èˆ‡å¸¸è¦‹å•é¡Œ\n",
    "\n",
    "### 6.1 æœ€ä½³å¯¦è¸\n",
    "\n",
    "1. **é¸æ“‡åˆé©çš„æ¨¡å‹å¤§å°**:\n",
    "   - åŸå‹é–‹ç™¼: `distilbert`, `albert-base`\n",
    "   - ç”Ÿç”¢éƒ¨ç½²: `bert-base`, `roberta-base`\n",
    "   - å­¸è¡“ç ”ç©¶: `bert-large`, `roberta-large`\n",
    "\n",
    "2. **ä½¿ç”¨ `AutoModel` è€Œéå…·é«”æ¨¡å‹é¡**:\n",
    "   ```python\n",
    "   # âœ… æ¨è–¦\n",
    "   from transformers import AutoModel\n",
    "   model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "   \n",
    "   # âŒ ä¸æ¨è–¦\n",
    "   from transformers import BertModel\n",
    "   model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "   ```\n",
    "\n",
    "3. **å¿«å–æ¨¡å‹ä»¥åŠ é€Ÿè¼‰å…¥**:\n",
    "   ```python\n",
    "   # æ¨¡å‹æœƒè‡ªå‹•å¿«å–åˆ° ~/.cache/huggingface/\n",
    "   # ç¬¬äºŒæ¬¡è¼‰å…¥æ™‚æœƒç›´æ¥å¾æœ¬åœ°è®€å–\n",
    "   ```\n",
    "\n",
    "4. **ä½¿ç”¨ `device_map` é€²è¡Œå¤š GPU æ¨ç†**:\n",
    "   ```python\n",
    "   model = AutoModel.from_pretrained(\n",
    "       \"bert-large-uncased\",\n",
    "       device_map=\"auto\"  # è‡ªå‹•åˆ†é…åˆ°å¯ç”¨ GPU\n",
    "   )\n",
    "   ```\n",
    "\n",
    "### 6.2 å¸¸è¦‹å•é¡Œ\n",
    "\n",
    "**Q1: æ¨¡å‹ä¸‹è¼‰å¤±æ•—æ€éº¼è¾¦?**\n",
    "```python\n",
    "# ä½¿ç”¨é¡åƒç«™ (ä¸­åœ‹å¤§é™¸ç”¨æˆ¶)\n",
    "import os\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "```\n",
    "\n",
    "**Q2: è¨˜æ†¶é«”ä¸è¶³ (OOM)?**\n",
    "```python\n",
    "# 1. ä½¿ç”¨æ›´å°çš„æ¨¡å‹\n",
    "# 2. æ¸›å°‘ batch size\n",
    "# 3. ä½¿ç”¨é‡åŒ–æ¨¡å‹\n",
    "from transformers import AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    load_in_8bit=True  # 8-bit é‡åŒ–\n",
    ")\n",
    "```\n",
    "\n",
    "**Q3: å¦‚ä½•é›¢ç·šä½¿ç”¨æ¨¡å‹?**\n",
    "```python\n",
    "# é å…ˆä¸‹è¼‰æ¨¡å‹\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "model.save_pretrained(\"./local_model\")\n",
    "\n",
    "# é›¢ç·šè¼‰å…¥\n",
    "model = AutoModel.from_pretrained(\"./local_model\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 7. èª²å¾Œç·´ç¿’\n",
    "\n",
    "### ç·´ç¿’ 1: æ¢ç´¢ä¸åŒä»»å‹™çš„ Pipeline\n",
    "\n",
    "å˜—è©¦ä½¿ç”¨ä»¥ä¸‹ Pipeline:\n",
    "1. `fill-mask`: å®Œå½¢å¡«ç©º\n",
    "2. `question-answering`: å•ç­”ç³»çµ±\n",
    "3. `summarization`: æ–‡æœ¬æ‘˜è¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç·´ç¿’ 1: Fill-Mask\n",
    "from transformers import pipeline\n",
    "\n",
    "unmasker = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
    "result = unmasker(\"Hugging Face is [MASK] for NLP.\")\n",
    "print(\"Fill-Mask çµæœ:\")\n",
    "for r in result[:3]:\n",
    "    print(f\"  {r['sequence']} (score: {r['score']:.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç·´ç¿’ 2: Question Answering\n",
    "qa_pipeline = pipeline(\"question-answering\")\n",
    "\n",
    "context = \"\"\"\n",
    "Hugging Face is a company based in New York City. \n",
    "It was founded in 2016 and specializes in natural language processing.\n",
    "\"\"\"\n",
    "\n",
    "question = \"When was Hugging Face founded?\"\n",
    "result = qa_pipeline(question=question, context=context)\n",
    "\n",
    "print(f\"å•é¡Œ: {question}\")\n",
    "print(f\"ç­”æ¡ˆ: {result['answer']} (ä¿¡å¿ƒåº¦: {result['score']:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ç·´ç¿’ 2: æ¯”è¼ƒä¸åŒæ¨¡å‹æ•ˆèƒ½\n",
    "\n",
    "æ¯”è¼ƒ `distilbert-base-uncased` èˆ‡ `bert-base-uncased` çš„åƒæ•¸é‡èˆ‡æ¨ç†é€Ÿåº¦:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "models_to_compare = [\n",
    "    \"distilbert-base-uncased\",\n",
    "    \"bert-base-uncased\"\n",
    "]\n",
    "\n",
    "test_text = \"Comparing model performance\" * 10  # é‡è¤‡ 10 æ¬¡\n",
    "\n",
    "for model_name in models_to_compare:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    \n",
    "    inputs = tokenizer(test_text, return_tensors=\"pt\")\n",
    "    \n",
    "    # æ¸¬é‡æ¨ç†æ™‚é–“\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        _ = model(**inputs)\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    print(f\"\\næ¨¡å‹: {model_name}\")\n",
    "    print(f\"  åƒæ•¸é‡: {model.num_parameters():,}\")\n",
    "    print(f\"  æ¨ç†æ™‚é–“: {elapsed*1000:.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. æœ¬ç¯€ç¸½çµ\n",
    "\n",
    "### âœ… é—œéµè¦é»\n",
    "\n",
    "1. **Hugging Face ç”Ÿæ…‹ç³»çµ±**:\n",
    "   - Transformers (æ¨¡å‹), Datasets (æ•¸æ“š), Hub (å¹³å°)\n",
    "   - çµ±ä¸€ API,æ”¯æ´æ‰€æœ‰ä¸»æµæ¨¡å‹\n",
    "\n",
    "2. **ä¸‰å¤§æ ¸å¿ƒçµ„ä»¶**:\n",
    "   - Model: é è¨“ç·´ç¥ç¶“ç¶²è·¯\n",
    "   - Tokenizer: æ–‡æœ¬ç·¨ç¢¼/è§£ç¢¼\n",
    "   - Pipeline: ç«¯åˆ°ç«¯å¿«é€Ÿæ‡‰ç”¨\n",
    "\n",
    "3. **æ¨¡å‹é¸æ“‡ç­–ç•¥**:\n",
    "   - ä»»å‹™é¡å‹ â†’ æ¨¡å‹æ¶æ§‹\n",
    "   - è³‡æºé™åˆ¶ â†’ æ¨¡å‹å¤§å°\n",
    "   - èªè¨€éœ€æ±‚ â†’ é è¨“ç·´èªæ–™\n",
    "\n",
    "4. **æœ€ä½³å¯¦è¸**:\n",
    "   - ä½¿ç”¨ `AutoModel` æå‡å½ˆæ€§\n",
    "   - åˆ©ç”¨å¿«å–æ©Ÿåˆ¶åŠ é€Ÿé–‹ç™¼\n",
    "   - é–±è®€æ¨¡å‹å¡ç‰‡äº†è§£é™åˆ¶\n",
    "\n",
    "### ğŸ“š å»¶ä¼¸é–±è®€\n",
    "\n",
    "- [Hugging Face å®˜æ–¹æ–‡æª”](https://huggingface.co/docs/transformers)\n",
    "- [Hugging Face èª²ç¨‹](https://huggingface.co/course)\n",
    "- [æ¨¡å‹ Hub](https://huggingface.co/models)\n",
    "\n",
    "### ğŸš€ ä¸‹ä¸€ç¯€é å‘Š\n",
    "\n",
    "**CH08-02: Pipeline API å¿«é€Ÿå…¥é–€**\n",
    "- æ·±å…¥ Pipeline å…§éƒ¨æ©Ÿåˆ¶\n",
    "- è‡ªè¨‚ Pipeline åƒæ•¸\n",
    "- æ‰¹æ¬¡è™•ç†èˆ‡æ•ˆèƒ½å„ªåŒ–\n",
    "\n",
    "---\n",
    "\n",
    "**èª²ç¨‹**: iSpan Python NLP Cookbooks v2\n",
    "**è¬›å¸«**: Claude AI\n",
    "**æœ€å¾Œæ›´æ–°**: 2025-10-17"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
