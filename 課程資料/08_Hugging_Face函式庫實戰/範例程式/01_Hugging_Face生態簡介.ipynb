{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH08-01: Hugging Face 生態簡介\n",
    "\n",
    "**課程**: iSpan Python NLP Cookbooks v2\n",
    "**章節**: CH08 Hugging Face 實戰\n",
    "**版本**: v1.0\n",
    "**更新日期**: 2025-10-17\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 本節學習目標\n",
    "\n",
    "1. 理解 Hugging Face 生態系統的核心組件\n",
    "2. 掌握 Transformers 函式庫的基本架構\n",
    "3. 熟悉 Hugging Face Hub 模型與數據集資源\n",
    "4. 了解模型卡片 (Model Card) 的重要性\n",
    "5. 學會快速搜尋與選擇預訓練模型\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Hugging Face 生態系統概覽\n",
    "\n",
    "### 1.1 什麼是 Hugging Face?\n",
    "\n",
    "**Hugging Face** 是目前最流行的 NLP 開源社群平台,提供:\n",
    "\n",
    "- 🤗 **Transformers**: 預訓練模型函式庫 (PyTorch, TensorFlow, JAX)\n",
    "- 🗂️ **Datasets**: 數據集函式庫 (超過 50,000 個數據集)\n",
    "- 🏛️ **Hub**: 模型與數據集共享平台 (超過 500,000 個模型)\n",
    "- ⚡ **Accelerate**: 分散式訓練加速工具\n",
    "- 🔍 **Tokenizers**: 高效能分詞器\n",
    "\n",
    "**核心優勢**:\n",
    "- ✅ 統一 API,支援所有主流模型 (BERT, GPT, T5...)\n",
    "- ✅ 開箱即用,無需從零訓練\n",
    "- ✅ 活躍社群,持續更新最新模型\n",
    "- ✅ 完整文檔與教學資源"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安裝 Hugging Face 核心套件\n",
    "# !pip install transformers datasets accelerate tokenizers -q\n",
    "\n",
    "# 驗證安裝\n",
    "import transformers\n",
    "import datasets\n",
    "\n",
    "print(f\"✅ Transformers 版本: {transformers.__version__}\")\n",
    "print(f\"✅ Datasets 版本: {datasets.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 生態系統架構圖\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────┐\n",
    "│           Hugging Face 生態系統              │\n",
    "├─────────────────────────────────────────────┤\n",
    "│                                             │\n",
    "│  ┌──────────┐  ┌──────────┐  ┌──────────┐ │\n",
    "│  │   Hub    │  │Transformers│ │ Datasets │ │\n",
    "│  │ (模型庫) │  │ (模型API)  │ │ (數據集) │ │\n",
    "│  └──────────┘  └──────────┘  └──────────┘ │\n",
    "│       ↓              ↓              ↓      │\n",
    "│  ┌──────────┐  ┌──────────┐  ┌──────────┐ │\n",
    "│  │Tokenizers│  │Accelerate│  │ Evaluate │ │\n",
    "│  │ (分詞器) │  │ (加速器) │  │ (評估)   │ │\n",
    "│  └──────────┘  └──────────┘  └──────────┘ │\n",
    "└─────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Transformers 函式庫核心概念\n",
    "\n",
    "### 2.1 三大核心組件\n",
    "\n",
    "Transformers 函式庫圍繞三個核心概念:\n",
    "\n",
    "1. **Model (模型)**: 預訓練的神經網路\n",
    "2. **Tokenizer (分詞器)**: 文本 → 數字的轉換器\n",
    "3. **Pipeline (管道)**: 端到端的快速應用 API\n",
    "\n",
    "#### 2.1.1 Model (模型)\n",
    "\n",
    "**模型架構分類**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoModelForSequenceClassification\n",
    "\n",
    "# 載入預訓練模型\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "\n",
    "# 1. 基礎模型 (無任務頭)\n",
    "base_model = AutoModel.from_pretrained(model_name)\n",
    "print(f\"基礎模型: {base_model.__class__.__name__}\")\n",
    "print(f\"參數量: {base_model.num_parameters():,}\")\n",
    "\n",
    "# 2. 任務特定模型 (有分類頭)\n",
    "classifier_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=2  # 二分類任務\n",
    ")\n",
    "print(f\"\\n分類模型: {classifier_model.__class__.__name__}\")\n",
    "print(f\"參數量: {classifier_model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**模型命名規範**:\n",
    "\n",
    "| 前綴 | 說明 | 範例 |\n",
    "|------|------|------|\n",
    "| `AutoModel` | 基礎模型 (無任務頭) | `AutoModel.from_pretrained('bert-base')` |\n",
    "| `AutoModelForSequenceClassification` | 序列分類 | 情感分析、文本分類 |\n",
    "| `AutoModelForTokenClassification` | 標記分類 | 命名實體識別 (NER) |\n",
    "| `AutoModelForQuestionAnswering` | 問答系統 | SQuAD, DRCD |\n",
    "| `AutoModelForCausalLM` | 因果語言模型 | GPT, LLaMA (文本生成) |\n",
    "| `AutoModelForSeq2SeqLM` | 序列到序列 | T5, BART (翻譯、摘要) |\n",
    "\n",
    "#### 2.1.2 Tokenizer (分詞器)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 載入分詞器 (必須與模型匹配)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# 文本編碼\n",
    "text = \"Hugging Face is amazing!\"\n",
    "encoded = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "print(\"原始文本:\", text)\n",
    "print(\"\\n編碼結果:\")\n",
    "print(f\"input_ids: {encoded['input_ids']}\")\n",
    "print(f\"attention_mask: {encoded['attention_mask']}\")\n",
    "\n",
    "# 解碼回文本\n",
    "decoded = tokenizer.decode(encoded['input_ids'][0])\n",
    "print(f\"\\n解碼文本: {decoded}\")\n",
    "\n",
    "# 查看詞彙表大小\n",
    "print(f\"\\n詞彙表大小: {tokenizer.vocab_size:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenizer 重要參數**:\n",
    "\n",
    "| 參數 | 說明 | 範例 |\n",
    "|------|------|------|\n",
    "| `return_tensors` | 返回張量類型 | `\"pt\"` (PyTorch), `\"tf\"` (TensorFlow) |\n",
    "| `padding` | 填充策略 | `True`, `\"max_length\"`, `\"longest\"` |\n",
    "| `truncation` | 截斷策略 | `True`, `\"only_first\"`, `\"longest_first\"` |\n",
    "| `max_length` | 最大序列長度 | `512`, `128` |\n",
    "| `add_special_tokens` | 是否添加特殊標記 | `True` (默認) |\n",
    "\n",
    "#### 2.1.3 Pipeline (管道)\n",
    "\n",
    "**Pipeline 是最快速的使用方式**,封裝了模型、分詞器、後處理邏輯:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# 1. 情感分析 Pipeline\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "result = sentiment_pipeline(\"I love this tutorial!\")\n",
    "print(\"情感分析:\", result)\n",
    "\n",
    "# 2. 命名實體識別 Pipeline\n",
    "ner_pipeline = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
    "result = ner_pipeline(\"Hugging Face is based in New York City.\")\n",
    "print(\"\\n命名實體識別:\", result)\n",
    "\n",
    "# 3. 文本生成 Pipeline\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "result = generator(\n",
    "    \"Once upon a time\",\n",
    "    max_length=50,\n",
    "    num_return_sequences=1\n",
    ")\n",
    "print(\"\\n文本生成:\", result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**內建 Pipeline 任務列表**:\n",
    "\n",
    "| Pipeline 名稱 | 任務 | 範例應用 |\n",
    "|--------------|------|----------|\n",
    "| `sentiment-analysis` | 情感分析 | 評論正負面判斷 |\n",
    "| `ner` | 命名實體識別 | 抽取人名、地名、組織 |\n",
    "| `question-answering` | 問答系統 | 從文本中找答案 |\n",
    "| `text-generation` | 文本生成 | 自動寫作、續寫 |\n",
    "| `summarization` | 文本摘要 | 新聞摘要、論文總結 |\n",
    "| `translation` | 機器翻譯 | 多語言翻譯 |\n",
    "| `zero-shot-classification` | 零樣本分類 | 無需訓練的分類 |\n",
    "| `fill-mask` | 完形填空 | BERT 式填空任務 |\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Hugging Face Hub 模型資源\n",
    "\n",
    "### 3.1 瀏覽與搜尋模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import list_models\n",
    "\n",
    "# 搜尋情感分析模型 (限制前 5 個)\n",
    "models = list_models(\n",
    "    filter=\"text-classification\",\n",
    "    sort=\"downloads\",\n",
    "    direction=-1,\n",
    "    limit=5\n",
    ")\n",
    "\n",
    "print(\"🔝 最受歡迎的情感分析模型:\\n\")\n",
    "for i, model in enumerate(models, 1):\n",
    "    print(f\"{i}. {model.modelId}\")\n",
    "    print(f\"   下載次數: {model.downloads:,}\")\n",
    "    print(f\"   標籤: {model.tags[:5]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 模型卡片 (Model Card) 解讀\n",
    "\n",
    "**模型卡片包含的關鍵信息**:\n",
    "\n",
    "1. **Model Description**: 模型簡介與架構\n",
    "2. **Intended Use**: 預期用途與限制\n",
    "3. **Training Data**: 訓練數據來源\n",
    "4. **Training Procedure**: 訓練細節 (超參數、硬體)\n",
    "5. **Evaluation Results**: 評估指標與基準對比\n",
    "6. **Limitations**: 已知限制與偏差\n",
    "7. **How to Use**: 使用範例代碼\n",
    "\n",
    "**範例: 查看模型卡片**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import ModelCard\n",
    "\n",
    "# 載入模型卡片\n",
    "model_id = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "card = ModelCard.load(model_id)\n",
    "\n",
    "# 顯示模型卡片部分內容\n",
    "print(f\"📄 模型: {model_id}\")\n",
    "print(f\"\\n模型簡介:\\n{card.text[:500]}...\")  # 顯示前 500 字元"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 模型選擇決策樹\n",
    "\n",
    "```\n",
    "選擇 Hugging Face 模型的流程:\n",
    "\n",
    "1. 確定任務類型\n",
    "   ├─ 分類 → text-classification\n",
    "   ├─ NER → token-classification\n",
    "   ├─ 生成 → text-generation\n",
    "   └─ 問答 → question-answering\n",
    "\n",
    "2. 選擇模型大小\n",
    "   ├─ 資源受限 → distilbert, albert, mobile\n",
    "   ├─ 平衡效能 → bert-base, roberta-base\n",
    "   └─ 極致效能 → bert-large, roberta-large\n",
    "\n",
    "3. 考慮語言\n",
    "   ├─ 英文 → bert, roberta, gpt2\n",
    "   ├─ 中文 → bert-base-chinese, roberta-wwm-ext\n",
    "   └─ 多語言 → mbert, xlm-roberta\n",
    "\n",
    "4. 檢查 Fine-tune 狀態\n",
    "   ├─ 已微調 (task-specific) → 直接使用\n",
    "   └─ 預訓練 (pretrained) → 需自行微調\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Datasets 函式庫\n",
    "\n",
    "### 4.1 載入內建數據集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# 載入 IMDB 電影評論數據集\n",
    "dataset = load_dataset(\"imdb\", split=\"train[:100]\")  # 先載入 100 筆測試\n",
    "\n",
    "print(f\"數據集大小: {len(dataset)}\")\n",
    "print(f\"\\n欄位: {dataset.column_names}\")\n",
    "print(f\"\\n第一筆資料:\")\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 數據集基本操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 過濾數據\n",
    "positive_reviews = dataset.filter(lambda x: x['label'] == 1)\n",
    "print(f\"正面評論數量: {len(positive_reviews)}\")\n",
    "\n",
    "# 映射函數 (添加文本長度欄位)\n",
    "dataset_with_length = dataset.map(\n",
    "    lambda x: {\"text_length\": len(x['text'])}\n",
    ")\n",
    "print(f\"\\n新增欄位: {dataset_with_length.column_names}\")\n",
    "\n",
    "# 查看統計信息\n",
    "import numpy as np\n",
    "lengths = dataset_with_length['text_length']\n",
    "print(f\"\\n文本長度統計:\")\n",
    "print(f\"  平均: {np.mean(lengths):.0f} 字元\")\n",
    "print(f\"  最大: {max(lengths)} 字元\")\n",
    "print(f\"  最小: {min(lengths)} 字元\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 數據集格式轉換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 轉換為 Pandas DataFrame\n",
    "df = dataset.to_pandas()\n",
    "print(\"Pandas DataFrame:\")\n",
    "print(df.head(3))\n",
    "\n",
    "# 轉換為 PyTorch Dataset\n",
    "dataset.set_format(type=\"torch\", columns=[\"label\"])\n",
    "print(f\"\\nPyTorch 格式: {dataset[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. 實戰案例: 完整流程示範\n",
    "\n",
    "### 5.1 情感分析完整流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Step 1: 載入預訓練模型 (Pipeline 封裝)\n",
    "classifier = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    ")\n",
    "\n",
    "# Step 2: 準備測試數據\n",
    "test_texts = [\n",
    "    \"This movie is absolutely fantastic!\",\n",
    "    \"I hated every minute of it.\",\n",
    "    \"It was okay, nothing special.\",\n",
    "    \"Best film I've seen this year!\"\n",
    "]\n",
    "\n",
    "# Step 3: 批次預測\n",
    "results = classifier(test_texts)\n",
    "\n",
    "# Step 4: 顯示結果\n",
    "for text, result in zip(test_texts, results):\n",
    "    label = result['label']\n",
    "    score = result['score']\n",
    "    print(f\"文本: {text}\")\n",
    "    print(f\"預測: {label} (信心度: {score:.2%})\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 手動流程 (不使用 Pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Step 1: 載入模型與分詞器\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Step 2: 編碼文本\n",
    "text = \"Hugging Face makes NLP so easy!\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Step 3: 模型推理\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.softmax(logits, dim=-1)\n",
    "\n",
    "# Step 4: 解析結果\n",
    "predicted_class = torch.argmax(predictions, dim=-1).item()\n",
    "confidence = predictions[0][predicted_class].item()\n",
    "\n",
    "label_map = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "print(f\"文本: {text}\")\n",
    "print(f\"預測: {label_map[predicted_class]}\")\n",
    "print(f\"信心度: {confidence:.2%}\")\n",
    "print(f\"\\n原始 logits: {logits}\")\n",
    "print(f\"softmax 機率: {predictions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. 最佳實踐與常見問題\n",
    "\n",
    "### 6.1 最佳實踐\n",
    "\n",
    "1. **選擇合適的模型大小**:\n",
    "   - 原型開發: `distilbert`, `albert-base`\n",
    "   - 生產部署: `bert-base`, `roberta-base`\n",
    "   - 學術研究: `bert-large`, `roberta-large`\n",
    "\n",
    "2. **使用 `AutoModel` 而非具體模型類**:\n",
    "   ```python\n",
    "   # ✅ 推薦\n",
    "   from transformers import AutoModel\n",
    "   model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "   \n",
    "   # ❌ 不推薦\n",
    "   from transformers import BertModel\n",
    "   model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "   ```\n",
    "\n",
    "3. **快取模型以加速載入**:\n",
    "   ```python\n",
    "   # 模型會自動快取到 ~/.cache/huggingface/\n",
    "   # 第二次載入時會直接從本地讀取\n",
    "   ```\n",
    "\n",
    "4. **使用 `device_map` 進行多 GPU 推理**:\n",
    "   ```python\n",
    "   model = AutoModel.from_pretrained(\n",
    "       \"bert-large-uncased\",\n",
    "       device_map=\"auto\"  # 自動分配到可用 GPU\n",
    "   )\n",
    "   ```\n",
    "\n",
    "### 6.2 常見問題\n",
    "\n",
    "**Q1: 模型下載失敗怎麼辦?**\n",
    "```python\n",
    "# 使用鏡像站 (中國大陸用戶)\n",
    "import os\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "```\n",
    "\n",
    "**Q2: 記憶體不足 (OOM)?**\n",
    "```python\n",
    "# 1. 使用更小的模型\n",
    "# 2. 減少 batch size\n",
    "# 3. 使用量化模型\n",
    "from transformers import AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    load_in_8bit=True  # 8-bit 量化\n",
    ")\n",
    "```\n",
    "\n",
    "**Q3: 如何離線使用模型?**\n",
    "```python\n",
    "# 預先下載模型\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "model.save_pretrained(\"./local_model\")\n",
    "\n",
    "# 離線載入\n",
    "model = AutoModel.from_pretrained(\"./local_model\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 7. 課後練習\n",
    "\n",
    "### 練習 1: 探索不同任務的 Pipeline\n",
    "\n",
    "嘗試使用以下 Pipeline:\n",
    "1. `fill-mask`: 完形填空\n",
    "2. `question-answering`: 問答系統\n",
    "3. `summarization`: 文本摘要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 練習 1: Fill-Mask\n",
    "from transformers import pipeline\n",
    "\n",
    "unmasker = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
    "result = unmasker(\"Hugging Face is [MASK] for NLP.\")\n",
    "print(\"Fill-Mask 結果:\")\n",
    "for r in result[:3]:\n",
    "    print(f\"  {r['sequence']} (score: {r['score']:.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 練習 2: Question Answering\n",
    "qa_pipeline = pipeline(\"question-answering\")\n",
    "\n",
    "context = \"\"\"\n",
    "Hugging Face is a company based in New York City. \n",
    "It was founded in 2016 and specializes in natural language processing.\n",
    "\"\"\"\n",
    "\n",
    "question = \"When was Hugging Face founded?\"\n",
    "result = qa_pipeline(question=question, context=context)\n",
    "\n",
    "print(f\"問題: {question}\")\n",
    "print(f\"答案: {result['answer']} (信心度: {result['score']:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習 2: 比較不同模型效能\n",
    "\n",
    "比較 `distilbert-base-uncased` 與 `bert-base-uncased` 的參數量與推理速度:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "models_to_compare = [\n",
    "    \"distilbert-base-uncased\",\n",
    "    \"bert-base-uncased\"\n",
    "]\n",
    "\n",
    "test_text = \"Comparing model performance\" * 10  # 重複 10 次\n",
    "\n",
    "for model_name in models_to_compare:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    \n",
    "    inputs = tokenizer(test_text, return_tensors=\"pt\")\n",
    "    \n",
    "    # 測量推理時間\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        _ = model(**inputs)\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    print(f\"\\n模型: {model_name}\")\n",
    "    print(f\"  參數量: {model.num_parameters():,}\")\n",
    "    print(f\"  推理時間: {elapsed*1000:.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. 本節總結\n",
    "\n",
    "### ✅ 關鍵要點\n",
    "\n",
    "1. **Hugging Face 生態系統**:\n",
    "   - Transformers (模型), Datasets (數據), Hub (平台)\n",
    "   - 統一 API,支援所有主流模型\n",
    "\n",
    "2. **三大核心組件**:\n",
    "   - Model: 預訓練神經網路\n",
    "   - Tokenizer: 文本編碼/解碼\n",
    "   - Pipeline: 端到端快速應用\n",
    "\n",
    "3. **模型選擇策略**:\n",
    "   - 任務類型 → 模型架構\n",
    "   - 資源限制 → 模型大小\n",
    "   - 語言需求 → 預訓練語料\n",
    "\n",
    "4. **最佳實踐**:\n",
    "   - 使用 `AutoModel` 提升彈性\n",
    "   - 利用快取機制加速開發\n",
    "   - 閱讀模型卡片了解限制\n",
    "\n",
    "### 📚 延伸閱讀\n",
    "\n",
    "- [Hugging Face 官方文檔](https://huggingface.co/docs/transformers)\n",
    "- [Hugging Face 課程](https://huggingface.co/course)\n",
    "- [模型 Hub](https://huggingface.co/models)\n",
    "\n",
    "### 🚀 下一節預告\n",
    "\n",
    "**CH08-02: Pipeline API 快速入門**\n",
    "- 深入 Pipeline 內部機制\n",
    "- 自訂 Pipeline 參數\n",
    "- 批次處理與效能優化\n",
    "\n",
    "---\n",
    "\n",
    "**課程**: iSpan Python NLP Cookbooks v2\n",
    "**講師**: Claude AI\n",
    "**最後更新**: 2025-10-17"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
