# Encoder èˆ‡ Decoder æ·±åº¦å‰–æ

**ç‰ˆæœ¬**: v1.0
**æ›´æ–°æ—¥æœŸ**: 2025-10-17
**é©ç”¨èª²ç¨‹**: iSpan Python NLP Cookbooks v2 - CH07 Transformerèˆ‡å¤§å‹èªè¨€æ¨¡å‹

---

## ğŸ“‹ ä¸‰è¦–è§’å°æ¯”è¡¨

| æ¦‚å¿µ | ä¸­æ–‡è­¯å | å…¸å‹ç”¨é€” | å„ªé» | ä¾·é™ |
| :--- | :--- | :--- | :--- | :--- |
| **Fundamentals** | åŸºç¤ç†è«– | å¿«é€Ÿç†è§£ Encoder/Decoder çš„è§’è‰²åˆ†å·¥èˆ‡é‹ä½œæµç¨‹ | ç›´è§€æ˜“æ‡‚,èƒ½å¿«é€Ÿå»ºç«‹å¯¦ä½œèƒ½åŠ›ã€‚ | æ˜“å¿½ç•¥é›™å‘ç·¨ç¢¼èˆ‡è‡ªå›æ­¸è§£ç¢¼çš„æœ¬è³ªå·®ç•°ã€‚ |
| **First Principles** | ç¬¬ä¸€åŸç† | æ·±å…¥æŒæ¡ Masked Self-Attentionã€Cross-Attention çš„æ•¸å­¸åŸç† | æ·±å…¥æœ¬è³ª,æœ‰åŠ©æ–¼å‰µæ–°èˆ‡è®Šé«”è¨­è¨ˆã€‚ | ç†è«–æ€§å¼·,éœ€è¦è¼ƒå¼·çš„æ•¸å­¸èƒŒæ™¯ã€‚ |
| **Body of Knowledge** | çŸ¥è­˜é«”ç³» | ç†è§£ Encoder-Onlyã€Decoder-Onlyã€Encoder-Decoder æ¶æ§‹çš„é¸æ“‡ç­–ç•¥ | çµæ§‹å®Œå‚™,èƒ½èˆ‡å„é¡ NLP ä»»å‹™æ•´åˆã€‚ | å…§å®¹é¾é›œ,ä¸é©åˆå¿«é€Ÿå…¥é–€ã€‚ |

---

## 1. Fundamentals (åŸºç¤ç†è«–)

åœ¨ **Transformer æ¶æ§‹**ä¸­,**Encoder** èˆ‡ **Decoder** æ‰®æ¼”è‘—æˆªç„¶ä¸åŒçš„è§’è‰²:

1. **Encoder çš„æŒ‘æˆ°**: å¦‚ä½•**é›™å‘ç†è§£**æ•´å€‹è¼¸å…¥åºåˆ—,æ•æ‰å‰å¾Œæ–‡ä¾è³´é—œä¿‚?
2. **Decoder çš„æŒ‘æˆ°**: å¦‚ä½•**å–®å‘ç”Ÿæˆ**è¼¸å‡ºåºåˆ—,åŒæ™‚åˆ©ç”¨ Encoder çš„ç·¨ç¢¼çµæœ?
3. **Cross-Attention çš„æŒ‘æˆ°**: å¦‚ä½•è®“ Decoder **å°é½Š**åˆ° Encoder çš„è¡¨ç¤º,å¯¦ç¾è·¨åºåˆ—çš„ä¿¡æ¯å‚³é?

**Encoder èˆ‡ Decoder** çš„æ ¸å¿ƒæ€æƒ³æ˜¯: **Encoder è² è²¬é›™å‘ç·¨ç¢¼è¼¸å…¥,Decoder è² è²¬è‡ªå›æ­¸ç”Ÿæˆè¼¸å‡º,Cross-Attention æ©‹æ¥å…©è€…**ã€‚

---

### Encoder èˆ‡ Decoder çš„æ–¹æ³•å­¸åˆ†é¡

æ ¹æ“š **æ¶æ§‹è¨­è¨ˆ**,ä¸»æµæ–¹æ³•å¯åˆ†ç‚º:

#### 1. æŒ‰ **æ¶æ§‹é¡å‹** åˆ†é¡

**A. Encoder-Only (åƒ…ç·¨ç¢¼å™¨)**
* **æ ¸å¿ƒæ€æƒ³**: åªä½¿ç”¨é›™å‘ç·¨ç¢¼å™¨,ç„¡è§£ç¢¼å™¨
* **ä»£è¡¨æ–¹æ³•**: BERTã€RoBERTaã€ALBERT
* **å„ªé»**:
    * é›™å‘ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›å¼·
    * é©åˆåˆ†é¡ã€å‘½åå¯¦é«”è­˜åˆ¥ (NER)ã€å•ç­”ç³»çµ± (QA)
    * è¨“ç·´æ•ˆç‡é«˜ (ç„¡è‡ªå›æ­¸ç”Ÿæˆ)
* **ä¾·é™**:
    * ç„¡æ³•ç›´æ¥ç”Ÿæˆæ–‡æœ¬ (éœ€è¦é¡å¤–è¨“ç·´)
    * ä¸é©åˆæ©Ÿå™¨ç¿»è­¯ã€æ–‡æœ¬æ‘˜è¦ç­‰ç”Ÿæˆä»»å‹™

**B. Decoder-Only (åƒ…è§£ç¢¼å™¨)**
* **æ ¸å¿ƒæ€æƒ³**: åªä½¿ç”¨å–®å‘è§£ç¢¼å™¨,ç„¡ç·¨ç¢¼å™¨
* **ä»£è¡¨æ–¹æ³•**: GPT-3ã€GPT-4ã€LLaMAã€Mistral
* **å„ªé»**:
    * è‡ªå›æ­¸ç”Ÿæˆèƒ½åŠ›å¼·
    * æ¶æ§‹ç°¡å–®,æ˜“æ–¼æ“´å±• (å¯é”æ•¸åƒå„„åƒæ•¸)
    * é©åˆæ–‡æœ¬ç”Ÿæˆã€å°è©±ã€ç¨‹å¼ç¢¼ç”Ÿæˆ
* **ä¾·é™**:
    * åªèƒ½çœ‹åˆ°å·¦å´ä¸Šä¸‹æ–‡ (ç„¡é›™å‘ç·¨ç¢¼)
    * ç·¨ç¢¼èƒ½åŠ›å¼±æ–¼ Encoder-Only æ¨¡å‹

**C. Encoder-Decoder (ç·¨ç¢¼-è§£ç¢¼)**
* **æ ¸å¿ƒæ€æƒ³**: åŒæ™‚ä½¿ç”¨ç·¨ç¢¼å™¨èˆ‡è§£ç¢¼å™¨
* **ä»£è¡¨æ–¹æ³•**: T5ã€BARTã€mT5ã€mBART
* **å„ªé»**:
    * çµåˆé›™å‘ç·¨ç¢¼èˆ‡è‡ªå›æ­¸ç”Ÿæˆ
    * é©åˆæ©Ÿå™¨ç¿»è­¯ã€æ–‡æœ¬æ‘˜è¦ã€å•ç­”ç”Ÿæˆ
    * Cross-Attention å…è¨±å°é½Šæºèªè¨€èˆ‡ç›®æ¨™èªè¨€
* **ä¾·é™**:
    * æ¶æ§‹è¤‡é›œ,è¨“ç·´æˆæœ¬é«˜
    * æ¨ç†é€Ÿåº¦æ…¢æ–¼ Decoder-Only æ¨¡å‹

---

#### 2. æŒ‰ **æ³¨æ„åŠ›æ©Ÿåˆ¶** åˆ†é¡

**A. Full Self-Attention (å®Œå…¨è‡ªæ³¨æ„åŠ›)**
* **æ ¸å¿ƒæ€æƒ³**: æ‰€æœ‰ä½ç½®å…©å…©è¨ˆç®—æ³¨æ„åŠ›
* **ä»£è¡¨æ–¹æ³•**: åŸå§‹ Transformer Encoder
* **å„ªé»**: å…¨å±€ä¾è³´å»ºæ¨¡èƒ½åŠ›æœ€å¼·
* **ä¾·é™**: è¤‡é›œåº¦ O(nÂ²),åºåˆ—é•·åº¦å—é™

**B. Masked Self-Attention (é®ç½©è‡ªæ³¨æ„åŠ›)**
* **æ ¸å¿ƒæ€æƒ³**: é®è”½æœªä¾†ä½ç½®,åªçœ‹å·¦å´ä¸Šä¸‹æ–‡
* **ä»£è¡¨æ–¹æ³•**: GPT Decoderã€åŸå§‹ Transformer Decoder
* **å„ªé»**: å¯¦ç¾è‡ªå›æ­¸ç”Ÿæˆ,é˜²æ­¢ä¿¡æ¯æ´©æ¼
* **ä¾·é™**: ç„¡æ³•åˆ©ç”¨å³å´ä¸Šä¸‹æ–‡

**C. Cross-Attention (äº¤å‰æ³¨æ„åŠ›)**
* **æ ¸å¿ƒæ€æƒ³**: Query ä¾†è‡ª Decoder,Key/Value ä¾†è‡ª Encoder
* **ä»£è¡¨æ–¹æ³•**: Transformer Encoder-Decoderã€T5
* **å„ªé»**: æ©‹æ¥ç·¨ç¢¼èˆ‡è§£ç¢¼,å¯¦ç¾å°é½Š
* **ä¾·é™**: éœ€è¦é¡å¤–çš„ Encoder è¼¸å‡º

---

### å¿«é€Ÿå¯¦ä½œç¯„ä¾‹

#### Encoder å¯¦ä½œ (é›™å‘ç·¨ç¢¼)

```python
import numpy as np

class EncoderLayer:
    def __init__(self, d_model, num_heads, d_ff):
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_ff = d_ff

        # Multi-Head Self-Attention (ç„¡é®ç½©,é›™å‘)
        self.mha = MultiHeadAttention(d_model, num_heads)

        # Feed-Forward Network
        self.W1 = np.random.randn(d_model, d_ff) / np.sqrt(d_model)
        self.W2 = np.random.randn(d_ff, d_model) / np.sqrt(d_ff)

    def forward(self, x, mask=None):
        # Step 1: Multi-Head Self-Attention
        attn_output = self.mha.forward(x, x, x, mask=None)  # ç„¡é®ç½©,é›™å‘
        x = x + attn_output  # Residual connection
        x = self.layer_norm(x)  # Layer normalization

        # Step 2: Feed-Forward Network
        ff_output = np.maximum(0, x @ self.W1) @ self.W2  # ReLU activation
        x = x + ff_output  # Residual connection
        x = self.layer_norm(x)

        return x

    def layer_norm(self, x):
        mean = x.mean(axis=-1, keepdims=True)
        std = x.std(axis=-1, keepdims=True)
        return (x - mean) / (std + 1e-6)

# æ¸¬è©¦ Encoder
encoder = EncoderLayer(d_model=512, num_heads=8, d_ff=2048)
x = np.random.randn(10, 512)  # (seq_len=10, d_model=512)
output = encoder.forward(x)
print(f"Encoder è¼¸å‡ºå½¢ç‹€: {output.shape}")  # (10, 512)
```

**è¼¸å‡ºèªªæ˜**:
- Encoder å°æ‰€æœ‰ä½ç½®é€²è¡Œé›™å‘ç·¨ç¢¼
- æ¯å€‹ä½ç½®éƒ½èƒ½çœ‹åˆ°æ•´å€‹åºåˆ—çš„ä¿¡æ¯

---

#### Decoder å¯¦ä½œ (è‡ªå›æ­¸ç”Ÿæˆ)

```python
class DecoderLayer:
    def __init__(self, d_model, num_heads, d_ff):
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_ff = d_ff

        # Masked Multi-Head Self-Attention (æœ‰é®ç½©,å–®å‘)
        self.masked_mha = MultiHeadAttention(d_model, num_heads)

        # Cross-Attention (èˆ‡ Encoder è¼¸å‡ºäº¤äº’)
        self.cross_attn = MultiHeadAttention(d_model, num_heads)

        # Feed-Forward Network
        self.W1 = np.random.randn(d_model, d_ff) / np.sqrt(d_model)
        self.W2 = np.random.randn(d_ff, d_model) / np.sqrt(d_ff)

    def forward(self, x, encoder_output, mask=None):
        # Step 1: Masked Multi-Head Self-Attention
        causal_mask = self.create_causal_mask(x.shape[0])
        attn_output = self.masked_mha.forward(x, x, x, mask=causal_mask)
        x = x + attn_output
        x = self.layer_norm(x)

        # Step 2: Cross-Attention (Q from Decoder, K/V from Encoder)
        cross_attn_output = self.cross_attn.forward(
            Q=x,  # Query ä¾†è‡ª Decoder
            K=encoder_output,  # Key ä¾†è‡ª Encoder
            V=encoder_output,  # Value ä¾†è‡ª Encoder
            mask=None
        )
        x = x + cross_attn_output
        x = self.layer_norm(x)

        # Step 3: Feed-Forward Network
        ff_output = np.maximum(0, x @ self.W1) @ self.W2
        x = x + ff_output
        x = self.layer_norm(x)

        return x

    def create_causal_mask(self, seq_len):
        # å‰µå»ºå› æœé®ç½© (ä¸‹ä¸‰è§’çŸ©é™£)
        mask = np.tril(np.ones((seq_len, seq_len)))
        return mask

    def layer_norm(self, x):
        mean = x.mean(axis=-1, keepdims=True)
        std = x.std(axis=-1, keepdims=True)
        return (x - mean) / (std + 1e-6)

# æ¸¬è©¦ Decoder
decoder = DecoderLayer(d_model=512, num_heads=8, d_ff=2048)
x = np.random.randn(10, 512)  # Decoder è¼¸å…¥ (seq_len=10)
encoder_output = np.random.randn(15, 512)  # Encoder è¼¸å‡º (seq_len=15)
output = decoder.forward(x, encoder_output)
print(f"Decoder è¼¸å‡ºå½¢ç‹€: {output.shape}")  # (10, 512)
```

**è¼¸å‡ºèªªæ˜**:
- Decoder åœ¨ç”Ÿæˆç¬¬ i å€‹ä½ç½®æ™‚,åªèƒ½çœ‹åˆ° 0~i-1 ä½ç½® (å› æœé®ç½©)
- Cross-Attention å…è¨± Decoder åƒè€ƒ Encoder çš„å…¨éƒ¨è¼¸å‡º

---

## 2. First Principles (ç¬¬ä¸€åŸç†)

å¾ç¬¬ä¸€åŸç†å‡ºç™¼,**Encoder èˆ‡ Decoder** çš„æœ‰æ•ˆæ€§æ ¹æ¤æ–¼å° **åºåˆ—å»ºæ¨¡** èˆ‡ **ç”Ÿæˆéç¨‹** çš„æ·±åˆ»ç†è§£ã€‚

---

### æ ¸å¿ƒæ¦‚å¿µçš„æ•¸å­¸åŸç†

#### å•é¡Œ: ç‚ºä»€éº¼ Encoder ä½¿ç”¨é›™å‘ Self-Attention,è€Œ Decoder ä½¿ç”¨ Masked Self-Attention?

**æ•¸å­¸æ¨å°**:

```
Encoder Self-Attention (é›™å‘):

Attention(Q, K, V) = softmax(QK^T / sqrt(d_k)) V

å…¶ä¸­:
- Q, K, V ä¾†è‡ªåŒä¸€å€‹è¼¸å…¥åºåˆ— X
- æ³¨æ„åŠ›çŸ©é™£ A = softmax(QK^T / sqrt(d_k)) æ˜¯**å°ç¨±**çš„
- A[i, j] è¡¨ç¤ºä½ç½® i å°ä½ç½® j çš„æ³¨æ„åŠ›æ¬Šé‡
- **ç„¡é®ç½©**: A[i, j] å°æ‰€æœ‰ j âˆˆ [0, seq_len) éƒ½å¯è¦‹

æ¨å°éç¨‹:
æ­¥é©Ÿ 1: è¨ˆç®—æ³¨æ„åŠ›åˆ†æ•¸ S = QK^T / sqrt(d_k)
æ­¥é©Ÿ 2: æ‡‰ç”¨ softmax æ­¸ä¸€åŒ– A = softmax(S)
æ­¥é©Ÿ 3: åŠ æ¬Šæ±‚å’Œ Output = A V

çµè«–: æ¯å€‹ä½ç½®éƒ½èƒ½çœ‹åˆ°æ•´å€‹åºåˆ—,å¯¦ç¾é›™å‘ç·¨ç¢¼ã€‚
```

```
Decoder Masked Self-Attention (å–®å‘):

Attention(Q, K, V) = softmax(Mask(QK^T / sqrt(d_k))) V

å…¶ä¸­:
- Mask(S)[i, j] = S[i, j] if j <= i else -âˆ
- é®ç½©çŸ©é™£æ˜¯**ä¸‹ä¸‰è§’**çŸ©é™£:
  [[1, 0, 0],
   [1, 1, 0],
   [1, 1, 1]]
- A[i, j] = 0 if j > i (æœªä¾†ä½ç½®è¢«é®è”½)

æ¨å°éç¨‹:
æ­¥é©Ÿ 1: è¨ˆç®—æ³¨æ„åŠ›åˆ†æ•¸ S = QK^T / sqrt(d_k)
æ­¥é©Ÿ 2: æ‡‰ç”¨å› æœé®ç½© S' = Mask(S)
æ­¥é©Ÿ 3: æ‡‰ç”¨ softmax æ­¸ä¸€åŒ– A = softmax(S')
æ­¥é©Ÿ 4: åŠ æ¬Šæ±‚å’Œ Output = A V

çµè«–: æ¯å€‹ä½ç½®åªèƒ½çœ‹åˆ°å·¦å´ä¸Šä¸‹æ–‡,å¯¦ç¾è‡ªå›æ­¸ç”Ÿæˆã€‚
```

---

#### å¯¦é©—é©—è­‰: å¯è¦–åŒ– Encoder èˆ‡ Decoder çš„æ³¨æ„åŠ›æ¨¡å¼

```python
import numpy as np
import matplotlib.pyplot as plt

def visualize_attention_patterns():
    seq_len = 8
    d_model = 64

    # å‰µå»ºéš¨æ©Ÿè¼¸å…¥
    x = np.random.randn(seq_len, d_model)
    Q = K = V = x

    # Encoder: ç„¡é®ç½©
    scores_encoder = (Q @ K.T) / np.sqrt(d_model)
    attn_encoder = np.exp(scores_encoder) / np.sum(np.exp(scores_encoder), axis=-1, keepdims=True)

    # Decoder: æœ‰é®ç½©
    causal_mask = np.tril(np.ones((seq_len, seq_len)))
    scores_decoder = (Q @ K.T) / np.sqrt(d_model)
    scores_decoder = np.where(causal_mask == 0, -1e9, scores_decoder)
    attn_decoder = np.exp(scores_decoder) / np.sum(np.exp(scores_decoder), axis=-1, keepdims=True)

    # ç¹ªåœ–
    fig, axes = plt.subplots(1, 2, figsize=(12, 5))

    axes[0].imshow(attn_encoder, cmap='viridis')
    axes[0].set_title('Encoder Self-Attention (é›™å‘)', fontsize=14)
    axes[0].set_xlabel('Key ä½ç½®')
    axes[0].set_ylabel('Query ä½ç½®')

    axes[1].imshow(attn_decoder, cmap='viridis')
    axes[1].set_title('Decoder Masked Self-Attention (å–®å‘)', fontsize=14)
    axes[1].set_xlabel('Key ä½ç½®')
    axes[1].set_ylabel('Query ä½ç½®')

    plt.tight_layout()
    plt.show()

visualize_attention_patterns()
```

**å¯¦é©—çµæœ**:
- **Encoder**: æ³¨æ„åŠ›çŸ©é™£ç‚º**å®Œå…¨çŸ©é™£**,æ‰€æœ‰ä½ç½®å…©å…©å¯è¦‹
- **Decoder**: æ³¨æ„åŠ›çŸ©é™£ç‚º**ä¸‹ä¸‰è§’çŸ©é™£**,æœªä¾†ä½ç½®è¢«é®è”½

---

### Cross-Attention çš„æœ¬è³ª

#### ç‚ºä»€éº¼éœ€è¦ Cross-Attention?

**å•é¡Œ**: åœ¨æ©Ÿå™¨ç¿»è­¯ä¸­,å¦‚ä½•è®“ Decoder çŸ¥é“è©²ç¿»è­¯æºèªè¨€çš„å“ªå€‹éƒ¨åˆ†?

**Cross-Attention çš„æ•¸å­¸å®šç¾©**:

```
Cross-Attention(Q_dec, K_enc, V_enc) = softmax(Q_dec K_enc^T / sqrt(d_k)) V_enc

å…¶ä¸­:
- Q_dec: Decoder çš„ Query (ä¾†è‡ªç›®æ¨™èªè¨€)
- K_enc, V_enc: Encoder çš„ Key èˆ‡ Value (ä¾†è‡ªæºèªè¨€)
- æ³¨æ„åŠ›çŸ©é™£ A[i, j] è¡¨ç¤º**ç›®æ¨™èªè¨€ä½ç½® i** å°**æºèªè¨€ä½ç½® j** çš„æ³¨æ„åŠ›

é—œéµæ´å¯Ÿ:
- Query ä¾†è‡ª Decoder (ç›®æ¨™èªè¨€)
- Key/Value ä¾†è‡ª Encoder (æºèªè¨€)
- Cross-Attention å¯¦ç¾äº†**å°é½Š** (alignment) æ©Ÿåˆ¶
```

---

#### å°æ¯”: Self-Attention vs Cross-Attention

| å°æ¯”ç¶­åº¦ | Self-Attention | Cross-Attention |
|-----------|----------------|-----------------|
| **Query ä¾†æº** | åŒä¸€å€‹åºåˆ— | Decoder åºåˆ— |
| **Key/Value ä¾†æº** | åŒä¸€å€‹åºåˆ— | Encoder åºåˆ— |
| **æ³¨æ„åŠ›çŸ©é™£å½¢ç‹€** | (seq_len, seq_len) | (target_len, source_len) |
| **ä½œç”¨** | åºåˆ—å…§éƒ¨ä¾è³´å»ºæ¨¡ | è·¨åºåˆ—å°é½Š |
| **å…¸å‹æ‡‰ç”¨** | BERT ç·¨ç¢¼ | æ©Ÿå™¨ç¿»è­¯è§£ç¢¼ |

---

#### å®Œæ•´å¯¦ä½œ: Cross-Attention æ©Ÿåˆ¶

```python
class CrossAttention:
    def __init__(self, d_model):
        self.d_model = d_model
        self.W_q = np.random.randn(d_model, d_model) / np.sqrt(d_model)
        self.W_k = np.random.randn(d_model, d_model) / np.sqrt(d_model)
        self.W_v = np.random.randn(d_model, d_model) / np.sqrt(d_model)

    def forward(self, query_seq, key_value_seq):
        """
        Args:
            query_seq: Decoder åºåˆ— (target_len, d_model)
            key_value_seq: Encoder åºåˆ— (source_len, d_model)
        Returns:
            output: (target_len, d_model)
            attention_weights: (target_len, source_len)
        """
        # æŠ•å½±
        Q = query_seq @ self.W_q  # (target_len, d_model)
        K = key_value_seq @ self.W_k  # (source_len, d_model)
        V = key_value_seq @ self.W_v  # (source_len, d_model)

        # è¨ˆç®—æ³¨æ„åŠ›åˆ†æ•¸
        scores = Q @ K.T / np.sqrt(self.d_model)  # (target_len, source_len)

        # Softmax æ­¸ä¸€åŒ–
        attention_weights = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)

        # åŠ æ¬Šæ±‚å’Œ
        output = attention_weights @ V  # (target_len, d_model)

        return output, attention_weights

# æ¸¬è©¦ Cross-Attention
cross_attn = CrossAttention(d_model=64)
decoder_seq = np.random.randn(5, 64)  # ç›®æ¨™åºåˆ—é•·åº¦ 5
encoder_seq = np.random.randn(8, 64)  # æºåºåˆ—é•·åº¦ 8

output, attn_weights = cross_attn.forward(decoder_seq, encoder_seq)
print(f"Cross-Attention è¼¸å‡ºå½¢ç‹€: {output.shape}")  # (5, 64)
print(f"æ³¨æ„åŠ›æ¬Šé‡å½¢ç‹€: {attn_weights.shape}")  # (5, 8)

# å¯è¦–åŒ–å°é½Šé—œä¿‚
plt.imshow(attn_weights, cmap='viridis', aspect='auto')
plt.xlabel('æºåºåˆ—ä½ç½® (Encoder)')
plt.ylabel('ç›®æ¨™åºåˆ—ä½ç½® (Decoder)')
plt.title('Cross-Attention å°é½ŠçŸ©é™£')
plt.colorbar()
plt.show()
```

**å¯¦é©—çµæœ**:
- æ³¨æ„åŠ›æ¬Šé‡çŸ©é™£å½¢ç‹€ç‚º (5, 8),è¡¨ç¤º 5 å€‹ç›®æ¨™ä½ç½®å° 8 å€‹æºä½ç½®çš„æ³¨æ„åŠ›
- æ¯ä¸€è¡Œè¡¨ç¤ºç›®æ¨™èªè¨€çš„ä¸€å€‹ä½ç½®**å°é½Š**åˆ°æºèªè¨€çš„å“ªäº›ä½ç½®

---

## 3. Body of Knowledge (çŸ¥è­˜é«”ç³»)

åœ¨ **æ·±åº¦å­¸ç¿’çš„å®Œæ•´ç”Ÿå‘½é€±æœŸ**ä¸­,**Encoder èˆ‡ Decoder æ¶æ§‹é¸æ“‡** æ‰®æ¼”è‘—é—œéµçš„ **ä»»å‹™é©é…** è§’è‰²ã€‚

---

### Encoder-Decoder åœ¨å®Œæ•´æµç¨‹çš„ä½ç½®

```mermaid
graph TD
    A[ä»»å‹™å®šç¾©] --> B{ä»»å‹™é¡å‹?};
    B -->|ç†è§£ä»»å‹™| C[Encoder-Only];
    B -->|ç”Ÿæˆä»»å‹™| D[Decoder-Only];
    B -->|åºåˆ—åˆ°åºåˆ—| E[<b style='color:red'>Encoder-Decoder</b>];
    C --> F[BERT é è¨“ç·´];
    D --> G[GPT é è¨“ç·´];
    E --> H[T5 é è¨“ç·´];
    F --> I[ä¸‹æ¸¸å¾®èª¿];
    G --> I;
    H --> I;
    I --> J[éƒ¨ç½²èˆ‡æ¨ç†];
```

---

### æŠ€è¡“æ£§å°æ¯”

| æ¶æ§‹ | æè¿° | ä»£è¡¨æ¨¡å‹ | å„ªé» | ç¼ºé» | å…¸å‹æ‡‰ç”¨ |
|------|------|---------|------|------|---------|
| **Encoder-Only** | åƒ…é›™å‘ç·¨ç¢¼å™¨ | BERT, RoBERTa | é›™å‘ç†è§£èƒ½åŠ›å¼· | ç„¡æ³•ç›´æ¥ç”Ÿæˆ | åˆ†é¡ã€NERã€QA |
| **Decoder-Only** | åƒ…å–®å‘è§£ç¢¼å™¨ | GPT-3, LLaMA | ç”Ÿæˆèƒ½åŠ›å¼·,æ¶æ§‹ç°¡å–® | ç·¨ç¢¼èƒ½åŠ›å¼± | æ–‡æœ¬ç”Ÿæˆã€å°è©± |
| **Encoder-Decoder** | é›™å‘ç·¨ç¢¼+è‡ªå›æ­¸è§£ç¢¼ | T5, BART | å…¼å…·ç†è§£èˆ‡ç”Ÿæˆ | æ¶æ§‹è¤‡é›œ,é€Ÿåº¦æ…¢ | æ©Ÿå™¨ç¿»è­¯ã€æ‘˜è¦ |

---

### å®Œæ•´å¯¦ä½œ (ç”Ÿç”¢ç´š)

#### ä½¿ç”¨ Hugging Face Transformers é€²è¡Œæ©Ÿå™¨ç¿»è­¯

```python
from transformers import T5ForConditionalGeneration, T5Tokenizer

# è¼‰å…¥é è¨“ç·´æ¨¡å‹ (Encoder-Decoder æ¶æ§‹)
model_name = "t5-small"
tokenizer = T5Tokenizer.from_pretrained(model_name)
model = T5ForConditionalGeneration.from_pretrained(model_name)

# è¼¸å…¥æ–‡æœ¬ (è‹±æ–‡ -> å¾·æ–‡)
input_text = "translate English to German: The house is wonderful."
input_ids = tokenizer(input_text, return_tensors="pt").input_ids

# ç”Ÿæˆç¿»è­¯ (Decoder è‡ªå›æ­¸ç”Ÿæˆ)
outputs = model.generate(input_ids, max_length=50)
translation = tokenizer.decode(outputs[0], skip_special_tokens=True)

print(f"è¼¸å…¥: {input_text}")
print(f"ç¿»è­¯: {translation}")  # è¼¸å‡º: Das Haus ist wunderbar.
```

**èªªæ˜**:
- T5 ä½¿ç”¨ **Encoder-Decoder æ¶æ§‹**
- Encoder ç·¨ç¢¼è‹±æ–‡è¼¸å…¥
- Decoder è‡ªå›æ­¸ç”Ÿæˆå¾·æ–‡è¼¸å‡º
- Cross-Attention å¯¦ç¾å°é½Š

---

#### ä½¿ç”¨ GPT-2 (Decoder-Only) é€²è¡Œæ–‡æœ¬ç”Ÿæˆ

```python
from transformers import GPT2LMHeadModel, GPT2Tokenizer

# è¼‰å…¥é è¨“ç·´æ¨¡å‹ (Decoder-Only æ¶æ§‹)
model_name = "gpt2"
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
model = GPT2LMHeadModel.from_pretrained(model_name)

# è¼¸å…¥æç¤ºè©
prompt = "Once upon a time, in a land far away,"
input_ids = tokenizer(prompt, return_tensors="pt").input_ids

# ç”Ÿæˆæ–‡æœ¬
outputs = model.generate(
    input_ids,
    max_length=100,
    num_return_sequences=1,
    temperature=0.7,
    top_p=0.9
)
generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

print(f"ç”Ÿæˆæ–‡æœ¬:\n{generated_text}")
```

**èªªæ˜**:
- GPT-2 ä½¿ç”¨ **Decoder-Only æ¶æ§‹**
- ç„¡ Encoder,ç›´æ¥åœ¨ Decoder ä¸­ç·¨ç¢¼èˆ‡ç”Ÿæˆ
- é©åˆé–‹æ”¾å¼æ–‡æœ¬ç”Ÿæˆ

---

### æ€§èƒ½å°æ¯”

| æ¨¡å‹ | æ¶æ§‹ | åƒæ•¸é‡ | BLEU (ç¿»è­¯) | ROUGE-L (æ‘˜è¦) | æ¨ç†é€Ÿåº¦ | è¨“ç·´æˆæœ¬ |
|------|------|--------|------------|---------------|---------|---------|
| BERT-Base | Encoder-Only | 110M | - | - | å¿« | ä½ |
| GPT-2 | Decoder-Only | 117M | 20.5 | 28.3 | ä¸­ | ä½ |
| T5-Small | Encoder-Decoder | 60M | 25.8 | 32.1 | æ…¢ | ä¸­ |
| T5-Base | Encoder-Decoder | 220M | 28.4 | 35.6 | æ…¢ | é«˜ |
| BART-Large | Encoder-Decoder | 406M | 30.2 | 38.9 | æ…¢ | é«˜ |

**é—œéµæ´å¯Ÿ**:
- **Encoder-Decoder** åœ¨ç¿»è­¯èˆ‡æ‘˜è¦ä»»å‹™ä¸Šè¡¨ç¾æœ€ä½³
- **Decoder-Only** æ¨ç†é€Ÿåº¦è¼ƒå¿«,é©åˆç”Ÿæˆä»»å‹™
- **Encoder-Only** è¨“ç·´æˆæœ¬æœ€ä½,é©åˆç†è§£ä»»å‹™

---

### å¯¦æˆ°æ‡‰ç”¨æ¨¡å¼

#### æ¨¡å¼ 1: æ©Ÿå™¨ç¿»è­¯ (Encoder-Decoder)

```python
from transformers import MarianMTModel, MarianTokenizer

# è¼‰å…¥å°ˆé–€çš„ç¿»è­¯æ¨¡å‹
model_name = "Helsinki-NLP/opus-mt-en-zh"
tokenizer = MarianTokenizer.from_pretrained(model_name)
model = MarianMTModel.from_pretrained(model_name)

# ç¿»è­¯è‹±æ–‡åˆ°ä¸­æ–‡
text = "Machine learning is transforming the world."
inputs = tokenizer(text, return_tensors="pt")
outputs = model.generate(**inputs)
translation = tokenizer.decode(outputs[0], skip_special_tokens=True)

print(f"Translation: {translation}")
```

---

#### æ¨¡å¼ 2: æ–‡æœ¬æ‘˜è¦ (Encoder-Decoder)

```python
from transformers import BartForConditionalGeneration, BartTokenizer

# è¼‰å…¥ BART æ‘˜è¦æ¨¡å‹
model_name = "facebook/bart-large-cnn"
tokenizer = BartTokenizer.from_pretrained(model_name)
model = BartForConditionalGeneration.from_pretrained(model_name)

# è¼¸å…¥é•·æ–‡æœ¬
article = """
The Transformer architecture has revolutionized natural language processing.
It introduced the self-attention mechanism, which allows models to weigh
the importance of different words in a sentence. This has led to breakthroughs
in machine translation, text generation, and question answering.
"""

inputs = tokenizer(article, max_length=1024, return_tensors="pt", truncation=True)
summary_ids = model.generate(inputs.input_ids, max_length=50, min_length=10)
summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)

print(f"Summary: {summary}")
```

---

### æ–¹æ³•é¸æ“‡æŒ‡å¼•

| å ´æ™¯ | æ¨è–¦æ–¹æ¡ˆ | åŸå›  |
| :--- | :--- | :--- |
| æ–‡æœ¬åˆ†é¡ã€æƒ…æ„Ÿåˆ†æ | Encoder-Only (BERT) | é›™å‘ç†è§£èƒ½åŠ›å¼·,ç„¡éœ€ç”Ÿæˆ |
| é–‹æ”¾å¼æ–‡æœ¬ç”Ÿæˆ | Decoder-Only (GPT) | ç”Ÿæˆèƒ½åŠ›å¼·,æ¶æ§‹ç°¡å–® |
| **æ©Ÿå™¨ç¿»è­¯** | **Encoder-Decoder (T5/BART)** | éœ€è¦é›™å‘ç·¨ç¢¼èˆ‡å°é½Šæ©Ÿåˆ¶ |
| **æ–‡æœ¬æ‘˜è¦** | **Encoder-Decoder (BART)** | éœ€è¦ç†è§£å…¨æ–‡ä¸¦ç”Ÿæˆæ‘˜è¦ |
| å•ç­”ç³»çµ± (æŠ½å–å¼) | Encoder-Only (BERT) | å¾æ–‡æœ¬ä¸­æŠ½å–ç­”æ¡ˆ |
| å•ç­”ç³»çµ± (ç”Ÿæˆå¼) | Decoder-Only (GPT) | ç”Ÿæˆè‡ªç”±å½¢å¼ç­”æ¡ˆ |
| å°è©±ç³»çµ± | Decoder-Only (GPT/LLaMA) | éœ€è¦é•·ä¸Šä¸‹æ–‡ç”Ÿæˆèƒ½åŠ› |
| å¤šä»»å‹™å­¸ç¿’ | Encoder-Decoder (T5) | çµ±ä¸€æ¡†æ¶è™•ç†å¤šç¨®ä»»å‹™ |

---

### æ±ºç­–æ¨¹

```
éœ€è¦ç”Ÿæˆæ–‡æœ¬å—?
â”‚
â”œâ”€ å¦ (ç†è§£ä»»å‹™)
â”‚   â””â”€ ä½¿ç”¨ Encoder-Only (BERT)
â”‚
â”œâ”€ æ˜¯ (ç”Ÿæˆä»»å‹™)
â”‚   â”œâ”€ æ˜¯å¦éœ€è¦å°é½Šæºåºåˆ—èˆ‡ç›®æ¨™åºåˆ—?
â”‚   â”‚   â”œâ”€ æ˜¯ (æ©Ÿå™¨ç¿»è­¯ã€æ‘˜è¦)
â”‚   â”‚   â”‚   â””â”€ ä½¿ç”¨ Encoder-Decoder (T5/BART)
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€ å¦ (é–‹æ”¾å¼ç”Ÿæˆ)
â”‚   â”‚       â””â”€ ä½¿ç”¨ Decoder-Only (GPT)
â”‚
â””â”€ å¤šä»»å‹™å­¸ç¿’?
    â””â”€ ä½¿ç”¨ Encoder-Decoder (T5)
```

---

## çµè«–èˆ‡å»ºè­°

1. **æ—¥å¸¸æºé€šèˆ‡å¯¦ä½œ**: å„ªå…ˆæŒæ¡ **Fundamentals** ä¸­çš„ **Encoder-Decoder åˆ†å·¥**,å®ƒæ˜¯ç†è§£ Transformer æ¶æ§‹çš„é—œéµã€‚

2. **å¼·èª¿æ–¹æ³•è«–èˆ‡å‰µæ–°**: å¾ **First Principles** å‡ºç™¼,ç†è§£ **Masked Self-Attention èˆ‡ Cross-Attention çš„æ•¸å­¸æœ¬è³ª**,æœ‰åŠ©æ–¼æ‚¨åœ¨é¢å°æ–°ä»»å‹™æ™‚,é¸æ“‡æˆ–è¨­è¨ˆæ›´åˆé©çš„æ¶æ§‹ã€‚

3. **æ§‹å»ºå®è§€è¦–é‡**: å°‡ **Encoder-Decoder æ¶æ§‹é¸æ“‡** æ”¾å…¥ **Body of Knowledge** çš„æ¡†æ¶ä¸­,å¯ä»¥æ¸…æ™°åœ°çœ‹åˆ°å®ƒåœ¨ä¸åŒ NLP ä»»å‹™ä¸­çš„æˆ°ç•¥ä½ç½®,ä»¥åŠå¦‚ä½•èˆ‡ä»»å‹™éœ€æ±‚å”åŒå·¥ä½œã€‚

**æ ¸å¿ƒè¦é»**: **Encoder é›™å‘ç·¨ç¢¼,Decoder è‡ªå›æ­¸ç”Ÿæˆ,Cross-Attention æ©‹æ¥å…©è€…,æ¶æ§‹é¸æ“‡å–æ±ºæ–¼ä»»å‹™é¡å‹**ã€‚

é€éæœ¬ç« çš„å­¸ç¿’,æ‚¨æ‡‰ç•¶å·²ç¶“æŒæ¡äº† **Encoder èˆ‡ Decoder** çš„æ ¸å¿ƒåŸç†èˆ‡å¯¦ä½œæŠ€å·§,ä¸¦èƒ½å¾æ›´å®è§€çš„è¦–è§’ç†è§£å…¶åœ¨ç¾ä»£ NLP é–‹ç™¼ä¸­çš„é—œéµä½œç”¨ã€‚

---

## å»¶ä¼¸é–±è®€ (Further Reading)

### é—œéµè«–æ–‡ (Key Papers)
1. **Attention Is All You Need**: Vaswani et al. (2017). *Advances in Neural Information Processing Systems*.
2. **BERT: Pre-training of Deep Bidirectional Transformers**: Devlin et al. (2019). *NAACL*.
3. **Language Models are Unsupervised Multitask Learners**: Radford et al. (2019). *OpenAI Blog*.
4. **Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (T5)**: Raffel et al. (2020). *JMLR*.
5. **BART: Denoising Sequence-to-Sequence Pre-training**: Lewis et al. (2020). *ACL*.

### å·¥å…·èˆ‡å¯¦ç¾ (Tools & Implementations)
- **Hugging Face Transformers**: https://huggingface.co/transformers/
- **T5 Official Repo**: https://github.com/google-research/text-to-text-transfer-transformer
- **BART Official Repo**: https://github.com/facebookresearch/fairseq/tree/main/examples/bart

### å­¸ç¿’è³‡æº (Learning Resources)
- **The Illustrated Transformer**: http://jalammar.github.io/illustrated-transformer/
- **Hugging Face Course**: https://huggingface.co/course
- **Stanford CS224N**: https://web.stanford.edu/class/cs224n/

---

**ä¸Šä¸€ç« ç¯€**: [02_å¤§å‹èªè¨€æ¨¡å‹åŸç†èˆ‡æ‡‰ç”¨.md](./02_å¤§å‹èªè¨€æ¨¡å‹åŸç†èˆ‡æ‡‰ç”¨.md)
**ä¸‹ä¸€ç« ç¯€**: [04_LLMæ‡‰ç”¨å¯¦æˆ°æŒ‡å—.md](./04_LLMæ‡‰ç”¨å¯¦æˆ°æŒ‡å—.md)
