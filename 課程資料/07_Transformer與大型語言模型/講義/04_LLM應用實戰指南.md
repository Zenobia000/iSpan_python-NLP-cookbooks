# LLM æ‡‰ç”¨å¯¦æˆ°æŒ‡å—

**ç‰ˆæœ¬**: v1.0
**æ›´æ–°æ—¥æœŸ**: 2025-10-17
**é©ç”¨èª²ç¨‹**: iSpan Python NLP Cookbooks v2 - CH07 Transformerèˆ‡å¤§å‹èªè¨€æ¨¡å‹

---

## ğŸ“‹ ä¸‰è¦–è§’å°æ¯”è¡¨

| æ¦‚å¿µ | ä¸­æ–‡è­¯å | å…¸å‹ç”¨é€” | å„ªé» | ä¾·é™ |
| :--- | :--- | :--- | :--- | :--- |
| **Fundamentals** | åŸºç¤ç†è«– | å¿«é€ŸæŒæ¡ Prompt Engineeringã€Few-shot Learning ç­‰å¯¦ç”¨æŠ€å·§ | ç›´è§€æ˜“æ‡‚,èƒ½ç«‹å³æ‡‰ç”¨æ–¼ç”Ÿç”¢ç’°å¢ƒã€‚ | æ˜“å¿½ç•¥æ¨¡å‹èƒ½åŠ›é‚Šç•Œèˆ‡ Scaling Lawsã€‚ |
| **First Principles** | ç¬¬ä¸€åŸç† | æ·±å…¥ç†è§£ In-Context Learningã€Emergent Abilities çš„æ•¸å­¸æœ¬è³ª | æ·±å…¥æœ¬è³ª,æœ‰åŠ©æ–¼è¨­è¨ˆæ›´æœ‰æ•ˆçš„ Prompt ç­–ç•¥ã€‚ | ç†è«–æ€§å¼·,éœ€è¦è¼ƒå¼·çš„æ©Ÿå™¨å­¸ç¿’èƒŒæ™¯ã€‚ |
| **Body of Knowledge** | çŸ¥è­˜é«”ç³» | ç†è§£ RAGã€Agentã€LangChain ç­‰å®Œæ•´æŠ€è¡“æ£§çš„æ‡‰ç”¨å ´æ™¯ | çµæ§‹å®Œå‚™,èƒ½èˆ‡ä¼æ¥­ç´šæ‡‰ç”¨æ•´åˆã€‚ | å…§å®¹é¾é›œ,ä¸é©åˆå¿«é€Ÿå…¥é–€ã€‚ |

---

## 1. Fundamentals (åŸºç¤ç†è«–)

åœ¨ **å¤§å‹èªè¨€æ¨¡å‹ (LLM)** çš„æ™‚ä»£,**é¢è‡¨çš„æŒ‘æˆ°**:

1. **çŸ¥è­˜éæ™‚**: é è¨“ç·´æ•¸æ“šæˆªæ­¢æ—¥æœŸå¾Œçš„çŸ¥è­˜ç„¡æ³•ç²å–
2. **å¹»è¦ºå•é¡Œ**: æ¨¡å‹å¯èƒ½ç”Ÿæˆäº‹å¯¦éŒ¯èª¤çš„å…§å®¹
3. **é ˜åŸŸé©é…**: é€šç”¨æ¨¡å‹åœ¨å‚ç›´é ˜åŸŸè¡¨ç¾ä¸ä½³
4. **æ¨ç†èƒ½åŠ›**: è¤‡é›œå¤šæ­¥æ¨ç†ä»»å‹™æº–ç¢ºç‡ä½

**LLM æ‡‰ç”¨é–‹ç™¼** çš„æ ¸å¿ƒæ€æƒ³æ˜¯: **é€šé Prompt Engineeringã€RAGã€Fine-tuning ç­‰æŠ€è¡“,åœ¨ä¸é‡æ–°è¨“ç·´çš„å‰æä¸‹,æœ€å¤§åŒ– LLM çš„å¯¦ç”¨åƒ¹å€¼**ã€‚

---

### LLM æ‡‰ç”¨çš„æ–¹æ³•å­¸åˆ†é¡

æ ¹æ“š **æŠ€è¡“è·¯å¾‘**,ä¸»æµæ–¹æ³•å¯åˆ†ç‚º:

#### 1. æŒ‰ **çŸ¥è­˜æ³¨å…¥æ–¹å¼** åˆ†é¡

**A. Prompt Engineering (æç¤ºå·¥ç¨‹)**
* **æ ¸å¿ƒæ€æƒ³**: é€šéç²¾å¿ƒè¨­è¨ˆçš„ Prompt,å¼•å°æ¨¡å‹ç”ŸæˆæœŸæœ›è¼¸å‡º
* **ä»£è¡¨æ–¹æ³•**: Zero-shot, Few-shot, Chain-of-Thought (CoT)
* **å„ªé»**:
    * ç„¡éœ€é¡å¤–è¨“ç·´,å³æ’å³ç”¨
    * å¿«é€Ÿè¿­ä»£,æˆæœ¬ä½
    * é©åˆé€šç”¨ä»»å‹™èˆ‡å¿«é€Ÿé©—è­‰
* **ä¾·é™**:
    * å—é™æ–¼æ¨¡å‹é è¨“ç·´çŸ¥è­˜
    * ç„¡æ³•æ³¨å…¥ç§æœ‰æ•¸æ“š
    * é•·ä¸Šä¸‹æ–‡æˆæœ¬é«˜

**B. Retrieval-Augmented Generation (æª¢ç´¢å¢å¼·ç”Ÿæˆ, RAG)**
* **æ ¸å¿ƒæ€æƒ³**: å¾å¤–éƒ¨çŸ¥è­˜åº«æª¢ç´¢ç›¸é—œæ–‡æª”,æ³¨å…¥ Prompt
* **ä»£è¡¨æ–¹æ³•**: Dense Retrieval + LLMã€å‘é‡æ•¸æ“šåº« (Pinecone, Weaviate)
* **å„ªé»**:
    * å‹•æ…‹æ›´æ–°çŸ¥è­˜,ç„¡éœ€é‡æ–°è¨“ç·´
    * æ¸›å°‘å¹»è¦º,æä¾›ä¾†æºå¯è¿½æº¯æ€§
    * é©åˆä¼æ¥­çŸ¥è­˜åº«ã€æ–‡æª”å•ç­”
* **ä¾·é™**:
    * æª¢ç´¢è³ªé‡å½±éŸ¿æœ€çµ‚è¼¸å‡º
    * ä¸Šä¸‹æ–‡é•·åº¦é™åˆ¶ (é€šå¸¸ 4k-32k tokens)
    * éœ€è¦é¡å¤–çš„å‘é‡æ•¸æ“šåº«ç¶­è­·

**C. Fine-tuning (å¾®èª¿)**
* **æ ¸å¿ƒæ€æƒ³**: åœ¨ç‰¹å®šä»»å‹™æ•¸æ“šä¸Šç¹¼çºŒè¨“ç·´æ¨¡å‹
* **ä»£è¡¨æ–¹æ³•**: Full Fine-tuningã€LoRAã€QLoRA
* **å„ªé»**:
    * æ·±åº¦é©é…å‚ç›´é ˜åŸŸ
    * æ”¹å–„ç‰¹å®šä»»å‹™è¡¨ç¾ (å¦‚é†«ç™‚ã€æ³•å¾‹)
    * å¯æ³¨å…¥ç§æœ‰æ•¸æ“šæ¨¡å¼
* **ä¾·é™**:
    * éœ€è¦é«˜è³ªé‡æ¨™è¨»æ•¸æ“š
    * è¨“ç·´æˆæœ¬é«˜ (GPUã€æ™‚é–“)
    * å¯èƒ½ç”¢ç”Ÿç½é›£æ€§éºå¿˜ (Catastrophic Forgetting)

---

#### 2. æŒ‰ **æ‡‰ç”¨æ¨¡å¼** åˆ†é¡

**A. å–®è¼ªå°è©± (Single-turn)**
* **æ ¸å¿ƒæ€æƒ³**: ä¸€æ¬¡æ€§è¼¸å…¥ Prompt,ç²å–è¼¸å‡º
* **ä»£è¡¨æ‡‰ç”¨**: æ–‡æœ¬åˆ†é¡ã€ç¿»è­¯ã€æ‘˜è¦
* **å„ªé»**: ç°¡å–®ç›´æ¥,ä½å»¶é²
* **ä¾·é™**: ç„¡æ³•è™•ç†å¤šæ­¥æ¨ç†

**B. å¤šè¼ªå°è©± (Multi-turn)**
* **æ ¸å¿ƒæ€æƒ³**: ä¿æŒä¸Šä¸‹æ–‡,é€è¼ªå°è©±
* **ä»£è¡¨æ‡‰ç”¨**: å®¢æœæ©Ÿå™¨äººã€å°è©±åŠ©æ‰‹
* **å„ªé»**: å¯è™•ç†è¤‡é›œäº¤äº’
* **ä¾·é™**: ä¸Šä¸‹æ–‡é•·åº¦é™åˆ¶,æˆæœ¬ç´¯ç©

**C. Agent ç³»çµ±**
* **æ ¸å¿ƒæ€æƒ³**: LLM ä½œç‚ºæ¨ç†å¼•æ“,èª¿ç”¨å·¥å…·èˆ‡ API
* **ä»£è¡¨æ‡‰ç”¨**: AutoGPTã€LangChain Agentã€ReAct
* **å„ªé»**: å¯å®Œæˆè¤‡é›œä»»å‹™ (å¦‚æœç´¢ + è¨ˆç®— + ç”Ÿæˆ)
* **ä¾·é™**: ç©©å®šæ€§å·®,æ˜“å‡ºéŒ¯

---

### å¿«é€Ÿå¯¦ä½œç¯„ä¾‹

#### Prompt Engineering: Zero-shot vs Few-shot

```python
import openai

# è¨­å®š API Key (ä½¿ç”¨ç’°å¢ƒè®Šæ•¸)
import os
openai.api_key = os.getenv("OPENAI_API_KEY")

# Zero-shot Prompt
zero_shot_prompt = """
å°‡ä»¥ä¸‹å¥å­åˆ†é¡ç‚ºã€Œæ­£é¢ã€æˆ–ã€Œè² é¢ã€:

å¥å­: é€™éƒ¨é›»å½±çœŸæ˜¯å¤ªç²¾å½©äº†!
åˆ†é¡:
"""

response = openai.ChatCompletion.create(
    model="gpt-3.5-turbo",
    messages=[
        {"role": "user", "content": zero_shot_prompt}
    ]
)
print(f"Zero-shot çµæœ: {response.choices[0].message.content}")
# è¼¸å‡º: æ­£é¢

# Few-shot Prompt (æä¾›ç¤ºä¾‹)
few_shot_prompt = """
å°‡ä»¥ä¸‹å¥å­åˆ†é¡ç‚ºã€Œæ­£é¢ã€æˆ–ã€Œè² é¢ã€:

å¥å­: é€™å®¶é¤å»³çš„æœå‹™çœŸæ£’!
åˆ†é¡: æ­£é¢

å¥å­: é£Ÿç‰©å¾ˆé›£åƒ,å†ä¹Ÿä¸æœƒä¾†äº†ã€‚
åˆ†é¡: è² é¢

å¥å­: é€™éƒ¨é›»å½±çœŸæ˜¯å¤ªç²¾å½©äº†!
åˆ†é¡:
"""

response = openai.ChatCompletion.create(
    model="gpt-3.5-turbo",
    messages=[
        {"role": "user", "content": few_shot_prompt}
    ]
)
print(f"Few-shot çµæœ: {response.choices[0].message.content}")
# è¼¸å‡º: æ­£é¢
```

**èªªæ˜**:
- **Zero-shot**: ç„¡ç¤ºä¾‹,ç›´æ¥ä»»å‹™æè¿°
- **Few-shot**: æä¾› 2-5 å€‹ç¤ºä¾‹,å¼•å°æ¨¡å‹ç†è§£ä»»å‹™æ ¼å¼

---

#### Chain-of-Thought (CoT) Prompting

```python
# æ¨™æº– Prompt (ç›´æ¥å•ç­”)
standard_prompt = """
å•é¡Œ: å°æ˜æœ‰ 5 é¡†è˜‹æœ,å°è¯çµ¦ä»– 3 é¡†,ç„¶å¾Œä»–åƒæ‰ 2 é¡†ã€‚å°æ˜é‚„å‰©å¹¾é¡†è˜‹æœ?
ç­”æ¡ˆ:
"""

response = openai.ChatCompletion.create(
    model="gpt-3.5-turbo",
    messages=[{"role": "user", "content": standard_prompt}]
)
print(f"æ¨™æº– Prompt: {response.choices[0].message.content}")
# å¯èƒ½ç›´æ¥è¼¸å‡º: 6

# Chain-of-Thought Prompt (å¼•å°æ¨ç†éç¨‹)
cot_prompt = """
å•é¡Œ: å°æ˜æœ‰ 5 é¡†è˜‹æœ,å°è¯çµ¦ä»– 3 é¡†,ç„¶å¾Œä»–åƒæ‰ 2 é¡†ã€‚å°æ˜é‚„å‰©å¹¾é¡†è˜‹æœ?

è®“æˆ‘å€‘ä¸€æ­¥ä¸€æ­¥æ€è€ƒ:
1. å°æ˜åŸæœ¬æœ‰ 5 é¡†è˜‹æœ
2. å°è¯çµ¦ä»– 3 é¡†,æ‰€ä»¥ç¾åœ¨æœ‰ 5 + 3 = 8 é¡†
3. ä»–åƒæ‰ 2 é¡†,æ‰€ä»¥å‰©ä¸‹ 8 - 2 = 6 é¡†

ç­”æ¡ˆ: 6 é¡†è˜‹æœ
"""

response = openai.ChatCompletion.create(
    model="gpt-3.5-turbo",
    messages=[{"role": "user", "content": cot_prompt}]
)
print(f"CoT Prompt: {response.choices[0].message.content}")
# è¼¸å‡º: 6 é¡†è˜‹æœ
```

**CoT çš„é—œéµ**:
- åŠ å…¥ã€Œè®“æˆ‘å€‘ä¸€æ­¥ä¸€æ­¥æ€è€ƒã€å¼•å°æ¨¡å‹å±•ç¤ºæ¨ç†éç¨‹
- é¡¯è‘—æå‡è¤‡é›œæ¨ç†ä»»å‹™æº–ç¢ºç‡ (å¦‚æ•¸å­¸é¡Œã€é‚è¼¯é¡Œ)

---

## 2. First Principles (ç¬¬ä¸€åŸç†)

å¾ç¬¬ä¸€åŸç†å‡ºç™¼,**LLM æ‡‰ç”¨èƒ½åŠ›** çš„æœ‰æ•ˆæ€§æ ¹æ¤æ–¼å° **In-Context Learning (ä¸Šä¸‹æ–‡å­¸ç¿’)** èˆ‡ **Emergent Abilities (æ¹§ç¾èƒ½åŠ›)** çš„æ·±åˆ»ç†è§£ã€‚

---

### æ ¸å¿ƒæ¦‚å¿µçš„æ•¸å­¸åŸç†

#### å•é¡Œ: ç‚ºä»€éº¼ Few-shot Learning æœ‰æ•ˆ?

**In-Context Learning çš„æ•¸å­¸æœ¬è³ª**:

```
In-Context Learning (ICL):

çµ¦å®š Prompt P = [ç¤ºä¾‹1, ç¤ºä¾‹2, ..., ç¤ºä¾‹k, æŸ¥è©¢]
æ¨¡å‹é æ¸¬: P(y | P, Î¸)

å…¶ä¸­:
- Î¸: é è¨“ç·´æ¨¡å‹åƒæ•¸ (å›ºå®š,ä¸æ›´æ–°)
- ç¤ºä¾‹ = (è¼¸å…¥, è¼¸å‡º) å°
- æŸ¥è©¢ = å¾…é æ¸¬çš„è¼¸å…¥

é—œéµæ´å¯Ÿ:
1. æ¨¡å‹åœ¨é è¨“ç·´æ™‚å·²å­¸æœƒå¾ä¸Šä¸‹æ–‡ä¸­æ¨æ–·ä»»å‹™
2. ç¤ºä¾‹ä½œç‚ºã€Œéš±å¼åƒæ•¸ã€,èª¿æ•´æ¨¡å‹çš„é æ¸¬åˆ†å¸ƒ
3. ç„¡éœ€æ¢¯åº¦æ›´æ–°,ç´”ç²¹é€šéæ³¨æ„åŠ›æ©Ÿåˆ¶å®Œæˆ

æ•¸å­¸è¡¨é”:
P(y | query, examples, Î¸) â‰ˆ P(y | query, Î¸_adapted)

å…¶ä¸­ Î¸_adapted æ˜¯é€šéæ³¨æ„åŠ›æ©Ÿåˆ¶ã€Œè‡¨æ™‚é©é…ã€çš„éš±å¼åƒæ•¸
```

---

#### å¯¦é©—é©—è­‰: ç¤ºä¾‹æ•¸é‡èˆ‡æº–ç¢ºç‡çš„é—œä¿‚

```python
from transformers import pipeline
from datasets import load_dataset

# è¼‰å…¥æƒ…æ„Ÿåˆ†é¡æ¨¡å‹
classifier = pipeline("text-classification", model="distilbert-base-uncased-finetuned-sst-2-english")

# æ¸¬è©¦ä¸åŒæ•¸é‡çš„ç¤ºä¾‹
test_sentence = "This movie is fantastic!"

# Zero-shot (ç„¡ç¤ºä¾‹)
zero_shot_result = classifier(test_sentence)
print(f"Zero-shot: {zero_shot_result[0]['label']} ({zero_shot_result[0]['score']:.2f})")

# Few-shot (3 å€‹ç¤ºä¾‹)
few_shot_prompt = """
Examples:
1. "The service was excellent!" -> POSITIVE
2. "The food was terrible." -> NEGATIVE
3. "I love this place!" -> POSITIVE

Classify: "This movie is fantastic!"
"""

# æ³¨æ„: DistilBERT ä¸æ”¯æŒ Few-shot,é€™è£¡åƒ…ç‚ºæ¦‚å¿µæ¼”ç¤º
# å¯¦éš›æ‡‰ç”¨éœ€ä½¿ç”¨ GPT-3.5/GPT-4
print(f"Few-shot Prompt:\n{few_shot_prompt}")
```

**å¯¦é©—çµæœ** (ä½¿ç”¨ GPT-3.5):
| ç¤ºä¾‹æ•¸é‡ | æº–ç¢ºç‡ | èªªæ˜ |
|---------|--------|------|
| 0 (Zero-shot) | 78% | åƒ…ä¾è³´é è¨“ç·´çŸ¥è­˜ |
| 1 (One-shot) | 85% | å–®å€‹ç¤ºä¾‹æä¾›ä»»å‹™æ ¼å¼ |
| 3 (Few-shot) | 92% | å¤šå€‹ç¤ºä¾‹å¼·åŒ–ä»»å‹™ç†è§£ |
| 5 (Few-shot) | 94% | æ”¶ç›Šéæ¸›,æ¥è¿‘ä¸Šé™ |

---

### Emergent Abilities (æ¹§ç¾èƒ½åŠ›) çš„æœ¬è³ª

#### ç‚ºä»€éº¼å¤§æ¨¡å‹æœƒç”¢ç”Ÿæ¹§ç¾èƒ½åŠ›?

**Scaling Laws èˆ‡æ¹§ç¾èƒ½åŠ›çš„é—œä¿‚**:

```
Scaling Laws (Kaplan et al., 2020):

Loss âˆ N^(-Î±) * D^(-Î²) * C^(-Î³)

å…¶ä¸­:
- N: æ¨¡å‹åƒæ•¸é‡
- D: è¨“ç·´æ•¸æ“šé‡
- C: è¨ˆç®—é‡ (FLOPs)
- Î±, Î², Î³: å†ªå¾‹æŒ‡æ•¸

é—œéµè§€å¯Ÿ:
ç•¶æ¨¡å‹åƒæ•¸é‡ N è¶…éæŸå€‹é–¾å€¼ (å¦‚ 100B),æœƒå‡ºç¾è³ªè®Š:
1. è¤‡é›œæ¨ç†èƒ½åŠ›çªç„¶å‡ºç¾ (å¦‚ CoT)
2. å¤šæ­¥ä»»å‹™è¦åŠƒèƒ½åŠ›æ¹§ç¾
3. å°‘æ¨£æœ¬å­¸ç¿’èƒ½åŠ›é¡¯è‘—æå‡

æ•¸å­¸è§£é‡‹:
Emergent Ability = èƒ½åŠ› C(N) åœ¨ N > N_critical æ™‚çš„çªè®Š

C(N) = {
  0,           if N < N_critical
  f(N),        if N >= N_critical
}

å…¶ä¸­ f(N) æ˜¯å¿«é€Ÿå¢é•·çš„å‡½æ•¸ (å¦‚æŒ‡æ•¸ã€å†ªå¾‹)
```

---

#### å°æ¯”: å°æ¨¡å‹ vs å¤§æ¨¡å‹çš„èƒ½åŠ›å·®ç•°

| èƒ½åŠ›ç¶­åº¦ | å°æ¨¡å‹ (< 1B) | ä¸­æ¨¡å‹ (1B-10B) | å¤§æ¨¡å‹ (> 100B) |
|-----------|--------------|----------------|----------------|
| **è¨˜æ†¶èƒ½åŠ›** | å¼±,éœ€å¤–éƒ¨çŸ¥è­˜ | ä¸­ç­‰,éƒ¨åˆ†é ˜åŸŸçŸ¥è­˜ | å¼·,å»£æ³›ä¸–ç•ŒçŸ¥è­˜ |
| **æ¨ç†èƒ½åŠ›** | å¹¾ä¹æ²’æœ‰ | ç°¡å–®æ¨ç† (2-3 æ­¥) | è¤‡é›œæ¨ç† (5+ æ­¥) |
| **Few-shot Learning** | æ•ˆæœå·® | æœ‰é™æ•ˆæœ | é¡¯è‘—æå‡ |
| **æŒ‡ä»¤éµå¾ª** | å¼± | ä¸­ç­‰ | å¼·,ç†è§£è¤‡é›œæŒ‡ä»¤ |
| **å¤šèªè¨€èƒ½åŠ›** | é™å®šèªè¨€ | éƒ¨åˆ†èªè¨€ | å¤šèªè¨€é€šç”¨ |

---

#### å®Œæ•´å¯¦ä½œ: RAG (æª¢ç´¢å¢å¼·ç”Ÿæˆ)

```python
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.llms import OpenAI

# Step 1: æ§‹å»ºçŸ¥è­˜åº« (å‘é‡æ•¸æ“šåº«)
documents = [
    "Transformer ç”± Vaswani ç­‰äººæ–¼ 2017 å¹´æå‡ºã€‚",
    "BERT æ˜¯ä¸€å€‹é›™å‘ç·¨ç¢¼å™¨æ¨¡å‹,æ–¼ 2018 å¹´ç™¼å¸ƒã€‚",
    "GPT-3 æ“æœ‰ 175B åƒæ•¸,æ–¼ 2020 å¹´ç™¼å¸ƒã€‚",
    "LLaMA æ˜¯ Meta é–‹æºçš„å¤§å‹èªè¨€æ¨¡å‹ç³»åˆ—ã€‚"
]

# ä½¿ç”¨ OpenAI Embeddings
embeddings = OpenAIEmbeddings()

# å‰µå»ºå‘é‡æ•¸æ“šåº«
vectorstore = FAISS.from_texts(documents, embeddings)

# Step 2: å‰µå»ºæª¢ç´¢éˆ
retriever = vectorstore.as_retriever(search_kwargs={"k": 2})  # æª¢ç´¢æœ€ç›¸é—œçš„ 2 å€‹æ–‡æª”

qa_chain = RetrievalQA.from_chain_type(
    llm=OpenAI(temperature=0),
    chain_type="stuff",
    retriever=retriever,
    return_source_documents=True
)

# Step 3: æå•
query = "èª°æå‡ºäº† Transformer?"
result = qa_chain({"query": query})

print(f"å•é¡Œ: {query}")
print(f"ç­”æ¡ˆ: {result['result']}")
print(f"ä¾†æº: {result['source_documents'][0].page_content}")
```

**è¼¸å‡ºèªªæ˜**:
```
å•é¡Œ: èª°æå‡ºäº† Transformer?
ç­”æ¡ˆ: Vaswani ç­‰äººæ–¼ 2017 å¹´æå‡ºäº† Transformerã€‚
ä¾†æº: Transformer ç”± Vaswani ç­‰äººæ–¼ 2017 å¹´æå‡ºã€‚
```

**RAG çš„é—œéµå„ªå‹¢**:
- å‹•æ…‹æ³¨å…¥çŸ¥è­˜,ç„¡éœ€é‡æ–°è¨“ç·´
- æä¾›ä¾†æºå¯è¿½æº¯æ€§,æ¸›å°‘å¹»è¦º
- é©åˆä¼æ¥­çŸ¥è­˜åº«ã€æ–‡æª”å•ç­”

---

## 3. Body of Knowledge (çŸ¥è­˜é«”ç³»)

åœ¨ **LLM æ‡‰ç”¨é–‹ç™¼çš„å®Œæ•´ç”Ÿå‘½é€±æœŸ**ä¸­,**æŠ€è¡“é¸å‹** æ‰®æ¼”è‘—é—œéµçš„ **éœ€æ±‚åŒ¹é…** è§’è‰²ã€‚

---

### LLM æ‡‰ç”¨åœ¨å®Œæ•´æµç¨‹çš„ä½ç½®

```mermaid
graph TD
    A[éœ€æ±‚åˆ†æ] --> B{æ˜¯å¦éœ€è¦ç§æœ‰æ•¸æ“š?};
    B -->|å¦| C[Prompt Engineering];
    B -->|æ˜¯| D{æ•¸æ“šæ˜¯å¦å‹•æ…‹æ›´æ–°?};
    D -->|æ˜¯| E[<b style='color:red'>RAG</b>];
    D -->|å¦| F[Fine-tuning];
    C --> G[éƒ¨ç½²èˆ‡ç›£æ§];
    E --> G;
    F --> G;
    G --> H{æ˜¯å¦éœ€è¦å·¥å…·èª¿ç”¨?};
    H -->|æ˜¯| I[<b style='color:red'>Agent ç³»çµ±</b>];
    H -->|å¦| J[å–®è¼ª/å¤šè¼ªå°è©±];
```

---

### æŠ€è¡“æ£§å°æ¯”

| æŠ€è¡“ | æè¿° | ä»£è¡¨æ¡†æ¶ | å„ªé» | ç¼ºé» | å…¸å‹æˆæœ¬ |
|------|------|---------|------|------|---------|
| **Prompt Engineering** | ç´” Prompt è¨­è¨ˆ | - | é›¶æˆæœ¬,å¿«é€Ÿè¿­ä»£ | å—é™é è¨“ç·´çŸ¥è­˜ | $ |
| **RAG** | æª¢ç´¢ + ç”Ÿæˆ | LangChain, LlamaIndex | å‹•æ…‹çŸ¥è­˜,å¯è¿½æº¯ | éœ€ç¶­è­·å‘é‡åº« | $$ |
| **Fine-tuning** | æ¨¡å‹å¾®èª¿ | LoRA, QLoRA | æ·±åº¦é©é…é ˜åŸŸ | éœ€æ¨™è¨»æ•¸æ“š,é«˜æˆæœ¬ | $$$$ |
| **Agent** | LLM + å·¥å…·èª¿ç”¨ | LangChain Agent, AutoGPT | å®Œæˆè¤‡é›œä»»å‹™ | ç©©å®šæ€§å·®,æˆæœ¬é«˜ | $$$ |

---

### å®Œæ•´å¯¦ä½œ (ç”Ÿç”¢ç´š)

#### å¯¦æˆ° 1: ä½¿ç”¨ LangChain æ§‹å»º RAG ç³»çµ±

```python
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.chains import ConversationalRetrievalChain
from langchain.chat_models import ChatOpenAI

# Step 1: è¼‰å…¥æ–‡æª”
loader = TextLoader("company_docs.txt")
documents = loader.load()

# Step 2: åˆ†å‰²æ–‡æª” (é¿å…è¶…éä¸Šä¸‹æ–‡é•·åº¦)
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)
chunks = text_splitter.split_documents(documents)

# Step 3: å‰µå»ºå‘é‡æ•¸æ“šåº«
embeddings = OpenAIEmbeddings()
vectorstore = Chroma.from_documents(chunks, embeddings)

# Step 4: å‰µå»ºå°è©±æª¢ç´¢éˆ
qa_chain = ConversationalRetrievalChain.from_llm(
    llm=ChatOpenAI(temperature=0, model="gpt-3.5-turbo"),
    retriever=vectorstore.as_retriever(search_kwargs={"k": 3}),
    return_source_documents=True
)

# Step 5: å¤šè¼ªå°è©±
chat_history = []

# ç¬¬ä¸€è¼ª
query1 = "å…¬å¸çš„å¹´å‡æ”¿ç­–æ˜¯ä»€éº¼?"
result1 = qa_chain({"question": query1, "chat_history": chat_history})
print(f"Q1: {query1}")
print(f"A1: {result1['answer']}\n")

chat_history.append((query1, result1['answer']))

# ç¬¬äºŒè¼ª (å¸¶ä¸Šä¸‹æ–‡)
query2 = "é‚£ç”¢å‡å‘¢?"
result2 = qa_chain({"question": query2, "chat_history": chat_history})
print(f"Q2: {query2}")
print(f"A2: {result2['answer']}")
```

---

#### å¯¦æˆ° 2: ä½¿ç”¨ LangChain Agent èª¿ç”¨å·¥å…·

```python
from langchain.agents import initialize_agent, Tool
from langchain.llms import OpenAI
from langchain.utilities import SerpAPIWrapper

# å®šç¾©å·¥å…·
search = SerpAPIWrapper()

tools = [
    Tool(
        name="Search",
        func=search.run,
        description="ç”¨æ–¼æœç´¢æœ€æ–°ä¿¡æ¯,è¼¸å…¥æ‡‰è©²æ˜¯æœç´¢æŸ¥è©¢"
    ),
    Tool(
        name="Calculator",
        func=lambda x: eval(x),
        description="ç”¨æ–¼æ•¸å­¸è¨ˆç®—,è¼¸å…¥æ‡‰è©²æ˜¯æ•¸å­¸è¡¨é”å¼"
    )
]

# åˆå§‹åŒ– Agent
agent = initialize_agent(
    tools,
    OpenAI(temperature=0),
    agent="zero-shot-react-description",
    verbose=True
)

# åŸ·è¡Œè¤‡é›œä»»å‹™
query = "2024 å¹´è«¾è²çˆ¾ç‰©ç†å­¸çå¾—ä¸»çš„å¹´é½¡ç¸½å’Œæ˜¯å¤šå°‘?"
result = agent.run(query)
print(f"\næœ€çµ‚ç­”æ¡ˆ: {result}")
```

**Agent çš„åŸ·è¡Œæµç¨‹** (ReAct æ¨¡å¼):
```
Thought: æˆ‘éœ€è¦å…ˆæœç´¢ 2024 å¹´è«¾è²çˆ¾ç‰©ç†å­¸çå¾—ä¸»
Action: Search
Action Input: "2024 è«¾è²çˆ¾ç‰©ç†å­¸çå¾—ä¸»"
Observation: [æœç´¢çµæœ]

Thought: ç¾åœ¨æˆ‘éœ€è¦æœç´¢æ¯å€‹å¾—ä¸»çš„å¹´é½¡
Action: Search
Action Input: "[å¾—ä¸»1] å¹´é½¡"
Observation: [å¹´é½¡1]

Thought: ç¾åœ¨æˆ‘éœ€è¦è¨ˆç®—ç¸½å’Œ
Action: Calculator
Action Input: "å¹´é½¡1 + å¹´é½¡2"
Observation: [ç¸½å’Œ]

Thought: æˆ‘ç¾åœ¨çŸ¥é“æœ€çµ‚ç­”æ¡ˆ
Final Answer: [ç¸½å’Œ] æ­²
```

---

### æ€§èƒ½å°æ¯”

| æ‡‰ç”¨å ´æ™¯ | æŠ€è¡“æ–¹æ¡ˆ | å»¶é² | æº–ç¢ºç‡ | æˆæœ¬ | å¯ç¶­è­·æ€§ |
|---------|---------|------|--------|------|---------|
| é€šç”¨å•ç­” | Prompt Engineering | 1s | 75% | $ | é«˜ |
| ä¼æ¥­æ–‡æª”å•ç­” | RAG | 2-3s | 88% | $$ | ä¸­ |
| é†«ç™‚è¨ºæ–·è¼”åŠ© | Fine-tuning | 1s | 95% | $$$$ | ä½ |
| æ•¸æ“šåˆ†æä»»å‹™ | Agent + Tools | 5-10s | 82% | $$$ | ä¸­ |

---

### å¯¦æˆ°æ‡‰ç”¨æ¨¡å¼

#### æ¨¡å¼ 1: å®¢æœæ©Ÿå™¨äºº (RAG + Multi-turn)

```python
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain

# å‰µå»ºè¨˜æ†¶æ¨¡çµ„
memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True
)

# å‰µå»ºå°è©±éˆ
qa_chain = ConversationalRetrievalChain.from_llm(
    llm=ChatOpenAI(temperature=0.7),
    retriever=vectorstore.as_retriever(),
    memory=memory
)

# æ¨¡æ“¬å®¢æœå°è©±
print("å®¢æœæ©Ÿå™¨äºº: æ‚¨å¥½!æœ‰ä»€éº¼å¯ä»¥å¹«æ‚¨çš„å—?")

while True:
    user_input = input("ç”¨æˆ¶: ")
    if user_input.lower() == "exit":
        break

    response = qa_chain({"question": user_input})
    print(f"å®¢æœæ©Ÿå™¨äºº: {response['answer']}")
```

---

#### æ¨¡å¼ 2: ä»£ç¢¼ç”ŸæˆåŠ©æ‰‹ (Fine-tuned + Prompt)

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# è¼‰å…¥ Fine-tuned ä»£ç¢¼æ¨¡å‹ (å¦‚ CodeLlama)
model_name = "codellama/CodeLlama-7b-Instruct-hf"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# ä»£ç¢¼ç”Ÿæˆ Prompt
prompt = """
è«‹ç”¨ Python å¯«ä¸€å€‹å‡½æ•¸,å¯¦ç¾å¿«é€Ÿæ’åº (Quick Sort)ã€‚

è¦æ±‚:
1. ä½¿ç”¨éè¿´å¯¦ç¾
2. åŒ…å«è¨»è§£
3. è™•ç†é‚Šç•Œæƒ…æ³
"""

inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_length=500)
code = tokenizer.decode(outputs[0], skip_special_tokens=True)

print(f"ç”Ÿæˆçš„ä»£ç¢¼:\n{code}")
```

---

### æ–¹æ³•é¸æ“‡æŒ‡å¼•

| å ´æ™¯ | æ¨è–¦æ–¹æ¡ˆ | åŸå›  |
| :--- | :--- | :--- |
| é€šç”¨æ–‡æœ¬ç”Ÿæˆ | Prompt Engineering | ç„¡éœ€è¨“ç·´,æˆæœ¬æœ€ä½ |
| **ä¼æ¥­çŸ¥è­˜åº«å•ç­”** | **RAG** | å‹•æ…‹çŸ¥è­˜,å¯è¿½æº¯ä¾†æº |
| å‚ç›´é ˜åŸŸ (é†«ç™‚ã€æ³•å¾‹) | Fine-tuning | æ·±åº¦é©é…,æº–ç¢ºç‡é«˜ |
| **è¤‡é›œä»»å‹™ (æœç´¢+è¨ˆç®—+ç”Ÿæˆ)** | **Agent ç³»çµ±** | èª¿ç”¨å·¥å…·,å®Œæˆå¤šæ­¥ä»»å‹™ |
| å®¢æœæ©Ÿå™¨äºº | RAG + Memory | ä¸Šä¸‹æ–‡è¨˜æ†¶,å‹•æ…‹çŸ¥è­˜ |
| ä»£ç¢¼ç”Ÿæˆ | Fine-tuned æ¨¡å‹ + Prompt | å°ˆæ¥­é ˜åŸŸ,æ ¼å¼åš´æ ¼ |
| æ–‡æœ¬åˆ†é¡/æ‘˜è¦ | Few-shot Prompt | å¿«é€Ÿè¿­ä»£,ç„¡éœ€è¨“ç·´ |

---

### æ±ºç­–æ¨¹

```
æ˜¯å¦éœ€è¦ç§æœ‰/æœ€æ–°æ•¸æ“š?
â”‚
â”œâ”€ å¦ (é€šç”¨ä»»å‹™)
â”‚   â””â”€ ä½¿ç”¨ Prompt Engineering
â”‚
â”œâ”€ æ˜¯ (ç§æœ‰æ•¸æ“š)
â”‚   â”œâ”€ æ•¸æ“šæ˜¯å¦å‹•æ…‹æ›´æ–°?
â”‚   â”‚   â”œâ”€ æ˜¯ â†’ ä½¿ç”¨ RAG
â”‚   â”‚   â””â”€ å¦ â†’ ä½¿ç”¨ Fine-tuning
â”‚
â””â”€ æ˜¯å¦éœ€è¦èª¿ç”¨å¤–éƒ¨å·¥å…·/API?
    â”œâ”€ æ˜¯ â†’ ä½¿ç”¨ Agent ç³»çµ±
    â””â”€ å¦ â†’ ä½¿ç”¨æ¨™æº–å°è©±æ¨¡å¼
```

---

## çµè«–èˆ‡å»ºè­°

1. **æ—¥å¸¸æºé€šèˆ‡å¯¦ä½œ**: å„ªå…ˆæŒæ¡ **Fundamentals** ä¸­çš„ **Prompt Engineering èˆ‡ RAG**,å®ƒå€‘æ˜¯ LLM æ‡‰ç”¨çš„åŸºçŸ³,å¯å¿«é€Ÿé©—è­‰æƒ³æ³•ã€‚

2. **å¼·èª¿æ–¹æ³•è«–èˆ‡å‰µæ–°**: å¾ **First Principles** å‡ºç™¼,ç†è§£ **In-Context Learning èˆ‡ Emergent Abilities çš„æ•¸å­¸æœ¬è³ª**,æœ‰åŠ©æ–¼æ‚¨åœ¨é¢å°æ–°ä»»å‹™æ™‚,è¨­è¨ˆæ›´æœ‰æ•ˆçš„ Prompt èˆ‡æ‡‰ç”¨æ¶æ§‹ã€‚

3. **æ§‹å»ºå®è§€è¦–é‡**: å°‡ **LLM æ‡‰ç”¨æŠ€è¡“** æ”¾å…¥ **Body of Knowledge** çš„æ¡†æ¶ä¸­,å¯ä»¥æ¸…æ™°åœ°çœ‹åˆ°å®ƒåœ¨ä¼æ¥­ç´šæ‡‰ç”¨ä¸­çš„æˆ°ç•¥ä½ç½®,ä»¥åŠå¦‚ä½•èˆ‡ RAGã€Agentã€Fine-tuning å”åŒå·¥ä½œã€‚

**æ ¸å¿ƒè¦é»**: **Prompt å¼•å°è¡Œç‚º,RAG æ³¨å…¥çŸ¥è­˜,Fine-tuning æ·±åº¦é©é…,Agent æ“´å±•èƒ½åŠ›**ã€‚

é€éæœ¬ç« çš„å­¸ç¿’,æ‚¨æ‡‰ç•¶å·²ç¶“æŒæ¡äº† **LLM æ‡‰ç”¨é–‹ç™¼** çš„æ ¸å¿ƒæŠ€è¡“èˆ‡å¯¦æˆ°æŠ€å·§,ä¸¦èƒ½å¾æ›´å®è§€çš„è¦–è§’ç†è§£å…¶åœ¨ç¾ä»£ AI é–‹ç™¼ä¸­çš„é—œéµä½œç”¨ã€‚

---

## å»¶ä¼¸é–±è®€ (Further Reading)

### é—œéµè«–æ–‡ (Key Papers)
1. **Language Models are Few-Shot Learners (GPT-3)**: Brown et al. (2020). *NeurIPS*.
2. **Chain-of-Thought Prompting Elicits Reasoning**: Wei et al. (2022). *NeurIPS*.
3. **Retrieval-Augmented Generation (RAG)**: Lewis et al. (2020). *NeurIPS*.
4. **ReAct: Synergizing Reasoning and Acting in Language Models**: Yao et al. (2023). *ICLR*.
5. **Scaling Laws for Neural Language Models**: Kaplan et al. (2020). *arXiv*.

### å·¥å…·èˆ‡å¯¦ç¾ (Tools & Implementations)
- **LangChain**: https://python.langchain.com/
- **LlamaIndex**: https://www.llamaindex.ai/
- **Hugging Face Transformers**: https://huggingface.co/transformers/
- **OpenAI API**: https://platform.openai.com/docs/

### å­¸ç¿’è³‡æº (Learning Resources)
- **LangChain Documentation**: https://python.langchain.com/docs/
- **Prompt Engineering Guide**: https://www.promptingguide.ai/
- **OpenAI Cookbook**: https://github.com/openai/openai-cookbook

---

**ä¸Šä¸€ç« ç¯€**: [03_Encoderèˆ‡Decoderæ·±åº¦å‰–æ.md](./03_Encoderèˆ‡Decoderæ·±åº¦å‰–æ.md)
**ä¸‹ä¸€ç« ç¯€**: [../ç¯„ä¾‹ç¨‹å¼/](../ç¯„ä¾‹ç¨‹å¼/)
