{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# CH07-06: 三大模型架構對比\n",
    "## Encoder vs Decoder vs Encoder-Decoder\n",
    "\n",
    "**課程時長**: 75 分鐘  \n",
    "**難度**: ⭐⭐⭐⭐  \n",
    "**前置知識**: CH07-01 至 CH07-05  \n",
    "\n",
    "---\n",
    "\n",
    "## 📚 本節學習目標\n",
    "\n",
    "1. ✅ 理解三大架構的設計哲學與適用場景\n",
    "2. ✅ 掌握 BERT, GPT, T5 的核心差異\n",
    "3. ✅ 學會根據任務選擇最佳架構\n",
    "4. ✅ 理解預訓練任務對模型能力的影響\n",
    "5. ✅ 實際對比三種架構的效能\n",
    "\n",
    "---\n",
    "\n",
    "## 📖 目錄\n",
    "\n",
    "1. [三大架構概覽](#1-overview)\n",
    "2. [Encoder-Only 架構 (BERT)](#2-encoder-only)\n",
    "3. [Decoder-Only 架構 (GPT)](#3-decoder-only)\n",
    "4. [Encoder-Decoder 架構 (T5)](#4-encoder-decoder)\n",
    "5. [任務與架構的匹配](#5-task-matching)\n",
    "6. [實戰對比與選型建議](#6-comparison)\n",
    "7. [總結](#7-summary)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1",
   "metadata": {},
   "source": [
    "## 1. 三大架構概覽 {#1-overview}\n",
    "\n",
    "### 1.1 架構分類\n",
    "\n",
    "```\n",
    "Transformer\n",
    "    ├─ Encoder-Only (雙向理解)\n",
    "    │   └─ BERT, RoBERTa, ALBERT, ELECTRA\n",
    "    │\n",
    "    ├─ Decoder-Only (單向生成)\n",
    "    │   └─ GPT, GPT-2, GPT-3, LLaMA, Mistral\n",
    "    │\n",
    "    └─ Encoder-Decoder (序列轉換)\n",
    "        └─ T5, BART, mBART, Pegasus\n",
    "```\n",
    "\n",
    "### 1.2 核心差異總覽\n",
    "\n",
    "| 特性 | Encoder-Only | Decoder-Only | Encoder-Decoder |\n",
    "|------|--------------|--------------|------------------|\n",
    "| **注意力類型** | 雙向 Self-Attention | 單向 Masked Self-Attention | 雙向 (Encoder) + 單向 (Decoder) + Cross-Attention |\n",
    "| **輸入-輸出** | 輸入 = 輸出長度 | 自回歸生成 | 輸入 ≠ 輸出長度 |\n",
    "| **預訓練任務** | MLM, NSP | Causal LM | Span Corruption, Translation |\n",
    "| **擅長任務** | 分類, NER, QA | 文本生成, 對話 | 翻譯, 摘要, 轉換 |\n",
    "| **代表模型** | BERT | GPT | T5 |\n",
    "| **參數效率** | 高 (理解) | 中 (生成) | 低 (需兩部分) |\n",
    "| **推論速度** | 快 (並行) | 慢 (逐步生成) | 中 (Encoder並行, Decoder逐步) |\n",
    "\n",
    "### 1.3 設計哲學\n",
    "\n",
    "**Encoder-Only** (BERT 哲學):\n",
    "> \"理解語言的最佳方式是同時看到上下文的兩側\"\n",
    "\n",
    "**Decoder-Only** (GPT 哲學):\n",
    "> \"語言生成本質上是預測下一個詞，這個任務足以學會理解\"\n",
    "\n",
    "**Encoder-Decoder** (T5 哲學):\n",
    "> \"所有 NLP 任務都可以統一為 Text-to-Text 格式\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入必要套件\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Tuple\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['font.sans-serif'] = ['Microsoft JhengHei', 'Arial Unicode MS']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(\"✅ 套件載入完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2",
   "metadata": {},
   "source": [
    "## 2. Encoder-Only 架構 (BERT) {#2-encoder-only}\n",
    "\n",
    "### 2.1 架構特點\n",
    "\n",
    "```\n",
    "Input: \"The cat [MASK] on the mat\"\n",
    "         ↓\n",
    "    Token + Segment + Position Embeddings\n",
    "         ↓\n",
    "    Encoder Layer 1 (雙向 Self-Attention)\n",
    "         ↓\n",
    "    Encoder Layer 2\n",
    "         ↓\n",
    "         ...\n",
    "         ↓\n",
    "    Encoder Layer 12\n",
    "         ↓\n",
    "Output: Contextual Representations\n",
    "         ↓\n",
    "    Task-Specific Head (分類/NER/QA...)\n",
    "```\n",
    "\n",
    "### 2.2 BERT 系列模型\n",
    "\n",
    "| 模型 | 層數 | d_model | 參數量 | 特點 |\n",
    "|------|------|---------|--------|------|\n",
    "| **BERT-Base** | 12 | 768 | 110M | 原始版本 |\n",
    "| **BERT-Large** | 24 | 1024 | 340M | 更大容量 |\n",
    "| **RoBERTa** | 12/24 | 768/1024 | 125M/355M | 移除 NSP, 更多數據 |\n",
    "| **ALBERT** | 12/24 | 768/1024 | 12M/18M | 參數共享, 大幅減少參數 |\n",
    "| **ELECTRA** | 12/24 | 768/1024 | 110M/335M | 改用 RTD 任務, 效率更高 |\n",
    "| **DeBERTa** | 12/24 | 768/1024 | 140M/380M | Disentangled Attention |\n",
    "\n",
    "### 2.3 預訓練任務\n",
    "\n",
    "**① Masked Language Model (MLM)**:\n",
    "```python\n",
    "# 範例\n",
    "Input:  \"The cat [MASK] on the [MASK]\"\n",
    "Target: \"The cat  sat   on the  mat \"\n",
    "\n",
    "# 遮蔽策略\n",
    "- 80%: 替換為 [MASK]\n",
    "- 10%: 替換為隨機詞\n",
    "- 10%: 保持不變\n",
    "```\n",
    "\n",
    "**② Next Sentence Prediction (NSP)** (BERT 原始版本):\n",
    "```python\n",
    "# 正例\n",
    "Sentence A: \"I love NLP.\"\n",
    "Sentence B: \"It is very interesting.\"  # IsNext = True\n",
    "\n",
    "# 負例\n",
    "Sentence A: \"I love NLP.\"\n",
    "Sentence B: \"The weather is nice today.\"  # IsNext = False\n",
    "```\n",
    "\n",
    "**RoBERTa 的改進**: 移除 NSP，只用 MLM，證明 NSP 對效果無益。\n",
    "\n",
    "### 2.4 雙向注意力的優勢\n",
    "\n",
    "**範例**: \"The animal didn't cross the street because it was too tired.\"\n",
    "\n",
    "- **單向 (Decoder)**: 處理 \"it\" 時只能看到左側，無法確定指代\n",
    "- **雙向 (Encoder)**: 同時看到 \"animal\" 和 \"tired\"，能準確判斷 \"it\" = \"animal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bert-attention-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化 BERT 的雙向注意力\n",
    "\n",
    "def visualize_bidirectional_attention():\n",
    "    \"\"\"比較單向 vs 雙向注意力的可見範圍\"\"\"\n",
    "    \n",
    "    seq_len = 8\n",
    "    tokens = ['The', 'cat', 'sat', 'on', 'the', 'mat', 'yesterday', '.']\n",
    "    \n",
    "    # 單向注意力 (Causal Mask)\n",
    "    causal_mask = np.tril(np.ones((seq_len, seq_len)))\n",
    "    \n",
    "    # 雙向注意力 (No Mask)\n",
    "    bidirectional_mask = np.ones((seq_len, seq_len))\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # 單向注意力\n",
    "    sns.heatmap(\n",
    "        causal_mask, \n",
    "        annot=True, \n",
    "        fmt='.0f', \n",
    "        cmap='Blues',\n",
    "        xticklabels=tokens,\n",
    "        yticklabels=tokens,\n",
    "        ax=axes[0],\n",
    "        cbar_kws={'label': 'Can Attend'}\n",
    "    )\n",
    "    axes[0].set_title('Decoder-Only (GPT)\\n單向注意力 - 只能看到過去', \n",
    "                      fontsize=13, fontweight='bold')\n",
    "    axes[0].set_xlabel('Key Position')\n",
    "    axes[0].set_ylabel('Query Position')\n",
    "    \n",
    "    # 雙向注意力\n",
    "    sns.heatmap(\n",
    "        bidirectional_mask, \n",
    "        annot=True, \n",
    "        fmt='.0f', \n",
    "        cmap='Greens',\n",
    "        xticklabels=tokens,\n",
    "        yticklabels=tokens,\n",
    "        ax=axes[1],\n",
    "        cbar_kws={'label': 'Can Attend'}\n",
    "    )\n",
    "    axes[1].set_title('Encoder-Only (BERT)\\n雙向注意力 - 可以看到完整上下文', \n",
    "                      fontsize=13, fontweight='bold')\n",
    "    axes[1].set_xlabel('Key Position')\n",
    "    axes[1].set_ylabel('Query Position')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n關鍵差異:\")\n",
    "    print(\"• GPT: 處理 'sat' 時只能看到 [The, cat, sat]\")\n",
    "    print(\"• BERT: 處理 'sat' 時可以看到整個句子 [The ... .]\")\n",
    "    print(\"\\n優勢:\")\n",
    "    print(\"• BERT 更適合理解任務 (分類、NER、問答)\")\n",
    "    print(\"• GPT 更適合生成任務 (文本生成、對話)\")\n",
    "\n",
    "visualize_bidirectional_attention()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bert-use-cases",
   "metadata": {},
   "source": [
    "### 2.5 BERT 的典型應用\n",
    "\n",
    "**✅ 最適合的任務**:\n",
    "1. **文本分類**: 情感分析、主題分類、垃圾郵件檢測\n",
    "2. **命名實體識別 (NER)**: 人名、地名、組織名提取\n",
    "3. **問答系統**: SQuAD, Natural Questions\n",
    "4. **語義相似度**: 句子配對、文本蘊涵\n",
    "5. **資訊檢索**: 搜尋排序、文檔相關性\n",
    "\n",
    "**❌ 不適合的任務**:\n",
    "- 文本生成 (需要 Decoder)\n",
    "- 長文本摘要 (需要 Encoder-Decoder)\n",
    "- 機器翻譯 (需要 Encoder-Decoder)\n",
    "\n",
    "### 2.6 BERT 的優勢與限制\n",
    "\n",
    "**優勢**:\n",
    "- ✅ 雙向上下文理解最強\n",
    "- ✅ 並行計算速度快\n",
    "- ✅ 微調簡單高效\n",
    "- ✅ 參數效率高\n",
    "\n",
    "**限制**:\n",
    "- ❌ 無法直接生成文本\n",
    "- ❌ 預訓練與微調存在差距 ([MASK] 只出現在訓練時)\n",
    "- ❌ 輸入長度受限 (通常 512 tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3",
   "metadata": {},
   "source": [
    "## 3. Decoder-Only 架構 (GPT) {#3-decoder-only}\n",
    "\n",
    "### 3.1 架構特點\n",
    "\n",
    "```\n",
    "Input: \"The cat sat\"\n",
    "         ↓\n",
    "    Token + Position Embeddings\n",
    "         ↓\n",
    "    Decoder Layer 1 (Masked Self-Attention)\n",
    "         ↓\n",
    "    Decoder Layer 2\n",
    "         ↓\n",
    "         ...\n",
    "         ↓\n",
    "    Decoder Layer N\n",
    "         ↓\n",
    "    Language Model Head\n",
    "         ↓\n",
    "Output: Next Token Probabilities → \"on\"\n",
    "```\n",
    "\n",
    "**關鍵**: 沒有 Cross-Attention，只有 Masked Self-Attention\n",
    "\n",
    "### 3.2 GPT 系列演進\n",
    "\n",
    "| 模型 | 層數 | d_model | 參數量 | 訓練數據 | 發布年份 |\n",
    "|------|------|---------|--------|----------|----------|\n",
    "| **GPT** | 12 | 768 | 117M | BooksCorpus (5GB) | 2018 |\n",
    "| **GPT-2** | 48 | 1600 | 1.5B | WebText (40GB) | 2019 |\n",
    "| **GPT-3** | 96 | 12288 | 175B | Common Crawl (570GB) | 2020 |\n",
    "| **GPT-3.5** | - | - | ~175B | + Code + Instructions | 2022 |\n",
    "| **GPT-4** | - | - | ~1.8T? | Multimodal | 2023 |\n",
    "\n",
    "**開源替代品**:\n",
    "- **LLaMA** (Meta): 7B/13B/33B/65B\n",
    "- **Mistral** (Mistral AI): 7B, MoE 架構\n",
    "- **Falcon** (TII): 7B/40B/180B\n",
    "\n",
    "### 3.3 預訓練任務: Causal Language Modeling\n",
    "\n",
    "**任務定義**: 給定前面的詞，預測下一個詞\n",
    "\n",
    "```python\n",
    "# 訓練範例\n",
    "Input:  \"The cat sat on\"\n",
    "Target: \"the\"\n",
    "\n",
    "Input:  \"The cat sat on the\"\n",
    "Target: \"mat\"\n",
    "\n",
    "# 數學形式\n",
    "P(x_t | x_1, x_2, ..., x_{t-1})\n",
    "\n",
    "# 損失函數 (Negative Log-Likelihood)\n",
    "Loss = -∑ log P(x_t | x_{<t})\n",
    "```\n",
    "\n",
    "### 3.4 自回歸生成範例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gpt-generation-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_autoregressive_generation():\n",
    "    \"\"\"視覺化 GPT 的自回歸生成過程\"\"\"\n",
    "    \n",
    "    # 生成步驟\n",
    "    steps = [\n",
    "        {\"input\": \"[BOS]\", \"context\": [\"[BOS]\"], \"predict\": \"I\"},\n",
    "        {\"input\": \"I\", \"context\": [\"[BOS]\", \"I\"], \"predict\": \"love\"},\n",
    "        {\"input\": \"love\", \"context\": [\"[BOS]\", \"I\", \"love\"], \"predict\": \"NLP\"},\n",
    "        {\"input\": \"NLP\", \"context\": [\"[BOS]\", \"I\", \"love\", \"NLP\"], \"predict\": \".\"},\n",
    "        {\"input\": \".\", \"context\": [\"[BOS]\", \"I\", \"love\", \"NLP\", \".\"], \"predict\": \"[EOS]\"},\n",
    "    ]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    \n",
    "    y_positions = list(range(len(steps), 0, -1))\n",
    "    \n",
    "    for i, (step, y_pos) in enumerate(zip(steps, y_positions)):\n",
    "        # 繪製上下文\n",
    "        context_str = \" \".join(step[\"context\"])\n",
    "        ax.text(0.1, y_pos, f\"Step {i+1}\", fontsize=12, fontweight='bold', va='center')\n",
    "        ax.text(0.25, y_pos, f\"Context: {context_str}\", fontsize=11, va='center', \n",
    "                bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5))\n",
    "        \n",
    "        # 箭頭\n",
    "        ax.annotate('', xy=(0.75, y_pos), xytext=(0.65, y_pos),\n",
    "                   arrowprops=dict(arrowstyle='->', lw=2, color='green'))\n",
    "        \n",
    "        # 預測結果\n",
    "        ax.text(0.8, y_pos, f'Predict: \"{step[\"predict\"]}\"', fontsize=11, va='center',\n",
    "                bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.7))\n",
    "    \n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, len(steps) + 1)\n",
    "    ax.axis('off')\n",
    "    ax.set_title('GPT 自回歸生成過程', fontsize=15, fontweight='bold', pad=20)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n自回歸生成特點:\")\n",
    "    print(\"• 每步只預測一個 token\")\n",
    "    print(\"• 上下文逐步增長\")\n",
    "    print(\"• 每個 token 只能看到左側的歷史\")\n",
    "    print(\"• 生成長度不確定 (直到遇到 [EOS])\")\n",
    "\n",
    "visualize_autoregressive_generation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gpt-capabilities",
   "metadata": {},
   "source": [
    "### 3.5 GPT 的湧現能力 (Emergent Abilities)\n",
    "\n",
    "隨著模型規模增長，GPT 展現出未經特別訓練的能力:\n",
    "\n",
    "**GPT-3 的 Few-Shot Learning**:\n",
    "```\n",
    "# Zero-Shot (無範例)\n",
    "Translate to French: \"Hello\" → \n",
    "\n",
    "# One-Shot (1 個範例)\n",
    "Translate to French: \"Hello\" → \"Bonjour\"\n",
    "Translate to French: \"Goodbye\" → \n",
    "\n",
    "# Few-Shot (多個範例)\n",
    "Translate to French: \"Hello\" → \"Bonjour\"\n",
    "Translate to French: \"Thank you\" → \"Merci\"\n",
    "Translate to French: \"Goodbye\" → \n",
    "```\n",
    "\n",
    "**In-Context Learning**: 模型在推論時從 prompt 中的範例學習，無需梯度更新。\n",
    "\n",
    "### 3.6 GPT 的典型應用\n",
    "\n",
    "**✅ 最適合的任務**:\n",
    "1. **文本生成**: 故事、文章、詩歌創作\n",
    "2. **對話系統**: ChatGPT, 客服機器人\n",
    "3. **程式碼生成**: GitHub Copilot, CodeX\n",
    "4. **文本補全**: IDE 自動補全、Email 續寫\n",
    "5. **創意寫作**: 廣告文案、劇本\n",
    "\n",
    "**⚠️ 需要 Prompt Engineering 的任務**:\n",
    "- 分類 (需要設計 prompt 如 \"Classify sentiment: positive or negative?\")\n",
    "- 資訊抽取 (需要示範格式)\n",
    "\n",
    "### 3.7 GPT 的優勢與限制\n",
    "\n",
    "**優勢**:\n",
    "- ✅ 生成能力最強\n",
    "- ✅ Zero/Few-shot 學習能力\n",
    "- ✅ 統一架構適用多種任務\n",
    "- ✅ 規模增長帶來能力提升\n",
    "\n",
    "**限制**:\n",
    "- ❌ 單向上下文理解較弱\n",
    "- ❌ 推論慢 (逐步生成)\n",
    "- ❌ 訓練成本極高\n",
    "- ❌ 容易產生幻覺 (Hallucination)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4",
   "metadata": {},
   "source": [
    "## 4. Encoder-Decoder 架構 (T5) {#4-encoder-decoder}\n",
    "\n",
    "### 4.1 架構特點\n",
    "\n",
    "```\n",
    "Source: \"translate English to German: Hello\"\n",
    "           ↓\n",
    "    Encoder (雙向理解)\n",
    "           ↓\n",
    "    Encoder Output ────┐\n",
    "                       │\n",
    "Target: \"<BOS>\"        │\n",
    "           ↓           │\n",
    "    Decoder (單向生成) │\n",
    "           ↑           │\n",
    "    Cross-Attention ←──┘\n",
    "           ↓\n",
    "Output: \"Hallo\"\n",
    "```\n",
    "\n",
    "**核心**: 結合 BERT 的理解能力 + GPT 的生成能力\n",
    "\n",
    "### 4.2 T5 系列模型\n",
    "\n",
    "| 模型 | Encoder 層數 | Decoder 層數 | d_model | 參數量 |\n",
    "|------|--------------|--------------|---------|--------|\n",
    "| **T5-Small** | 6 | 6 | 512 | 60M |\n",
    "| **T5-Base** | 12 | 12 | 768 | 220M |\n",
    "| **T5-Large** | 24 | 24 | 1024 | 770M |\n",
    "| **T5-3B** | 24 | 24 | 1024 | 3B |\n",
    "| **T5-11B** | 24 | 24 | 1024 | 11B |\n",
    "\n",
    "**其他 Encoder-Decoder 模型**:\n",
    "- **BART** (Facebook): Encoder-Decoder with denoising objectives\n",
    "- **mBART** (Multilingual BART): 支援 50+ 語言\n",
    "- **Pegasus** (Google): 專為摘要任務優化\n",
    "\n",
    "### 4.3 T5 的統一框架: Text-to-Text\n",
    "\n",
    "**核心思想**: 所有 NLP 任務都轉換為 \"輸入文本 → 輸出文本\"\n",
    "\n",
    "```python\n",
    "# 機器翻譯\n",
    "Input:  \"translate English to German: Hello\"\n",
    "Output: \"Hallo\"\n",
    "\n",
    "# 文本分類\n",
    "Input:  \"sentiment: This movie is amazing!\"\n",
    "Output: \"positive\"\n",
    "\n",
    "# 摘要\n",
    "Input:  \"summarize: [長篇文章...]\"\n",
    "Output: \"[摘要...]\"\n",
    "\n",
    "# 問答\n",
    "Input:  \"question: What is NLP? context: [文章...]\"\n",
    "Output: \"Natural Language Processing\"\n",
    "\n",
    "# 命名實體識別\n",
    "Input:  \"ner: John lives in New York\"\n",
    "Output: \"John: PERSON, New York: LOCATION\"\n",
    "```\n",
    "\n",
    "### 4.4 預訓練任務: Span Corruption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "t5-span-corruption",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_span_corruption():\n",
    "    \"\"\"視覺化 T5 的 Span Corruption 預訓練任務\"\"\"\n",
    "    \n",
    "    original = \"Thank you for inviting me to your party last week\"\n",
    "    tokens = original.split()\n",
    "    \n",
    "    # 模擬 span corruption\n",
    "    # Mask spans of varying lengths\n",
    "    input_text = \"Thank you <X> inviting <Y> to your party <Z> week\"\n",
    "    target_text = \"<X> for <Y> me <Z> last\"\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    \n",
    "    # 原始文本\n",
    "    ax.text(0.5, 0.85, \"原始文本\", ha='center', fontsize=14, fontweight='bold')\n",
    "    ax.text(0.5, 0.75, original, ha='center', fontsize=12,\n",
    "            bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.5))\n",
    "    \n",
    "    # 箭頭\n",
    "    ax.annotate('', xy=(0.5, 0.65), xytext=(0.5, 0.70),\n",
    "               arrowprops=dict(arrowstyle='->', lw=2))\n",
    "    ax.text(0.52, 0.67, 'Span Corruption', fontsize=10, style='italic')\n",
    "    \n",
    "    # Encoder Input\n",
    "    ax.text(0.25, 0.55, \"Encoder Input\", ha='center', fontsize=13, fontweight='bold')\n",
    "    ax.text(0.25, 0.45, input_text, ha='center', fontsize=11,\n",
    "            bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.6))\n",
    "    \n",
    "    # Decoder Target\n",
    "    ax.text(0.75, 0.55, \"Decoder Target\", ha='center', fontsize=13, fontweight='bold')\n",
    "    ax.text(0.75, 0.45, target_text, ha='center', fontsize=11,\n",
    "            bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.6))\n",
    "    \n",
    "    # 說明\n",
    "    explanation = [\n",
    "        \"• 隨機選擇多個 spans (連續 token 片段) 進行遮蔽\",\n",
    "        \"• 使用特殊 token <X>, <Y>, <Z> 替換被遮蔽的 spans\",\n",
    "        \"• Decoder 需要依序重建被遮蔽的 spans\",\n",
    "        \"• 比 BERT 的 MLM 更適合序列生成任務\"\n",
    "    ]\n",
    "    \n",
    "    y_start = 0.30\n",
    "    for i, line in enumerate(explanation):\n",
    "        ax.text(0.1, y_start - i*0.05, line, fontsize=10, va='top')\n",
    "    \n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.axis('off')\n",
    "    ax.set_title('T5 預訓練任務: Span Corruption', fontsize=15, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_span_corruption()\n",
    "\n",
    "print(\"\\nSpan Corruption vs MLM (BERT):\")\n",
    "print(\"• BERT MLM: 遮蔽單個 token, 預測每個位置的 token\")\n",
    "print(\"• T5 Span Corruption: 遮蔽連續片段, 生成式地重建\")\n",
    "print(\"\\n優勢: 更符合下游生成任務 (摘要、翻譯) 的需求\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t5-use-cases",
   "metadata": {},
   "source": [
    "### 4.5 T5 的典型應用\n",
    "\n",
    "**✅ 最適合的任務**:\n",
    "1. **機器翻譯**: 多語言互譯\n",
    "2. **文本摘要**: 長文摘要、新聞標題生成\n",
    "3. **問答系統**: 生成式問答 (而非抽取式)\n",
    "4. **資料增強**: Paraphrase, Back-translation\n",
    "5. **結構化輸出**: JSON 生成、格式轉換\n",
    "\n",
    "**範例 - 多任務統一**:\n",
    "```python\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "\n",
    "# 任務 1: 翻譯\n",
    "input_text = \"translate English to French: Hello, how are you?\"\n",
    "# 任務 2: 摘要\n",
    "input_text = \"summarize: [長篇文章]\"\n",
    "# 任務 3: 問答\n",
    "input_text = \"question: What is NLP? context: [段落]\"\n",
    "```\n",
    "\n",
    "### 4.6 T5 的優勢與限制\n",
    "\n",
    "**優勢**:\n",
    "- ✅ 統一框架簡化多任務學習\n",
    "- ✅ 結合雙向理解 + 單向生成\n",
    "- ✅ 適合需要深度理解的生成任務\n",
    "- ✅ 靈活的輸入輸出長度\n",
    "\n",
    "**限制**:\n",
    "- ❌ 參數量大 (需要 Encoder + Decoder)\n",
    "- ❌ 推論速度較慢\n",
    "- ❌ 對 prompt 格式敏感\n",
    "- ❌ 訓練成本高"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-5",
   "metadata": {},
   "source": [
    "## 5. 任務與架構的匹配 {#5-task-matching}\n",
    "\n",
    "### 5.1 任務分類決策樹\n",
    "\n",
    "```\n",
    "你的任務需要生成文本嗎?\n",
    "├─ 否 (理解任務)\n",
    "│   ├─ 分類、NER、問答 (抽取式) → Encoder-Only (BERT)\n",
    "│   └─ 語義相似度、資訊檢索 → Encoder-Only (BERT)\n",
    "│\n",
    "└─ 是 (生成任務)\n",
    "    ├─ 輸入和輸出是否有明確對應?\n",
    "    │   ├─ 是 (翻譯、摘要、轉換) → Encoder-Decoder (T5/BART)\n",
    "    │   └─ 否 (續寫、對話、創作) → Decoder-Only (GPT)\n",
    "    │\n",
    "    └─ 需要深度理解源文本嗎?\n",
    "        ├─ 是 (摘要、問答生成) → Encoder-Decoder (T5)\n",
    "        └─ 否 (自由創作) → Decoder-Only (GPT)\n",
    "```\n",
    "\n",
    "### 5.2 任務-架構匹配表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task-architecture-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 創建任務-架構匹配表\n",
    "task_data = {\n",
    "    '任務類型': [\n",
    "        '情感分類',\n",
    "        '命名實體識別',\n",
    "        '問答 (抽取式)',\n",
    "        '語義相似度',\n",
    "        '機器翻譯',\n",
    "        '文本摘要',\n",
    "        '問答 (生成式)',\n",
    "        '文本生成',\n",
    "        '對話系統',\n",
    "        '程式碼生成'\n",
    "    ],\n",
    "    'Encoder-Only (BERT)': [\n",
    "        '✅ 最佳', '✅ 最佳', '✅ 最佳', '✅ 最佳',\n",
    "        '❌ 不適合', '❌ 不適合', '⚠️ 可行但不佳',\n",
    "        '❌ 不適合', '❌ 不適合', '❌ 不適合'\n",
    "    ],\n",
    "    'Decoder-Only (GPT)': [\n",
    "        '⚠️ 需 Prompt', '⚠️ 需 Prompt', '✅ 可用',\n",
    "        '⚠️ 需 Prompt', '✅ 可用', '✅ 可用', '✅ 最佳',\n",
    "        '✅ 最佳', '✅ 最佳', '✅ 最佳'\n",
    "    ],\n",
    "    'Encoder-Decoder (T5)': [\n",
    "        '✅ 可用', '✅ 可用', '✅ 可用', '⚠️ 較複雜',\n",
    "        '✅ 最佳', '✅ 最佳', '✅ 最佳',\n",
    "        '✅ 可用', '✅ 可用', '⚠️ 較複雜'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(task_data)\n",
    "\n",
    "# 設置顏色對應\n",
    "def color_cells(val):\n",
    "    if '✅ 最佳' in val:\n",
    "        return 'background-color: #90EE90; font-weight: bold'\n",
    "    elif '✅' in val:\n",
    "        return 'background-color: #FFFACD'\n",
    "    elif '⚠️' in val:\n",
    "        return 'background-color: #FFE4B5'\n",
    "    elif '❌' in val:\n",
    "        return 'background-color: #FFB6C1'\n",
    "    return ''\n",
    "\n",
    "# 顯示表格\n",
    "styled_df = df.style.applymap(color_cells, subset=df.columns[1:])\n",
    "display(styled_df)\n",
    "\n",
    "print(\"\\n圖例:\")\n",
    "print(\"✅ 最佳 - 此架構專為該任務設計\")\n",
    "print(\"✅ 可用 - 表現良好，但可能需要調整\")\n",
    "print(\"⚠️ 需 Prompt/較複雜 - 可行但需要特殊處理\")\n",
    "print(\"❌ 不適合 - 架構限制導致無法有效完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-6",
   "metadata": {},
   "source": [
    "## 6. 實戰對比與選型建議 {#6-comparison}\n",
    "\n",
    "### 6.1 效能對比 (相同參數量級)\n",
    "\n",
    "**情境**: 假設都是 ~300M 參數的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "performance-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模擬效能對比數據\n",
    "comparison_data = {\n",
    "    '指標': [\n",
    "        '文本分類 (GLUE)',\n",
    "        '命名實體識別 (CoNLL)',\n",
    "        '問答 (SQuAD 2.0)',\n",
    "        '機器翻譯 (BLEU)',\n",
    "        '文本摘要 (ROUGE)',\n",
    "        '文本生成 (人工評分)',\n",
    "        '訓練時間 (相對)',\n",
    "        '推論速度 (tokens/sec)',\n",
    "        '記憶體使用 (GB)'\n",
    "    ],\n",
    "    'BERT-Large': [92, 94, 88, 0, 0, 30, 1.0, 850, 12],\n",
    "    'GPT-2-Medium': [85, 78, 82, 68, 72, 95, 0.8, 320, 10],\n",
    "    'T5-Large': [91, 90, 90, 92, 88, 85, 1.5, 280, 18]\n",
    "}\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "\n",
    "# 視覺化\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. 理解任務對比\n",
    "understanding_tasks = df_comparison.iloc[:3, :]\n",
    "x = np.arange(len(understanding_tasks['指標']))\n",
    "width = 0.25\n",
    "\n",
    "axes[0, 0].bar(x - width, understanding_tasks['BERT-Large'], width, label='BERT-Large', color='skyblue')\n",
    "axes[0, 0].bar(x, understanding_tasks['GPT-2-Medium'], width, label='GPT-2-Medium', color='lightcoral')\n",
    "axes[0, 0].bar(x + width, understanding_tasks['T5-Large'], width, label='T5-Large', color='lightgreen')\n",
    "axes[0, 0].set_ylabel('分數', fontsize=11)\n",
    "axes[0, 0].set_title('理解任務效能對比', fontsize=13, fontweight='bold')\n",
    "axes[0, 0].set_xticks(x)\n",
    "axes[0, 0].set_xticklabels(understanding_tasks['指標'], rotation=15, ha='right')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].set_ylim(0, 100)\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 2. 生成任務對比\n",
    "generation_tasks = df_comparison.iloc[3:6, :]\n",
    "x = np.arange(len(generation_tasks['指標']))\n",
    "\n",
    "axes[0, 1].bar(x - width, generation_tasks['BERT-Large'], width, label='BERT-Large', color='skyblue')\n",
    "axes[0, 1].bar(x, generation_tasks['GPT-2-Medium'], width, label='GPT-2-Medium', color='lightcoral')\n",
    "axes[0, 1].bar(x + width, generation_tasks['T5-Large'], width, label='T5-Large', color='lightgreen')\n",
    "axes[0, 1].set_ylabel('分數', fontsize=11)\n",
    "axes[0, 1].set_title('生成任務效能對比', fontsize=13, fontweight='bold')\n",
    "axes[0, 1].set_xticks(x)\n",
    "axes[0, 1].set_xticklabels(generation_tasks['指標'], rotation=15, ha='right')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].set_ylim(0, 100)\n",
    "axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 3. 推論速度對比\n",
    "speed_data = df_comparison.iloc[7:8, 1:].values[0]\n",
    "models = ['BERT-Large', 'GPT-2-Medium', 'T5-Large']\n",
    "colors_speed = ['skyblue', 'lightcoral', 'lightgreen']\n",
    "\n",
    "axes[1, 0].barh(models, speed_data, color=colors_speed)\n",
    "axes[1, 0].set_xlabel('Tokens/sec', fontsize=11)\n",
    "axes[1, 0].set_title('推論速度對比 (越高越好)', fontsize=13, fontweight='bold')\n",
    "axes[1, 0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "for i, v in enumerate(speed_data):\n",
    "    axes[1, 0].text(v + 20, i, str(v), va='center', fontweight='bold')\n",
    "\n",
    "# 4. 記憶體使用對比\n",
    "memory_data = df_comparison.iloc[8:9, 1:].values[0]\n",
    "\n",
    "axes[1, 1].barh(models, memory_data, color=colors_speed)\n",
    "axes[1, 1].set_xlabel('記憶體 (GB)', fontsize=11)\n",
    "axes[1, 1].set_title('記憶體使用對比 (越低越好)', fontsize=13, fontweight='bold')\n",
    "axes[1, 1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "for i, v in enumerate(memory_data):\n",
    "    axes[1, 1].text(v + 0.5, i, str(v), va='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n關鍵觀察:\")\n",
    "print(\"• BERT: 理解任務最強，推論最快，但無法生成\")\n",
    "print(\"• GPT: 生成任務最強，但理解任務需要 prompt engineering\")\n",
    "print(\"• T5: 平衡型選手，適合多任務場景，但資源消耗最大\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selection-guide",
   "metadata": {},
   "source": [
    "### 6.2 選型決策指南\n",
    "\n",
    "#### 場景 1: 企業級文本分類系統\n",
    "\n",
    "**需求**:\n",
    "- 客戶評論情感分析\n",
    "- 需要高準確率和快速推論\n",
    "- 資料已標註\n",
    "\n",
    "**推薦**: ✅ **BERT / RoBERTa**\n",
    "\n",
    "**理由**:\n",
    "1. 分類任務是 BERT 的強項\n",
    "2. 微調簡單高效\n",
    "3. 推論速度快，適合生產環境\n",
    "\n",
    "---\n",
    "\n",
    "#### 場景 2: AI 寫作助手\n",
    "\n",
    "**需求**:\n",
    "- 文章續寫、創意寫作\n",
    "- 需要自然流暢的生成\n",
    "- 支援多種風格\n",
    "\n",
    "**推薦**: ✅ **GPT-3 / LLaMA / Mistral**\n",
    "\n",
    "**理由**:\n",
    "1. 生成能力最強\n",
    "2. Few-shot learning 適應不同風格\n",
    "3. 長文本生成連貫性好\n",
    "\n",
    "---\n",
    "\n",
    "#### 場景 3: 多語言新聞摘要系統\n",
    "\n",
    "**需求**:\n",
    "- 將長新聞摘要為短標題\n",
    "- 支援多種語言\n",
    "- 需要保持原文重點\n",
    "\n",
    "**推薦**: ✅ **T5 / mBART / Pegasus**\n",
    "\n",
    "**理由**:\n",
    "1. Encoder-Decoder 專為序列轉換設計\n",
    "2. Encoder 深度理解源文本\n",
    "3. Decoder 生成簡潔摘要\n",
    "\n",
    "---\n",
    "\n",
    "#### 場景 4: 智能客服系統\n",
    "\n",
    "**需求**:\n",
    "- 理解用戶意圖\n",
    "- 生成合適回覆\n",
    "- 需要多輪對話能力\n",
    "\n",
    "**推薦**: ✅ **GPT (小規模) + BERT (意圖分類)**\n",
    "\n",
    "**理由**:\n",
    "1. BERT 快速分類用戶意圖\n",
    "2. GPT 生成自然回覆\n",
    "3. 混合架構平衡效能與成本\n",
    "\n",
    "---\n",
    "\n",
    "### 6.3 實用選型檢查表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selection-checklist",
   "metadata": {},
   "outputs": [],
   "source": [
    "def architecture_selector():\n",
    "    \"\"\"\n",
    "    互動式架構選擇工具\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"        Transformer 架構選擇助手\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "    \n",
    "    questions = [\n",
    "        {\n",
    "            'q': '你的任務需要生成文本嗎？',\n",
    "            'options': ['是 - 需要生成', '否 - 只需理解'],\n",
    "            'scores': {'BERT': [0, 2], 'GPT': [2, 0], 'T5': [1, 1]}\n",
    "        },\n",
    "        {\n",
    "            'q': '輸入和輸出是否有明確對應關係？(如翻譯、摘要)',\n",
    "            'options': ['是 - 有對應', '否 - 自由生成'],\n",
    "            'scores': {'BERT': [1, 0], 'GPT': [1, 2], 'T5': [2, 1]}\n",
    "        },\n",
    "        {\n",
    "            'q': '推論速度是否為關鍵考量？',\n",
    "            'options': ['是 - 需要快速推論', '否 - 品質優先'],\n",
    "            'scores': {'BERT': [2, 1], 'GPT': [1, 1], 'T5': [0, 2]}\n",
    "        },\n",
    "        {\n",
    "            'q': '是否有大量標註數據可供微調？',\n",
    "            'options': ['是 - 有標註數據', '否 - 數據稀缺'],\n",
    "            'scores': {'BERT': [2, 0], 'GPT': [1, 2], 'T5': [2, 1]}\n",
    "        },\n",
    "        {\n",
    "            'q': '部署資源限制如何？',\n",
    "            'options': ['受限 - 需要小模型', '充足 - 可用大模型'],\n",
    "            'scores': {'BERT': [2, 1], 'GPT': [1, 1], 'T5': [0, 2]}\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # 模擬用戶選擇 (實際使用時可以改為 input())\n",
    "    user_choices = [0, 0, 0, 0, 1]  # 範例選擇\n",
    "    \n",
    "    scores = {'BERT': 0, 'GPT': 0, 'T5': 0}\n",
    "    \n",
    "    for i, (question, choice) in enumerate(zip(questions, user_choices)):\n",
    "        print(f\"Q{i+1}. {question['q']}\")\n",
    "        print(f\"   選擇: {question['options'][choice]}\")\n",
    "        print()\n",
    "        \n",
    "        for arch in scores:\n",
    "            scores[arch] += question['scores'][arch][choice]\n",
    "    \n",
    "    # 排序結果\n",
    "    sorted_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"推薦結果:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for rank, (arch, score) in enumerate(sorted_scores, 1):\n",
    "        stars = '⭐' * score\n",
    "        print(f\"{rank}. {arch:8s} - {stars} ({score} 分)\")\n",
    "    \n",
    "    best = sorted_scores[0][0]\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"✅ 建議使用: {best}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    recommendations = {\n",
    "        'BERT': '適合分類、NER、問答等理解任務，推論快速，易於微調',\n",
    "        'GPT': '適合文本生成、對話系統，支援 few-shot learning',\n",
    "        'T5': '適合翻譯、摘要等序列轉換任務，統一多任務框架'\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n理由: {recommendations[best]}\")\n",
    "\n",
    "architecture_selector()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-7",
   "metadata": {},
   "source": [
    "## 7. 總結 {#7-summary}\n",
    "\n",
    "### 7.1 三大架構核心要點\n",
    "\n",
    "| 架構 | 核心優勢 | 最佳應用 | 代表模型 |\n",
    "|------|----------|----------|----------|\n",
    "| **Encoder-Only** | 雙向理解、快速推論 | 分類、NER、問答 | BERT, RoBERTa |\n",
    "| **Decoder-Only** | 強大生成、Few-shot | 文本生成、對話 | GPT-3, LLaMA |\n",
    "| **Encoder-Decoder** | 平衡理解與生成 | 翻譯、摘要 | T5, BART |\n",
    "\n",
    "### 7.2 選型三原則\n",
    "\n",
    "1. **任務導向**: 先確定任務類型(理解 vs 生成 vs 轉換)\n",
    "2. **資源考量**: 評估計算資源、推論延遲需求\n",
    "3. **數據情況**: 考慮標註數據量、是否需要 few-shot\n",
    "\n",
    "### 7.3 未來趨勢\n",
    "\n",
    "**架構融合**:\n",
    "- **Instruction-tuned Models**: GPT-3.5, ChatGPT, Claude\n",
    "  - Decoder-Only 架構 + 指令微調\n",
    "  - 同時具備理解與生成能力\n",
    "\n",
    "- **Mixture of Experts (MoE)**: Mistral 8x7B\n",
    "  - 稀疏激活降低推論成本\n",
    "  - 保持大模型能力\n",
    "\n",
    "- **Multimodal Transformers**: GPT-4, Gemini\n",
    "  - 跨模態理解(文本、圖像、音訊)\n",
    "\n",
    "**實用建議**:\n",
    "\n",
    "✅ **2024 年的最佳實踐**:\n",
    "1. **理解任務**: 優先使用 DeBERTa, RoBERTa (效率高)\n",
    "2. **生成任務**: 使用開源 LLM (LLaMA 2, Mistral) + Fine-tuning\n",
    "3. **轉換任務**: T5 或 BART (專業領域可微調)\n",
    "4. **通用場景**: 考慮 API (GPT-4, Claude) vs 自建模型的成本\n",
    "\n",
    "### 7.4 延伸閱讀\n",
    "\n",
    "1. **論文**:\n",
    "   - [BERT: Pre-training of Deep Bidirectional Transformers](https://arxiv.org/abs/1810.04805)\n",
    "   - [Language Models are Unsupervised Multitask Learners (GPT-2)](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
    "   - [Exploring the Limits of Transfer Learning with T5](https://arxiv.org/abs/1910.10683)\n",
    "\n",
    "2. **實戰資源**:\n",
    "   - [Hugging Face Model Hub](https://huggingface.co/models)\n",
    "   - [Papers with Code - NLP Leaderboards](https://paperswithcode.com/area/natural-language-processing)\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 下一節預告\n",
    "\n",
    "**CH07-07: 大型語言模型 (LLMs)**\n",
    "- LLM 的規模定律 (Scaling Laws)\n",
    "- Instruction Tuning 與 RLHF\n",
    "- Prompt Engineering 技巧\n",
    "- In-Context Learning 原理\n",
    "- LLM 的能力邊界與局限\n",
    "\n",
    "---\n",
    "\n",
    "**課程完成時間**: `____年____月____日`  \n",
    "**學習心得**: ___________________________________"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
