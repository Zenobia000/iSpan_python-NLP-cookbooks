{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# CH07-06: ä¸‰å¤§æ¨¡å‹æ¶æ§‹å°æ¯”\n",
    "## Encoder vs Decoder vs Encoder-Decoder\n",
    "\n",
    "**èª²ç¨‹æ™‚é•·**: 75 åˆ†é˜  \n",
    "**é›£åº¦**: â­â­â­â­  \n",
    "**å‰ç½®çŸ¥è­˜**: CH07-01 è‡³ CH07-05  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š æœ¬ç¯€å­¸ç¿’ç›®æ¨™\n",
    "\n",
    "1. âœ… ç†è§£ä¸‰å¤§æ¶æ§‹çš„è¨­è¨ˆå“²å­¸èˆ‡é©ç”¨å ´æ™¯\n",
    "2. âœ… æŒæ¡ BERT, GPT, T5 çš„æ ¸å¿ƒå·®ç•°\n",
    "3. âœ… å­¸æœƒæ ¹æ“šä»»å‹™é¸æ“‡æœ€ä½³æ¶æ§‹\n",
    "4. âœ… ç†è§£é è¨“ç·´ä»»å‹™å°æ¨¡å‹èƒ½åŠ›çš„å½±éŸ¿\n",
    "5. âœ… å¯¦éš›å°æ¯”ä¸‰ç¨®æ¶æ§‹çš„æ•ˆèƒ½\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“– ç›®éŒ„\n",
    "\n",
    "1. [ä¸‰å¤§æ¶æ§‹æ¦‚è¦½](#1-overview)\n",
    "2. [Encoder-Only æ¶æ§‹ (BERT)](#2-encoder-only)\n",
    "3. [Decoder-Only æ¶æ§‹ (GPT)](#3-decoder-only)\n",
    "4. [Encoder-Decoder æ¶æ§‹ (T5)](#4-encoder-decoder)\n",
    "5. [ä»»å‹™èˆ‡æ¶æ§‹çš„åŒ¹é…](#5-task-matching)\n",
    "6. [å¯¦æˆ°å°æ¯”èˆ‡é¸å‹å»ºè­°](#6-comparison)\n",
    "7. [ç¸½çµ](#7-summary)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1",
   "metadata": {},
   "source": [
    "## 1. ä¸‰å¤§æ¶æ§‹æ¦‚è¦½ {#1-overview}\n",
    "\n",
    "### 1.1 æ¶æ§‹åˆ†é¡\n",
    "\n",
    "```\n",
    "Transformer\n",
    "    â”œâ”€ Encoder-Only (é›™å‘ç†è§£)\n",
    "    â”‚   â””â”€ BERT, RoBERTa, ALBERT, ELECTRA\n",
    "    â”‚\n",
    "    â”œâ”€ Decoder-Only (å–®å‘ç”Ÿæˆ)\n",
    "    â”‚   â””â”€ GPT, GPT-2, GPT-3, LLaMA, Mistral\n",
    "    â”‚\n",
    "    â””â”€ Encoder-Decoder (åºåˆ—è½‰æ›)\n",
    "        â””â”€ T5, BART, mBART, Pegasus\n",
    "```\n",
    "\n",
    "### 1.2 æ ¸å¿ƒå·®ç•°ç¸½è¦½\n",
    "\n",
    "| ç‰¹æ€§ | Encoder-Only | Decoder-Only | Encoder-Decoder |\n",
    "|------|--------------|--------------|------------------|\n",
    "| **æ³¨æ„åŠ›é¡å‹** | é›™å‘ Self-Attention | å–®å‘ Masked Self-Attention | é›™å‘ (Encoder) + å–®å‘ (Decoder) + Cross-Attention |\n",
    "| **è¼¸å…¥-è¼¸å‡º** | è¼¸å…¥ = è¼¸å‡ºé•·åº¦ | è‡ªå›æ­¸ç”Ÿæˆ | è¼¸å…¥ â‰  è¼¸å‡ºé•·åº¦ |\n",
    "| **é è¨“ç·´ä»»å‹™** | MLM, NSP | Causal LM | Span Corruption, Translation |\n",
    "| **æ“…é•·ä»»å‹™** | åˆ†é¡, NER, QA | æ–‡æœ¬ç”Ÿæˆ, å°è©± | ç¿»è­¯, æ‘˜è¦, è½‰æ› |\n",
    "| **ä»£è¡¨æ¨¡å‹** | BERT | GPT | T5 |\n",
    "| **åƒæ•¸æ•ˆç‡** | é«˜ (ç†è§£) | ä¸­ (ç”Ÿæˆ) | ä½ (éœ€å…©éƒ¨åˆ†) |\n",
    "| **æ¨è«–é€Ÿåº¦** | å¿« (ä¸¦è¡Œ) | æ…¢ (é€æ­¥ç”Ÿæˆ) | ä¸­ (Encoderä¸¦è¡Œ, Decoderé€æ­¥) |\n",
    "\n",
    "### 1.3 è¨­è¨ˆå“²å­¸\n",
    "\n",
    "**Encoder-Only** (BERT å“²å­¸):\n",
    "> \"ç†è§£èªè¨€çš„æœ€ä½³æ–¹å¼æ˜¯åŒæ™‚çœ‹åˆ°ä¸Šä¸‹æ–‡çš„å…©å´\"\n",
    "\n",
    "**Decoder-Only** (GPT å“²å­¸):\n",
    "> \"èªè¨€ç”Ÿæˆæœ¬è³ªä¸Šæ˜¯é æ¸¬ä¸‹ä¸€å€‹è©ï¼Œé€™å€‹ä»»å‹™è¶³ä»¥å­¸æœƒç†è§£\"\n",
    "\n",
    "**Encoder-Decoder** (T5 å“²å­¸):\n",
    "> \"æ‰€æœ‰ NLP ä»»å‹™éƒ½å¯ä»¥çµ±ä¸€ç‚º Text-to-Text æ ¼å¼\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¼‰å…¥å¿…è¦å¥—ä»¶\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Tuple\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['font.sans-serif'] = ['Microsoft JhengHei', 'Arial Unicode MS']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(\"âœ… å¥—ä»¶è¼‰å…¥å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2",
   "metadata": {},
   "source": [
    "## 2. Encoder-Only æ¶æ§‹ (BERT) {#2-encoder-only}\n",
    "\n",
    "### 2.1 æ¶æ§‹ç‰¹é»\n",
    "\n",
    "```\n",
    "Input: \"The cat [MASK] on the mat\"\n",
    "         â†“\n",
    "    Token + Segment + Position Embeddings\n",
    "         â†“\n",
    "    Encoder Layer 1 (é›™å‘ Self-Attention)\n",
    "         â†“\n",
    "    Encoder Layer 2\n",
    "         â†“\n",
    "         ...\n",
    "         â†“\n",
    "    Encoder Layer 12\n",
    "         â†“\n",
    "Output: Contextual Representations\n",
    "         â†“\n",
    "    Task-Specific Head (åˆ†é¡/NER/QA...)\n",
    "```\n",
    "\n",
    "### 2.2 BERT ç³»åˆ—æ¨¡å‹\n",
    "\n",
    "| æ¨¡å‹ | å±¤æ•¸ | d_model | åƒæ•¸é‡ | ç‰¹é» |\n",
    "|------|------|---------|--------|------|\n",
    "| **BERT-Base** | 12 | 768 | 110M | åŸå§‹ç‰ˆæœ¬ |\n",
    "| **BERT-Large** | 24 | 1024 | 340M | æ›´å¤§å®¹é‡ |\n",
    "| **RoBERTa** | 12/24 | 768/1024 | 125M/355M | ç§»é™¤ NSP, æ›´å¤šæ•¸æ“š |\n",
    "| **ALBERT** | 12/24 | 768/1024 | 12M/18M | åƒæ•¸å…±äº«, å¤§å¹…æ¸›å°‘åƒæ•¸ |\n",
    "| **ELECTRA** | 12/24 | 768/1024 | 110M/335M | æ”¹ç”¨ RTD ä»»å‹™, æ•ˆç‡æ›´é«˜ |\n",
    "| **DeBERTa** | 12/24 | 768/1024 | 140M/380M | Disentangled Attention |\n",
    "\n",
    "### 2.3 é è¨“ç·´ä»»å‹™\n",
    "\n",
    "**â‘  Masked Language Model (MLM)**:\n",
    "```python\n",
    "# ç¯„ä¾‹\n",
    "Input:  \"The cat [MASK] on the [MASK]\"\n",
    "Target: \"The cat  sat   on the  mat \"\n",
    "\n",
    "# é®è”½ç­–ç•¥\n",
    "- 80%: æ›¿æ›ç‚º [MASK]\n",
    "- 10%: æ›¿æ›ç‚ºéš¨æ©Ÿè©\n",
    "- 10%: ä¿æŒä¸è®Š\n",
    "```\n",
    "\n",
    "**â‘¡ Next Sentence Prediction (NSP)** (BERT åŸå§‹ç‰ˆæœ¬):\n",
    "```python\n",
    "# æ­£ä¾‹\n",
    "Sentence A: \"I love NLP.\"\n",
    "Sentence B: \"It is very interesting.\"  # IsNext = True\n",
    "\n",
    "# è² ä¾‹\n",
    "Sentence A: \"I love NLP.\"\n",
    "Sentence B: \"The weather is nice today.\"  # IsNext = False\n",
    "```\n",
    "\n",
    "**RoBERTa çš„æ”¹é€²**: ç§»é™¤ NSPï¼Œåªç”¨ MLMï¼Œè­‰æ˜ NSP å°æ•ˆæœç„¡ç›Šã€‚\n",
    "\n",
    "### 2.4 é›™å‘æ³¨æ„åŠ›çš„å„ªå‹¢\n",
    "\n",
    "**ç¯„ä¾‹**: \"The animal didn't cross the street because it was too tired.\"\n",
    "\n",
    "- **å–®å‘ (Decoder)**: è™•ç† \"it\" æ™‚åªèƒ½çœ‹åˆ°å·¦å´ï¼Œç„¡æ³•ç¢ºå®šæŒ‡ä»£\n",
    "- **é›™å‘ (Encoder)**: åŒæ™‚çœ‹åˆ° \"animal\" å’Œ \"tired\"ï¼Œèƒ½æº–ç¢ºåˆ¤æ–· \"it\" = \"animal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bert-attention-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¦–è¦ºåŒ– BERT çš„é›™å‘æ³¨æ„åŠ›\n",
    "\n",
    "def visualize_bidirectional_attention():\n",
    "    \"\"\"æ¯”è¼ƒå–®å‘ vs é›™å‘æ³¨æ„åŠ›çš„å¯è¦‹ç¯„åœ\"\"\"\n",
    "    \n",
    "    seq_len = 8\n",
    "    tokens = ['The', 'cat', 'sat', 'on', 'the', 'mat', 'yesterday', '.']\n",
    "    \n",
    "    # å–®å‘æ³¨æ„åŠ› (Causal Mask)\n",
    "    causal_mask = np.tril(np.ones((seq_len, seq_len)))\n",
    "    \n",
    "    # é›™å‘æ³¨æ„åŠ› (No Mask)\n",
    "    bidirectional_mask = np.ones((seq_len, seq_len))\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # å–®å‘æ³¨æ„åŠ›\n",
    "    sns.heatmap(\n",
    "        causal_mask, \n",
    "        annot=True, \n",
    "        fmt='.0f', \n",
    "        cmap='Blues',\n",
    "        xticklabels=tokens,\n",
    "        yticklabels=tokens,\n",
    "        ax=axes[0],\n",
    "        cbar_kws={'label': 'Can Attend'}\n",
    "    )\n",
    "    axes[0].set_title('Decoder-Only (GPT)\\nå–®å‘æ³¨æ„åŠ› - åªèƒ½çœ‹åˆ°éå»', \n",
    "                      fontsize=13, fontweight='bold')\n",
    "    axes[0].set_xlabel('Key Position')\n",
    "    axes[0].set_ylabel('Query Position')\n",
    "    \n",
    "    # é›™å‘æ³¨æ„åŠ›\n",
    "    sns.heatmap(\n",
    "        bidirectional_mask, \n",
    "        annot=True, \n",
    "        fmt='.0f', \n",
    "        cmap='Greens',\n",
    "        xticklabels=tokens,\n",
    "        yticklabels=tokens,\n",
    "        ax=axes[1],\n",
    "        cbar_kws={'label': 'Can Attend'}\n",
    "    )\n",
    "    axes[1].set_title('Encoder-Only (BERT)\\né›™å‘æ³¨æ„åŠ› - å¯ä»¥çœ‹åˆ°å®Œæ•´ä¸Šä¸‹æ–‡', \n",
    "                      fontsize=13, fontweight='bold')\n",
    "    axes[1].set_xlabel('Key Position')\n",
    "    axes[1].set_ylabel('Query Position')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\né—œéµå·®ç•°:\")\n",
    "    print(\"â€¢ GPT: è™•ç† 'sat' æ™‚åªèƒ½çœ‹åˆ° [The, cat, sat]\")\n",
    "    print(\"â€¢ BERT: è™•ç† 'sat' æ™‚å¯ä»¥çœ‹åˆ°æ•´å€‹å¥å­ [The ... .]\")\n",
    "    print(\"\\nå„ªå‹¢:\")\n",
    "    print(\"â€¢ BERT æ›´é©åˆç†è§£ä»»å‹™ (åˆ†é¡ã€NERã€å•ç­”)\")\n",
    "    print(\"â€¢ GPT æ›´é©åˆç”Ÿæˆä»»å‹™ (æ–‡æœ¬ç”Ÿæˆã€å°è©±)\")\n",
    "\n",
    "visualize_bidirectional_attention()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bert-use-cases",
   "metadata": {},
   "source": [
    "### 2.5 BERT çš„å…¸å‹æ‡‰ç”¨\n",
    "\n",
    "**âœ… æœ€é©åˆçš„ä»»å‹™**:\n",
    "1. **æ–‡æœ¬åˆ†é¡**: æƒ…æ„Ÿåˆ†æã€ä¸»é¡Œåˆ†é¡ã€åƒåœ¾éƒµä»¶æª¢æ¸¬\n",
    "2. **å‘½åå¯¦é«”è­˜åˆ¥ (NER)**: äººåã€åœ°åã€çµ„ç¹”åæå–\n",
    "3. **å•ç­”ç³»çµ±**: SQuAD, Natural Questions\n",
    "4. **èªç¾©ç›¸ä¼¼åº¦**: å¥å­é…å°ã€æ–‡æœ¬è˜Šæ¶µ\n",
    "5. **è³‡è¨Šæª¢ç´¢**: æœå°‹æ’åºã€æ–‡æª”ç›¸é—œæ€§\n",
    "\n",
    "**âŒ ä¸é©åˆçš„ä»»å‹™**:\n",
    "- æ–‡æœ¬ç”Ÿæˆ (éœ€è¦ Decoder)\n",
    "- é•·æ–‡æœ¬æ‘˜è¦ (éœ€è¦ Encoder-Decoder)\n",
    "- æ©Ÿå™¨ç¿»è­¯ (éœ€è¦ Encoder-Decoder)\n",
    "\n",
    "### 2.6 BERT çš„å„ªå‹¢èˆ‡é™åˆ¶\n",
    "\n",
    "**å„ªå‹¢**:\n",
    "- âœ… é›™å‘ä¸Šä¸‹æ–‡ç†è§£æœ€å¼·\n",
    "- âœ… ä¸¦è¡Œè¨ˆç®—é€Ÿåº¦å¿«\n",
    "- âœ… å¾®èª¿ç°¡å–®é«˜æ•ˆ\n",
    "- âœ… åƒæ•¸æ•ˆç‡é«˜\n",
    "\n",
    "**é™åˆ¶**:\n",
    "- âŒ ç„¡æ³•ç›´æ¥ç”Ÿæˆæ–‡æœ¬\n",
    "- âŒ é è¨“ç·´èˆ‡å¾®èª¿å­˜åœ¨å·®è· ([MASK] åªå‡ºç¾åœ¨è¨“ç·´æ™‚)\n",
    "- âŒ è¼¸å…¥é•·åº¦å—é™ (é€šå¸¸ 512 tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3",
   "metadata": {},
   "source": [
    "## 3. Decoder-Only æ¶æ§‹ (GPT) {#3-decoder-only}\n",
    "\n",
    "### 3.1 æ¶æ§‹ç‰¹é»\n",
    "\n",
    "```\n",
    "Input: \"The cat sat\"\n",
    "         â†“\n",
    "    Token + Position Embeddings\n",
    "         â†“\n",
    "    Decoder Layer 1 (Masked Self-Attention)\n",
    "         â†“\n",
    "    Decoder Layer 2\n",
    "         â†“\n",
    "         ...\n",
    "         â†“\n",
    "    Decoder Layer N\n",
    "         â†“\n",
    "    Language Model Head\n",
    "         â†“\n",
    "Output: Next Token Probabilities â†’ \"on\"\n",
    "```\n",
    "\n",
    "**é—œéµ**: æ²’æœ‰ Cross-Attentionï¼Œåªæœ‰ Masked Self-Attention\n",
    "\n",
    "### 3.2 GPT ç³»åˆ—æ¼”é€²\n",
    "\n",
    "| æ¨¡å‹ | å±¤æ•¸ | d_model | åƒæ•¸é‡ | è¨“ç·´æ•¸æ“š | ç™¼å¸ƒå¹´ä»½ |\n",
    "|------|------|---------|--------|----------|----------|\n",
    "| **GPT** | 12 | 768 | 117M | BooksCorpus (5GB) | 2018 |\n",
    "| **GPT-2** | 48 | 1600 | 1.5B | WebText (40GB) | 2019 |\n",
    "| **GPT-3** | 96 | 12288 | 175B | Common Crawl (570GB) | 2020 |\n",
    "| **GPT-3.5** | - | - | ~175B | + Code + Instructions | 2022 |\n",
    "| **GPT-4** | - | - | ~1.8T? | Multimodal | 2023 |\n",
    "\n",
    "**é–‹æºæ›¿ä»£å“**:\n",
    "- **LLaMA** (Meta): 7B/13B/33B/65B\n",
    "- **Mistral** (Mistral AI): 7B, MoE æ¶æ§‹\n",
    "- **Falcon** (TII): 7B/40B/180B\n",
    "\n",
    "### 3.3 é è¨“ç·´ä»»å‹™: Causal Language Modeling\n",
    "\n",
    "**ä»»å‹™å®šç¾©**: çµ¦å®šå‰é¢çš„è©ï¼Œé æ¸¬ä¸‹ä¸€å€‹è©\n",
    "\n",
    "```python\n",
    "# è¨“ç·´ç¯„ä¾‹\n",
    "Input:  \"The cat sat on\"\n",
    "Target: \"the\"\n",
    "\n",
    "Input:  \"The cat sat on the\"\n",
    "Target: \"mat\"\n",
    "\n",
    "# æ•¸å­¸å½¢å¼\n",
    "P(x_t | x_1, x_2, ..., x_{t-1})\n",
    "\n",
    "# æå¤±å‡½æ•¸ (Negative Log-Likelihood)\n",
    "Loss = -âˆ‘ log P(x_t | x_{<t})\n",
    "```\n",
    "\n",
    "### 3.4 è‡ªå›æ­¸ç”Ÿæˆç¯„ä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gpt-generation-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_autoregressive_generation():\n",
    "    \"\"\"è¦–è¦ºåŒ– GPT çš„è‡ªå›æ­¸ç”Ÿæˆéç¨‹\"\"\"\n",
    "    \n",
    "    # ç”Ÿæˆæ­¥é©Ÿ\n",
    "    steps = [\n",
    "        {\"input\": \"[BOS]\", \"context\": [\"[BOS]\"], \"predict\": \"I\"},\n",
    "        {\"input\": \"I\", \"context\": [\"[BOS]\", \"I\"], \"predict\": \"love\"},\n",
    "        {\"input\": \"love\", \"context\": [\"[BOS]\", \"I\", \"love\"], \"predict\": \"NLP\"},\n",
    "        {\"input\": \"NLP\", \"context\": [\"[BOS]\", \"I\", \"love\", \"NLP\"], \"predict\": \".\"},\n",
    "        {\"input\": \".\", \"context\": [\"[BOS]\", \"I\", \"love\", \"NLP\", \".\"], \"predict\": \"[EOS]\"},\n",
    "    ]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    \n",
    "    y_positions = list(range(len(steps), 0, -1))\n",
    "    \n",
    "    for i, (step, y_pos) in enumerate(zip(steps, y_positions)):\n",
    "        # ç¹ªè£½ä¸Šä¸‹æ–‡\n",
    "        context_str = \" \".join(step[\"context\"])\n",
    "        ax.text(0.1, y_pos, f\"Step {i+1}\", fontsize=12, fontweight='bold', va='center')\n",
    "        ax.text(0.25, y_pos, f\"Context: {context_str}\", fontsize=11, va='center', \n",
    "                bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5))\n",
    "        \n",
    "        # ç®­é ­\n",
    "        ax.annotate('', xy=(0.75, y_pos), xytext=(0.65, y_pos),\n",
    "                   arrowprops=dict(arrowstyle='->', lw=2, color='green'))\n",
    "        \n",
    "        # é æ¸¬çµæœ\n",
    "        ax.text(0.8, y_pos, f'Predict: \"{step[\"predict\"]}\"', fontsize=11, va='center',\n",
    "                bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.7))\n",
    "    \n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, len(steps) + 1)\n",
    "    ax.axis('off')\n",
    "    ax.set_title('GPT è‡ªå›æ­¸ç”Ÿæˆéç¨‹', fontsize=15, fontweight='bold', pad=20)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nè‡ªå›æ­¸ç”Ÿæˆç‰¹é»:\")\n",
    "    print(\"â€¢ æ¯æ­¥åªé æ¸¬ä¸€å€‹ token\")\n",
    "    print(\"â€¢ ä¸Šä¸‹æ–‡é€æ­¥å¢é•·\")\n",
    "    print(\"â€¢ æ¯å€‹ token åªèƒ½çœ‹åˆ°å·¦å´çš„æ­·å²\")\n",
    "    print(\"â€¢ ç”Ÿæˆé•·åº¦ä¸ç¢ºå®š (ç›´åˆ°é‡åˆ° [EOS])\")\n",
    "\n",
    "visualize_autoregressive_generation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gpt-capabilities",
   "metadata": {},
   "source": [
    "### 3.5 GPT çš„æ¹§ç¾èƒ½åŠ› (Emergent Abilities)\n",
    "\n",
    "éš¨è‘—æ¨¡å‹è¦æ¨¡å¢é•·ï¼ŒGPT å±•ç¾å‡ºæœªç¶“ç‰¹åˆ¥è¨“ç·´çš„èƒ½åŠ›:\n",
    "\n",
    "**GPT-3 çš„ Few-Shot Learning**:\n",
    "```\n",
    "# Zero-Shot (ç„¡ç¯„ä¾‹)\n",
    "Translate to French: \"Hello\" â†’ \n",
    "\n",
    "# One-Shot (1 å€‹ç¯„ä¾‹)\n",
    "Translate to French: \"Hello\" â†’ \"Bonjour\"\n",
    "Translate to French: \"Goodbye\" â†’ \n",
    "\n",
    "# Few-Shot (å¤šå€‹ç¯„ä¾‹)\n",
    "Translate to French: \"Hello\" â†’ \"Bonjour\"\n",
    "Translate to French: \"Thank you\" â†’ \"Merci\"\n",
    "Translate to French: \"Goodbye\" â†’ \n",
    "```\n",
    "\n",
    "**In-Context Learning**: æ¨¡å‹åœ¨æ¨è«–æ™‚å¾ prompt ä¸­çš„ç¯„ä¾‹å­¸ç¿’ï¼Œç„¡éœ€æ¢¯åº¦æ›´æ–°ã€‚\n",
    "\n",
    "### 3.6 GPT çš„å…¸å‹æ‡‰ç”¨\n",
    "\n",
    "**âœ… æœ€é©åˆçš„ä»»å‹™**:\n",
    "1. **æ–‡æœ¬ç”Ÿæˆ**: æ•…äº‹ã€æ–‡ç« ã€è©©æ­Œå‰µä½œ\n",
    "2. **å°è©±ç³»çµ±**: ChatGPT, å®¢æœæ©Ÿå™¨äºº\n",
    "3. **ç¨‹å¼ç¢¼ç”Ÿæˆ**: GitHub Copilot, CodeX\n",
    "4. **æ–‡æœ¬è£œå…¨**: IDE è‡ªå‹•è£œå…¨ã€Email çºŒå¯«\n",
    "5. **å‰µæ„å¯«ä½œ**: å»£å‘Šæ–‡æ¡ˆã€åŠ‡æœ¬\n",
    "\n",
    "**âš ï¸ éœ€è¦ Prompt Engineering çš„ä»»å‹™**:\n",
    "- åˆ†é¡ (éœ€è¦è¨­è¨ˆ prompt å¦‚ \"Classify sentiment: positive or negative?\")\n",
    "- è³‡è¨ŠæŠ½å– (éœ€è¦ç¤ºç¯„æ ¼å¼)\n",
    "\n",
    "### 3.7 GPT çš„å„ªå‹¢èˆ‡é™åˆ¶\n",
    "\n",
    "**å„ªå‹¢**:\n",
    "- âœ… ç”Ÿæˆèƒ½åŠ›æœ€å¼·\n",
    "- âœ… Zero/Few-shot å­¸ç¿’èƒ½åŠ›\n",
    "- âœ… çµ±ä¸€æ¶æ§‹é©ç”¨å¤šç¨®ä»»å‹™\n",
    "- âœ… è¦æ¨¡å¢é•·å¸¶ä¾†èƒ½åŠ›æå‡\n",
    "\n",
    "**é™åˆ¶**:\n",
    "- âŒ å–®å‘ä¸Šä¸‹æ–‡ç†è§£è¼ƒå¼±\n",
    "- âŒ æ¨è«–æ…¢ (é€æ­¥ç”Ÿæˆ)\n",
    "- âŒ è¨“ç·´æˆæœ¬æ¥µé«˜\n",
    "- âŒ å®¹æ˜“ç”¢ç”Ÿå¹»è¦º (Hallucination)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4",
   "metadata": {},
   "source": [
    "## 4. Encoder-Decoder æ¶æ§‹ (T5) {#4-encoder-decoder}\n",
    "\n",
    "### 4.1 æ¶æ§‹ç‰¹é»\n",
    "\n",
    "```\n",
    "Source: \"translate English to German: Hello\"\n",
    "           â†“\n",
    "    Encoder (é›™å‘ç†è§£)\n",
    "           â†“\n",
    "    Encoder Output â”€â”€â”€â”€â”\n",
    "                       â”‚\n",
    "Target: \"<BOS>\"        â”‚\n",
    "           â†“           â”‚\n",
    "    Decoder (å–®å‘ç”Ÿæˆ) â”‚\n",
    "           â†‘           â”‚\n",
    "    Cross-Attention â†â”€â”€â”˜\n",
    "           â†“\n",
    "Output: \"Hallo\"\n",
    "```\n",
    "\n",
    "**æ ¸å¿ƒ**: çµåˆ BERT çš„ç†è§£èƒ½åŠ› + GPT çš„ç”Ÿæˆèƒ½åŠ›\n",
    "\n",
    "### 4.2 T5 ç³»åˆ—æ¨¡å‹\n",
    "\n",
    "| æ¨¡å‹ | Encoder å±¤æ•¸ | Decoder å±¤æ•¸ | d_model | åƒæ•¸é‡ |\n",
    "|------|--------------|--------------|---------|--------|\n",
    "| **T5-Small** | 6 | 6 | 512 | 60M |\n",
    "| **T5-Base** | 12 | 12 | 768 | 220M |\n",
    "| **T5-Large** | 24 | 24 | 1024 | 770M |\n",
    "| **T5-3B** | 24 | 24 | 1024 | 3B |\n",
    "| **T5-11B** | 24 | 24 | 1024 | 11B |\n",
    "\n",
    "**å…¶ä»– Encoder-Decoder æ¨¡å‹**:\n",
    "- **BART** (Facebook): Encoder-Decoder with denoising objectives\n",
    "- **mBART** (Multilingual BART): æ”¯æ´ 50+ èªè¨€\n",
    "- **Pegasus** (Google): å°ˆç‚ºæ‘˜è¦ä»»å‹™å„ªåŒ–\n",
    "\n",
    "### 4.3 T5 çš„çµ±ä¸€æ¡†æ¶: Text-to-Text\n",
    "\n",
    "**æ ¸å¿ƒæ€æƒ³**: æ‰€æœ‰ NLP ä»»å‹™éƒ½è½‰æ›ç‚º \"è¼¸å…¥æ–‡æœ¬ â†’ è¼¸å‡ºæ–‡æœ¬\"\n",
    "\n",
    "```python\n",
    "# æ©Ÿå™¨ç¿»è­¯\n",
    "Input:  \"translate English to German: Hello\"\n",
    "Output: \"Hallo\"\n",
    "\n",
    "# æ–‡æœ¬åˆ†é¡\n",
    "Input:  \"sentiment: This movie is amazing!\"\n",
    "Output: \"positive\"\n",
    "\n",
    "# æ‘˜è¦\n",
    "Input:  \"summarize: [é•·ç¯‡æ–‡ç« ...]\"\n",
    "Output: \"[æ‘˜è¦...]\"\n",
    "\n",
    "# å•ç­”\n",
    "Input:  \"question: What is NLP? context: [æ–‡ç« ...]\"\n",
    "Output: \"Natural Language Processing\"\n",
    "\n",
    "# å‘½åå¯¦é«”è­˜åˆ¥\n",
    "Input:  \"ner: John lives in New York\"\n",
    "Output: \"John: PERSON, New York: LOCATION\"\n",
    "```\n",
    "\n",
    "### 4.4 é è¨“ç·´ä»»å‹™: Span Corruption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "t5-span-corruption",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_span_corruption():\n",
    "    \"\"\"è¦–è¦ºåŒ– T5 çš„ Span Corruption é è¨“ç·´ä»»å‹™\"\"\"\n",
    "    \n",
    "    original = \"Thank you for inviting me to your party last week\"\n",
    "    tokens = original.split()\n",
    "    \n",
    "    # æ¨¡æ“¬ span corruption\n",
    "    # Mask spans of varying lengths\n",
    "    input_text = \"Thank you <X> inviting <Y> to your party <Z> week\"\n",
    "    target_text = \"<X> for <Y> me <Z> last\"\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    \n",
    "    # åŸå§‹æ–‡æœ¬\n",
    "    ax.text(0.5, 0.85, \"åŸå§‹æ–‡æœ¬\", ha='center', fontsize=14, fontweight='bold')\n",
    "    ax.text(0.5, 0.75, original, ha='center', fontsize=12,\n",
    "            bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.5))\n",
    "    \n",
    "    # ç®­é ­\n",
    "    ax.annotate('', xy=(0.5, 0.65), xytext=(0.5, 0.70),\n",
    "               arrowprops=dict(arrowstyle='->', lw=2))\n",
    "    ax.text(0.52, 0.67, 'Span Corruption', fontsize=10, style='italic')\n",
    "    \n",
    "    # Encoder Input\n",
    "    ax.text(0.25, 0.55, \"Encoder Input\", ha='center', fontsize=13, fontweight='bold')\n",
    "    ax.text(0.25, 0.45, input_text, ha='center', fontsize=11,\n",
    "            bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.6))\n",
    "    \n",
    "    # Decoder Target\n",
    "    ax.text(0.75, 0.55, \"Decoder Target\", ha='center', fontsize=13, fontweight='bold')\n",
    "    ax.text(0.75, 0.45, target_text, ha='center', fontsize=11,\n",
    "            bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.6))\n",
    "    \n",
    "    # èªªæ˜\n",
    "    explanation = [\n",
    "        \"â€¢ éš¨æ©Ÿé¸æ“‡å¤šå€‹ spans (é€£çºŒ token ç‰‡æ®µ) é€²è¡Œé®è”½\",\n",
    "        \"â€¢ ä½¿ç”¨ç‰¹æ®Š token <X>, <Y>, <Z> æ›¿æ›è¢«é®è”½çš„ spans\",\n",
    "        \"â€¢ Decoder éœ€è¦ä¾åºé‡å»ºè¢«é®è”½çš„ spans\",\n",
    "        \"â€¢ æ¯” BERT çš„ MLM æ›´é©åˆåºåˆ—ç”Ÿæˆä»»å‹™\"\n",
    "    ]\n",
    "    \n",
    "    y_start = 0.30\n",
    "    for i, line in enumerate(explanation):\n",
    "        ax.text(0.1, y_start - i*0.05, line, fontsize=10, va='top')\n",
    "    \n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.axis('off')\n",
    "    ax.set_title('T5 é è¨“ç·´ä»»å‹™: Span Corruption', fontsize=15, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_span_corruption()\n",
    "\n",
    "print(\"\\nSpan Corruption vs MLM (BERT):\")\n",
    "print(\"â€¢ BERT MLM: é®è”½å–®å€‹ token, é æ¸¬æ¯å€‹ä½ç½®çš„ token\")\n",
    "print(\"â€¢ T5 Span Corruption: é®è”½é€£çºŒç‰‡æ®µ, ç”Ÿæˆå¼åœ°é‡å»º\")\n",
    "print(\"\\nå„ªå‹¢: æ›´ç¬¦åˆä¸‹æ¸¸ç”Ÿæˆä»»å‹™ (æ‘˜è¦ã€ç¿»è­¯) çš„éœ€æ±‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t5-use-cases",
   "metadata": {},
   "source": [
    "### 4.5 T5 çš„å…¸å‹æ‡‰ç”¨\n",
    "\n",
    "**âœ… æœ€é©åˆçš„ä»»å‹™**:\n",
    "1. **æ©Ÿå™¨ç¿»è­¯**: å¤šèªè¨€äº’è­¯\n",
    "2. **æ–‡æœ¬æ‘˜è¦**: é•·æ–‡æ‘˜è¦ã€æ–°èæ¨™é¡Œç”Ÿæˆ\n",
    "3. **å•ç­”ç³»çµ±**: ç”Ÿæˆå¼å•ç­” (è€ŒéæŠ½å–å¼)\n",
    "4. **è³‡æ–™å¢å¼·**: Paraphrase, Back-translation\n",
    "5. **çµæ§‹åŒ–è¼¸å‡º**: JSON ç”Ÿæˆã€æ ¼å¼è½‰æ›\n",
    "\n",
    "**ç¯„ä¾‹ - å¤šä»»å‹™çµ±ä¸€**:\n",
    "```python\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "\n",
    "# ä»»å‹™ 1: ç¿»è­¯\n",
    "input_text = \"translate English to French: Hello, how are you?\"\n",
    "# ä»»å‹™ 2: æ‘˜è¦\n",
    "input_text = \"summarize: [é•·ç¯‡æ–‡ç« ]\"\n",
    "# ä»»å‹™ 3: å•ç­”\n",
    "input_text = \"question: What is NLP? context: [æ®µè½]\"\n",
    "```\n",
    "\n",
    "### 4.6 T5 çš„å„ªå‹¢èˆ‡é™åˆ¶\n",
    "\n",
    "**å„ªå‹¢**:\n",
    "- âœ… çµ±ä¸€æ¡†æ¶ç°¡åŒ–å¤šä»»å‹™å­¸ç¿’\n",
    "- âœ… çµåˆé›™å‘ç†è§£ + å–®å‘ç”Ÿæˆ\n",
    "- âœ… é©åˆéœ€è¦æ·±åº¦ç†è§£çš„ç”Ÿæˆä»»å‹™\n",
    "- âœ… éˆæ´»çš„è¼¸å…¥è¼¸å‡ºé•·åº¦\n",
    "\n",
    "**é™åˆ¶**:\n",
    "- âŒ åƒæ•¸é‡å¤§ (éœ€è¦ Encoder + Decoder)\n",
    "- âŒ æ¨è«–é€Ÿåº¦è¼ƒæ…¢\n",
    "- âŒ å° prompt æ ¼å¼æ•æ„Ÿ\n",
    "- âŒ è¨“ç·´æˆæœ¬é«˜"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-5",
   "metadata": {},
   "source": [
    "## 5. ä»»å‹™èˆ‡æ¶æ§‹çš„åŒ¹é… {#5-task-matching}\n",
    "\n",
    "### 5.1 ä»»å‹™åˆ†é¡æ±ºç­–æ¨¹\n",
    "\n",
    "```\n",
    "ä½ çš„ä»»å‹™éœ€è¦ç”Ÿæˆæ–‡æœ¬å—?\n",
    "â”œâ”€ å¦ (ç†è§£ä»»å‹™)\n",
    "â”‚   â”œâ”€ åˆ†é¡ã€NERã€å•ç­” (æŠ½å–å¼) â†’ Encoder-Only (BERT)\n",
    "â”‚   â””â”€ èªç¾©ç›¸ä¼¼åº¦ã€è³‡è¨Šæª¢ç´¢ â†’ Encoder-Only (BERT)\n",
    "â”‚\n",
    "â””â”€ æ˜¯ (ç”Ÿæˆä»»å‹™)\n",
    "    â”œâ”€ è¼¸å…¥å’Œè¼¸å‡ºæ˜¯å¦æœ‰æ˜ç¢ºå°æ‡‰?\n",
    "    â”‚   â”œâ”€ æ˜¯ (ç¿»è­¯ã€æ‘˜è¦ã€è½‰æ›) â†’ Encoder-Decoder (T5/BART)\n",
    "    â”‚   â””â”€ å¦ (çºŒå¯«ã€å°è©±ã€å‰µä½œ) â†’ Decoder-Only (GPT)\n",
    "    â”‚\n",
    "    â””â”€ éœ€è¦æ·±åº¦ç†è§£æºæ–‡æœ¬å—?\n",
    "        â”œâ”€ æ˜¯ (æ‘˜è¦ã€å•ç­”ç”Ÿæˆ) â†’ Encoder-Decoder (T5)\n",
    "        â””â”€ å¦ (è‡ªç”±å‰µä½œ) â†’ Decoder-Only (GPT)\n",
    "```\n",
    "\n",
    "### 5.2 ä»»å‹™-æ¶æ§‹åŒ¹é…è¡¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task-architecture-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‰µå»ºä»»å‹™-æ¶æ§‹åŒ¹é…è¡¨\n",
    "task_data = {\n",
    "    'ä»»å‹™é¡å‹': [\n",
    "        'æƒ…æ„Ÿåˆ†é¡',\n",
    "        'å‘½åå¯¦é«”è­˜åˆ¥',\n",
    "        'å•ç­” (æŠ½å–å¼)',\n",
    "        'èªç¾©ç›¸ä¼¼åº¦',\n",
    "        'æ©Ÿå™¨ç¿»è­¯',\n",
    "        'æ–‡æœ¬æ‘˜è¦',\n",
    "        'å•ç­” (ç”Ÿæˆå¼)',\n",
    "        'æ–‡æœ¬ç”Ÿæˆ',\n",
    "        'å°è©±ç³»çµ±',\n",
    "        'ç¨‹å¼ç¢¼ç”Ÿæˆ'\n",
    "    ],\n",
    "    'Encoder-Only (BERT)': [\n",
    "        'âœ… æœ€ä½³', 'âœ… æœ€ä½³', 'âœ… æœ€ä½³', 'âœ… æœ€ä½³',\n",
    "        'âŒ ä¸é©åˆ', 'âŒ ä¸é©åˆ', 'âš ï¸ å¯è¡Œä½†ä¸ä½³',\n",
    "        'âŒ ä¸é©åˆ', 'âŒ ä¸é©åˆ', 'âŒ ä¸é©åˆ'\n",
    "    ],\n",
    "    'Decoder-Only (GPT)': [\n",
    "        'âš ï¸ éœ€ Prompt', 'âš ï¸ éœ€ Prompt', 'âœ… å¯ç”¨',\n",
    "        'âš ï¸ éœ€ Prompt', 'âœ… å¯ç”¨', 'âœ… å¯ç”¨', 'âœ… æœ€ä½³',\n",
    "        'âœ… æœ€ä½³', 'âœ… æœ€ä½³', 'âœ… æœ€ä½³'\n",
    "    ],\n",
    "    'Encoder-Decoder (T5)': [\n",
    "        'âœ… å¯ç”¨', 'âœ… å¯ç”¨', 'âœ… å¯ç”¨', 'âš ï¸ è¼ƒè¤‡é›œ',\n",
    "        'âœ… æœ€ä½³', 'âœ… æœ€ä½³', 'âœ… æœ€ä½³',\n",
    "        'âœ… å¯ç”¨', 'âœ… å¯ç”¨', 'âš ï¸ è¼ƒè¤‡é›œ'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(task_data)\n",
    "\n",
    "# è¨­ç½®é¡è‰²å°æ‡‰\n",
    "def color_cells(val):\n",
    "    if 'âœ… æœ€ä½³' in val:\n",
    "        return 'background-color: #90EE90; font-weight: bold'\n",
    "    elif 'âœ…' in val:\n",
    "        return 'background-color: #FFFACD'\n",
    "    elif 'âš ï¸' in val:\n",
    "        return 'background-color: #FFE4B5'\n",
    "    elif 'âŒ' in val:\n",
    "        return 'background-color: #FFB6C1'\n",
    "    return ''\n",
    "\n",
    "# é¡¯ç¤ºè¡¨æ ¼\n",
    "styled_df = df.style.applymap(color_cells, subset=df.columns[1:])\n",
    "display(styled_df)\n",
    "\n",
    "print(\"\\nåœ–ä¾‹:\")\n",
    "print(\"âœ… æœ€ä½³ - æ­¤æ¶æ§‹å°ˆç‚ºè©²ä»»å‹™è¨­è¨ˆ\")\n",
    "print(\"âœ… å¯ç”¨ - è¡¨ç¾è‰¯å¥½ï¼Œä½†å¯èƒ½éœ€è¦èª¿æ•´\")\n",
    "print(\"âš ï¸ éœ€ Prompt/è¼ƒè¤‡é›œ - å¯è¡Œä½†éœ€è¦ç‰¹æ®Šè™•ç†\")\n",
    "print(\"âŒ ä¸é©åˆ - æ¶æ§‹é™åˆ¶å°è‡´ç„¡æ³•æœ‰æ•ˆå®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-6",
   "metadata": {},
   "source": [
    "## 6. å¯¦æˆ°å°æ¯”èˆ‡é¸å‹å»ºè­° {#6-comparison}\n",
    "\n",
    "### 6.1 æ•ˆèƒ½å°æ¯” (ç›¸åŒåƒæ•¸é‡ç´š)\n",
    "\n",
    "**æƒ…å¢ƒ**: å‡è¨­éƒ½æ˜¯ ~300M åƒæ•¸çš„æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "performance-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¨¡æ“¬æ•ˆèƒ½å°æ¯”æ•¸æ“š\n",
    "comparison_data = {\n",
    "    'æŒ‡æ¨™': [\n",
    "        'æ–‡æœ¬åˆ†é¡ (GLUE)',\n",
    "        'å‘½åå¯¦é«”è­˜åˆ¥ (CoNLL)',\n",
    "        'å•ç­” (SQuAD 2.0)',\n",
    "        'æ©Ÿå™¨ç¿»è­¯ (BLEU)',\n",
    "        'æ–‡æœ¬æ‘˜è¦ (ROUGE)',\n",
    "        'æ–‡æœ¬ç”Ÿæˆ (äººå·¥è©•åˆ†)',\n",
    "        'è¨“ç·´æ™‚é–“ (ç›¸å°)',\n",
    "        'æ¨è«–é€Ÿåº¦ (tokens/sec)',\n",
    "        'è¨˜æ†¶é«”ä½¿ç”¨ (GB)'\n",
    "    ],\n",
    "    'BERT-Large': [92, 94, 88, 0, 0, 30, 1.0, 850, 12],\n",
    "    'GPT-2-Medium': [85, 78, 82, 68, 72, 95, 0.8, 320, 10],\n",
    "    'T5-Large': [91, 90, 90, 92, 88, 85, 1.5, 280, 18]\n",
    "}\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "\n",
    "# è¦–è¦ºåŒ–\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. ç†è§£ä»»å‹™å°æ¯”\n",
    "understanding_tasks = df_comparison.iloc[:3, :]\n",
    "x = np.arange(len(understanding_tasks['æŒ‡æ¨™']))\n",
    "width = 0.25\n",
    "\n",
    "axes[0, 0].bar(x - width, understanding_tasks['BERT-Large'], width, label='BERT-Large', color='skyblue')\n",
    "axes[0, 0].bar(x, understanding_tasks['GPT-2-Medium'], width, label='GPT-2-Medium', color='lightcoral')\n",
    "axes[0, 0].bar(x + width, understanding_tasks['T5-Large'], width, label='T5-Large', color='lightgreen')\n",
    "axes[0, 0].set_ylabel('åˆ†æ•¸', fontsize=11)\n",
    "axes[0, 0].set_title('ç†è§£ä»»å‹™æ•ˆèƒ½å°æ¯”', fontsize=13, fontweight='bold')\n",
    "axes[0, 0].set_xticks(x)\n",
    "axes[0, 0].set_xticklabels(understanding_tasks['æŒ‡æ¨™'], rotation=15, ha='right')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].set_ylim(0, 100)\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 2. ç”Ÿæˆä»»å‹™å°æ¯”\n",
    "generation_tasks = df_comparison.iloc[3:6, :]\n",
    "x = np.arange(len(generation_tasks['æŒ‡æ¨™']))\n",
    "\n",
    "axes[0, 1].bar(x - width, generation_tasks['BERT-Large'], width, label='BERT-Large', color='skyblue')\n",
    "axes[0, 1].bar(x, generation_tasks['GPT-2-Medium'], width, label='GPT-2-Medium', color='lightcoral')\n",
    "axes[0, 1].bar(x + width, generation_tasks['T5-Large'], width, label='T5-Large', color='lightgreen')\n",
    "axes[0, 1].set_ylabel('åˆ†æ•¸', fontsize=11)\n",
    "axes[0, 1].set_title('ç”Ÿæˆä»»å‹™æ•ˆèƒ½å°æ¯”', fontsize=13, fontweight='bold')\n",
    "axes[0, 1].set_xticks(x)\n",
    "axes[0, 1].set_xticklabels(generation_tasks['æŒ‡æ¨™'], rotation=15, ha='right')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].set_ylim(0, 100)\n",
    "axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 3. æ¨è«–é€Ÿåº¦å°æ¯”\n",
    "speed_data = df_comparison.iloc[7:8, 1:].values[0]\n",
    "models = ['BERT-Large', 'GPT-2-Medium', 'T5-Large']\n",
    "colors_speed = ['skyblue', 'lightcoral', 'lightgreen']\n",
    "\n",
    "axes[1, 0].barh(models, speed_data, color=colors_speed)\n",
    "axes[1, 0].set_xlabel('Tokens/sec', fontsize=11)\n",
    "axes[1, 0].set_title('æ¨è«–é€Ÿåº¦å°æ¯” (è¶Šé«˜è¶Šå¥½)', fontsize=13, fontweight='bold')\n",
    "axes[1, 0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "for i, v in enumerate(speed_data):\n",
    "    axes[1, 0].text(v + 20, i, str(v), va='center', fontweight='bold')\n",
    "\n",
    "# 4. è¨˜æ†¶é«”ä½¿ç”¨å°æ¯”\n",
    "memory_data = df_comparison.iloc[8:9, 1:].values[0]\n",
    "\n",
    "axes[1, 1].barh(models, memory_data, color=colors_speed)\n",
    "axes[1, 1].set_xlabel('è¨˜æ†¶é«” (GB)', fontsize=11)\n",
    "axes[1, 1].set_title('è¨˜æ†¶é«”ä½¿ç”¨å°æ¯” (è¶Šä½è¶Šå¥½)', fontsize=13, fontweight='bold')\n",
    "axes[1, 1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "for i, v in enumerate(memory_data):\n",
    "    axes[1, 1].text(v + 0.5, i, str(v), va='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\né—œéµè§€å¯Ÿ:\")\n",
    "print(\"â€¢ BERT: ç†è§£ä»»å‹™æœ€å¼·ï¼Œæ¨è«–æœ€å¿«ï¼Œä½†ç„¡æ³•ç”Ÿæˆ\")\n",
    "print(\"â€¢ GPT: ç”Ÿæˆä»»å‹™æœ€å¼·ï¼Œä½†ç†è§£ä»»å‹™éœ€è¦ prompt engineering\")\n",
    "print(\"â€¢ T5: å¹³è¡¡å‹é¸æ‰‹ï¼Œé©åˆå¤šä»»å‹™å ´æ™¯ï¼Œä½†è³‡æºæ¶ˆè€—æœ€å¤§\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selection-guide",
   "metadata": {},
   "source": [
    "### 6.2 é¸å‹æ±ºç­–æŒ‡å—\n",
    "\n",
    "#### å ´æ™¯ 1: ä¼æ¥­ç´šæ–‡æœ¬åˆ†é¡ç³»çµ±\n",
    "\n",
    "**éœ€æ±‚**:\n",
    "- å®¢æˆ¶è©•è«–æƒ…æ„Ÿåˆ†æ\n",
    "- éœ€è¦é«˜æº–ç¢ºç‡å’Œå¿«é€Ÿæ¨è«–\n",
    "- è³‡æ–™å·²æ¨™è¨»\n",
    "\n",
    "**æ¨è–¦**: âœ… **BERT / RoBERTa**\n",
    "\n",
    "**ç†ç”±**:\n",
    "1. åˆ†é¡ä»»å‹™æ˜¯ BERT çš„å¼·é …\n",
    "2. å¾®èª¿ç°¡å–®é«˜æ•ˆ\n",
    "3. æ¨è«–é€Ÿåº¦å¿«ï¼Œé©åˆç”Ÿç”¢ç’°å¢ƒ\n",
    "\n",
    "---\n",
    "\n",
    "#### å ´æ™¯ 2: AI å¯«ä½œåŠ©æ‰‹\n",
    "\n",
    "**éœ€æ±‚**:\n",
    "- æ–‡ç« çºŒå¯«ã€å‰µæ„å¯«ä½œ\n",
    "- éœ€è¦è‡ªç„¶æµæš¢çš„ç”Ÿæˆ\n",
    "- æ”¯æ´å¤šç¨®é¢¨æ ¼\n",
    "\n",
    "**æ¨è–¦**: âœ… **GPT-3 / LLaMA / Mistral**\n",
    "\n",
    "**ç†ç”±**:\n",
    "1. ç”Ÿæˆèƒ½åŠ›æœ€å¼·\n",
    "2. Few-shot learning é©æ‡‰ä¸åŒé¢¨æ ¼\n",
    "3. é•·æ–‡æœ¬ç”Ÿæˆé€£è²«æ€§å¥½\n",
    "\n",
    "---\n",
    "\n",
    "#### å ´æ™¯ 3: å¤šèªè¨€æ–°èæ‘˜è¦ç³»çµ±\n",
    "\n",
    "**éœ€æ±‚**:\n",
    "- å°‡é•·æ–°èæ‘˜è¦ç‚ºçŸ­æ¨™é¡Œ\n",
    "- æ”¯æ´å¤šç¨®èªè¨€\n",
    "- éœ€è¦ä¿æŒåŸæ–‡é‡é»\n",
    "\n",
    "**æ¨è–¦**: âœ… **T5 / mBART / Pegasus**\n",
    "\n",
    "**ç†ç”±**:\n",
    "1. Encoder-Decoder å°ˆç‚ºåºåˆ—è½‰æ›è¨­è¨ˆ\n",
    "2. Encoder æ·±åº¦ç†è§£æºæ–‡æœ¬\n",
    "3. Decoder ç”Ÿæˆç°¡æ½”æ‘˜è¦\n",
    "\n",
    "---\n",
    "\n",
    "#### å ´æ™¯ 4: æ™ºèƒ½å®¢æœç³»çµ±\n",
    "\n",
    "**éœ€æ±‚**:\n",
    "- ç†è§£ç”¨æˆ¶æ„åœ–\n",
    "- ç”Ÿæˆåˆé©å›è¦†\n",
    "- éœ€è¦å¤šè¼ªå°è©±èƒ½åŠ›\n",
    "\n",
    "**æ¨è–¦**: âœ… **GPT (å°è¦æ¨¡) + BERT (æ„åœ–åˆ†é¡)**\n",
    "\n",
    "**ç†ç”±**:\n",
    "1. BERT å¿«é€Ÿåˆ†é¡ç”¨æˆ¶æ„åœ–\n",
    "2. GPT ç”Ÿæˆè‡ªç„¶å›è¦†\n",
    "3. æ··åˆæ¶æ§‹å¹³è¡¡æ•ˆèƒ½èˆ‡æˆæœ¬\n",
    "\n",
    "---\n",
    "\n",
    "### 6.3 å¯¦ç”¨é¸å‹æª¢æŸ¥è¡¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selection-checklist",
   "metadata": {},
   "outputs": [],
   "source": [
    "def architecture_selector():\n",
    "    \"\"\"\n",
    "    äº’å‹•å¼æ¶æ§‹é¸æ“‡å·¥å…·\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"        Transformer æ¶æ§‹é¸æ“‡åŠ©æ‰‹\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "    \n",
    "    questions = [\n",
    "        {\n",
    "            'q': 'ä½ çš„ä»»å‹™éœ€è¦ç”Ÿæˆæ–‡æœ¬å—ï¼Ÿ',\n",
    "            'options': ['æ˜¯ - éœ€è¦ç”Ÿæˆ', 'å¦ - åªéœ€ç†è§£'],\n",
    "            'scores': {'BERT': [0, 2], 'GPT': [2, 0], 'T5': [1, 1]}\n",
    "        },\n",
    "        {\n",
    "            'q': 'è¼¸å…¥å’Œè¼¸å‡ºæ˜¯å¦æœ‰æ˜ç¢ºå°æ‡‰é—œä¿‚ï¼Ÿ(å¦‚ç¿»è­¯ã€æ‘˜è¦)',\n",
    "            'options': ['æ˜¯ - æœ‰å°æ‡‰', 'å¦ - è‡ªç”±ç”Ÿæˆ'],\n",
    "            'scores': {'BERT': [1, 0], 'GPT': [1, 2], 'T5': [2, 1]}\n",
    "        },\n",
    "        {\n",
    "            'q': 'æ¨è«–é€Ÿåº¦æ˜¯å¦ç‚ºé—œéµè€ƒé‡ï¼Ÿ',\n",
    "            'options': ['æ˜¯ - éœ€è¦å¿«é€Ÿæ¨è«–', 'å¦ - å“è³ªå„ªå…ˆ'],\n",
    "            'scores': {'BERT': [2, 1], 'GPT': [1, 1], 'T5': [0, 2]}\n",
    "        },\n",
    "        {\n",
    "            'q': 'æ˜¯å¦æœ‰å¤§é‡æ¨™è¨»æ•¸æ“šå¯ä¾›å¾®èª¿ï¼Ÿ',\n",
    "            'options': ['æ˜¯ - æœ‰æ¨™è¨»æ•¸æ“š', 'å¦ - æ•¸æ“šç¨€ç¼º'],\n",
    "            'scores': {'BERT': [2, 0], 'GPT': [1, 2], 'T5': [2, 1]}\n",
    "        },\n",
    "        {\n",
    "            'q': 'éƒ¨ç½²è³‡æºé™åˆ¶å¦‚ä½•ï¼Ÿ',\n",
    "            'options': ['å—é™ - éœ€è¦å°æ¨¡å‹', 'å……è¶³ - å¯ç”¨å¤§æ¨¡å‹'],\n",
    "            'scores': {'BERT': [2, 1], 'GPT': [1, 1], 'T5': [0, 2]}\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # æ¨¡æ“¬ç”¨æˆ¶é¸æ“‡ (å¯¦éš›ä½¿ç”¨æ™‚å¯ä»¥æ”¹ç‚º input())\n",
    "    user_choices = [0, 0, 0, 0, 1]  # ç¯„ä¾‹é¸æ“‡\n",
    "    \n",
    "    scores = {'BERT': 0, 'GPT': 0, 'T5': 0}\n",
    "    \n",
    "    for i, (question, choice) in enumerate(zip(questions, user_choices)):\n",
    "        print(f\"Q{i+1}. {question['q']}\")\n",
    "        print(f\"   é¸æ“‡: {question['options'][choice]}\")\n",
    "        print()\n",
    "        \n",
    "        for arch in scores:\n",
    "            scores[arch] += question['scores'][arch][choice]\n",
    "    \n",
    "    # æ’åºçµæœ\n",
    "    sorted_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"æ¨è–¦çµæœ:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for rank, (arch, score) in enumerate(sorted_scores, 1):\n",
    "        stars = 'â­' * score\n",
    "        print(f\"{rank}. {arch:8s} - {stars} ({score} åˆ†)\")\n",
    "    \n",
    "    best = sorted_scores[0][0]\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"âœ… å»ºè­°ä½¿ç”¨: {best}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    recommendations = {\n",
    "        'BERT': 'é©åˆåˆ†é¡ã€NERã€å•ç­”ç­‰ç†è§£ä»»å‹™ï¼Œæ¨è«–å¿«é€Ÿï¼Œæ˜“æ–¼å¾®èª¿',\n",
    "        'GPT': 'é©åˆæ–‡æœ¬ç”Ÿæˆã€å°è©±ç³»çµ±ï¼Œæ”¯æ´ few-shot learning',\n",
    "        'T5': 'é©åˆç¿»è­¯ã€æ‘˜è¦ç­‰åºåˆ—è½‰æ›ä»»å‹™ï¼Œçµ±ä¸€å¤šä»»å‹™æ¡†æ¶'\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nç†ç”±: {recommendations[best]}\")\n",
    "\n",
    "architecture_selector()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-7",
   "metadata": {},
   "source": [
    "## 7. ç¸½çµ {#7-summary}\n",
    "\n",
    "### 7.1 ä¸‰å¤§æ¶æ§‹æ ¸å¿ƒè¦é»\n",
    "\n",
    "| æ¶æ§‹ | æ ¸å¿ƒå„ªå‹¢ | æœ€ä½³æ‡‰ç”¨ | ä»£è¡¨æ¨¡å‹ |\n",
    "|------|----------|----------|----------|\n",
    "| **Encoder-Only** | é›™å‘ç†è§£ã€å¿«é€Ÿæ¨è«– | åˆ†é¡ã€NERã€å•ç­” | BERT, RoBERTa |\n",
    "| **Decoder-Only** | å¼·å¤§ç”Ÿæˆã€Few-shot | æ–‡æœ¬ç”Ÿæˆã€å°è©± | GPT-3, LLaMA |\n",
    "| **Encoder-Decoder** | å¹³è¡¡ç†è§£èˆ‡ç”Ÿæˆ | ç¿»è­¯ã€æ‘˜è¦ | T5, BART |\n",
    "\n",
    "### 7.2 é¸å‹ä¸‰åŸå‰‡\n",
    "\n",
    "1. **ä»»å‹™å°å‘**: å…ˆç¢ºå®šä»»å‹™é¡å‹(ç†è§£ vs ç”Ÿæˆ vs è½‰æ›)\n",
    "2. **è³‡æºè€ƒé‡**: è©•ä¼°è¨ˆç®—è³‡æºã€æ¨è«–å»¶é²éœ€æ±‚\n",
    "3. **æ•¸æ“šæƒ…æ³**: è€ƒæ…®æ¨™è¨»æ•¸æ“šé‡ã€æ˜¯å¦éœ€è¦ few-shot\n",
    "\n",
    "### 7.3 æœªä¾†è¶¨å‹¢\n",
    "\n",
    "**æ¶æ§‹èåˆ**:\n",
    "- **Instruction-tuned Models**: GPT-3.5, ChatGPT, Claude\n",
    "  - Decoder-Only æ¶æ§‹ + æŒ‡ä»¤å¾®èª¿\n",
    "  - åŒæ™‚å…·å‚™ç†è§£èˆ‡ç”Ÿæˆèƒ½åŠ›\n",
    "\n",
    "- **Mixture of Experts (MoE)**: Mistral 8x7B\n",
    "  - ç¨€ç–æ¿€æ´»é™ä½æ¨è«–æˆæœ¬\n",
    "  - ä¿æŒå¤§æ¨¡å‹èƒ½åŠ›\n",
    "\n",
    "- **Multimodal Transformers**: GPT-4, Gemini\n",
    "  - è·¨æ¨¡æ…‹ç†è§£(æ–‡æœ¬ã€åœ–åƒã€éŸ³è¨Š)\n",
    "\n",
    "**å¯¦ç”¨å»ºè­°**:\n",
    "\n",
    "âœ… **2024 å¹´çš„æœ€ä½³å¯¦è¸**:\n",
    "1. **ç†è§£ä»»å‹™**: å„ªå…ˆä½¿ç”¨ DeBERTa, RoBERTa (æ•ˆç‡é«˜)\n",
    "2. **ç”Ÿæˆä»»å‹™**: ä½¿ç”¨é–‹æº LLM (LLaMA 2, Mistral) + Fine-tuning\n",
    "3. **è½‰æ›ä»»å‹™**: T5 æˆ– BART (å°ˆæ¥­é ˜åŸŸå¯å¾®èª¿)\n",
    "4. **é€šç”¨å ´æ™¯**: è€ƒæ…® API (GPT-4, Claude) vs è‡ªå»ºæ¨¡å‹çš„æˆæœ¬\n",
    "\n",
    "### 7.4 å»¶ä¼¸é–±è®€\n",
    "\n",
    "1. **è«–æ–‡**:\n",
    "   - [BERT: Pre-training of Deep Bidirectional Transformers](https://arxiv.org/abs/1810.04805)\n",
    "   - [Language Models are Unsupervised Multitask Learners (GPT-2)](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
    "   - [Exploring the Limits of Transfer Learning with T5](https://arxiv.org/abs/1910.10683)\n",
    "\n",
    "2. **å¯¦æˆ°è³‡æº**:\n",
    "   - [Hugging Face Model Hub](https://huggingface.co/models)\n",
    "   - [Papers with Code - NLP Leaderboards](https://paperswithcode.com/area/natural-language-processing)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ ä¸‹ä¸€ç¯€é å‘Š\n",
    "\n",
    "**CH07-07: å¤§å‹èªè¨€æ¨¡å‹ (LLMs)**\n",
    "- LLM çš„è¦æ¨¡å®šå¾‹ (Scaling Laws)\n",
    "- Instruction Tuning èˆ‡ RLHF\n",
    "- Prompt Engineering æŠ€å·§\n",
    "- In-Context Learning åŸç†\n",
    "- LLM çš„èƒ½åŠ›é‚Šç•Œèˆ‡å±€é™\n",
    "\n",
    "---\n",
    "\n",
    "**èª²ç¨‹å®Œæˆæ™‚é–“**: `____å¹´____æœˆ____æ—¥`  \n",
    "**å­¸ç¿’å¿ƒå¾—**: ___________________________________"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
