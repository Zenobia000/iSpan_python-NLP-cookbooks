{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH07-03: 注意力機制 (Attention Mechanism)\n",
    "\n",
    "**課程目標:**\n",
    "- 掌握 Scaled Dot-Product Attention 的數學原理與實作\n",
    "- 理解 Multi-Head Attention 的設計動機與優勢\n",
    "- 實作完整可運行的 Attention 模組\n",
    "- 學會 Attention 權重的視覺化與解釋\n",
    "- 了解 Masked Attention 在 Decoder 中的應用\n",
    "\n",
    "**學習時間:** 約 120 分鐘\n",
    "\n",
    "**前置知識:**\n",
    "- 線性代數基礎 (矩陣乘法)\n",
    "- Softmax 函數\n",
    "- Transformer 架構概覽 (CH07-01)\n",
    "- 詞嵌入基礎 (CH07-02)\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 目錄\n",
    "\n",
    "1. [Attention 機制的直覺理解](#1)\n",
    "2. [Scaled Dot-Product Attention 數學推導](#2)\n",
    "3. [從零實作 Self-Attention](#3)\n",
    "4. [Multi-Head Attention 深入](#4)\n",
    "5. [Masked Attention 機制](#5)\n",
    "6. [Attention 視覺化與解釋性](#6)\n",
    "7. [完整 Attention 模組實作](#7)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 環境設定與套件導入\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Optional, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 設定中文顯示\n",
    "plt.rcParams['font.sans-serif'] = ['Microsoft JhengHei', 'SimHei', 'Arial Unicode MS']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 設定隨機種子\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"✅ 環境設定完成\")\n",
    "print(f\"NumPy 版本: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "## 1. Attention 機制的直覺理解\n",
    "\n",
    "### 1.1 從人類注意力到機器注意力\n",
    "\n",
    "#### 人類閱讀的例子:\n",
    "\n",
    "句子: *\"The **animal** didn't cross the street because **it** was too tired.\"*\n",
    "\n",
    "問: \"it\" 指什麼?\n",
    "\n",
    "**人類的思考過程:**\n",
    "1. 看到 \"it\"\n",
    "2. 向前搜尋可能的指代對象\n",
    "3. \"animal\" 最相關 ✅\n",
    "4. \"street\" 不太相關 ❌\n",
    "\n",
    "**這就是 Attention!**\n",
    "\n",
    "---\n",
    "\n",
    "### 1.2 Attention 的核心思想\n",
    "\n",
    "**定義:** 給定一個查詢 (Query),從一組鍵值對 (Key-Value pairs) 中找到最相關的訊息\n",
    "\n",
    "```\n",
    "Query:  \"it\" 是什麼?\n",
    "  ↓\n",
    "Key:    \"The\"   \"animal\"  \"didn't\"  \"cross\"  \"street\" ...\n",
    "  ↓       ↓         ↓          ↓         ↓         ↓\n",
    "Score:  0.05     0.82       0.03      0.01      0.09   ... ← 相關度\n",
    "  ↓\n",
    "Value:  [vec1]   [vec2]     [vec3]    [vec4]    [vec5] ...\n",
    "  ↓\n",
    "Output: 0.05*vec1 + 0.82*vec2 + 0.03*vec3 + ...  ← 加權組合\n",
    "```\n",
    "\n",
    "**三個關鍵矩陣:**\n",
    "- **Query (Q)**: \"我想查詢什麼?\"\n",
    "- **Key (K)**: \"我是什麼?\"\n",
    "- **Value (V)**: \"我包含什麼訊息?\"\n",
    "\n",
    "---\n",
    "\n",
    "### 1.3 Self-Attention vs Cross-Attention\n",
    "\n",
    "#### Self-Attention (自注意力)\n",
    "```python\n",
    "Q, K, V 來自同一個序列\n",
    "句子關注自己: \"The cat sat\" → 每個詞關注其他詞\n",
    "```\n",
    "\n",
    "#### Cross-Attention (交叉注意力)\n",
    "```python\n",
    "Q 來自一個序列, K/V 來自另一個序列\n",
    "翻譯: \"Le chat\" (法文) → \"The cat\" (英文)\n",
    "      Q (英文)    K/V (法文)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 直覺示範: 模擬 Attention 分數計算\n",
    "sentence = [\"The\", \"animal\", \"didn't\", \"cross\", \"the\", \"street\", \"because\", \"it\", \"was\", \"tired\"]\n",
    "query_word = \"it\"  # 我們想知道 \"it\" 關注哪些詞\n",
    "\n",
    "# 手工設定相關性分數 (實際由神經網路學習)\n",
    "relevance_scores = {\n",
    "    \"The\": 0.05,\n",
    "    \"animal\": 0.82,  # 最相關!\n",
    "    \"didn't\": 0.03,\n",
    "    \"cross\": 0.01,\n",
    "    \"the\": 0.04,\n",
    "    \"street\": 0.09,\n",
    "    \"because\": 0.05,\n",
    "    \"it\": 0.15,      # 自己也有點相關\n",
    "    \"was\": 0.08,\n",
    "    \"tired\": 0.12,\n",
    "}\n",
    "\n",
    "# Softmax 歸一化 (確保總和為 1)\n",
    "scores = np.array(list(relevance_scores.values()))\n",
    "attention_weights = np.exp(scores) / np.sum(np.exp(scores))\n",
    "\n",
    "# 視覺化\n",
    "plt.figure(figsize=(14, 6))\n",
    "colors = ['red' if word == 'animal' else 'steelblue' for word in sentence]\n",
    "bars = plt.bar(sentence, attention_weights, color=colors, alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('Words in Sentence', fontsize=12)\n",
    "plt.ylabel('Attention Weight', fontsize=12)\n",
    "plt.title(f'Attention 權重分布: \"{query_word}\" 關注序列中哪些詞?', fontsize=14, fontweight='bold')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 標註最高分\n",
    "max_idx = np.argmax(attention_weights)\n",
    "plt.annotate(f'最高關注\\n{attention_weights[max_idx]:.2%}', \n",
    "             xy=(max_idx, attention_weights[max_idx]),\n",
    "             xytext=(max_idx, attention_weights[max_idx] + 0.1),\n",
    "             fontsize=11, fontweight='bold', color='red',\n",
    "             arrowprops=dict(arrowstyle='->', color='red', lw=2))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n🎯 '{query_word}' 最關注: '{sentence[max_idx]}' (權重: {attention_weights[max_idx]:.2%})\")\n",
    "print(f\"\\n✅ 權重總和驗證: {attention_weights.sum():.6f} (應為 1.0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "## 2. Scaled Dot-Product Attention 數學推導\n",
    "\n",
    "### 2.1 完整公式\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V\n",
    "$$\n",
    "\n",
    "其中:\n",
    "- $Q \\in \\mathbb{R}^{n \\times d_k}$: Query 矩陣 (n 個查詢,每個 $d_k$ 維)\n",
    "- $K \\in \\mathbb{R}^{m \\times d_k}$: Key 矩陣 (m 個鍵)\n",
    "- $V \\in \\mathbb{R}^{m \\times d_v}$: Value 矩陣 (m 個值,每個 $d_v$ 維)\n",
    "- $d_k$: Key/Query 的維度\n",
    "- $\\sqrt{d_k}$: 縮放因子\n",
    "\n",
    "---\n",
    "\n",
    "### 2.2 逐步計算過程\n",
    "\n",
    "#### Step 1: 計算相似度分數\n",
    "\n",
    "$$\n",
    "\\text{Scores} = QK^T \\in \\mathbb{R}^{n \\times m}\n",
    "$$\n",
    "\n",
    "- 使用**點積** (Dot Product) 衡量相似度\n",
    "- 分數越高表示越相關\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 2: 縮放 (Scaling)\n",
    "\n",
    "$$\n",
    "\\text{Scaled Scores} = \\frac{QK^T}{\\sqrt{d_k}}\n",
    "$$\n",
    "\n",
    "**為什麼需要縮放?**\n",
    "\n",
    "當 $d_k$ 很大時,點積結果可能非常大:\n",
    "\n",
    "```python\n",
    "d_k = 512 時:\n",
    "QK^T 可能達到 ±50 以上\n",
    "  ↓\n",
    "Softmax 會進入飽和區 (梯度接近 0)\n",
    "  ↓\n",
    "訓練困難!\n",
    "```\n",
    "\n",
    "除以 $\\sqrt{d_k}$ 可以穩定梯度:\n",
    "\n",
    "```python\n",
    "50 / √512 ≈ 2.2  ← 合理範圍\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 3: Softmax 歸一化\n",
    "\n",
    "$$\n",
    "\\text{Attention Weights} = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)\n",
    "$$\n",
    "\n",
    "- 將分數轉換為概率分布 (總和為 1)\n",
    "- 每一行獨立做 Softmax\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 4: 加權組合 Value\n",
    "\n",
    "$$\n",
    "\\text{Output} = \\text{Attention Weights} \\cdot V \\in \\mathbb{R}^{n \\times d_v}\n",
    "$$\n",
    "\n",
    "- 使用 Attention 權重對 Value 進行加權平均\n",
    "- 最終輸出融合了所有相關位置的訊息\n",
    "\n",
    "---\n",
    "\n",
    "### 2.3 矩陣維度分析\n",
    "\n",
    "```\n",
    "假設: 句子長度 seq_len=6, 嵌入維度 d_model=512\n",
    "\n",
    "Q: (6, 64)   ← batch 內每個詞的 Query\n",
    "K: (6, 64)   ← batch 內每個詞的 Key\n",
    "V: (6, 64)   ← batch 內每個詞的 Value\n",
    "\n",
    "QK^T: (6, 64) × (64, 6) = (6, 6)  ← 相似度矩陣\n",
    "       ↓ Softmax\n",
    "Weights: (6, 6)  ← Attention 權重矩陣\n",
    "       ↓ × V\n",
    "Output: (6, 6) × (6, 64) = (6, 64)  ← 最終輸出\n",
    "```\n",
    "\n",
    "**觀察:** 輸出形狀與輸入相同!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化: Attention 計算流程\n",
    "def visualize_attention_flow():\n",
    "    \"\"\"視覺化 Attention 的四個步驟\"\"\"\n",
    "    \n",
    "    # 模擬輸入\n",
    "    seq_len = 4\n",
    "    d_k = 3\n",
    "    \n",
    "    Q = np.random.randn(seq_len, d_k)\n",
    "    K = np.random.randn(seq_len, d_k)\n",
    "    V = np.random.randn(seq_len, d_k)\n",
    "    \n",
    "    # Step 1: QK^T\n",
    "    scores = np.dot(Q, K.T)\n",
    "    \n",
    "    # Step 2: Scaling\n",
    "    scaled_scores = scores / np.sqrt(d_k)\n",
    "    \n",
    "    # Step 3: Softmax\n",
    "    attention_weights = np.exp(scaled_scores) / np.sum(np.exp(scaled_scores), axis=-1, keepdims=True)\n",
    "    \n",
    "    # Step 4: Weighted Sum\n",
    "    output = np.dot(attention_weights, V)\n",
    "    \n",
    "    # 視覺化四個步驟\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "    \n",
    "    # Step 1\n",
    "    sns.heatmap(scores, annot=True, fmt='.2f', cmap='coolwarm', ax=axes[0, 0], cbar_kws={'label': 'Score'})\n",
    "    axes[0, 0].set_title('Step 1: QK^T (相似度矩陣)', fontsize=12, fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Key Index')\n",
    "    axes[0, 0].set_ylabel('Query Index')\n",
    "    \n",
    "    # Step 2\n",
    "    sns.heatmap(scaled_scores, annot=True, fmt='.2f', cmap='coolwarm', ax=axes[0, 1], cbar_kws={'label': 'Scaled Score'})\n",
    "    axes[0, 1].set_title(f'Step 2: 縮放 (÷√{d_k})', fontsize=12, fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Key Index')\n",
    "    axes[0, 1].set_ylabel('Query Index')\n",
    "    \n",
    "    # Step 3\n",
    "    sns.heatmap(attention_weights, annot=True, fmt='.2f', cmap='YlOrRd', ax=axes[1, 0], \n",
    "                cbar_kws={'label': 'Probability'}, vmin=0, vmax=1)\n",
    "    axes[1, 0].set_title('Step 3: Softmax (歸一化)', fontsize=12, fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Key Index')\n",
    "    axes[1, 0].set_ylabel('Query Index')\n",
    "    \n",
    "    # Step 4\n",
    "    sns.heatmap(output.T, annot=True, fmt='.2f', cmap='viridis', ax=axes[1, 1], cbar_kws={'label': 'Value'})\n",
    "    axes[1, 1].set_title('Step 4: Attention Weights × V (輸出)', fontsize=12, fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('Query Index')\n",
    "    axes[1, 1].set_ylabel('Dimension')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 驗證\n",
    "    print(\"\\n✅ 驗證:\")\n",
    "    print(f\"  Step 3 每行總和: {attention_weights.sum(axis=1)}  ← 應全為 1.0\")\n",
    "    print(f\"  輸入形狀: Q{Q.shape}, K{K.shape}, V{V.shape}\")\n",
    "    print(f\"  輸出形狀: {output.shape}  ← 與 Q 相同!\")\n",
    "\n",
    "visualize_attention_flow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "## 3. 從零實作 Self-Attention\n",
    "\n",
    "### 3.1 基礎版本 (無投影矩陣)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(\n",
    "    Q: np.ndarray,\n",
    "    K: np.ndarray,\n",
    "    V: np.ndarray,\n",
    "    mask: Optional[np.ndarray] = None\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Scaled Dot-Product Attention 完整實作\n",
    "    \n",
    "    Args:\n",
    "        Q: Query 矩陣, shape (seq_len_q, d_k)\n",
    "        K: Key 矩陣, shape (seq_len_k, d_k)\n",
    "        V: Value 矩陣, shape (seq_len_k, d_v)\n",
    "        mask: 可選的遮罩矩陣, shape (seq_len_q, seq_len_k)\n",
    "    \n",
    "    Returns:\n",
    "        output: Attention 輸出, shape (seq_len_q, d_v)\n",
    "        attention_weights: Attention 權重, shape (seq_len_q, seq_len_k)\n",
    "    \"\"\"\n",
    "    # 獲取 Key 的維度\n",
    "    d_k = K.shape[-1]\n",
    "    \n",
    "    # Step 1: 計算相似度分數 QK^T\n",
    "    scores = np.matmul(Q, K.T)  # (seq_len_q, seq_len_k)\n",
    "    \n",
    "    # Step 2: 縮放\n",
    "    scaled_scores = scores / np.sqrt(d_k)\n",
    "    \n",
    "    # (可選) 應用遮罩 (將遮罩位置設為極小值)\n",
    "    if mask is not None:\n",
    "        scaled_scores = np.where(mask == 0, -1e9, scaled_scores)\n",
    "    \n",
    "    # Step 3: Softmax 歸一化\n",
    "    attention_weights = np.exp(scaled_scores) / np.sum(\n",
    "        np.exp(scaled_scores), axis=-1, keepdims=True\n",
    "    )\n",
    "    \n",
    "    # Step 4: 加權組合 Value\n",
    "    output = np.matmul(attention_weights, V)  # (seq_len_q, d_v)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "\n",
    "# 測試\n",
    "print(\"🧪 測試 Scaled Dot-Product Attention\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "seq_len = 5\n",
    "d_k = 64\n",
    "d_v = 64\n",
    "\n",
    "# 模擬輸入\n",
    "Q = np.random.randn(seq_len, d_k)\n",
    "K = np.random.randn(seq_len, d_k)\n",
    "V = np.random.randn(seq_len, d_v)\n",
    "\n",
    "# 計算 Attention\n",
    "output, attn_weights = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "print(f\"\\n輸入:\")\n",
    "print(f\"  Q 形狀: {Q.shape}\")\n",
    "print(f\"  K 形狀: {K.shape}\")\n",
    "print(f\"  V 形狀: {V.shape}\")\n",
    "\n",
    "print(f\"\\n輸出:\")\n",
    "print(f\"  Output 形狀: {output.shape}\")\n",
    "print(f\"  Attention Weights 形狀: {attn_weights.shape}\")\n",
    "\n",
    "print(f\"\\n✅ 驗證:\")\n",
    "print(f\"  每行 Attention 權重總和: {attn_weights.sum(axis=1)}\")\n",
    "print(f\"  (應全為 1.0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化 Attention 權重矩陣\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(attn_weights, annot=True, fmt='.3f', cmap='YlOrRd',\n",
    "            xticklabels=[f'Key {i+1}' for i in range(seq_len)],\n",
    "            yticklabels=[f'Query {i+1}' for i in range(seq_len)],\n",
    "            cbar_kws={'label': 'Attention Score'})\n",
    "plt.xlabel('Key (被關注的位置)', fontsize=12)\n",
    "plt.ylabel('Query (查詢位置)', fontsize=12)\n",
    "plt.title('Self-Attention 權重矩陣\\n(每行總和為 1)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n📊 解讀:\")\n",
    "print(\"  - 每行代表一個 Query 對所有 Key 的關注分布\")\n",
    "print(\"  - 顏色越深 (紅色) 表示關注度越高\")\n",
    "print(\"  - 對角線通常較亮: 每個詞會關注自己\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 完整版本 (含投影矩陣)\n",
    "\n",
    "實際 Transformer 中, Q, K, V 是通過**線性投影**獲得:\n",
    "\n",
    "$$\n",
    "Q = XW^Q, \\quad K = XW^K, \\quad V = XW^V\n",
    "$$\n",
    "\n",
    "其中:\n",
    "- $X \\in \\mathbb{R}^{seq\\_len \\times d_{model}}$: 輸入 (嵌入向量)\n",
    "- $W^Q, W^K \\in \\mathbb{R}^{d_{model} \\times d_k}$: Query/Key 投影矩陣\n",
    "- $W^V \\in \\mathbb{R}^{d_{model} \\times d_v}$: Value 投影矩陣\n",
    "\n",
    "**為什麼需要投影?**\n",
    "- 學習不同的查詢/鍵/值表示\n",
    "- 降維 (通常 $d_k = d_v = d_{model} / h$, h 為頭數)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention:\n",
    "    \"\"\"\n",
    "    完整 Self-Attention 層實作 (含投影矩陣)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, d_k: int, d_v: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: 輸入嵌入維度\n",
    "            d_k: Query/Key 維度\n",
    "            d_v: Value 維度\n",
    "        \"\"\"\n",
    "        self.d_k = d_k\n",
    "        \n",
    "        # 初始化投影矩陣 (Xavier 初始化)\n",
    "        self.W_q = np.random.randn(d_model, d_k) / np.sqrt(d_model)\n",
    "        self.W_k = np.random.randn(d_model, d_k) / np.sqrt(d_model)\n",
    "        self.W_v = np.random.randn(d_model, d_v) / np.sqrt(d_model)\n",
    "    \n",
    "    def forward(self, X: np.ndarray, mask: Optional[np.ndarray] = None):\n",
    "        \"\"\"\n",
    "        前向傳播\n",
    "        \n",
    "        Args:\n",
    "            X: 輸入, shape (seq_len, d_model)\n",
    "            mask: 可選遮罩, shape (seq_len, seq_len)\n",
    "            \n",
    "        Returns:\n",
    "            output: shape (seq_len, d_v)\n",
    "            attention_weights: shape (seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        # 線性投影獲得 Q, K, V\n",
    "        Q = np.dot(X, self.W_q)  # (seq_len, d_k)\n",
    "        K = np.dot(X, self.W_k)  # (seq_len, d_k)\n",
    "        V = np.dot(X, self.W_v)  # (seq_len, d_v)\n",
    "        \n",
    "        # 計算 Scaled Dot-Product Attention\n",
    "        output, attention_weights = scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        return output, attention_weights\n",
    "    \n",
    "    def __call__(self, X, mask=None):\n",
    "        return self.forward(X, mask)\n",
    "\n",
    "\n",
    "# 測試完整版 Self-Attention\n",
    "print(\"🧪 測試完整 Self-Attention 層\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "seq_len = 6\n",
    "d_model = 512\n",
    "d_k = d_v = 64\n",
    "\n",
    "# 創建 Self-Attention 層\n",
    "self_attn = SelfAttention(d_model, d_k, d_v)\n",
    "\n",
    "# 模擬輸入 (例如詞嵌入 + 位置編碼)\n",
    "X = np.random.randn(seq_len, d_model)\n",
    "\n",
    "# 前向傳播\n",
    "output, attn_weights = self_attn(X)\n",
    "\n",
    "print(f\"\\n輸入形狀: {X.shape}\")\n",
    "print(f\"輸出形狀: {output.shape}\")\n",
    "print(f\"Attention 權重形狀: {attn_weights.shape}\")\n",
    "\n",
    "print(f\"\\n參數量:\")\n",
    "print(f\"  W_q: {self_attn.W_q.shape} = {np.prod(self_attn.W_q.shape):,} 參數\")\n",
    "print(f\"  W_k: {self_attn.W_k.shape} = {np.prod(self_attn.W_k.shape):,} 參數\")\n",
    "print(f\"  W_v: {self_attn.W_v.shape} = {np.prod(self_attn.W_v.shape):,} 參數\")\n",
    "total_params = np.prod(self_attn.W_q.shape) + np.prod(self_attn.W_k.shape) + np.prod(self_attn.W_v.shape)\n",
    "print(f\"  總計: {total_params:,} 參數\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "## 4. Multi-Head Attention 深入\n",
    "\n",
    "### 4.1 為什麼需要多頭?\n",
    "\n",
    "**問題:** 單個 Attention 頭可能無法同時捕捉多種關係\n",
    "\n",
    "#### 句子中的多種關係:\n",
    "\n",
    "```\n",
    "\"The cat sat on the mat because it was tired.\"\n",
    "\n",
    "語法關係:\n",
    "  cat ← sat (主詞-動詞)\n",
    "  sat → mat (動詞-受詞)\n",
    "\n",
    "語義關係:\n",
    "  it → cat (指代關係)\n",
    "  tired → cat (狀態描述)\n",
    "\n",
    "位置關係:\n",
    "  cat ← 鄰近詞\n",
    "```\n",
    "\n",
    "**解決方案:** 使用多個頭,各自學習不同關係!\n",
    "\n",
    "---\n",
    "\n",
    "### 4.2 Multi-Head Attention 公式\n",
    "\n",
    "$$\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1, ..., head_h) W^O\n",
    "$$\n",
    "\n",
    "其中每個頭:\n",
    "\n",
    "$$\n",
    "head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n",
    "$$\n",
    "\n",
    "**參數:**\n",
    "- $h$: 頭數 (論文中 = 8)\n",
    "- $W_i^Q, W_i^K \\in \\mathbb{R}^{d_{model} \\times d_k}$: 第 i 個頭的 Q/K 投影\n",
    "- $W_i^V \\in \\mathbb{R}^{d_{model} \\times d_v}$: 第 i 個頭的 V 投影\n",
    "- $W^O \\in \\mathbb{R}^{hd_v \\times d_{model}}$: 輸出投影\n",
    "\n",
    "**論文設定:**\n",
    "- $d_{model} = 512$\n",
    "- $h = 8$\n",
    "- $d_k = d_v = d_{model} / h = 64$\n",
    "\n",
    "---\n",
    "\n",
    "### 4.3 為什麼維度是 d_model / h?\n",
    "\n",
    "**計算成本考量:**\n",
    "\n",
    "```\n",
    "單頭 Attention (d_k = 512):\n",
    "  計算成本: O(seq_len^2 × 512)\n",
    "\n",
    "多頭 Attention (8 頭, d_k = 64):\n",
    "  每個頭: O(seq_len^2 × 64)\n",
    "  總成本: 8 × O(seq_len^2 × 64) = O(seq_len^2 × 512)\n",
    "```\n",
    "\n",
    "**結論:** 計算成本相同,但表達能力更強!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention:\n",
    "    \"\"\"\n",
    "    Multi-Head Self-Attention 完整實作\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, num_heads: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: 模型維度 (嵌入維度)\n",
    "            num_heads: 頭數\n",
    "        \"\"\"\n",
    "        assert d_model % num_heads == 0, \"d_model 必須可被 num_heads 整除\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads  # 每個頭的維度\n",
    "        \n",
    "        # 每個頭的投影矩陣\n",
    "        self.W_q = np.random.randn(num_heads, d_model, self.d_k) / np.sqrt(d_model)\n",
    "        self.W_k = np.random.randn(num_heads, d_model, self.d_k) / np.sqrt(d_model)\n",
    "        self.W_v = np.random.randn(num_heads, d_model, self.d_k) / np.sqrt(d_model)\n",
    "        \n",
    "        # 輸出投影矩陣\n",
    "        self.W_o = np.random.randn(d_model, d_model) / np.sqrt(d_model)\n",
    "    \n",
    "    def forward(self, X: np.ndarray, mask: Optional[np.ndarray] = None):\n",
    "        \"\"\"\n",
    "        前向傳播\n",
    "        \n",
    "        Args:\n",
    "            X: 輸入, shape (seq_len, d_model)\n",
    "            mask: 可選遮罩, shape (seq_len, seq_len)\n",
    "            \n",
    "        Returns:\n",
    "            output: shape (seq_len, d_model)\n",
    "            all_attention_weights: list of (seq_len, seq_len), 每個頭的權重\n",
    "        \"\"\"\n",
    "        seq_len = X.shape[0]\n",
    "        head_outputs = []\n",
    "        all_attention_weights = []\n",
    "        \n",
    "        # 對每個頭分別計算\n",
    "        for i in range(self.num_heads):\n",
    "            # 投影到 Q, K, V\n",
    "            Q = np.dot(X, self.W_q[i])  # (seq_len, d_k)\n",
    "            K = np.dot(X, self.W_k[i])\n",
    "            V = np.dot(X, self.W_v[i])\n",
    "            \n",
    "            # 計算 Attention\n",
    "            head_output, attn_weights = scaled_dot_product_attention(Q, K, V, mask)\n",
    "            \n",
    "            head_outputs.append(head_output)\n",
    "            all_attention_weights.append(attn_weights)\n",
    "        \n",
    "        # Concatenate all heads\n",
    "        multi_head_output = np.concatenate(head_outputs, axis=-1)  # (seq_len, num_heads * d_k)\n",
    "        \n",
    "        # 最終線性投影\n",
    "        output = np.dot(multi_head_output, self.W_o)  # (seq_len, d_model)\n",
    "        \n",
    "        return output, all_attention_weights\n",
    "    \n",
    "    def __call__(self, X, mask=None):\n",
    "        return self.forward(X, mask)\n",
    "\n",
    "\n",
    "# 測試 Multi-Head Attention\n",
    "print(\"🧪 測試 Multi-Head Attention\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "seq_len = 6\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "\n",
    "# 創建 Multi-Head Attention 層\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "# 模擬輸入\n",
    "X = np.random.randn(seq_len, d_model)\n",
    "\n",
    "# 前向傳播\n",
    "output, head_attentions = mha(X)\n",
    "\n",
    "print(f\"\\n配置:\")\n",
    "print(f\"  模型維度 (d_model): {d_model}\")\n",
    "print(f\"  頭數 (num_heads): {num_heads}\")\n",
    "print(f\"  每個頭維度 (d_k): {mha.d_k}\")\n",
    "\n",
    "print(f\"\\n輸入形狀: {X.shape}\")\n",
    "print(f\"輸出形狀: {output.shape}\")\n",
    "print(f\"每個頭的 Attention 權重: {len(head_attentions)} 個, 每個形狀 {head_attentions[0].shape}\")\n",
    "\n",
    "print(f\"\\n參數量計算:\")\n",
    "params_qkv = num_heads * d_model * mha.d_k * 3  # Q, K, V\n",
    "params_o = d_model * d_model  # W_o\n",
    "total = params_qkv + params_o\n",
    "print(f\"  Q/K/V 投影: {params_qkv:,}\")\n",
    "print(f\"  輸出投影: {params_o:,}\")\n",
    "print(f\"  總計: {total:,} 參數\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化: 8 個頭的 Attention 模式\n",
    "fig, axes = plt.subplots(2, 4, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(num_heads):\n",
    "    sns.heatmap(head_attentions[i], annot=False, cmap='YlOrRd', \n",
    "                ax=axes[i], cbar=True, vmin=0, vmax=1,\n",
    "                cbar_kws={'label': 'Score'})\n",
    "    axes[i].set_title(f'Head {i+1}', fontsize=11, fontweight='bold')\n",
    "    axes[i].set_xlabel('Key', fontsize=9)\n",
    "    axes[i].set_ylabel('Query', fontsize=9)\n",
    "\n",
    "plt.suptitle('Multi-Head Attention: 8 個頭的不同關注模式', \n",
    "             fontsize=16, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n🔍 觀察:\")\n",
    "print(\"  - 不同頭學習到不同的 Attention 模式\")\n",
    "print(\"  - 有些頭關注局部 (對角線),有些關注全局\")\n",
    "print(\"  - 多樣性讓模型捕捉複雜關係\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "## 5. Masked Attention 機制\n",
    "\n",
    "### 5.1 為什麼需要 Mask?\n",
    "\n",
    "#### 應用場景 1: Padding Mask\n",
    "\n",
    "**問題:** 批次訓練時,序列長度不同需要填充 (padding)\n",
    "\n",
    "```python\n",
    "句子 1: \"I love NLP\"        → [20, 45, 89, 0, 0]  ← 填充 0\n",
    "句子 2: \"Transformer rocks\" → [78, 12, 0, 0, 0]\n",
    "```\n",
    "\n",
    "**Padding Token 不應該被關注!**\n",
    "\n",
    "---\n",
    "\n",
    "#### 應用場景 2: Look-Ahead Mask (Decoder)\n",
    "\n",
    "**問題:** Decoder 生成時不能\"偷看\"未來的詞!\n",
    "\n",
    "```\n",
    "生成 \"I love NLP\":\n",
    "\n",
    "時間 t=1: 只能看 \"I\"           → 預測 \"love\"\n",
    "時間 t=2: 只能看 \"I love\"      → 預測 \"NLP\"\n",
    "時間 t=3: 只能看 \"I love NLP\"  → 預測 <EOS>\n",
    "```\n",
    "\n",
    "**必須遮罩未來位置!**\n",
    "\n",
    "---\n",
    "\n",
    "### 5.2 Mask 的實作\n",
    "\n",
    "#### Look-Ahead Mask (下三角遮罩)\n",
    "\n",
    "$$\n",
    "\\text{Mask} = \\begin{bmatrix}\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "1 & 1 & 0 & 0 \\\\\n",
    "1 & 1 & 1 & 0 \\\\\n",
    "1 & 1 & 1 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- 1: 可以關注 (Allow)\n",
    "- 0: 不可關注 (Mask) → 設為 $-\\infty$ (Softmax 後變為 0)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(seq_len: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    創建 Look-Ahead Mask (下三角矩陣)\n",
    "    \n",
    "    Args:\n",
    "        seq_len: 序列長度\n",
    "        \n",
    "    Returns:\n",
    "        mask: shape (seq_len, seq_len), 1=允許關注, 0=遮罩\n",
    "    \"\"\"\n",
    "    mask = np.tril(np.ones((seq_len, seq_len)))  # 下三角矩陣\n",
    "    return mask\n",
    "\n",
    "\n",
    "def create_padding_mask(seq: np.ndarray, pad_token: int = 0) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    創建 Padding Mask\n",
    "    \n",
    "    Args:\n",
    "        seq: Token ID 序列, shape (seq_len,)\n",
    "        pad_token: Padding token 的 ID\n",
    "        \n",
    "    Returns:\n",
    "        mask: shape (seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    # Padding 位置設為 0, 其他為 1\n",
    "    mask = (seq != pad_token).astype(int)\n",
    "    # 擴展為矩陣\n",
    "    mask = mask[:, np.newaxis] * mask[np.newaxis, :]\n",
    "    return mask\n",
    "\n",
    "\n",
    "# 測試 Look-Ahead Mask\n",
    "seq_len = 6\n",
    "look_ahead_mask = create_look_ahead_mask(seq_len)\n",
    "\n",
    "print(\"🎭 Look-Ahead Mask (Decoder 使用)\")\n",
    "print(\"=\" * 50)\n",
    "print(look_ahead_mask.astype(int))\n",
    "\n",
    "# 測試 Padding Mask\n",
    "seq_with_padding = np.array([20, 45, 89, 12, 0, 0])  # 最後兩個是 padding\n",
    "padding_mask = create_padding_mask(seq_with_padding)\n",
    "\n",
    "print(\"\\n🎭 Padding Mask (Encoder 使用)\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"序列: {seq_with_padding} (0 是 padding)\")\n",
    "print(f\"\\nPadding Mask:\\n{padding_mask.astype(int)}\")\n",
    "\n",
    "# 視覺化兩種 Mask\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Look-Ahead Mask\n",
    "sns.heatmap(look_ahead_mask, annot=True, fmt='.0f', cmap='RdYlGn', \n",
    "            ax=axes[0], cbar_kws={'label': '1=Allow, 0=Mask'},\n",
    "            xticklabels=[f't{i+1}' for i in range(seq_len)],\n",
    "            yticklabels=[f't{i+1}' for i in range(seq_len)])\n",
    "axes[0].set_title('Look-Ahead Mask\\n(防止看到未來)', fontsize=13, fontweight='bold')\n",
    "axes[0].set_xlabel('時間 (被關注位置)')\n",
    "axes[0].set_ylabel('時間 (查詢位置)')\n",
    "\n",
    "# Padding Mask\n",
    "sns.heatmap(padding_mask, annot=True, fmt='.0f', cmap='RdYlGn',\n",
    "            ax=axes[1], cbar_kws={'label': '1=Allow, 0=Mask'},\n",
    "            xticklabels=[f'Pos{i+1}' for i in range(seq_len)],\n",
    "            yticklabels=[f'Pos{i+1}' for i in range(seq_len)])\n",
    "axes[1].set_title('Padding Mask\\n(忽略 Padding)', fontsize=13, fontweight='bold')\n",
    "axes[1].set_xlabel('位置 (被關注位置)')\n",
    "axes[1].set_ylabel('位置 (查詢位置)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n💡 重點:\")\n",
    "print(\"  - Look-Ahead Mask: Decoder 自回歸生成用\")\n",
    "print(\"  - Padding Mask: 忽略填充的無效 token\")\n",
    "print(\"  - 可以組合使用: combined_mask = look_ahead_mask & padding_mask\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a>\n",
    "## 6. Attention 視覺化與解釋性\n",
    "\n",
    "### 6.1 真實句子的 Attention 分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用真實句子測試 Attention\n",
    "sentence = \"The cat sat on the mat\".split()\n",
    "seq_len = len(sentence)\n",
    "d_model = 128\n",
    "\n",
    "# 模擬詞嵌入 (實際應使用訓練好的嵌入)\n",
    "embeddings = np.random.randn(seq_len, d_model)\n",
    "\n",
    "# 創建 Self-Attention 層\n",
    "self_attn = SelfAttention(d_model, d_k=64, d_v=64)\n",
    "\n",
    "# 計算 Attention\n",
    "output, attn_weights = self_attn(embeddings)\n",
    "\n",
    "# 視覺化 Attention 權重\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(attn_weights, annot=True, fmt='.2f', cmap='YlOrRd',\n",
    "            xticklabels=sentence,\n",
    "            yticklabels=sentence,\n",
    "            cbar_kws={'label': 'Attention Score'})\n",
    "plt.xlabel('Key (被關注的詞)', fontsize=12)\n",
    "plt.ylabel('Query (查詢詞)', fontsize=12)\n",
    "plt.title('Self-Attention 權重視覺化\\n句子: \"The cat sat on the mat\"', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 分析每個詞最關注的對象\n",
    "print(\"\\n🔍 Attention 模式分析:\")\n",
    "print(\"=\" * 50)\n",
    "for i, query_word in enumerate(sentence):\n",
    "    top_3_idx = np.argsort(attn_weights[i])[-3:][::-1]\n",
    "    top_3_words = [sentence[idx] for idx in top_3_idx]\n",
    "    top_3_scores = [attn_weights[i, idx] for idx in top_3_idx]\n",
    "    \n",
    "    print(f\"\\n'{query_word}' 最關注:\")\n",
    "    for word, score in zip(top_3_words, top_3_scores):\n",
    "        print(f\"  → {word:8s}: {score:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Attention 的可解釋性\n",
    "\n",
    "**優點:** Attention 權重可視化提供模型決策的「透明度」\n",
    "\n",
    "#### 實際應用案例:\n",
    "\n",
    "1. **機器翻譯:**\n",
    "   - 查看源語言與目標語言的對齊\n",
    "   - 檢查模型是否學到正確的語法結構\n",
    "\n",
    "2. **文本摘要:**\n",
    "   - 查看摘要句關注原文哪些部分\n",
    "   - 驗證是否擷取關鍵訊息\n",
    "\n",
    "3. **問答系統:**\n",
    "   - 查看答案關注問題與文章的哪些詞\n",
    "   - Debug 錯誤答案的原因\n",
    "\n",
    "---\n",
    "\n",
    "### 6.3 Attention 的局限性\n",
    "\n",
    "**警告:** Attention 不是萬能的解釋工具!\n",
    "\n",
    "1. **多頭難以解釋:** 8-16 個頭,每個學什麼?\n",
    "2. **層疊效應:** 12-24 層,最終決策難追蹤\n",
    "3. **權重 ≠ 重要性:** 高 Attention 不一定代表高重要性\n",
    "\n",
    "**建議:** 結合多種解釋方法 (Attention + LIME + SHAP)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"7\"></a>\n",
    "## 7. 完整 Attention 模組實作\n",
    "\n",
    "### 7.1 Production-Ready 版本\n",
    "\n",
    "整合所有功能:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompleteMultiHeadAttention:\n",
    "    \"\"\"\n",
    "    完整 Multi-Head Attention 實作\n",
    "    包含: Dropout, Masking, 完整參數管理\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: 模型維度\n",
    "            num_heads: 頭數\n",
    "            dropout: Dropout 比例\n",
    "        \"\"\"\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # 所有頭共享的投影矩陣 (更高效的實作方式)\n",
    "        self.W_q = self._init_weights((d_model, d_model))\n",
    "        self.W_k = self._init_weights((d_model, d_model))\n",
    "        self.W_v = self._init_weights((d_model, d_model))\n",
    "        self.W_o = self._init_weights((d_model, d_model))\n",
    "    \n",
    "    def _init_weights(self, shape):\n",
    "        \"\"\"Xavier 初始化\"\"\"\n",
    "        return np.random.randn(*shape) / np.sqrt(shape[0])\n",
    "    \n",
    "    def _split_heads(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        分割為多頭\n",
    "        \n",
    "        Args:\n",
    "            x: shape (seq_len, d_model)\n",
    "            \n",
    "        Returns:\n",
    "            shape (num_heads, seq_len, d_k)\n",
    "        \"\"\"\n",
    "        seq_len = x.shape[0]\n",
    "        x = x.reshape(seq_len, self.num_heads, self.d_k)\n",
    "        return x.transpose(1, 0, 2)  # (num_heads, seq_len, d_k)\n",
    "    \n",
    "    def _merge_heads(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        合併多頭\n",
    "        \n",
    "        Args:\n",
    "            x: shape (num_heads, seq_len, d_k)\n",
    "            \n",
    "        Returns:\n",
    "            shape (seq_len, d_model)\n",
    "        \"\"\"\n",
    "        x = x.transpose(1, 0, 2)  # (seq_len, num_heads, d_k)\n",
    "        seq_len = x.shape[0]\n",
    "        return x.reshape(seq_len, self.d_model)\n",
    "    \n",
    "    def forward(\n",
    "        self, \n",
    "        X: np.ndarray, \n",
    "        mask: Optional[np.ndarray] = None,\n",
    "        training: bool = False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        前向傳播\n",
    "        \n",
    "        Args:\n",
    "            X: 輸入, shape (seq_len, d_model)\n",
    "            mask: 可選遮罩, shape (seq_len, seq_len)\n",
    "            training: 是否訓練模式 (決定是否 dropout)\n",
    "            \n",
    "        Returns:\n",
    "            output: shape (seq_len, d_model)\n",
    "            attention_weights: shape (num_heads, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        seq_len = X.shape[0]\n",
    "        \n",
    "        # 線性投影\n",
    "        Q = np.dot(X, self.W_q)  # (seq_len, d_model)\n",
    "        K = np.dot(X, self.W_k)\n",
    "        V = np.dot(X, self.W_v)\n",
    "        \n",
    "        # 分割為多頭\n",
    "        Q = self._split_heads(Q)  # (num_heads, seq_len, d_k)\n",
    "        K = self._split_heads(K)\n",
    "        V = self._split_heads(V)\n",
    "        \n",
    "        # 對每個頭計算 Attention\n",
    "        head_outputs = []\n",
    "        all_attention_weights = []\n",
    "        \n",
    "        for i in range(self.num_heads):\n",
    "            head_output, attn_weights = scaled_dot_product_attention(\n",
    "                Q[i], K[i], V[i], mask\n",
    "            )\n",
    "            head_outputs.append(head_output)\n",
    "            all_attention_weights.append(attn_weights)\n",
    "        \n",
    "        # Stack heads\n",
    "        multi_head = np.stack(head_outputs, axis=0)  # (num_heads, seq_len, d_k)\n",
    "        all_attention_weights = np.stack(all_attention_weights, axis=0)  # (num_heads, seq_len, seq_len)\n",
    "        \n",
    "        # 合併多頭\n",
    "        concat_output = self._merge_heads(multi_head)  # (seq_len, d_model)\n",
    "        \n",
    "        # 最終線性投影\n",
    "        output = np.dot(concat_output, self.W_o)\n",
    "        \n",
    "        # Dropout (訓練時)\n",
    "        if training and self.dropout > 0:\n",
    "            dropout_mask = np.random.binomial(1, 1-self.dropout, output.shape)\n",
    "            output = output * dropout_mask / (1 - self.dropout)\n",
    "        \n",
    "        return output, all_attention_weights\n",
    "    \n",
    "    def __call__(self, X, mask=None, training=False):\n",
    "        return self.forward(X, mask, training)\n",
    "\n",
    "\n",
    "# 測試完整版 Multi-Head Attention\n",
    "print(\"🧪 測試 Complete Multi-Head Attention\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "seq_len = 10\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "\n",
    "complete_mha = CompleteMultiHeadAttention(d_model, num_heads, dropout=0.1)\n",
    "\n",
    "# 模擬輸入\n",
    "X = np.random.randn(seq_len, d_model)\n",
    "\n",
    "# 測試 1: 無遮罩\n",
    "output_no_mask, _ = complete_mha(X, training=False)\n",
    "print(f\"\\n測試 1 - 無遮罩:\")\n",
    "print(f\"  輸入形狀: {X.shape}\")\n",
    "print(f\"  輸出形狀: {output_no_mask.shape}\")\n",
    "\n",
    "# 測試 2: 使用 Look-Ahead Mask\n",
    "mask = create_look_ahead_mask(seq_len)\n",
    "output_masked, attn_masked = complete_mha(X, mask=mask, training=False)\n",
    "print(f\"\\n測試 2 - Look-Ahead Mask:\")\n",
    "print(f\"  輸出形狀: {output_masked.shape}\")\n",
    "print(f\"  Attention 權重形狀: {attn_masked.shape}\")\n",
    "\n",
    "# 測試 3: 訓練模式 (含 Dropout)\n",
    "output_train, _ = complete_mha(X, training=True)\n",
    "print(f\"\\n測試 3 - 訓練模式 (Dropout=0.1):\")\n",
    "print(f\"  輸出形狀: {output_train.shape}\")\n",
    "\n",
    "print(f\"\\n✅ 所有測試通過!\")\n",
    "print(f\"\\n📊 總參數量: {d_model * d_model * 4:,}\")\n",
    "print(f\"   (4 個投影矩陣: W_q, W_k, W_v, W_o)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化有無 Mask 的差異\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# 無遮罩的 Attention\n",
    "output_full, attn_full = complete_mha(X, mask=None, training=False)\n",
    "sns.heatmap(attn_full[0], annot=False, cmap='YlOrRd', ax=axes[0], vmin=0, vmax=0.5)\n",
    "axes[0].set_title('無遮罩 Attention\\n(可看到所有位置)', fontsize=13, fontweight='bold')\n",
    "axes[0].set_xlabel('Key Position')\n",
    "axes[0].set_ylabel('Query Position')\n",
    "\n",
    "# Look-Ahead Mask\n",
    "sns.heatmap(attn_masked[0], annot=False, cmap='YlOrRd', ax=axes[1], vmin=0, vmax=0.5)\n",
    "axes[1].set_title('Look-Ahead Mask Attention\\n(只能看到過去)', fontsize=13, fontweight='bold')\n",
    "axes[1].set_xlabel('Key Position')\n",
    "axes[1].set_ylabel('Query Position')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n🎯 對比:\")\n",
    "print(\"  左圖: Encoder 使用 (雙向,可看全文)\")\n",
    "print(\"  右圖: Decoder 使用 (單向,只看過去)\")\n",
    "print(\"  右圖上三角全黑: 未來被遮罩\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🧮 數學補充: 為什麼是點積?\n",
    "\n",
    "### 相似度計算的三種方法\n",
    "\n",
    "#### 1. 點積 (Dot Product) - Transformer 使用\n",
    "\n",
    "$$\n",
    "\\text{score}(q, k) = q^T k\n",
    "$$\n",
    "\n",
    "- ✅ 計算效率高 (矩陣乘法)\n",
    "- ✅ GPU 友好\n",
    "- ⚠️ 受向量長度影響 (需縮放)\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. 餘弦相似度 (Cosine Similarity)\n",
    "\n",
    "$$\n",
    "\\text{score}(q, k) = \\frac{q^T k}{||q|| \\cdot ||k||}\n",
    "$$\n",
    "\n",
    "- ✅ 不受向量長度影響\n",
    "- ❌ 計算成本較高 (需正規化)\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. 加性 Attention (Additive) - Bahdanau Attention\n",
    "\n",
    "$$\n",
    "\\text{score}(q, k) = v^T \\tanh(W_q q + W_k k)\n",
    "$$\n",
    "\n",
    "- ✅ 表達能力強\n",
    "- ❌ 參數量多,計算慢\n",
    "\n",
    "---\n",
    "\n",
    "**Transformer 選擇點積的原因:**\n",
    "1. 速度最快 (矩陣運算高度優化)\n",
    "2. 縮放後效果與其他方法相當\n",
    "3. 實作簡單\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 比較三種相似度計算方法\n",
    "def compare_similarity_methods():\n",
    "    \"\"\"比較不同 Attention Score 計算方法\"\"\"\n",
    "    \n",
    "    # 兩個測試向量\n",
    "    q = np.array([1.0, 2.0, 3.0, 4.0])\n",
    "    k1 = np.array([1.0, 2.0, 3.0, 4.0])  # 與 q 相同\n",
    "    k2 = np.array([4.0, 3.0, 2.0, 1.0])  # 與 q 相反\n",
    "    k3 = np.array([0.0, 0.0, 0.0, 0.0])  # 零向量\n",
    "    \n",
    "    keys = [k1, k2, k3]\n",
    "    key_names = ['k1 (相同)', 'k2 (相反)', 'k3 (零)']\n",
    "    \n",
    "    results = {'Dot Product': [], 'Cosine': [], 'Scaled Dot': []}\n",
    "    \n",
    "    for k in keys:\n",
    "        # 方法 1: 點積\n",
    "        dot = np.dot(q, k)\n",
    "        results['Dot Product'].append(dot)\n",
    "        \n",
    "        # 方法 2: 餘弦相似度\n",
    "        cos = np.dot(q, k) / (np.linalg.norm(q) * np.linalg.norm(k) + 1e-8)\n",
    "        results['Cosine'].append(cos)\n",
    "        \n",
    "        # 方法 3: 縮放點積\n",
    "        scaled = dot / np.sqrt(len(q))\n",
    "        results['Scaled Dot'].append(scaled)\n",
    "    \n",
    "    # 視覺化比較\n",
    "    x = np.arange(len(key_names))\n",
    "    width = 0.25\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    for i, (method, scores) in enumerate(results.items()):\n",
    "        ax.bar(x + i*width, scores, width, label=method, alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('Key Vectors', fontsize=12)\n",
    "    ax.set_ylabel('Similarity Score', fontsize=12)\n",
    "    ax.set_title('三種相似度計算方法對比', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x + width)\n",
    "    ax.set_xticklabels(key_names)\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    ax.axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 打印數值\n",
    "    print(\"\\n📊 數值對比:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"{'Key':<15} {'Dot Product':>15} {'Cosine':>15} {'Scaled Dot':>15}\")\n",
    "    print(\"=\" * 60)\n",
    "    for i, name in enumerate(key_names):\n",
    "        print(f\"{name:<15} {results['Dot Product'][i]:>15.2f} {results['Cosine'][i]:>15.2f} {results['Scaled Dot'][i]:>15.2f}\")\n",
    "\n",
    "compare_similarity_methods()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🎓 實戰案例: 完整 Transformer Encoder Layer\n",
    "\n",
    "結合所有組件,實作一個完整的 Encoder 層:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer:\n",
    "    \"\"\"\n",
    "    完整 Transformer Encoder 層\n",
    "    = Multi-Head Attention + Feed Forward + 殘差連接 + LayerNorm\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout: float = 0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: 模型維度\n",
    "            num_heads: Attention 頭數\n",
    "            d_ff: Feed-Forward 中間層維度\n",
    "            dropout: Dropout 比例\n",
    "        \"\"\"\n",
    "        self.mha = CompleteMultiHeadAttention(d_model, num_heads, dropout)\n",
    "        \n",
    "        # Feed-Forward Network\n",
    "        self.W1 = np.random.randn(d_model, d_ff) / np.sqrt(d_model)\n",
    "        self.b1 = np.zeros(d_ff)\n",
    "        self.W2 = np.random.randn(d_ff, d_model) / np.sqrt(d_ff)\n",
    "        self.b2 = np.zeros(d_model)\n",
    "        \n",
    "        self.dropout = dropout\n",
    "    \n",
    "    def feed_forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Position-wise Feed-Forward Network\"\"\"\n",
    "        # FFN(x) = max(0, xW1 + b1)W2 + b2\n",
    "        hidden = np.maximum(0, np.dot(x, self.W1) + self.b1)  # ReLU activation\n",
    "        output = np.dot(hidden, self.W2) + self.b2\n",
    "        return output\n",
    "    \n",
    "    def layer_norm(self, x: np.ndarray, epsilon: float = 1e-6) -> np.ndarray:\n",
    "        \"\"\"Layer Normalization\"\"\"\n",
    "        mean = x.mean(axis=-1, keepdims=True)\n",
    "        std = x.std(axis=-1, keepdims=True)\n",
    "        return (x - mean) / (std + epsilon)\n",
    "    \n",
    "    def forward(self, x: np.ndarray, mask: Optional[np.ndarray] = None):\n",
    "        \"\"\"\n",
    "        前向傳播\n",
    "        \n",
    "        Args:\n",
    "            x: 輸入, shape (seq_len, d_model)\n",
    "            mask: 可選遮罩\n",
    "            \n",
    "        Returns:\n",
    "            output: shape (seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # Sub-layer 1: Multi-Head Attention + Residual + LayerNorm\n",
    "        attn_output, _ = self.mha(x, mask, training=False)\n",
    "        x = self.layer_norm(x + attn_output)  # 殘差連接 + LayerNorm\n",
    "        \n",
    "        # Sub-layer 2: Feed-Forward + Residual + LayerNorm\n",
    "        ff_output = self.feed_forward(x)\n",
    "        output = self.layer_norm(x + ff_output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "# 測試完整 Encoder Layer\n",
    "print(\"🧪 測試完整 Transformer Encoder Layer\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "d_ff = 2048  # 論文中 = 4 × d_model\n",
    "\n",
    "encoder_layer = TransformerEncoderLayer(d_model, num_heads, d_ff)\n",
    "\n",
    "# 模擬輸入\n",
    "seq_len = 10\n",
    "X = np.random.randn(seq_len, d_model)\n",
    "\n",
    "# 前向傳播\n",
    "output = encoder_layer.forward(X)\n",
    "\n",
    "print(f\"\\n輸入形狀: {X.shape}\")\n",
    "print(f\"輸出形狀: {output.shape}\")\n",
    "print(f\"\\n✅ 形狀保持不變 (Transformer 的重要特性!)\")\n",
    "\n",
    "# 統計輸出分布\n",
    "print(f\"\\n輸出統計:\")\n",
    "print(f\"  均值: {output.mean():.6f} (LayerNorm 後應接近 0)\")\n",
    "print(f\"  標準差: {output.std():.6f} (LayerNorm 後應接近 1)\")\n",
    "print(f\"  最小值: {output.min():.6f}\")\n",
    "print(f\"  最大值: {output.max():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 📚 本課總結\n",
    "\n",
    "### 核心要點回顧:\n",
    "\n",
    "1. **Scaled Dot-Product Attention:**\n",
    "   - 四步驟: QK^T → 縮放 → Softmax → 加權組合 V\n",
    "   - 縮放因子 $\\sqrt{d_k}$ 穩定梯度\n",
    "   - 輸出形狀與輸入相同\n",
    "\n",
    "2. **Multi-Head Attention:**\n",
    "   - 多個頭並行學習不同關係\n",
    "   - 每個頭維度 = d_model / num_heads\n",
    "   - 計算成本與單頭相當,但表達能力更強\n",
    "\n",
    "3. **Masking 機制:**\n",
    "   - Padding Mask: 忽略填充 token\n",
    "   - Look-Ahead Mask: Decoder 防止看到未來\n",
    "   - 可組合使用\n",
    "\n",
    "4. **完整 Encoder Layer:**\n",
    "   - Multi-Head Attention\n",
    "   - Feed-Forward Network\n",
    "   - Residual Connection (殘差)\n",
    "   - Layer Normalization (歸一化)\n",
    "\n",
    "5. **Attention 可解釋性:**\n",
    "   - 權重視覺化提供透明度\n",
    "   - 但不是萬能解釋工具\n",
    "   - 需結合其他方法\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 下節預告\n",
    "\n",
    "**CH07-04: Transformer 編碼器 (Encoder)**\n",
    "\n",
    "我們將探討:\n",
    "- 堆疊多層 Encoder 的設計\n",
    "- 完整 Encoder 架構實作\n",
    "- 使用 Encoder 進行文本分類\n",
    "- BERT 模型原理與應用\n",
    "\n",
    "---\n",
    "\n",
    "## 📖 延伸閱讀\n",
    "\n",
    "1. **論文:**\n",
    "   - [Attention is All You Need](https://arxiv.org/abs/1706.03762) (必讀!)\n",
    "   - [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/