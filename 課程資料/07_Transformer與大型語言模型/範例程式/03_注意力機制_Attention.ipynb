{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH07-03: æ³¨æ„åŠ›æ©Ÿåˆ¶ (Attention Mechanism)\n",
    "\n",
    "**èª²ç¨‹ç›®æ¨™:**\n",
    "- æŒæ¡ Scaled Dot-Product Attention çš„æ•¸å­¸åŸç†èˆ‡å¯¦ä½œ\n",
    "- ç†è§£ Multi-Head Attention çš„è¨­è¨ˆå‹•æ©Ÿèˆ‡å„ªå‹¢\n",
    "- å¯¦ä½œå®Œæ•´å¯é‹è¡Œçš„ Attention æ¨¡çµ„\n",
    "- å­¸æœƒ Attention æ¬Šé‡çš„è¦–è¦ºåŒ–èˆ‡è§£é‡‹\n",
    "- äº†è§£ Masked Attention åœ¨ Decoder ä¸­çš„æ‡‰ç”¨\n",
    "\n",
    "**å­¸ç¿’æ™‚é–“:** ç´„ 120 åˆ†é˜\n",
    "\n",
    "**å‰ç½®çŸ¥è­˜:**\n",
    "- ç·šæ€§ä»£æ•¸åŸºç¤ (çŸ©é™£ä¹˜æ³•)\n",
    "- Softmax å‡½æ•¸\n",
    "- Transformer æ¶æ§‹æ¦‚è¦½ (CH07-01)\n",
    "- è©åµŒå…¥åŸºç¤ (CH07-02)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š ç›®éŒ„\n",
    "\n",
    "1. [Attention æ©Ÿåˆ¶çš„ç›´è¦ºç†è§£](#1)\n",
    "2. [Scaled Dot-Product Attention æ•¸å­¸æ¨å°](#2)\n",
    "3. [å¾é›¶å¯¦ä½œ Self-Attention](#3)\n",
    "4. [Multi-Head Attention æ·±å…¥](#4)\n",
    "5. [Masked Attention æ©Ÿåˆ¶](#5)\n",
    "6. [Attention è¦–è¦ºåŒ–èˆ‡è§£é‡‹æ€§](#6)\n",
    "7. [å®Œæ•´ Attention æ¨¡çµ„å¯¦ä½œ](#7)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç’°å¢ƒè¨­å®šèˆ‡å¥—ä»¶å°å…¥\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Optional, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# è¨­å®šä¸­æ–‡é¡¯ç¤º\n",
    "plt.rcParams['font.sans-serif'] = ['Microsoft JhengHei', 'SimHei', 'Arial Unicode MS']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# è¨­å®šéš¨æ©Ÿç¨®å­\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"âœ… ç’°å¢ƒè¨­å®šå®Œæˆ\")\n",
    "print(f\"NumPy ç‰ˆæœ¬: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "## 1. Attention æ©Ÿåˆ¶çš„ç›´è¦ºç†è§£\n",
    "\n",
    "### 1.1 å¾äººé¡æ³¨æ„åŠ›åˆ°æ©Ÿå™¨æ³¨æ„åŠ›\n",
    "\n",
    "#### äººé¡é–±è®€çš„ä¾‹å­:\n",
    "\n",
    "å¥å­: *\"The **animal** didn't cross the street because **it** was too tired.\"*\n",
    "\n",
    "å•: \"it\" æŒ‡ä»€éº¼?\n",
    "\n",
    "**äººé¡çš„æ€è€ƒéç¨‹:**\n",
    "1. çœ‹åˆ° \"it\"\n",
    "2. å‘å‰æœå°‹å¯èƒ½çš„æŒ‡ä»£å°è±¡\n",
    "3. \"animal\" æœ€ç›¸é—œ âœ…\n",
    "4. \"street\" ä¸å¤ªç›¸é—œ âŒ\n",
    "\n",
    "**é€™å°±æ˜¯ Attention!**\n",
    "\n",
    "---\n",
    "\n",
    "### 1.2 Attention çš„æ ¸å¿ƒæ€æƒ³\n",
    "\n",
    "**å®šç¾©:** çµ¦å®šä¸€å€‹æŸ¥è©¢ (Query),å¾ä¸€çµ„éµå€¼å° (Key-Value pairs) ä¸­æ‰¾åˆ°æœ€ç›¸é—œçš„è¨Šæ¯\n",
    "\n",
    "```\n",
    "Query:  \"it\" æ˜¯ä»€éº¼?\n",
    "  â†“\n",
    "Key:    \"The\"   \"animal\"  \"didn't\"  \"cross\"  \"street\" ...\n",
    "  â†“       â†“         â†“          â†“         â†“         â†“\n",
    "Score:  0.05     0.82       0.03      0.01      0.09   ... â† ç›¸é—œåº¦\n",
    "  â†“\n",
    "Value:  [vec1]   [vec2]     [vec3]    [vec4]    [vec5] ...\n",
    "  â†“\n",
    "Output: 0.05*vec1 + 0.82*vec2 + 0.03*vec3 + ...  â† åŠ æ¬Šçµ„åˆ\n",
    "```\n",
    "\n",
    "**ä¸‰å€‹é—œéµçŸ©é™£:**\n",
    "- **Query (Q)**: \"æˆ‘æƒ³æŸ¥è©¢ä»€éº¼?\"\n",
    "- **Key (K)**: \"æˆ‘æ˜¯ä»€éº¼?\"\n",
    "- **Value (V)**: \"æˆ‘åŒ…å«ä»€éº¼è¨Šæ¯?\"\n",
    "\n",
    "---\n",
    "\n",
    "### 1.3 Self-Attention vs Cross-Attention\n",
    "\n",
    "#### Self-Attention (è‡ªæ³¨æ„åŠ›)\n",
    "```python\n",
    "Q, K, V ä¾†è‡ªåŒä¸€å€‹åºåˆ—\n",
    "å¥å­é—œæ³¨è‡ªå·±: \"The cat sat\" â†’ æ¯å€‹è©é—œæ³¨å…¶ä»–è©\n",
    "```\n",
    "\n",
    "#### Cross-Attention (äº¤å‰æ³¨æ„åŠ›)\n",
    "```python\n",
    "Q ä¾†è‡ªä¸€å€‹åºåˆ—, K/V ä¾†è‡ªå¦ä¸€å€‹åºåˆ—\n",
    "ç¿»è­¯: \"Le chat\" (æ³•æ–‡) â†’ \"The cat\" (è‹±æ–‡)\n",
    "      Q (è‹±æ–‡)    K/V (æ³•æ–‡)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç›´è¦ºç¤ºç¯„: æ¨¡æ“¬ Attention åˆ†æ•¸è¨ˆç®—\n",
    "sentence = [\"The\", \"animal\", \"didn't\", \"cross\", \"the\", \"street\", \"because\", \"it\", \"was\", \"tired\"]\n",
    "query_word = \"it\"  # æˆ‘å€‘æƒ³çŸ¥é“ \"it\" é—œæ³¨å“ªäº›è©\n",
    "\n",
    "# æ‰‹å·¥è¨­å®šç›¸é—œæ€§åˆ†æ•¸ (å¯¦éš›ç”±ç¥ç¶“ç¶²è·¯å­¸ç¿’)\n",
    "relevance_scores = {\n",
    "    \"The\": 0.05,\n",
    "    \"animal\": 0.82,  # æœ€ç›¸é—œ!\n",
    "    \"didn't\": 0.03,\n",
    "    \"cross\": 0.01,\n",
    "    \"the\": 0.04,\n",
    "    \"street\": 0.09,\n",
    "    \"because\": 0.05,\n",
    "    \"it\": 0.15,      # è‡ªå·±ä¹Ÿæœ‰é»ç›¸é—œ\n",
    "    \"was\": 0.08,\n",
    "    \"tired\": 0.12,\n",
    "}\n",
    "\n",
    "# Softmax æ­¸ä¸€åŒ– (ç¢ºä¿ç¸½å’Œç‚º 1)\n",
    "scores = np.array(list(relevance_scores.values()))\n",
    "attention_weights = np.exp(scores) / np.sum(np.exp(scores))\n",
    "\n",
    "# è¦–è¦ºåŒ–\n",
    "plt.figure(figsize=(14, 6))\n",
    "colors = ['red' if word == 'animal' else 'steelblue' for word in sentence]\n",
    "bars = plt.bar(sentence, attention_weights, color=colors, alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('Words in Sentence', fontsize=12)\n",
    "plt.ylabel('Attention Weight', fontsize=12)\n",
    "plt.title(f'Attention æ¬Šé‡åˆ†å¸ƒ: \"{query_word}\" é—œæ³¨åºåˆ—ä¸­å“ªäº›è©?', fontsize=14, fontweight='bold')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# æ¨™è¨»æœ€é«˜åˆ†\n",
    "max_idx = np.argmax(attention_weights)\n",
    "plt.annotate(f'æœ€é«˜é—œæ³¨\\n{attention_weights[max_idx]:.2%}', \n",
    "             xy=(max_idx, attention_weights[max_idx]),\n",
    "             xytext=(max_idx, attention_weights[max_idx] + 0.1),\n",
    "             fontsize=11, fontweight='bold', color='red',\n",
    "             arrowprops=dict(arrowstyle='->', color='red', lw=2))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nğŸ¯ '{query_word}' æœ€é—œæ³¨: '{sentence[max_idx]}' (æ¬Šé‡: {attention_weights[max_idx]:.2%})\")\n",
    "print(f\"\\nâœ… æ¬Šé‡ç¸½å’Œé©—è­‰: {attention_weights.sum():.6f} (æ‡‰ç‚º 1.0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "## 2. Scaled Dot-Product Attention æ•¸å­¸æ¨å°\n",
    "\n",
    "### 2.1 å®Œæ•´å…¬å¼\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V\n",
    "$$\n",
    "\n",
    "å…¶ä¸­:\n",
    "- $Q \\in \\mathbb{R}^{n \\times d_k}$: Query çŸ©é™£ (n å€‹æŸ¥è©¢,æ¯å€‹ $d_k$ ç¶­)\n",
    "- $K \\in \\mathbb{R}^{m \\times d_k}$: Key çŸ©é™£ (m å€‹éµ)\n",
    "- $V \\in \\mathbb{R}^{m \\times d_v}$: Value çŸ©é™£ (m å€‹å€¼,æ¯å€‹ $d_v$ ç¶­)\n",
    "- $d_k$: Key/Query çš„ç¶­åº¦\n",
    "- $\\sqrt{d_k}$: ç¸®æ”¾å› å­\n",
    "\n",
    "---\n",
    "\n",
    "### 2.2 é€æ­¥è¨ˆç®—éç¨‹\n",
    "\n",
    "#### Step 1: è¨ˆç®—ç›¸ä¼¼åº¦åˆ†æ•¸\n",
    "\n",
    "$$\n",
    "\\text{Scores} = QK^T \\in \\mathbb{R}^{n \\times m}\n",
    "$$\n",
    "\n",
    "- ä½¿ç”¨**é»ç©** (Dot Product) è¡¡é‡ç›¸ä¼¼åº¦\n",
    "- åˆ†æ•¸è¶Šé«˜è¡¨ç¤ºè¶Šç›¸é—œ\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 2: ç¸®æ”¾ (Scaling)\n",
    "\n",
    "$$\n",
    "\\text{Scaled Scores} = \\frac{QK^T}{\\sqrt{d_k}}\n",
    "$$\n",
    "\n",
    "**ç‚ºä»€éº¼éœ€è¦ç¸®æ”¾?**\n",
    "\n",
    "ç•¶ $d_k$ å¾ˆå¤§æ™‚,é»ç©çµæœå¯èƒ½éå¸¸å¤§:\n",
    "\n",
    "```python\n",
    "d_k = 512 æ™‚:\n",
    "QK^T å¯èƒ½é”åˆ° Â±50 ä»¥ä¸Š\n",
    "  â†“\n",
    "Softmax æœƒé€²å…¥é£½å’Œå€ (æ¢¯åº¦æ¥è¿‘ 0)\n",
    "  â†“\n",
    "è¨“ç·´å›°é›£!\n",
    "```\n",
    "\n",
    "é™¤ä»¥ $\\sqrt{d_k}$ å¯ä»¥ç©©å®šæ¢¯åº¦:\n",
    "\n",
    "```python\n",
    "50 / âˆš512 â‰ˆ 2.2  â† åˆç†ç¯„åœ\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 3: Softmax æ­¸ä¸€åŒ–\n",
    "\n",
    "$$\n",
    "\\text{Attention Weights} = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)\n",
    "$$\n",
    "\n",
    "- å°‡åˆ†æ•¸è½‰æ›ç‚ºæ¦‚ç‡åˆ†å¸ƒ (ç¸½å’Œç‚º 1)\n",
    "- æ¯ä¸€è¡Œç¨ç«‹åš Softmax\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 4: åŠ æ¬Šçµ„åˆ Value\n",
    "\n",
    "$$\n",
    "\\text{Output} = \\text{Attention Weights} \\cdot V \\in \\mathbb{R}^{n \\times d_v}\n",
    "$$\n",
    "\n",
    "- ä½¿ç”¨ Attention æ¬Šé‡å° Value é€²è¡ŒåŠ æ¬Šå¹³å‡\n",
    "- æœ€çµ‚è¼¸å‡ºèåˆäº†æ‰€æœ‰ç›¸é—œä½ç½®çš„è¨Šæ¯\n",
    "\n",
    "---\n",
    "\n",
    "### 2.3 çŸ©é™£ç¶­åº¦åˆ†æ\n",
    "\n",
    "```\n",
    "å‡è¨­: å¥å­é•·åº¦ seq_len=6, åµŒå…¥ç¶­åº¦ d_model=512\n",
    "\n",
    "Q: (6, 64)   â† batch å…§æ¯å€‹è©çš„ Query\n",
    "K: (6, 64)   â† batch å…§æ¯å€‹è©çš„ Key\n",
    "V: (6, 64)   â† batch å…§æ¯å€‹è©çš„ Value\n",
    "\n",
    "QK^T: (6, 64) Ã— (64, 6) = (6, 6)  â† ç›¸ä¼¼åº¦çŸ©é™£\n",
    "       â†“ Softmax\n",
    "Weights: (6, 6)  â† Attention æ¬Šé‡çŸ©é™£\n",
    "       â†“ Ã— V\n",
    "Output: (6, 6) Ã— (6, 64) = (6, 64)  â† æœ€çµ‚è¼¸å‡º\n",
    "```\n",
    "\n",
    "**è§€å¯Ÿ:** è¼¸å‡ºå½¢ç‹€èˆ‡è¼¸å…¥ç›¸åŒ!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¦–è¦ºåŒ–: Attention è¨ˆç®—æµç¨‹\n",
    "def visualize_attention_flow():\n",
    "    \"\"\"è¦–è¦ºåŒ– Attention çš„å››å€‹æ­¥é©Ÿ\"\"\"\n",
    "    \n",
    "    # æ¨¡æ“¬è¼¸å…¥\n",
    "    seq_len = 4\n",
    "    d_k = 3\n",
    "    \n",
    "    Q = np.random.randn(seq_len, d_k)\n",
    "    K = np.random.randn(seq_len, d_k)\n",
    "    V = np.random.randn(seq_len, d_k)\n",
    "    \n",
    "    # Step 1: QK^T\n",
    "    scores = np.dot(Q, K.T)\n",
    "    \n",
    "    # Step 2: Scaling\n",
    "    scaled_scores = scores / np.sqrt(d_k)\n",
    "    \n",
    "    # Step 3: Softmax\n",
    "    attention_weights = np.exp(scaled_scores) / np.sum(np.exp(scaled_scores), axis=-1, keepdims=True)\n",
    "    \n",
    "    # Step 4: Weighted Sum\n",
    "    output = np.dot(attention_weights, V)\n",
    "    \n",
    "    # è¦–è¦ºåŒ–å››å€‹æ­¥é©Ÿ\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "    \n",
    "    # Step 1\n",
    "    sns.heatmap(scores, annot=True, fmt='.2f', cmap='coolwarm', ax=axes[0, 0], cbar_kws={'label': 'Score'})\n",
    "    axes[0, 0].set_title('Step 1: QK^T (ç›¸ä¼¼åº¦çŸ©é™£)', fontsize=12, fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Key Index')\n",
    "    axes[0, 0].set_ylabel('Query Index')\n",
    "    \n",
    "    # Step 2\n",
    "    sns.heatmap(scaled_scores, annot=True, fmt='.2f', cmap='coolwarm', ax=axes[0, 1], cbar_kws={'label': 'Scaled Score'})\n",
    "    axes[0, 1].set_title(f'Step 2: ç¸®æ”¾ (Ã·âˆš{d_k})', fontsize=12, fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Key Index')\n",
    "    axes[0, 1].set_ylabel('Query Index')\n",
    "    \n",
    "    # Step 3\n",
    "    sns.heatmap(attention_weights, annot=True, fmt='.2f', cmap='YlOrRd', ax=axes[1, 0], \n",
    "                cbar_kws={'label': 'Probability'}, vmin=0, vmax=1)\n",
    "    axes[1, 0].set_title('Step 3: Softmax (æ­¸ä¸€åŒ–)', fontsize=12, fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Key Index')\n",
    "    axes[1, 0].set_ylabel('Query Index')\n",
    "    \n",
    "    # Step 4\n",
    "    sns.heatmap(output.T, annot=True, fmt='.2f', cmap='viridis', ax=axes[1, 1], cbar_kws={'label': 'Value'})\n",
    "    axes[1, 1].set_title('Step 4: Attention Weights Ã— V (è¼¸å‡º)', fontsize=12, fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('Query Index')\n",
    "    axes[1, 1].set_ylabel('Dimension')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # é©—è­‰\n",
    "    print(\"\\nâœ… é©—è­‰:\")\n",
    "    print(f\"  Step 3 æ¯è¡Œç¸½å’Œ: {attention_weights.sum(axis=1)}  â† æ‡‰å…¨ç‚º 1.0\")\n",
    "    print(f\"  è¼¸å…¥å½¢ç‹€: Q{Q.shape}, K{K.shape}, V{V.shape}\")\n",
    "    print(f\"  è¼¸å‡ºå½¢ç‹€: {output.shape}  â† èˆ‡ Q ç›¸åŒ!\")\n",
    "\n",
    "visualize_attention_flow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "## 3. å¾é›¶å¯¦ä½œ Self-Attention\n",
    "\n",
    "### 3.1 åŸºç¤ç‰ˆæœ¬ (ç„¡æŠ•å½±çŸ©é™£)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(\n",
    "    Q: np.ndarray,\n",
    "    K: np.ndarray,\n",
    "    V: np.ndarray,\n",
    "    mask: Optional[np.ndarray] = None\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Scaled Dot-Product Attention å®Œæ•´å¯¦ä½œ\n",
    "    \n",
    "    Args:\n",
    "        Q: Query çŸ©é™£, shape (seq_len_q, d_k)\n",
    "        K: Key çŸ©é™£, shape (seq_len_k, d_k)\n",
    "        V: Value çŸ©é™£, shape (seq_len_k, d_v)\n",
    "        mask: å¯é¸çš„é®ç½©çŸ©é™£, shape (seq_len_q, seq_len_k)\n",
    "    \n",
    "    Returns:\n",
    "        output: Attention è¼¸å‡º, shape (seq_len_q, d_v)\n",
    "        attention_weights: Attention æ¬Šé‡, shape (seq_len_q, seq_len_k)\n",
    "    \"\"\"\n",
    "    # ç²å– Key çš„ç¶­åº¦\n",
    "    d_k = K.shape[-1]\n",
    "    \n",
    "    # Step 1: è¨ˆç®—ç›¸ä¼¼åº¦åˆ†æ•¸ QK^T\n",
    "    scores = np.matmul(Q, K.T)  # (seq_len_q, seq_len_k)\n",
    "    \n",
    "    # Step 2: ç¸®æ”¾\n",
    "    scaled_scores = scores / np.sqrt(d_k)\n",
    "    \n",
    "    # (å¯é¸) æ‡‰ç”¨é®ç½© (å°‡é®ç½©ä½ç½®è¨­ç‚ºæ¥µå°å€¼)\n",
    "    if mask is not None:\n",
    "        scaled_scores = np.where(mask == 0, -1e9, scaled_scores)\n",
    "    \n",
    "    # Step 3: Softmax æ­¸ä¸€åŒ–\n",
    "    attention_weights = np.exp(scaled_scores) / np.sum(\n",
    "        np.exp(scaled_scores), axis=-1, keepdims=True\n",
    "    )\n",
    "    \n",
    "    # Step 4: åŠ æ¬Šçµ„åˆ Value\n",
    "    output = np.matmul(attention_weights, V)  # (seq_len_q, d_v)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "\n",
    "# æ¸¬è©¦\n",
    "print(\"ğŸ§ª æ¸¬è©¦ Scaled Dot-Product Attention\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "seq_len = 5\n",
    "d_k = 64\n",
    "d_v = 64\n",
    "\n",
    "# æ¨¡æ“¬è¼¸å…¥\n",
    "Q = np.random.randn(seq_len, d_k)\n",
    "K = np.random.randn(seq_len, d_k)\n",
    "V = np.random.randn(seq_len, d_v)\n",
    "\n",
    "# è¨ˆç®— Attention\n",
    "output, attn_weights = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "print(f\"\\nè¼¸å…¥:\")\n",
    "print(f\"  Q å½¢ç‹€: {Q.shape}\")\n",
    "print(f\"  K å½¢ç‹€: {K.shape}\")\n",
    "print(f\"  V å½¢ç‹€: {V.shape}\")\n",
    "\n",
    "print(f\"\\nè¼¸å‡º:\")\n",
    "print(f\"  Output å½¢ç‹€: {output.shape}\")\n",
    "print(f\"  Attention Weights å½¢ç‹€: {attn_weights.shape}\")\n",
    "\n",
    "print(f\"\\nâœ… é©—è­‰:\")\n",
    "print(f\"  æ¯è¡Œ Attention æ¬Šé‡ç¸½å’Œ: {attn_weights.sum(axis=1)}\")\n",
    "print(f\"  (æ‡‰å…¨ç‚º 1.0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¦–è¦ºåŒ– Attention æ¬Šé‡çŸ©é™£\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(attn_weights, annot=True, fmt='.3f', cmap='YlOrRd',\n",
    "            xticklabels=[f'Key {i+1}' for i in range(seq_len)],\n",
    "            yticklabels=[f'Query {i+1}' for i in range(seq_len)],\n",
    "            cbar_kws={'label': 'Attention Score'})\n",
    "plt.xlabel('Key (è¢«é—œæ³¨çš„ä½ç½®)', fontsize=12)\n",
    "plt.ylabel('Query (æŸ¥è©¢ä½ç½®)', fontsize=12)\n",
    "plt.title('Self-Attention æ¬Šé‡çŸ©é™£\\n(æ¯è¡Œç¸½å’Œç‚º 1)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ“Š è§£è®€:\")\n",
    "print(\"  - æ¯è¡Œä»£è¡¨ä¸€å€‹ Query å°æ‰€æœ‰ Key çš„é—œæ³¨åˆ†å¸ƒ\")\n",
    "print(\"  - é¡è‰²è¶Šæ·± (ç´…è‰²) è¡¨ç¤ºé—œæ³¨åº¦è¶Šé«˜\")\n",
    "print(\"  - å°è§’ç·šé€šå¸¸è¼ƒäº®: æ¯å€‹è©æœƒé—œæ³¨è‡ªå·±\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 å®Œæ•´ç‰ˆæœ¬ (å«æŠ•å½±çŸ©é™£)\n",
    "\n",
    "å¯¦éš› Transformer ä¸­, Q, K, V æ˜¯é€šé**ç·šæ€§æŠ•å½±**ç²å¾—:\n",
    "\n",
    "$$\n",
    "Q = XW^Q, \\quad K = XW^K, \\quad V = XW^V\n",
    "$$\n",
    "\n",
    "å…¶ä¸­:\n",
    "- $X \\in \\mathbb{R}^{seq\\_len \\times d_{model}}$: è¼¸å…¥ (åµŒå…¥å‘é‡)\n",
    "- $W^Q, W^K \\in \\mathbb{R}^{d_{model} \\times d_k}$: Query/Key æŠ•å½±çŸ©é™£\n",
    "- $W^V \\in \\mathbb{R}^{d_{model} \\times d_v}$: Value æŠ•å½±çŸ©é™£\n",
    "\n",
    "**ç‚ºä»€éº¼éœ€è¦æŠ•å½±?**\n",
    "- å­¸ç¿’ä¸åŒçš„æŸ¥è©¢/éµ/å€¼è¡¨ç¤º\n",
    "- é™ç¶­ (é€šå¸¸ $d_k = d_v = d_{model} / h$, h ç‚ºé ­æ•¸)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention:\n",
    "    \"\"\"\n",
    "    å®Œæ•´ Self-Attention å±¤å¯¦ä½œ (å«æŠ•å½±çŸ©é™£)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, d_k: int, d_v: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: è¼¸å…¥åµŒå…¥ç¶­åº¦\n",
    "            d_k: Query/Key ç¶­åº¦\n",
    "            d_v: Value ç¶­åº¦\n",
    "        \"\"\"\n",
    "        self.d_k = d_k\n",
    "        \n",
    "        # åˆå§‹åŒ–æŠ•å½±çŸ©é™£ (Xavier åˆå§‹åŒ–)\n",
    "        self.W_q = np.random.randn(d_model, d_k) / np.sqrt(d_model)\n",
    "        self.W_k = np.random.randn(d_model, d_k) / np.sqrt(d_model)\n",
    "        self.W_v = np.random.randn(d_model, d_v) / np.sqrt(d_model)\n",
    "    \n",
    "    def forward(self, X: np.ndarray, mask: Optional[np.ndarray] = None):\n",
    "        \"\"\"\n",
    "        å‰å‘å‚³æ’­\n",
    "        \n",
    "        Args:\n",
    "            X: è¼¸å…¥, shape (seq_len, d_model)\n",
    "            mask: å¯é¸é®ç½©, shape (seq_len, seq_len)\n",
    "            \n",
    "        Returns:\n",
    "            output: shape (seq_len, d_v)\n",
    "            attention_weights: shape (seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        # ç·šæ€§æŠ•å½±ç²å¾— Q, K, V\n",
    "        Q = np.dot(X, self.W_q)  # (seq_len, d_k)\n",
    "        K = np.dot(X, self.W_k)  # (seq_len, d_k)\n",
    "        V = np.dot(X, self.W_v)  # (seq_len, d_v)\n",
    "        \n",
    "        # è¨ˆç®— Scaled Dot-Product Attention\n",
    "        output, attention_weights = scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        return output, attention_weights\n",
    "    \n",
    "    def __call__(self, X, mask=None):\n",
    "        return self.forward(X, mask)\n",
    "\n",
    "\n",
    "# æ¸¬è©¦å®Œæ•´ç‰ˆ Self-Attention\n",
    "print(\"ğŸ§ª æ¸¬è©¦å®Œæ•´ Self-Attention å±¤\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "seq_len = 6\n",
    "d_model = 512\n",
    "d_k = d_v = 64\n",
    "\n",
    "# å‰µå»º Self-Attention å±¤\n",
    "self_attn = SelfAttention(d_model, d_k, d_v)\n",
    "\n",
    "# æ¨¡æ“¬è¼¸å…¥ (ä¾‹å¦‚è©åµŒå…¥ + ä½ç½®ç·¨ç¢¼)\n",
    "X = np.random.randn(seq_len, d_model)\n",
    "\n",
    "# å‰å‘å‚³æ’­\n",
    "output, attn_weights = self_attn(X)\n",
    "\n",
    "print(f\"\\nè¼¸å…¥å½¢ç‹€: {X.shape}\")\n",
    "print(f\"è¼¸å‡ºå½¢ç‹€: {output.shape}\")\n",
    "print(f\"Attention æ¬Šé‡å½¢ç‹€: {attn_weights.shape}\")\n",
    "\n",
    "print(f\"\\nåƒæ•¸é‡:\")\n",
    "print(f\"  W_q: {self_attn.W_q.shape} = {np.prod(self_attn.W_q.shape):,} åƒæ•¸\")\n",
    "print(f\"  W_k: {self_attn.W_k.shape} = {np.prod(self_attn.W_k.shape):,} åƒæ•¸\")\n",
    "print(f\"  W_v: {self_attn.W_v.shape} = {np.prod(self_attn.W_v.shape):,} åƒæ•¸\")\n",
    "total_params = np.prod(self_attn.W_q.shape) + np.prod(self_attn.W_k.shape) + np.prod(self_attn.W_v.shape)\n",
    "print(f\"  ç¸½è¨ˆ: {total_params:,} åƒæ•¸\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "## 4. Multi-Head Attention æ·±å…¥\n",
    "\n",
    "### 4.1 ç‚ºä»€éº¼éœ€è¦å¤šé ­?\n",
    "\n",
    "**å•é¡Œ:** å–®å€‹ Attention é ­å¯èƒ½ç„¡æ³•åŒæ™‚æ•æ‰å¤šç¨®é—œä¿‚\n",
    "\n",
    "#### å¥å­ä¸­çš„å¤šç¨®é—œä¿‚:\n",
    "\n",
    "```\n",
    "\"The cat sat on the mat because it was tired.\"\n",
    "\n",
    "èªæ³•é—œä¿‚:\n",
    "  cat â† sat (ä¸»è©-å‹•è©)\n",
    "  sat â†’ mat (å‹•è©-å—è©)\n",
    "\n",
    "èªç¾©é—œä¿‚:\n",
    "  it â†’ cat (æŒ‡ä»£é—œä¿‚)\n",
    "  tired â†’ cat (ç‹€æ…‹æè¿°)\n",
    "\n",
    "ä½ç½®é—œä¿‚:\n",
    "  cat â† é„°è¿‘è©\n",
    "```\n",
    "\n",
    "**è§£æ±ºæ–¹æ¡ˆ:** ä½¿ç”¨å¤šå€‹é ­,å„è‡ªå­¸ç¿’ä¸åŒé—œä¿‚!\n",
    "\n",
    "---\n",
    "\n",
    "### 4.2 Multi-Head Attention å…¬å¼\n",
    "\n",
    "$$\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1, ..., head_h) W^O\n",
    "$$\n",
    "\n",
    "å…¶ä¸­æ¯å€‹é ­:\n",
    "\n",
    "$$\n",
    "head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n",
    "$$\n",
    "\n",
    "**åƒæ•¸:**\n",
    "- $h$: é ­æ•¸ (è«–æ–‡ä¸­ = 8)\n",
    "- $W_i^Q, W_i^K \\in \\mathbb{R}^{d_{model} \\times d_k}$: ç¬¬ i å€‹é ­çš„ Q/K æŠ•å½±\n",
    "- $W_i^V \\in \\mathbb{R}^{d_{model} \\times d_v}$: ç¬¬ i å€‹é ­çš„ V æŠ•å½±\n",
    "- $W^O \\in \\mathbb{R}^{hd_v \\times d_{model}}$: è¼¸å‡ºæŠ•å½±\n",
    "\n",
    "**è«–æ–‡è¨­å®š:**\n",
    "- $d_{model} = 512$\n",
    "- $h = 8$\n",
    "- $d_k = d_v = d_{model} / h = 64$\n",
    "\n",
    "---\n",
    "\n",
    "### 4.3 ç‚ºä»€éº¼ç¶­åº¦æ˜¯ d_model / h?\n",
    "\n",
    "**è¨ˆç®—æˆæœ¬è€ƒé‡:**\n",
    "\n",
    "```\n",
    "å–®é ­ Attention (d_k = 512):\n",
    "  è¨ˆç®—æˆæœ¬: O(seq_len^2 Ã— 512)\n",
    "\n",
    "å¤šé ­ Attention (8 é ­, d_k = 64):\n",
    "  æ¯å€‹é ­: O(seq_len^2 Ã— 64)\n",
    "  ç¸½æˆæœ¬: 8 Ã— O(seq_len^2 Ã— 64) = O(seq_len^2 Ã— 512)\n",
    "```\n",
    "\n",
    "**çµè«–:** è¨ˆç®—æˆæœ¬ç›¸åŒ,ä½†è¡¨é”èƒ½åŠ›æ›´å¼·!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention:\n",
    "    \"\"\"\n",
    "    Multi-Head Self-Attention å®Œæ•´å¯¦ä½œ\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, num_heads: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: æ¨¡å‹ç¶­åº¦ (åµŒå…¥ç¶­åº¦)\n",
    "            num_heads: é ­æ•¸\n",
    "        \"\"\"\n",
    "        assert d_model % num_heads == 0, \"d_model å¿…é ˆå¯è¢« num_heads æ•´é™¤\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads  # æ¯å€‹é ­çš„ç¶­åº¦\n",
    "        \n",
    "        # æ¯å€‹é ­çš„æŠ•å½±çŸ©é™£\n",
    "        self.W_q = np.random.randn(num_heads, d_model, self.d_k) / np.sqrt(d_model)\n",
    "        self.W_k = np.random.randn(num_heads, d_model, self.d_k) / np.sqrt(d_model)\n",
    "        self.W_v = np.random.randn(num_heads, d_model, self.d_k) / np.sqrt(d_model)\n",
    "        \n",
    "        # è¼¸å‡ºæŠ•å½±çŸ©é™£\n",
    "        self.W_o = np.random.randn(d_model, d_model) / np.sqrt(d_model)\n",
    "    \n",
    "    def forward(self, X: np.ndarray, mask: Optional[np.ndarray] = None):\n",
    "        \"\"\"\n",
    "        å‰å‘å‚³æ’­\n",
    "        \n",
    "        Args:\n",
    "            X: è¼¸å…¥, shape (seq_len, d_model)\n",
    "            mask: å¯é¸é®ç½©, shape (seq_len, seq_len)\n",
    "            \n",
    "        Returns:\n",
    "            output: shape (seq_len, d_model)\n",
    "            all_attention_weights: list of (seq_len, seq_len), æ¯å€‹é ­çš„æ¬Šé‡\n",
    "        \"\"\"\n",
    "        seq_len = X.shape[0]\n",
    "        head_outputs = []\n",
    "        all_attention_weights = []\n",
    "        \n",
    "        # å°æ¯å€‹é ­åˆ†åˆ¥è¨ˆç®—\n",
    "        for i in range(self.num_heads):\n",
    "            # æŠ•å½±åˆ° Q, K, V\n",
    "            Q = np.dot(X, self.W_q[i])  # (seq_len, d_k)\n",
    "            K = np.dot(X, self.W_k[i])\n",
    "            V = np.dot(X, self.W_v[i])\n",
    "            \n",
    "            # è¨ˆç®— Attention\n",
    "            head_output, attn_weights = scaled_dot_product_attention(Q, K, V, mask)\n",
    "            \n",
    "            head_outputs.append(head_output)\n",
    "            all_attention_weights.append(attn_weights)\n",
    "        \n",
    "        # Concatenate all heads\n",
    "        multi_head_output = np.concatenate(head_outputs, axis=-1)  # (seq_len, num_heads * d_k)\n",
    "        \n",
    "        # æœ€çµ‚ç·šæ€§æŠ•å½±\n",
    "        output = np.dot(multi_head_output, self.W_o)  # (seq_len, d_model)\n",
    "        \n",
    "        return output, all_attention_weights\n",
    "    \n",
    "    def __call__(self, X, mask=None):\n",
    "        return self.forward(X, mask)\n",
    "\n",
    "\n",
    "# æ¸¬è©¦ Multi-Head Attention\n",
    "print(\"ğŸ§ª æ¸¬è©¦ Multi-Head Attention\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "seq_len = 6\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "\n",
    "# å‰µå»º Multi-Head Attention å±¤\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "# æ¨¡æ“¬è¼¸å…¥\n",
    "X = np.random.randn(seq_len, d_model)\n",
    "\n",
    "# å‰å‘å‚³æ’­\n",
    "output, head_attentions = mha(X)\n",
    "\n",
    "print(f\"\\né…ç½®:\")\n",
    "print(f\"  æ¨¡å‹ç¶­åº¦ (d_model): {d_model}\")\n",
    "print(f\"  é ­æ•¸ (num_heads): {num_heads}\")\n",
    "print(f\"  æ¯å€‹é ­ç¶­åº¦ (d_k): {mha.d_k}\")\n",
    "\n",
    "print(f\"\\nè¼¸å…¥å½¢ç‹€: {X.shape}\")\n",
    "print(f\"è¼¸å‡ºå½¢ç‹€: {output.shape}\")\n",
    "print(f\"æ¯å€‹é ­çš„ Attention æ¬Šé‡: {len(head_attentions)} å€‹, æ¯å€‹å½¢ç‹€ {head_attentions[0].shape}\")\n",
    "\n",
    "print(f\"\\nåƒæ•¸é‡è¨ˆç®—:\")\n",
    "params_qkv = num_heads * d_model * mha.d_k * 3  # Q, K, V\n",
    "params_o = d_model * d_model  # W_o\n",
    "total = params_qkv + params_o\n",
    "print(f\"  Q/K/V æŠ•å½±: {params_qkv:,}\")\n",
    "print(f\"  è¼¸å‡ºæŠ•å½±: {params_o:,}\")\n",
    "print(f\"  ç¸½è¨ˆ: {total:,} åƒæ•¸\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¦–è¦ºåŒ–: 8 å€‹é ­çš„ Attention æ¨¡å¼\n",
    "fig, axes = plt.subplots(2, 4, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(num_heads):\n",
    "    sns.heatmap(head_attentions[i], annot=False, cmap='YlOrRd', \n",
    "                ax=axes[i], cbar=True, vmin=0, vmax=1,\n",
    "                cbar_kws={'label': 'Score'})\n",
    "    axes[i].set_title(f'Head {i+1}', fontsize=11, fontweight='bold')\n",
    "    axes[i].set_xlabel('Key', fontsize=9)\n",
    "    axes[i].set_ylabel('Query', fontsize=9)\n",
    "\n",
    "plt.suptitle('Multi-Head Attention: 8 å€‹é ­çš„ä¸åŒé—œæ³¨æ¨¡å¼', \n",
    "             fontsize=16, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ” è§€å¯Ÿ:\")\n",
    "print(\"  - ä¸åŒé ­å­¸ç¿’åˆ°ä¸åŒçš„ Attention æ¨¡å¼\")\n",
    "print(\"  - æœ‰äº›é ­é—œæ³¨å±€éƒ¨ (å°è§’ç·š),æœ‰äº›é—œæ³¨å…¨å±€\")\n",
    "print(\"  - å¤šæ¨£æ€§è®“æ¨¡å‹æ•æ‰è¤‡é›œé—œä¿‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "## 5. Masked Attention æ©Ÿåˆ¶\n",
    "\n",
    "### 5.1 ç‚ºä»€éº¼éœ€è¦ Mask?\n",
    "\n",
    "#### æ‡‰ç”¨å ´æ™¯ 1: Padding Mask\n",
    "\n",
    "**å•é¡Œ:** æ‰¹æ¬¡è¨“ç·´æ™‚,åºåˆ—é•·åº¦ä¸åŒéœ€è¦å¡«å…… (padding)\n",
    "\n",
    "```python\n",
    "å¥å­ 1: \"I love NLP\"        â†’ [20, 45, 89, 0, 0]  â† å¡«å…… 0\n",
    "å¥å­ 2: \"Transformer rocks\" â†’ [78, 12, 0, 0, 0]\n",
    "```\n",
    "\n",
    "**Padding Token ä¸æ‡‰è©²è¢«é—œæ³¨!**\n",
    "\n",
    "---\n",
    "\n",
    "#### æ‡‰ç”¨å ´æ™¯ 2: Look-Ahead Mask (Decoder)\n",
    "\n",
    "**å•é¡Œ:** Decoder ç”Ÿæˆæ™‚ä¸èƒ½\"å·çœ‹\"æœªä¾†çš„è©!\n",
    "\n",
    "```\n",
    "ç”Ÿæˆ \"I love NLP\":\n",
    "\n",
    "æ™‚é–“ t=1: åªèƒ½çœ‹ \"I\"           â†’ é æ¸¬ \"love\"\n",
    "æ™‚é–“ t=2: åªèƒ½çœ‹ \"I love\"      â†’ é æ¸¬ \"NLP\"\n",
    "æ™‚é–“ t=3: åªèƒ½çœ‹ \"I love NLP\"  â†’ é æ¸¬ <EOS>\n",
    "```\n",
    "\n",
    "**å¿…é ˆé®ç½©æœªä¾†ä½ç½®!**\n",
    "\n",
    "---\n",
    "\n",
    "### 5.2 Mask çš„å¯¦ä½œ\n",
    "\n",
    "#### Look-Ahead Mask (ä¸‹ä¸‰è§’é®ç½©)\n",
    "\n",
    "$$\n",
    "\\text{Mask} = \\begin{bmatrix}\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "1 & 1 & 0 & 0 \\\\\n",
    "1 & 1 & 1 & 0 \\\\\n",
    "1 & 1 & 1 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- 1: å¯ä»¥é—œæ³¨ (Allow)\n",
    "- 0: ä¸å¯é—œæ³¨ (Mask) â†’ è¨­ç‚º $-\\infty$ (Softmax å¾Œè®Šç‚º 0)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(seq_len: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    å‰µå»º Look-Ahead Mask (ä¸‹ä¸‰è§’çŸ©é™£)\n",
    "    \n",
    "    Args:\n",
    "        seq_len: åºåˆ—é•·åº¦\n",
    "        \n",
    "    Returns:\n",
    "        mask: shape (seq_len, seq_len), 1=å…è¨±é—œæ³¨, 0=é®ç½©\n",
    "    \"\"\"\n",
    "    mask = np.tril(np.ones((seq_len, seq_len)))  # ä¸‹ä¸‰è§’çŸ©é™£\n",
    "    return mask\n",
    "\n",
    "\n",
    "def create_padding_mask(seq: np.ndarray, pad_token: int = 0) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    å‰µå»º Padding Mask\n",
    "    \n",
    "    Args:\n",
    "        seq: Token ID åºåˆ—, shape (seq_len,)\n",
    "        pad_token: Padding token çš„ ID\n",
    "        \n",
    "    Returns:\n",
    "        mask: shape (seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    # Padding ä½ç½®è¨­ç‚º 0, å…¶ä»–ç‚º 1\n",
    "    mask = (seq != pad_token).astype(int)\n",
    "    # æ“´å±•ç‚ºçŸ©é™£\n",
    "    mask = mask[:, np.newaxis] * mask[np.newaxis, :]\n",
    "    return mask\n",
    "\n",
    "\n",
    "# æ¸¬è©¦ Look-Ahead Mask\n",
    "seq_len = 6\n",
    "look_ahead_mask = create_look_ahead_mask(seq_len)\n",
    "\n",
    "print(\"ğŸ­ Look-Ahead Mask (Decoder ä½¿ç”¨)\")\n",
    "print(\"=\" * 50)\n",
    "print(look_ahead_mask.astype(int))\n",
    "\n",
    "# æ¸¬è©¦ Padding Mask\n",
    "seq_with_padding = np.array([20, 45, 89, 12, 0, 0])  # æœ€å¾Œå…©å€‹æ˜¯ padding\n",
    "padding_mask = create_padding_mask(seq_with_padding)\n",
    "\n",
    "print(\"\\nğŸ­ Padding Mask (Encoder ä½¿ç”¨)\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"åºåˆ—: {seq_with_padding} (0 æ˜¯ padding)\")\n",
    "print(f\"\\nPadding Mask:\\n{padding_mask.astype(int)}\")\n",
    "\n",
    "# è¦–è¦ºåŒ–å…©ç¨® Mask\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Look-Ahead Mask\n",
    "sns.heatmap(look_ahead_mask, annot=True, fmt='.0f', cmap='RdYlGn', \n",
    "            ax=axes[0], cbar_kws={'label': '1=Allow, 0=Mask'},\n",
    "            xticklabels=[f't{i+1}' for i in range(seq_len)],\n",
    "            yticklabels=[f't{i+1}' for i in range(seq_len)])\n",
    "axes[0].set_title('Look-Ahead Mask\\n(é˜²æ­¢çœ‹åˆ°æœªä¾†)', fontsize=13, fontweight='bold')\n",
    "axes[0].set_xlabel('æ™‚é–“ (è¢«é—œæ³¨ä½ç½®)')\n",
    "axes[0].set_ylabel('æ™‚é–“ (æŸ¥è©¢ä½ç½®)')\n",
    "\n",
    "# Padding Mask\n",
    "sns.heatmap(padding_mask, annot=True, fmt='.0f', cmap='RdYlGn',\n",
    "            ax=axes[1], cbar_kws={'label': '1=Allow, 0=Mask'},\n",
    "            xticklabels=[f'Pos{i+1}' for i in range(seq_len)],\n",
    "            yticklabels=[f'Pos{i+1}' for i in range(seq_len)])\n",
    "axes[1].set_title('Padding Mask\\n(å¿½ç•¥ Padding)', fontsize=13, fontweight='bold')\n",
    "axes[1].set_xlabel('ä½ç½® (è¢«é—œæ³¨ä½ç½®)')\n",
    "axes[1].set_ylabel('ä½ç½® (æŸ¥è©¢ä½ç½®)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ’¡ é‡é»:\")\n",
    "print(\"  - Look-Ahead Mask: Decoder è‡ªå›æ­¸ç”Ÿæˆç”¨\")\n",
    "print(\"  - Padding Mask: å¿½ç•¥å¡«å……çš„ç„¡æ•ˆ token\")\n",
    "print(\"  - å¯ä»¥çµ„åˆä½¿ç”¨: combined_mask = look_ahead_mask & padding_mask\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a>\n",
    "## 6. Attention è¦–è¦ºåŒ–èˆ‡è§£é‡‹æ€§\n",
    "\n",
    "### 6.1 çœŸå¯¦å¥å­çš„ Attention åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä½¿ç”¨çœŸå¯¦å¥å­æ¸¬è©¦ Attention\n",
    "sentence = \"The cat sat on the mat\".split()\n",
    "seq_len = len(sentence)\n",
    "d_model = 128\n",
    "\n",
    "# æ¨¡æ“¬è©åµŒå…¥ (å¯¦éš›æ‡‰ä½¿ç”¨è¨“ç·´å¥½çš„åµŒå…¥)\n",
    "embeddings = np.random.randn(seq_len, d_model)\n",
    "\n",
    "# å‰µå»º Self-Attention å±¤\n",
    "self_attn = SelfAttention(d_model, d_k=64, d_v=64)\n",
    "\n",
    "# è¨ˆç®— Attention\n",
    "output, attn_weights = self_attn(embeddings)\n",
    "\n",
    "# è¦–è¦ºåŒ– Attention æ¬Šé‡\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(attn_weights, annot=True, fmt='.2f', cmap='YlOrRd',\n",
    "            xticklabels=sentence,\n",
    "            yticklabels=sentence,\n",
    "            cbar_kws={'label': 'Attention Score'})\n",
    "plt.xlabel('Key (è¢«é—œæ³¨çš„è©)', fontsize=12)\n",
    "plt.ylabel('Query (æŸ¥è©¢è©)', fontsize=12)\n",
    "plt.title('Self-Attention æ¬Šé‡è¦–è¦ºåŒ–\\nå¥å­: \"The cat sat on the mat\"', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# åˆ†ææ¯å€‹è©æœ€é—œæ³¨çš„å°è±¡\n",
    "print(\"\\nğŸ” Attention æ¨¡å¼åˆ†æ:\")\n",
    "print(\"=\" * 50)\n",
    "for i, query_word in enumerate(sentence):\n",
    "    top_3_idx = np.argsort(attn_weights[i])[-3:][::-1]\n",
    "    top_3_words = [sentence[idx] for idx in top_3_idx]\n",
    "    top_3_scores = [attn_weights[i, idx] for idx in top_3_idx]\n",
    "    \n",
    "    print(f\"\\n'{query_word}' æœ€é—œæ³¨:\")\n",
    "    for word, score in zip(top_3_words, top_3_scores):\n",
    "        print(f\"  â†’ {word:8s}: {score:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Attention çš„å¯è§£é‡‹æ€§\n",
    "\n",
    "**å„ªé»:** Attention æ¬Šé‡å¯è¦–åŒ–æä¾›æ¨¡å‹æ±ºç­–çš„ã€Œé€æ˜åº¦ã€\n",
    "\n",
    "#### å¯¦éš›æ‡‰ç”¨æ¡ˆä¾‹:\n",
    "\n",
    "1. **æ©Ÿå™¨ç¿»è­¯:**\n",
    "   - æŸ¥çœ‹æºèªè¨€èˆ‡ç›®æ¨™èªè¨€çš„å°é½Š\n",
    "   - æª¢æŸ¥æ¨¡å‹æ˜¯å¦å­¸åˆ°æ­£ç¢ºçš„èªæ³•çµæ§‹\n",
    "\n",
    "2. **æ–‡æœ¬æ‘˜è¦:**\n",
    "   - æŸ¥çœ‹æ‘˜è¦å¥é—œæ³¨åŸæ–‡å“ªäº›éƒ¨åˆ†\n",
    "   - é©—è­‰æ˜¯å¦æ“·å–é—œéµè¨Šæ¯\n",
    "\n",
    "3. **å•ç­”ç³»çµ±:**\n",
    "   - æŸ¥çœ‹ç­”æ¡ˆé—œæ³¨å•é¡Œèˆ‡æ–‡ç« çš„å“ªäº›è©\n",
    "   - Debug éŒ¯èª¤ç­”æ¡ˆçš„åŸå› \n",
    "\n",
    "---\n",
    "\n",
    "### 6.3 Attention çš„å±€é™æ€§\n",
    "\n",
    "**è­¦å‘Š:** Attention ä¸æ˜¯è¬èƒ½çš„è§£é‡‹å·¥å…·!\n",
    "\n",
    "1. **å¤šé ­é›£ä»¥è§£é‡‹:** 8-16 å€‹é ­,æ¯å€‹å­¸ä»€éº¼?\n",
    "2. **å±¤ç–Šæ•ˆæ‡‰:** 12-24 å±¤,æœ€çµ‚æ±ºç­–é›£è¿½è¹¤\n",
    "3. **æ¬Šé‡ â‰  é‡è¦æ€§:** é«˜ Attention ä¸ä¸€å®šä»£è¡¨é«˜é‡è¦æ€§\n",
    "\n",
    "**å»ºè­°:** çµåˆå¤šç¨®è§£é‡‹æ–¹æ³• (Attention + LIME + SHAP)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"7\"></a>\n",
    "## 7. å®Œæ•´ Attention æ¨¡çµ„å¯¦ä½œ\n",
    "\n",
    "### 7.1 Production-Ready ç‰ˆæœ¬\n",
    "\n",
    "æ•´åˆæ‰€æœ‰åŠŸèƒ½:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompleteMultiHeadAttention:\n",
    "    \"\"\"\n",
    "    å®Œæ•´ Multi-Head Attention å¯¦ä½œ\n",
    "    åŒ…å«: Dropout, Masking, å®Œæ•´åƒæ•¸ç®¡ç†\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: æ¨¡å‹ç¶­åº¦\n",
    "            num_heads: é ­æ•¸\n",
    "            dropout: Dropout æ¯”ä¾‹\n",
    "        \"\"\"\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # æ‰€æœ‰é ­å…±äº«çš„æŠ•å½±çŸ©é™£ (æ›´é«˜æ•ˆçš„å¯¦ä½œæ–¹å¼)\n",
    "        self.W_q = self._init_weights((d_model, d_model))\n",
    "        self.W_k = self._init_weights((d_model, d_model))\n",
    "        self.W_v = self._init_weights((d_model, d_model))\n",
    "        self.W_o = self._init_weights((d_model, d_model))\n",
    "    \n",
    "    def _init_weights(self, shape):\n",
    "        \"\"\"Xavier åˆå§‹åŒ–\"\"\"\n",
    "        return np.random.randn(*shape) / np.sqrt(shape[0])\n",
    "    \n",
    "    def _split_heads(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        åˆ†å‰²ç‚ºå¤šé ­\n",
    "        \n",
    "        Args:\n",
    "            x: shape (seq_len, d_model)\n",
    "            \n",
    "        Returns:\n",
    "            shape (num_heads, seq_len, d_k)\n",
    "        \"\"\"\n",
    "        seq_len = x.shape[0]\n",
    "        x = x.reshape(seq_len, self.num_heads, self.d_k)\n",
    "        return x.transpose(1, 0, 2)  # (num_heads, seq_len, d_k)\n",
    "    \n",
    "    def _merge_heads(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        åˆä½µå¤šé ­\n",
    "        \n",
    "        Args:\n",
    "            x: shape (num_heads, seq_len, d_k)\n",
    "            \n",
    "        Returns:\n",
    "            shape (seq_len, d_model)\n",
    "        \"\"\"\n",
    "        x = x.transpose(1, 0, 2)  # (seq_len, num_heads, d_k)\n",
    "        seq_len = x.shape[0]\n",
    "        return x.reshape(seq_len, self.d_model)\n",
    "    \n",
    "    def forward(\n",
    "        self, \n",
    "        X: np.ndarray, \n",
    "        mask: Optional[np.ndarray] = None,\n",
    "        training: bool = False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        å‰å‘å‚³æ’­\n",
    "        \n",
    "        Args:\n",
    "            X: è¼¸å…¥, shape (seq_len, d_model)\n",
    "            mask: å¯é¸é®ç½©, shape (seq_len, seq_len)\n",
    "            training: æ˜¯å¦è¨“ç·´æ¨¡å¼ (æ±ºå®šæ˜¯å¦ dropout)\n",
    "            \n",
    "        Returns:\n",
    "            output: shape (seq_len, d_model)\n",
    "            attention_weights: shape (num_heads, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        seq_len = X.shape[0]\n",
    "        \n",
    "        # ç·šæ€§æŠ•å½±\n",
    "        Q = np.dot(X, self.W_q)  # (seq_len, d_model)\n",
    "        K = np.dot(X, self.W_k)\n",
    "        V = np.dot(X, self.W_v)\n",
    "        \n",
    "        # åˆ†å‰²ç‚ºå¤šé ­\n",
    "        Q = self._split_heads(Q)  # (num_heads, seq_len, d_k)\n",
    "        K = self._split_heads(K)\n",
    "        V = self._split_heads(V)\n",
    "        \n",
    "        # å°æ¯å€‹é ­è¨ˆç®— Attention\n",
    "        head_outputs = []\n",
    "        all_attention_weights = []\n",
    "        \n",
    "        for i in range(self.num_heads):\n",
    "            head_output, attn_weights = scaled_dot_product_attention(\n",
    "                Q[i], K[i], V[i], mask\n",
    "            )\n",
    "            head_outputs.append(head_output)\n",
    "            all_attention_weights.append(attn_weights)\n",
    "        \n",
    "        # Stack heads\n",
    "        multi_head = np.stack(head_outputs, axis=0)  # (num_heads, seq_len, d_k)\n",
    "        all_attention_weights = np.stack(all_attention_weights, axis=0)  # (num_heads, seq_len, seq_len)\n",
    "        \n",
    "        # åˆä½µå¤šé ­\n",
    "        concat_output = self._merge_heads(multi_head)  # (seq_len, d_model)\n",
    "        \n",
    "        # æœ€çµ‚ç·šæ€§æŠ•å½±\n",
    "        output = np.dot(concat_output, self.W_o)\n",
    "        \n",
    "        # Dropout (è¨“ç·´æ™‚)\n",
    "        if training and self.dropout > 0:\n",
    "            dropout_mask = np.random.binomial(1, 1-self.dropout, output.shape)\n",
    "            output = output * dropout_mask / (1 - self.dropout)\n",
    "        \n",
    "        return output, all_attention_weights\n",
    "    \n",
    "    def __call__(self, X, mask=None, training=False):\n",
    "        return self.forward(X, mask, training)\n",
    "\n",
    "\n",
    "# æ¸¬è©¦å®Œæ•´ç‰ˆ Multi-Head Attention\n",
    "print(\"ğŸ§ª æ¸¬è©¦ Complete Multi-Head Attention\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "seq_len = 10\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "\n",
    "complete_mha = CompleteMultiHeadAttention(d_model, num_heads, dropout=0.1)\n",
    "\n",
    "# æ¨¡æ“¬è¼¸å…¥\n",
    "X = np.random.randn(seq_len, d_model)\n",
    "\n",
    "# æ¸¬è©¦ 1: ç„¡é®ç½©\n",
    "output_no_mask, _ = complete_mha(X, training=False)\n",
    "print(f\"\\næ¸¬è©¦ 1 - ç„¡é®ç½©:\")\n",
    "print(f\"  è¼¸å…¥å½¢ç‹€: {X.shape}\")\n",
    "print(f\"  è¼¸å‡ºå½¢ç‹€: {output_no_mask.shape}\")\n",
    "\n",
    "# æ¸¬è©¦ 2: ä½¿ç”¨ Look-Ahead Mask\n",
    "mask = create_look_ahead_mask(seq_len)\n",
    "output_masked, attn_masked = complete_mha(X, mask=mask, training=False)\n",
    "print(f\"\\næ¸¬è©¦ 2 - Look-Ahead Mask:\")\n",
    "print(f\"  è¼¸å‡ºå½¢ç‹€: {output_masked.shape}\")\n",
    "print(f\"  Attention æ¬Šé‡å½¢ç‹€: {attn_masked.shape}\")\n",
    "\n",
    "# æ¸¬è©¦ 3: è¨“ç·´æ¨¡å¼ (å« Dropout)\n",
    "output_train, _ = complete_mha(X, training=True)\n",
    "print(f\"\\næ¸¬è©¦ 3 - è¨“ç·´æ¨¡å¼ (Dropout=0.1):\")\n",
    "print(f\"  è¼¸å‡ºå½¢ç‹€: {output_train.shape}\")\n",
    "\n",
    "print(f\"\\nâœ… æ‰€æœ‰æ¸¬è©¦é€šé!\")\n",
    "print(f\"\\nğŸ“Š ç¸½åƒæ•¸é‡: {d_model * d_model * 4:,}\")\n",
    "print(f\"   (4 å€‹æŠ•å½±çŸ©é™£: W_q, W_k, W_v, W_o)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¦–è¦ºåŒ–æœ‰ç„¡ Mask çš„å·®ç•°\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# ç„¡é®ç½©çš„ Attention\n",
    "output_full, attn_full = complete_mha(X, mask=None, training=False)\n",
    "sns.heatmap(attn_full[0], annot=False, cmap='YlOrRd', ax=axes[0], vmin=0, vmax=0.5)\n",
    "axes[0].set_title('ç„¡é®ç½© Attention\\n(å¯çœ‹åˆ°æ‰€æœ‰ä½ç½®)', fontsize=13, fontweight='bold')\n",
    "axes[0].set_xlabel('Key Position')\n",
    "axes[0].set_ylabel('Query Position')\n",
    "\n",
    "# Look-Ahead Mask\n",
    "sns.heatmap(attn_masked[0], annot=False, cmap='YlOrRd', ax=axes[1], vmin=0, vmax=0.5)\n",
    "axes[1].set_title('Look-Ahead Mask Attention\\n(åªèƒ½çœ‹åˆ°éå»)', fontsize=13, fontweight='bold')\n",
    "axes[1].set_xlabel('Key Position')\n",
    "axes[1].set_ylabel('Query Position')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ¯ å°æ¯”:\")\n",
    "print(\"  å·¦åœ–: Encoder ä½¿ç”¨ (é›™å‘,å¯çœ‹å…¨æ–‡)\")\n",
    "print(\"  å³åœ–: Decoder ä½¿ç”¨ (å–®å‘,åªçœ‹éå»)\")\n",
    "print(\"  å³åœ–ä¸Šä¸‰è§’å…¨é»‘: æœªä¾†è¢«é®ç½©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ§® æ•¸å­¸è£œå……: ç‚ºä»€éº¼æ˜¯é»ç©?\n",
    "\n",
    "### ç›¸ä¼¼åº¦è¨ˆç®—çš„ä¸‰ç¨®æ–¹æ³•\n",
    "\n",
    "#### 1. é»ç© (Dot Product) - Transformer ä½¿ç”¨\n",
    "\n",
    "$$\n",
    "\\text{score}(q, k) = q^T k\n",
    "$$\n",
    "\n",
    "- âœ… è¨ˆç®—æ•ˆç‡é«˜ (çŸ©é™£ä¹˜æ³•)\n",
    "- âœ… GPU å‹å¥½\n",
    "- âš ï¸ å—å‘é‡é•·åº¦å½±éŸ¿ (éœ€ç¸®æ”¾)\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. é¤˜å¼¦ç›¸ä¼¼åº¦ (Cosine Similarity)\n",
    "\n",
    "$$\n",
    "\\text{score}(q, k) = \\frac{q^T k}{||q|| \\cdot ||k||}\n",
    "$$\n",
    "\n",
    "- âœ… ä¸å—å‘é‡é•·åº¦å½±éŸ¿\n",
    "- âŒ è¨ˆç®—æˆæœ¬è¼ƒé«˜ (éœ€æ­£è¦åŒ–)\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. åŠ æ€§ Attention (Additive) - Bahdanau Attention\n",
    "\n",
    "$$\n",
    "\\text{score}(q, k) = v^T \\tanh(W_q q + W_k k)\n",
    "$$\n",
    "\n",
    "- âœ… è¡¨é”èƒ½åŠ›å¼·\n",
    "- âŒ åƒæ•¸é‡å¤š,è¨ˆç®—æ…¢\n",
    "\n",
    "---\n",
    "\n",
    "**Transformer é¸æ“‡é»ç©çš„åŸå› :**\n",
    "1. é€Ÿåº¦æœ€å¿« (çŸ©é™£é‹ç®—é«˜åº¦å„ªåŒ–)\n",
    "2. ç¸®æ”¾å¾Œæ•ˆæœèˆ‡å…¶ä»–æ–¹æ³•ç›¸ç•¶\n",
    "3. å¯¦ä½œç°¡å–®\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¯”è¼ƒä¸‰ç¨®ç›¸ä¼¼åº¦è¨ˆç®—æ–¹æ³•\n",
    "def compare_similarity_methods():\n",
    "    \"\"\"æ¯”è¼ƒä¸åŒ Attention Score è¨ˆç®—æ–¹æ³•\"\"\"\n",
    "    \n",
    "    # å…©å€‹æ¸¬è©¦å‘é‡\n",
    "    q = np.array([1.0, 2.0, 3.0, 4.0])\n",
    "    k1 = np.array([1.0, 2.0, 3.0, 4.0])  # èˆ‡ q ç›¸åŒ\n",
    "    k2 = np.array([4.0, 3.0, 2.0, 1.0])  # èˆ‡ q ç›¸å\n",
    "    k3 = np.array([0.0, 0.0, 0.0, 0.0])  # é›¶å‘é‡\n",
    "    \n",
    "    keys = [k1, k2, k3]\n",
    "    key_names = ['k1 (ç›¸åŒ)', 'k2 (ç›¸å)', 'k3 (é›¶)']\n",
    "    \n",
    "    results = {'Dot Product': [], 'Cosine': [], 'Scaled Dot': []}\n",
    "    \n",
    "    for k in keys:\n",
    "        # æ–¹æ³• 1: é»ç©\n",
    "        dot = np.dot(q, k)\n",
    "        results['Dot Product'].append(dot)\n",
    "        \n",
    "        # æ–¹æ³• 2: é¤˜å¼¦ç›¸ä¼¼åº¦\n",
    "        cos = np.dot(q, k) / (np.linalg.norm(q) * np.linalg.norm(k) + 1e-8)\n",
    "        results['Cosine'].append(cos)\n",
    "        \n",
    "        # æ–¹æ³• 3: ç¸®æ”¾é»ç©\n",
    "        scaled = dot / np.sqrt(len(q))\n",
    "        results['Scaled Dot'].append(scaled)\n",
    "    \n",
    "    # è¦–è¦ºåŒ–æ¯”è¼ƒ\n",
    "    x = np.arange(len(key_names))\n",
    "    width = 0.25\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    for i, (method, scores) in enumerate(results.items()):\n",
    "        ax.bar(x + i*width, scores, width, label=method, alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('Key Vectors', fontsize=12)\n",
    "    ax.set_ylabel('Similarity Score', fontsize=12)\n",
    "    ax.set_title('ä¸‰ç¨®ç›¸ä¼¼åº¦è¨ˆç®—æ–¹æ³•å°æ¯”', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x + width)\n",
    "    ax.set_xticklabels(key_names)\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    ax.axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # æ‰“å°æ•¸å€¼\n",
    "    print(\"\\nğŸ“Š æ•¸å€¼å°æ¯”:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"{'Key':<15} {'Dot Product':>15} {'Cosine':>15} {'Scaled Dot':>15}\")\n",
    "    print(\"=\" * 60)\n",
    "    for i, name in enumerate(key_names):\n",
    "        print(f\"{name:<15} {results['Dot Product'][i]:>15.2f} {results['Cosine'][i]:>15.2f} {results['Scaled Dot'][i]:>15.2f}\")\n",
    "\n",
    "compare_similarity_methods()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ å¯¦æˆ°æ¡ˆä¾‹: å®Œæ•´ Transformer Encoder Layer\n",
    "\n",
    "çµåˆæ‰€æœ‰çµ„ä»¶,å¯¦ä½œä¸€å€‹å®Œæ•´çš„ Encoder å±¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer:\n",
    "    \"\"\"\n",
    "    å®Œæ•´ Transformer Encoder å±¤\n",
    "    = Multi-Head Attention + Feed Forward + æ®˜å·®é€£æ¥ + LayerNorm\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout: float = 0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: æ¨¡å‹ç¶­åº¦\n",
    "            num_heads: Attention é ­æ•¸\n",
    "            d_ff: Feed-Forward ä¸­é–“å±¤ç¶­åº¦\n",
    "            dropout: Dropout æ¯”ä¾‹\n",
    "        \"\"\"\n",
    "        self.mha = CompleteMultiHeadAttention(d_model, num_heads, dropout)\n",
    "        \n",
    "        # Feed-Forward Network\n",
    "        self.W1 = np.random.randn(d_model, d_ff) / np.sqrt(d_model)\n",
    "        self.b1 = np.zeros(d_ff)\n",
    "        self.W2 = np.random.randn(d_ff, d_model) / np.sqrt(d_ff)\n",
    "        self.b2 = np.zeros(d_model)\n",
    "        \n",
    "        self.dropout = dropout\n",
    "    \n",
    "    def feed_forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Position-wise Feed-Forward Network\"\"\"\n",
    "        # FFN(x) = max(0, xW1 + b1)W2 + b2\n",
    "        hidden = np.maximum(0, np.dot(x, self.W1) + self.b1)  # ReLU activation\n",
    "        output = np.dot(hidden, self.W2) + self.b2\n",
    "        return output\n",
    "    \n",
    "    def layer_norm(self, x: np.ndarray, epsilon: float = 1e-6) -> np.ndarray:\n",
    "        \"\"\"Layer Normalization\"\"\"\n",
    "        mean = x.mean(axis=-1, keepdims=True)\n",
    "        std = x.std(axis=-1, keepdims=True)\n",
    "        return (x - mean) / (std + epsilon)\n",
    "    \n",
    "    def forward(self, x: np.ndarray, mask: Optional[np.ndarray] = None):\n",
    "        \"\"\"\n",
    "        å‰å‘å‚³æ’­\n",
    "        \n",
    "        Args:\n",
    "            x: è¼¸å…¥, shape (seq_len, d_model)\n",
    "            mask: å¯é¸é®ç½©\n",
    "            \n",
    "        Returns:\n",
    "            output: shape (seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # Sub-layer 1: Multi-Head Attention + Residual + LayerNorm\n",
    "        attn_output, _ = self.mha(x, mask, training=False)\n",
    "        x = self.layer_norm(x + attn_output)  # æ®˜å·®é€£æ¥ + LayerNorm\n",
    "        \n",
    "        # Sub-layer 2: Feed-Forward + Residual + LayerNorm\n",
    "        ff_output = self.feed_forward(x)\n",
    "        output = self.layer_norm(x + ff_output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "# æ¸¬è©¦å®Œæ•´ Encoder Layer\n",
    "print(\"ğŸ§ª æ¸¬è©¦å®Œæ•´ Transformer Encoder Layer\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "d_ff = 2048  # è«–æ–‡ä¸­ = 4 Ã— d_model\n",
    "\n",
    "encoder_layer = TransformerEncoderLayer(d_model, num_heads, d_ff)\n",
    "\n",
    "# æ¨¡æ“¬è¼¸å…¥\n",
    "seq_len = 10\n",
    "X = np.random.randn(seq_len, d_model)\n",
    "\n",
    "# å‰å‘å‚³æ’­\n",
    "output = encoder_layer.forward(X)\n",
    "\n",
    "print(f\"\\nè¼¸å…¥å½¢ç‹€: {X.shape}\")\n",
    "print(f\"è¼¸å‡ºå½¢ç‹€: {output.shape}\")\n",
    "print(f\"\\nâœ… å½¢ç‹€ä¿æŒä¸è®Š (Transformer çš„é‡è¦ç‰¹æ€§!)\")\n",
    "\n",
    "# çµ±è¨ˆè¼¸å‡ºåˆ†å¸ƒ\n",
    "print(f\"\\nè¼¸å‡ºçµ±è¨ˆ:\")\n",
    "print(f\"  å‡å€¼: {output.mean():.6f} (LayerNorm å¾Œæ‡‰æ¥è¿‘ 0)\")\n",
    "print(f\"  æ¨™æº–å·®: {output.std():.6f} (LayerNorm å¾Œæ‡‰æ¥è¿‘ 1)\")\n",
    "print(f\"  æœ€å°å€¼: {output.min():.6f}\")\n",
    "print(f\"  æœ€å¤§å€¼: {output.max():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“š æœ¬èª²ç¸½çµ\n",
    "\n",
    "### æ ¸å¿ƒè¦é»å›é¡§:\n",
    "\n",
    "1. **Scaled Dot-Product Attention:**\n",
    "   - å››æ­¥é©Ÿ: QK^T â†’ ç¸®æ”¾ â†’ Softmax â†’ åŠ æ¬Šçµ„åˆ V\n",
    "   - ç¸®æ”¾å› å­ $\\sqrt{d_k}$ ç©©å®šæ¢¯åº¦\n",
    "   - è¼¸å‡ºå½¢ç‹€èˆ‡è¼¸å…¥ç›¸åŒ\n",
    "\n",
    "2. **Multi-Head Attention:**\n",
    "   - å¤šå€‹é ­ä¸¦è¡Œå­¸ç¿’ä¸åŒé—œä¿‚\n",
    "   - æ¯å€‹é ­ç¶­åº¦ = d_model / num_heads\n",
    "   - è¨ˆç®—æˆæœ¬èˆ‡å–®é ­ç›¸ç•¶,ä½†è¡¨é”èƒ½åŠ›æ›´å¼·\n",
    "\n",
    "3. **Masking æ©Ÿåˆ¶:**\n",
    "   - Padding Mask: å¿½ç•¥å¡«å…… token\n",
    "   - Look-Ahead Mask: Decoder é˜²æ­¢çœ‹åˆ°æœªä¾†\n",
    "   - å¯çµ„åˆä½¿ç”¨\n",
    "\n",
    "4. **å®Œæ•´ Encoder Layer:**\n",
    "   - Multi-Head Attention\n",
    "   - Feed-Forward Network\n",
    "   - Residual Connection (æ®˜å·®)\n",
    "   - Layer Normalization (æ­¸ä¸€åŒ–)\n",
    "\n",
    "5. **Attention å¯è§£é‡‹æ€§:**\n",
    "   - æ¬Šé‡è¦–è¦ºåŒ–æä¾›é€æ˜åº¦\n",
    "   - ä½†ä¸æ˜¯è¬èƒ½è§£é‡‹å·¥å…·\n",
    "   - éœ€çµåˆå…¶ä»–æ–¹æ³•\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ ä¸‹ç¯€é å‘Š\n",
    "\n",
    "**CH07-04: Transformer ç·¨ç¢¼å™¨ (Encoder)**\n",
    "\n",
    "æˆ‘å€‘å°‡æ¢è¨:\n",
    "- å †ç–Šå¤šå±¤ Encoder çš„è¨­è¨ˆ\n",
    "- å®Œæ•´ Encoder æ¶æ§‹å¯¦ä½œ\n",
    "- ä½¿ç”¨ Encoder é€²è¡Œæ–‡æœ¬åˆ†é¡\n",
    "- BERT æ¨¡å‹åŸç†èˆ‡æ‡‰ç”¨\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“– å»¶ä¼¸é–±è®€\n",
    "\n",
    "1. **è«–æ–‡:**\n",
    "   - [Attention is All You Need](https://arxiv.org/abs/1706.03762) (å¿…è®€!)\n",
    "   - [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/