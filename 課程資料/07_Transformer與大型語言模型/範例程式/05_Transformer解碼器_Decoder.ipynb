{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# CH07-05: Transformer è§£ç¢¼å™¨ (Decoder)\n",
    "\n",
    "**èª²ç¨‹æ™‚é•·**: 90 åˆ†é˜  \n",
    "**é›£åº¦**: â­â­â­â­â­  \n",
    "**å‰ç½®çŸ¥è­˜**: CH07-01, CH07-02, CH07-03, CH07-04  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š æœ¬ç¯€å­¸ç¿’ç›®æ¨™\n",
    "\n",
    "1. âœ… ç†è§£ Decoder èˆ‡ Encoder çš„å·®ç•°\n",
    "2. âœ… å¯¦ä½œ Masked Self-Attention (Look-Ahead Mask)\n",
    "3. âœ… å¯¦ä½œ Cross-Attention (Encoder-Decoder Attention)\n",
    "4. âœ… çµ„åˆå®Œæ•´çš„ Encoder-Decoder æ¶æ§‹\n",
    "5. âœ… ç†è§£è‡ªå›æ­¸ç”Ÿæˆ (Autoregressive Generation)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“– ç›®éŒ„\n",
    "\n",
    "1. [Decoder æ¶æ§‹æ¦‚è¦½](#1-decoder-overview)\n",
    "2. [Masked Self-Attention å¯¦ä½œ](#2-masked-attention)\n",
    "3. [Cross-Attention å¯¦ä½œ](#3-cross-attention)\n",
    "4. [å®Œæ•´ Decoder Layer å¯¦ä½œ](#4-decoder-layer)\n",
    "5. [Encoder-Decoder çµ„åˆ](#5-encoder-decoder)\n",
    "6. [è‡ªå›æ­¸ç”Ÿæˆèˆ‡æ¨è«–](#6-generation)\n",
    "7. [ç¸½çµèˆ‡ç·´ç¿’](#7-summary)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1",
   "metadata": {},
   "source": [
    "## 1. Decoder æ¶æ§‹æ¦‚è¦½ {#1-decoder-overview}\n",
    "\n",
    "### 1.1 Decoder vs Encoder çš„é—œéµå·®ç•°\n",
    "\n",
    "| ç‰¹æ€§ | Encoder | Decoder |\n",
    "|------|---------|----------|\n",
    "| Self-Attention | é›™å‘ (Bidirectional) | å–®å‘ (Unidirectional) |\n",
    "| Masking | Padding Mask | Look-Ahead Mask + Padding Mask |\n",
    "| Cross-Attention | âŒ ç„¡ | âœ… æœ‰ï¼ˆèˆ‡ Encoder è¼¸å‡ºäº’å‹•ï¼‰|\n",
    "| å­å±¤æ•¸é‡ | 2 (MHA + FFN) | 3 (Masked MHA + Cross MHA + FFN) |\n",
    "| æ‡‰ç”¨å ´æ™¯ | æ–‡æœ¬ç†è§£ | æ–‡æœ¬ç”Ÿæˆ |\n",
    "\n",
    "### 1.2 Decoder Layer æ¶æ§‹\n",
    "\n",
    "```\n",
    "Input (Target Sequence)\n",
    "         â†“\n",
    "Masked Multi-Head Self-Attention  (çœ‹ä¸åˆ°æœªä¾†çš„ token)\n",
    "         â†“\n",
    "    Add & Norm\n",
    "         â†“\n",
    "Cross-Attention (èˆ‡ Encoder è¼¸å‡ºäº’å‹•)\n",
    "         â†“\n",
    "    Add & Norm\n",
    "         â†“\n",
    "Feed-Forward Network\n",
    "         â†“\n",
    "    Add & Norm\n",
    "         â†“\n",
    "       Output\n",
    "```\n",
    "\n",
    "### 1.3 ç‚ºä»€éº¼éœ€è¦ Masked Self-Attention?\n",
    "\n",
    "**å•é¡Œ**: åœ¨ç”Ÿæˆä»»å‹™ä¸­ï¼Œæ¨¡å‹ä¸æ‡‰è©²çœ‹åˆ°æœªä¾†çš„ token\n",
    "\n",
    "**ç¯„ä¾‹ (æ©Ÿå™¨ç¿»è­¯)**:\n",
    "```\n",
    "Source (Encoder):  I  love  NLP\n",
    "Target (Decoder):  æˆ‘  æ„›   è‡ªç„¶èªè¨€è™•ç†\n",
    "```\n",
    "\n",
    "ç•¶é æ¸¬ã€Œæ„›ã€æ™‚:\n",
    "- âœ… å¯ä»¥çœ‹åˆ°: ã€Œæˆ‘ã€ï¼ˆå·²ç”Ÿæˆçš„ tokenï¼‰\n",
    "- âŒ ä¸èƒ½çœ‹åˆ°: ã€Œè‡ªç„¶èªè¨€è™•ç†ã€ï¼ˆæœªä¾†çš„ tokenï¼‰\n",
    "\n",
    "**Look-Ahead Mask** ç¢ºä¿æ¯å€‹ä½ç½®åªèƒ½çœ‹åˆ°è‡ªå·±å’Œä¹‹å‰çš„ tokenã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¼‰å…¥å¿…è¦å¥—ä»¶\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Optional, Tuple\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['font.sans-serif'] = ['Microsoft JhengHei', 'Arial Unicode MS']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(\"âœ… å¥—ä»¶è¼‰å…¥å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2",
   "metadata": {},
   "source": [
    "## 2. Masked Self-Attention å¯¦ä½œ {#2-masked-attention}\n",
    "\n",
    "### 2.1 Look-Ahead Mask çš„æ•¸å­¸å®šç¾©\n",
    "\n",
    "å°æ–¼é•·åº¦ç‚º $n$ çš„åºåˆ—ï¼ŒLook-Ahead Mask æ˜¯ä¸€å€‹ $n \\times n$ çš„ä¸‹ä¸‰è§’çŸ©é™£:\n",
    "\n",
    "$$\n",
    "\\text{Mask}_{ij} = \\begin{cases}\n",
    "1 & \\text{if } i \\geq j \\\\\n",
    "0 & \\text{if } i < j\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**ç¯„ä¾‹ (åºåˆ—é•·åº¦ = 5)**:\n",
    "```\n",
    "     t0  t1  t2  t3  t4\n",
    "t0 [  1   0   0   0   0 ]  â† ä½ç½® 0 åªèƒ½çœ‹åˆ°è‡ªå·±\n",
    "t1 [  1   1   0   0   0 ]  â† ä½ç½® 1 å¯ä»¥çœ‹åˆ° 0, 1\n",
    "t2 [  1   1   1   0   0 ]  â† ä½ç½® 2 å¯ä»¥çœ‹åˆ° 0, 1, 2\n",
    "t3 [  1   1   1   1   0 ]  â† ä½ç½® 3 å¯ä»¥çœ‹åˆ° 0, 1, 2, 3\n",
    "t4 [  1   1   1   1   1 ]  â† ä½ç½® 4 å¯ä»¥çœ‹åˆ°æ‰€æœ‰\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "look-ahead-mask",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(seq_len: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Create look-ahead mask for decoder self-attention\n",
    "    \n",
    "    Args:\n",
    "        seq_len: Sequence length\n",
    "    \n",
    "    Returns:\n",
    "        mask: Lower triangular matrix (seq_len, seq_len)\n",
    "              1 = can attend, 0 = cannot attend\n",
    "    \"\"\"\n",
    "    mask = np.tril(np.ones((seq_len, seq_len)))\n",
    "    return mask\n",
    "\n",
    "\n",
    "# è¦–è¦ºåŒ– Look-Ahead Mask\n",
    "seq_len = 8\n",
    "look_ahead_mask = create_look_ahead_mask(seq_len)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    look_ahead_mask, \n",
    "    annot=True, \n",
    "    fmt='.0f', \n",
    "    cmap='YlGnBu',\n",
    "    cbar_kws={'label': 'Can Attend'},\n",
    "    xticklabels=[f't{i}' for i in range(seq_len)],\n",
    "    yticklabels=[f't{i}' for i in range(seq_len)]\n",
    ")\n",
    "plt.title('Look-Ahead Mask (Decoder Self-Attention)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Key Position (è¢«é—œæ³¨çš„ä½ç½®)')\n",
    "plt.ylabel('Query Position (ç•¶å‰ä½ç½®)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"èªªæ˜:\")\n",
    "print(\"â€¢ 1 (è—è‰²): å¯ä»¥é—œæ³¨ (attend)\")\n",
    "print(\"â€¢ 0 (ç™½è‰²): ä¸èƒ½é—œæ³¨ (masked out)\")\n",
    "print(\"â€¢ ä¸‹ä¸‰è§’çŸ©é™£ç¢ºä¿æ¯å€‹ä½ç½®åªèƒ½çœ‹åˆ°è‡ªå·±å’Œä¹‹å‰çš„ token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2-2",
   "metadata": {},
   "source": [
    "### 2.2 Masked Self-Attention å¯¦ä½œ\n",
    "\n",
    "èˆ‡ Encoder çš„ Self-Attention ç›¸æ¯”ï¼Œå”¯ä¸€å·®ç•°æ˜¯æ·»åŠ  Look-Ahead Maskã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "masked-attention-func",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(\n",
    "    Q: np.ndarray, \n",
    "    K: np.ndarray, \n",
    "    V: np.ndarray, \n",
    "    mask: Optional[np.ndarray] = None\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Scaled Dot-Product Attention with optional masking\n",
    "    \n",
    "    Args:\n",
    "        Q: Query matrix (seq_len, d_k)\n",
    "        K: Key matrix (seq_len, d_k)\n",
    "        V: Value matrix (seq_len, d_v)\n",
    "        mask: Optional mask (seq_len, seq_len), 1=attend, 0=mask\n",
    "    \n",
    "    Returns:\n",
    "        output: Attention output (seq_len, d_v)\n",
    "        attention_weights: Attention weights (seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    d_k = K.shape[-1]\n",
    "    \n",
    "    # Compute attention scores\n",
    "    scores = np.matmul(Q, K.T) / np.sqrt(d_k)\n",
    "    \n",
    "    # Apply mask: set masked positions to large negative value\n",
    "    if mask is not None:\n",
    "        scores = np.where(mask == 0, -1e9, scores)\n",
    "    \n",
    "    # Apply softmax\n",
    "    attention_weights = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)\n",
    "    \n",
    "    # Compute weighted sum of values\n",
    "    output = np.matmul(attention_weights, V)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "\n",
    "# æ¸¬è©¦ Masked Self-Attention\n",
    "seq_len = 5\n",
    "d_k = 64\n",
    "\n",
    "# éš¨æ©Ÿç”Ÿæˆ Q, K, V\n",
    "Q = K = V = np.random.randn(seq_len, d_k)\n",
    "\n",
    "# å»ºç«‹ Look-Ahead Mask\n",
    "mask = create_look_ahead_mask(seq_len)\n",
    "\n",
    "# è¨ˆç®— attention (æœ‰ mask vs ç„¡ mask)\n",
    "output_masked, attn_masked = scaled_dot_product_attention(Q, K, V, mask)\n",
    "output_unmasked, attn_unmasked = scaled_dot_product_attention(Q, K, V, None)\n",
    "\n",
    "# è¦–è¦ºåŒ–å°æ¯”\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "sns.heatmap(attn_unmasked, annot=True, fmt='.2f', cmap='YlOrRd', \n",
    "            xticklabels=[f't{i}' for i in range(seq_len)],\n",
    "            yticklabels=[f't{i}' for i in range(seq_len)],\n",
    "            ax=axes[0], cbar_kws={'label': 'Attention Weight'})\n",
    "axes[0].set_title('Encoder Self-Attention (ç„¡ Mask)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Key Position')\n",
    "axes[0].set_ylabel('Query Position')\n",
    "\n",
    "sns.heatmap(attn_masked, annot=True, fmt='.2f', cmap='YlOrRd', \n",
    "            xticklabels=[f't{i}' for i in range(seq_len)],\n",
    "            yticklabels=[f't{i}' for i in range(seq_len)],\n",
    "            ax=axes[1], cbar_kws={'label': 'Attention Weight'})\n",
    "axes[1].set_title('Decoder Masked Self-Attention (æœ‰ Mask)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Key Position')\n",
    "axes[1].set_ylabel('Query Position')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nè§€å¯Ÿ:\")\n",
    "print(\"â€¢ å·¦åœ–: Encoder å¯ä»¥çœ‹åˆ°æ‰€æœ‰ä½ç½®ï¼ˆé›™å‘æ³¨æ„åŠ›ï¼‰\")\n",
    "print(\"â€¢ å³åœ–: Decoder åªèƒ½çœ‹åˆ°ç•¶å‰åŠä¹‹å‰ä½ç½®ï¼ˆå–®å‘æ³¨æ„åŠ›ï¼‰\")\n",
    "print(\"â€¢ å³ä¸Šè§’ä¸‰è§’å€åŸŸçš„æ³¨æ„åŠ›æ¬Šé‡ç‚º 0ï¼ˆè¢« mask æ‰ï¼‰\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3",
   "metadata": {},
   "source": [
    "## 3. Cross-Attention å¯¦ä½œ {#3-cross-attention}\n",
    "\n",
    "### 3.1 Cross-Attention çš„ä½œç”¨\n",
    "\n",
    "**å®šç¾©**: Decoder ä½¿ç”¨ Encoder çš„è¼¸å‡ºä¾†è¨ˆç®—æ³¨æ„åŠ›\n",
    "\n",
    "**æ•¸å­¸å½¢å¼**:\n",
    "$$\n",
    "\\text{CrossAttention}(Q_{\\text{decoder}}, K_{\\text{encoder}}, V_{\\text{encoder}})\n",
    "$$\n",
    "\n",
    "**é—œéµå·®ç•°**:\n",
    "- **Self-Attention**: $Q, K, V$ éƒ½ä¾†è‡ªåŒä¸€åºåˆ—\n",
    "- **Cross-Attention**: $Q$ ä¾†è‡ª Decoderï¼Œ$K, V$ ä¾†è‡ª Encoder\n",
    "\n",
    "### 3.2 Cross-Attention åœ¨æ©Ÿå™¨ç¿»è­¯ä¸­çš„æ‡‰ç”¨\n",
    "\n",
    "```\n",
    "Encoder è¼¸å…¥:  I     love   NLP    (Source)\n",
    "               â†“      â†“      â†“\n",
    "Encoder è¼¸å‡º: [e1]   [e2]   [e3]   â† ä½œç‚º K, V\n",
    "                â†‘      â†‘      â†‘\n",
    "                â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜\n",
    "                       â†“\n",
    "Decoder Query: [æˆ‘]   [æ„›]   [?]    â† ä½œç‚º Q\n",
    "                       â†“\n",
    "Cross-Attention è¨ˆç®—æ¯å€‹ target token èˆ‡ source tokens çš„ç›¸é—œæ€§\n",
    "```\n",
    "\n",
    "**å¯¦ä¾‹**: ç•¶ç”Ÿæˆã€Œæ„›ã€æ™‚\n",
    "- Query: ã€Œæ„›ã€çš„è¡¨ç¤ºå‘é‡\n",
    "- Keys: [\"I\", \"love\", \"NLP\"] çš„è¡¨ç¤ºå‘é‡\n",
    "- Attention åˆ†æ•¸: ã€Œæ„›ã€æœƒé«˜åº¦é—œæ³¨ \"love\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-attention-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¨¡æ“¬ Cross-Attention ç¯„ä¾‹\n",
    "def cross_attention_demo():\n",
    "    \"\"\"\n",
    "    Demo: Visualize cross-attention between encoder and decoder\n",
    "    \"\"\"\n",
    "    # åºåˆ—é•·åº¦\n",
    "    encoder_seq_len = 4  # Source: \"I love NLP .\"\n",
    "    decoder_seq_len = 5  # Target: \"æˆ‘ æ„› è‡ªç„¶ èªè¨€ è™•ç†\"\n",
    "    d_k = 64\n",
    "    \n",
    "    # æ¨¡æ“¬ Encoder è¼¸å‡º (ä½œç‚º K, V)\n",
    "    encoder_output = np.random.randn(encoder_seq_len, d_k)\n",
    "    \n",
    "    # æ¨¡æ“¬ Decoder ç•¶å‰ç‹€æ…‹ (ä½œç‚º Q)\n",
    "    decoder_query = np.random.randn(decoder_seq_len, d_k)\n",
    "    \n",
    "    # Cross-Attention: Q from decoder, K,V from encoder\n",
    "    # æ³¨æ„: K å’Œ V çš„åºåˆ—é•·åº¦èˆ‡ encoder ä¸€è‡´\n",
    "    K = V = encoder_output\n",
    "    Q = decoder_query\n",
    "    \n",
    "    # è¨ˆç®— cross-attention scores\n",
    "    scores = np.matmul(Q, K.T) / np.sqrt(d_k)\n",
    "    attention_weights = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)\n",
    "    \n",
    "    # è¦–è¦ºåŒ–\n",
    "    source_tokens = ['I', 'love', 'NLP', '.']\n",
    "    target_tokens = ['æˆ‘', 'æ„›', 'è‡ªç„¶', 'èªè¨€', 'è™•ç†']\n",
    "    \n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(\n",
    "        attention_weights, \n",
    "        annot=True, \n",
    "        fmt='.3f', \n",
    "        cmap='RdYlGn',\n",
    "        xticklabels=source_tokens,\n",
    "        yticklabels=target_tokens,\n",
    "        cbar_kws={'label': 'Attention Weight'}\n",
    "    )\n",
    "    plt.title('Cross-Attention: Decoder â†’ Encoder', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Source (Encoder Keys/Values)', fontsize=12)\n",
    "    plt.ylabel('Target (Decoder Queries)', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"èªªæ˜:\")\n",
    "    print(\"â€¢ æ¯ä¸€è¡Œ: Decoder ä¸­ä¸€å€‹ token å° Encoder æ‰€æœ‰ tokens çš„æ³¨æ„åŠ›åˆ†ä½ˆ\")\n",
    "    print(\"â€¢ ä¾‹: 'æ„›' é€™ä¸€è¡Œé¡¯ç¤ºå®ƒå° ['I', 'love', 'NLP', '.'] çš„æ³¨æ„åŠ›\")\n",
    "    print(\"â€¢ ç†æƒ³æƒ…æ³: 'æ„›' æœƒé«˜åº¦é—œæ³¨ 'love'\")\n",
    "\n",
    "cross_attention_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3-2",
   "metadata": {},
   "source": [
    "### 3.3 Cross-Attention å¯¦ä½œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-attention-impl",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_attention(\n",
    "    Q_decoder: np.ndarray,\n",
    "    encoder_output: np.ndarray,\n",
    "    mask: Optional[np.ndarray] = None\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Cross-Attention between decoder and encoder\n",
    "    \n",
    "    Args:\n",
    "        Q_decoder: Decoder queries (decoder_seq_len, d_k)\n",
    "        encoder_output: Encoder output as K,V (encoder_seq_len, d_k)\n",
    "        mask: Optional padding mask for encoder (decoder_seq_len, encoder_seq_len)\n",
    "    \n",
    "    Returns:\n",
    "        output: Cross-attention output (decoder_seq_len, d_k)\n",
    "        attention_weights: (decoder_seq_len, encoder_seq_len)\n",
    "    \"\"\"\n",
    "    # Encoder output serves as both K and V\n",
    "    K = V = encoder_output\n",
    "    Q = Q_decoder\n",
    "    \n",
    "    d_k = K.shape[-1]\n",
    "    \n",
    "    # Compute attention scores\n",
    "    scores = np.matmul(Q, K.T) / np.sqrt(d_k)\n",
    "    \n",
    "    # Apply padding mask if provided\n",
    "    if mask is not None:\n",
    "        scores = np.where(mask == 0, -1e9, scores)\n",
    "    \n",
    "    # Softmax\n",
    "    attention_weights = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)\n",
    "    \n",
    "    # Weighted sum\n",
    "    output = np.matmul(attention_weights, V)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "\n",
    "# æ¸¬è©¦ Cross-Attention\n",
    "encoder_seq_len = 6\n",
    "decoder_seq_len = 4\n",
    "d_k = 64\n",
    "\n",
    "encoder_output = np.random.randn(encoder_seq_len, d_k)\n",
    "decoder_query = np.random.randn(decoder_seq_len, d_k)\n",
    "\n",
    "output, attn_weights = cross_attention(decoder_query, encoder_output)\n",
    "\n",
    "print(\"âœ… Cross-Attention æ¸¬è©¦é€šé\")\n",
    "print(f\"Decoder Query å½¢ç‹€: {decoder_query.shape}\")\n",
    "print(f\"Encoder Output å½¢ç‹€: {encoder_output.shape}\")\n",
    "print(f\"Cross-Attention è¼¸å‡ºå½¢ç‹€: {output.shape}\")\n",
    "print(f\"Attention Weights å½¢ç‹€: {attn_weights.shape}\")\n",
    "print(f\"\\næ¯å€‹ decoder token çš„æ³¨æ„åŠ›å’Œæ‡‰ç‚º 1.0:\")\n",
    "print(f\"  {np.sum(attn_weights, axis=1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4",
   "metadata": {},
   "source": [
    "## 4. å®Œæ•´ Decoder Layer å¯¦ä½œ {#4-decoder-layer}\n",
    "\n",
    "### 4.1 Decoder Layer çš„ä¸‰å€‹å­å±¤\n",
    "\n",
    "```\n",
    "Input\n",
    "  â†“\n",
    "â‘  Masked Multi-Head Self-Attention\n",
    "  â†“\n",
    "Add & Norm\n",
    "  â†“\n",
    "â‘¡ Cross-Attention (èˆ‡ Encoder è¼¸å‡º)\n",
    "  â†“\n",
    "Add & Norm\n",
    "  â†“\n",
    "â‘¢ Feed-Forward Network\n",
    "  â†“\n",
    "Add & Norm\n",
    "  â†“\n",
    "Output\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helper-components",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¼”åŠ©çµ„ä»¶ (å¾ CH07-04 è¤‡è£½)\n",
    "\n",
    "class LayerNormalization:\n",
    "    \"\"\"Layer Normalization\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, eps: float = 1e-6):\n",
    "        self.gamma = np.ones(d_model)\n",
    "        self.beta = np.zeros(d_model)\n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        mean = np.mean(x, axis=-1, keepdims=True)\n",
    "        variance = np.var(x, axis=-1, keepdims=True)\n",
    "        x_norm = (x - mean) / np.sqrt(variance + self.eps)\n",
    "        return self.gamma * x_norm + self.beta\n",
    "\n",
    "\n",
    "class FeedForwardNetwork:\n",
    "    \"\"\"Position-wise Feed-Forward Network\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, d_ff: int):\n",
    "        self.W1 = np.random.randn(d_model, d_ff) / np.sqrt(d_model)\n",
    "        self.b1 = np.zeros(d_ff)\n",
    "        self.W2 = np.random.randn(d_ff, d_model) / np.sqrt(d_ff)\n",
    "        self.b2 = np.zeros(d_model)\n",
    "    \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        hidden = np.maximum(0, np.dot(x, self.W1) + self.b1)\n",
    "        output = np.dot(hidden, self.W2) + self.b2\n",
    "        return output\n",
    "\n",
    "\n",
    "class MultiHeadAttention:\n",
    "    \"\"\"Multi-Head Attention (å¯ç”¨æ–¼ Self-Attention æˆ– Cross-Attention)\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, num_heads: int):\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.W_q = np.random.randn(d_model, d_model) / np.sqrt(d_model)\n",
    "        self.W_k = np.random.randn(d_model, d_model) / np.sqrt(d_model)\n",
    "        self.W_v = np.random.randn(d_model, d_model) / np.sqrt(d_model)\n",
    "        self.W_o = np.random.randn(d_model, d_model) / np.sqrt(d_model)\n",
    "    \n",
    "    def split_heads(self, x: np.ndarray) -> np.ndarray:\n",
    "        seq_len = x.shape[0]\n",
    "        x = x.reshape(seq_len, self.num_heads, self.d_k)\n",
    "        return x.transpose(1, 0, 2)\n",
    "    \n",
    "    def forward(\n",
    "        self, \n",
    "        Q_input: np.ndarray,\n",
    "        K_input: np.ndarray,\n",
    "        V_input: np.ndarray,\n",
    "        mask: Optional[np.ndarray] = None\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            Q_input: Query input (seq_len_q, d_model)\n",
    "            K_input: Key input (seq_len_k, d_model)\n",
    "            V_input: Value input (seq_len_v, d_model)\n",
    "            mask: Optional mask (seq_len_q, seq_len_k)\n",
    "        \"\"\"\n",
    "        Q = np.dot(Q_input, self.W_q)\n",
    "        K = np.dot(K_input, self.W_k)\n",
    "        V = np.dot(V_input, self.W_v)\n",
    "        \n",
    "        Q = self.split_heads(Q)\n",
    "        K = self.split_heads(K)\n",
    "        V = self.split_heads(V)\n",
    "        \n",
    "        outputs = []\n",
    "        attention_weights_all = []\n",
    "        \n",
    "        for i in range(self.num_heads):\n",
    "            output, attn_weights = scaled_dot_product_attention(\n",
    "                Q[i], K[i], V[i], mask\n",
    "            )\n",
    "            outputs.append(output)\n",
    "            attention_weights_all.append(attn_weights)\n",
    "        \n",
    "        concat_output = np.concatenate(outputs, axis=-1)\n",
    "        final_output = np.dot(concat_output, self.W_o)\n",
    "        attention_weights = np.stack(attention_weights_all, axis=0)\n",
    "        \n",
    "        return final_output, attention_weights\n",
    "\n",
    "print(\"âœ… è¼”åŠ©çµ„ä»¶è¼‰å…¥å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decoder-layer",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer:\n",
    "    \"\"\"Complete Transformer Decoder Layer\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout: float = 0.1):\n",
    "        # â‘  Masked Multi-Head Self-Attention\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        \n",
    "        # â‘¡ Cross-Attention\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        \n",
    "        # â‘¢ Feed-Forward Network\n",
    "        self.ffn = FeedForwardNetwork(d_model, d_ff)\n",
    "        \n",
    "        # Layer Normalization (3 å€‹)\n",
    "        self.layernorm1 = LayerNormalization(d_model)\n",
    "        self.layernorm2 = LayerNormalization(d_model)\n",
    "        self.layernorm3 = LayerNormalization(d_model)\n",
    "        \n",
    "        self.dropout = dropout\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        x: np.ndarray,\n",
    "        encoder_output: np.ndarray,\n",
    "        look_ahead_mask: Optional[np.ndarray] = None,\n",
    "        padding_mask: Optional[np.ndarray] = None\n",
    "    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Decoder input (decoder_seq_len, d_model)\n",
    "            encoder_output: Encoder output (encoder_seq_len, d_model)\n",
    "            look_ahead_mask: Mask for self-attention (decoder_seq_len, decoder_seq_len)\n",
    "            padding_mask: Mask for cross-attention (decoder_seq_len, encoder_seq_len)\n",
    "        \n",
    "        Returns:\n",
    "            output: (decoder_seq_len, d_model)\n",
    "            self_attn_weights: (num_heads, decoder_seq_len, decoder_seq_len)\n",
    "            cross_attn_weights: (num_heads, decoder_seq_len, encoder_seq_len)\n",
    "        \"\"\"\n",
    "        # â‘  Masked Multi-Head Self-Attention\n",
    "        self_attn_output, self_attn_weights = self.self_attn.forward(\n",
    "            x, x, x, look_ahead_mask\n",
    "        )\n",
    "        x = self.layernorm1.forward(x + self_attn_output)\n",
    "        \n",
    "        # â‘¡ Cross-Attention\n",
    "        cross_attn_output, cross_attn_weights = self.cross_attn.forward(\n",
    "            Q_input=x,  # Query from decoder\n",
    "            K_input=encoder_output,  # Key from encoder\n",
    "            V_input=encoder_output,  # Value from encoder\n",
    "            mask=padding_mask\n",
    "        )\n",
    "        x = self.layernorm2.forward(x + cross_attn_output)\n",
    "        \n",
    "        # â‘¢ Feed-Forward Network\n",
    "        ffn_output = self.ffn.forward(x)\n",
    "        x = self.layernorm3.forward(x + ffn_output)\n",
    "        \n",
    "        return x, self_attn_weights, cross_attn_weights\n",
    "\n",
    "\n",
    "# æ¸¬è©¦ Decoder Layer\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "d_ff = 2048\n",
    "encoder_seq_len = 10\n",
    "decoder_seq_len = 8\n",
    "\n",
    "# æ¨¡æ“¬è¼¸å…¥\n",
    "decoder_input = np.random.randn(decoder_seq_len, d_model)\n",
    "encoder_output = np.random.randn(encoder_seq_len, d_model)\n",
    "look_ahead_mask = create_look_ahead_mask(decoder_seq_len)\n",
    "\n",
    "# å»ºç«‹ä¸¦æ¸¬è©¦ Decoder Layer\n",
    "decoder_layer = TransformerDecoderLayer(d_model, num_heads, d_ff)\n",
    "output, self_attn, cross_attn = decoder_layer.forward(\n",
    "    decoder_input, encoder_output, look_ahead_mask\n",
    ")\n",
    "\n",
    "print(\"âœ… Transformer Decoder Layer æ¸¬è©¦é€šé\")\n",
    "print(f\"Decoder è¼¸å…¥: {decoder_input.shape}\")\n",
    "print(f\"Encoder è¼¸å‡º: {encoder_output.shape}\")\n",
    "print(f\"Decoder è¼¸å‡º: {output.shape}\")\n",
    "print(f\"Self-Attention æ¬Šé‡: {self_attn.shape}\")\n",
    "print(f\"Cross-Attention æ¬Šé‡: {cross_attn.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-5",
   "metadata": {},
   "source": [
    "## 5. Encoder-Decoder çµ„åˆ {#5-encoder-decoder}\n",
    "\n",
    "### 5.1 å®Œæ•´ Transformer æ¶æ§‹\n",
    "\n",
    "```\n",
    "Source Input (Encoder)\n",
    "         â†“\n",
    "    Embedding + PE\n",
    "         â†“\n",
    "    Encoder Layer 1\n",
    "         â†“\n",
    "         ...\n",
    "         â†“\n",
    "    Encoder Layer N\n",
    "         â†“\n",
    "    Encoder Output â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                                 â”‚\n",
    "Target Input (Decoder)           â”‚\n",
    "         â†“                       â”‚\n",
    "    Embedding + PE               â”‚\n",
    "         â†“                       â”‚\n",
    "    Decoder Layer 1 â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ (Cross-Attention)\n",
    "         â†“                       â”‚\n",
    "         ...                    â”‚\n",
    "         â†“                       â”‚\n",
    "    Decoder Layer N â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â†“\n",
    "    Linear + Softmax\n",
    "         â†“\n",
    "    Output Probabilities\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "full-transformer",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer:\n",
    "    \"\"\"Complete Transformer with Encoder and Decoder\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        num_encoder_layers: int,\n",
    "        num_decoder_layers: int,\n",
    "        d_model: int,\n",
    "        num_heads: int,\n",
    "        d_ff: int,\n",
    "        source_vocab_size: int,\n",
    "        target_vocab_size: int,\n",
    "        max_seq_len: int,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Embeddings\n",
    "        self.source_embedding = np.random.randn(source_vocab_size, d_model) / np.sqrt(d_model)\n",
    "        self.target_embedding = np.random.randn(target_vocab_size, d_model) / np.sqrt(d_model)\n",
    "        self.positional_encoding = self.get_positional_encoding(max_seq_len, d_model)\n",
    "        \n",
    "        # Encoder stack\n",
    "        self.encoder_layers = [\n",
    "            self._create_encoder_layer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_encoder_layers)\n",
    "        ]\n",
    "        \n",
    "        # Decoder stack\n",
    "        self.decoder_layers = [\n",
    "            TransformerDecoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_decoder_layers)\n",
    "        ]\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_projection = np.random.randn(d_model, target_vocab_size) / np.sqrt(d_model)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_positional_encoding(max_seq_len: int, d_model: int) -> np.ndarray:\n",
    "        pos_encoding = np.zeros((max_seq_len, d_model))\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pos_encoding[pos, i] = np.sin(pos / (10000 ** (i / d_model)))\n",
    "                if i + 1 < d_model:\n",
    "                    pos_encoding[pos, i + 1] = np.cos(pos / (10000 ** (i / d_model)))\n",
    "        return pos_encoding\n",
    "    \n",
    "    def _create_encoder_layer(self, d_model, num_heads, d_ff, dropout):\n",
    "        \"\"\"ç°¡åŒ–ç‰ˆ Encoder Layer (ç„¡ Cross-Attention)\"\"\"\n",
    "        class EncoderLayer:\n",
    "            def __init__(self):\n",
    "                self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "                self.ffn = FeedForwardNetwork(d_model, d_ff)\n",
    "                self.layernorm1 = LayerNormalization(d_model)\n",
    "                self.layernorm2 = LayerNormalization(d_model)\n",
    "            \n",
    "            def forward(self, x, mask=None):\n",
    "                attn_output, _ = self.self_attn.forward(x, x, x, mask)\n",
    "                x = self.layernorm1.forward(x + attn_output)\n",
    "                ffn_output = self.ffn.forward(x)\n",
    "                x = self.layernorm2.forward(x + ffn_output)\n",
    "                return x\n",
    "        \n",
    "        return EncoderLayer()\n",
    "    \n",
    "    def encode(self, source_ids: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Encode source sequence\"\"\"\n",
    "        seq_len = source_ids.shape[0]\n",
    "        \n",
    "        # Embedding + Positional Encoding\n",
    "        x = self.source_embedding[source_ids]\n",
    "        x = x + self.positional_encoding[:seq_len, :]\n",
    "        \n",
    "        # Pass through encoder layers\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            x = encoder_layer.forward(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def decode(\n",
    "        self,\n",
    "        target_ids: np.ndarray,\n",
    "        encoder_output: np.ndarray,\n",
    "        look_ahead_mask: Optional[np.ndarray] = None\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Decode target sequence\"\"\"\n",
    "        seq_len = target_ids.shape[0]\n",
    "        \n",
    "        # Embedding + Positional Encoding\n",
    "        x = self.target_embedding[target_ids]\n",
    "        x = x + self.positional_encoding[:seq_len, :]\n",
    "        \n",
    "        # Pass through decoder layers\n",
    "        for decoder_layer in self.decoder_layers:\n",
    "            x, _, _ = decoder_layer.forward(x, encoder_output, look_ahead_mask)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        source_ids: np.ndarray,\n",
    "        target_ids: np.ndarray\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Complete forward pass\n",
    "        \n",
    "        Args:\n",
    "            source_ids: Source token IDs (source_seq_len,)\n",
    "            target_ids: Target token IDs (target_seq_len,)\n",
    "        \n",
    "        Returns:\n",
    "            logits: Output logits (target_seq_len, target_vocab_size)\n",
    "        \"\"\"\n",
    "        # Encode\n",
    "        encoder_output = self.encode(source_ids)\n",
    "        \n",
    "        # Create look-ahead mask for decoder\n",
    "        target_seq_len = target_ids.shape[0]\n",
    "        look_ahead_mask = create_look_ahead_mask(target_seq_len)\n",
    "        \n",
    "        # Decode\n",
    "        decoder_output = self.decode(target_ids, encoder_output, look_ahead_mask)\n",
    "        \n",
    "        # Project to vocabulary\n",
    "        logits = np.dot(decoder_output, self.output_projection)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "# æ¸¬è©¦å®Œæ•´ Transformer\n",
    "transformer = Transformer(\n",
    "    num_encoder_layers=6,\n",
    "    num_decoder_layers=6,\n",
    "    d_model=512,\n",
    "    num_heads=8,\n",
    "    d_ff=2048,\n",
    "    source_vocab_size=10000,\n",
    "    target_vocab_size=8000,\n",
    "    max_seq_len=100\n",
    ")\n",
    "\n",
    "# æ¨¡æ“¬è¼¸å…¥\n",
    "source_ids = np.random.randint(0, 10000, size=12)\n",
    "target_ids = np.random.randint(0, 8000, size=10)\n",
    "\n",
    "# å‰å‘å‚³æ’­\n",
    "logits = transformer.forward(source_ids, target_ids)\n",
    "\n",
    "print(\"âœ… å®Œæ•´ Transformer æ¸¬è©¦é€šé\")\n",
    "print(f\"Source åºåˆ—é•·åº¦: {len(source_ids)}\")\n",
    "print(f\"Target åºåˆ—é•·åº¦: {len(target_ids)}\")\n",
    "print(f\"è¼¸å‡º logits å½¢ç‹€: {logits.shape}\")\n",
    "print(f\"  â†’ æ¯å€‹ target token æœ‰ {logits.shape[1]} å€‹è©å½™çš„æ©Ÿç‡åˆ†æ•¸\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-6",
   "metadata": {},
   "source": [
    "## 6. è‡ªå›æ­¸ç”Ÿæˆèˆ‡æ¨è«– {#6-generation}\n",
    "\n",
    "### 6.1 è¨“ç·´ vs æ¨è«–çš„å·®ç•°\n",
    "\n",
    "**è¨“ç·´æ™‚ (Teacher Forcing)**:\n",
    "- è¼¸å…¥: å®Œæ•´çš„ target åºåˆ—ï¼ˆå·²çŸ¥æ‰€æœ‰ tokenï¼‰\n",
    "- ç›®æ¨™: ä¸€æ¬¡æ€§é æ¸¬æ‰€æœ‰ä½ç½®çš„ token\n",
    "- æ•ˆç‡: é«˜ï¼ˆä¸¦è¡Œè¨ˆç®—ï¼‰\n",
    "\n",
    "**æ¨è«–æ™‚ (Autoregressive Generation)**:\n",
    "- è¼¸å…¥: é€æ­¥ç”Ÿæˆçš„åºåˆ—ï¼ˆå¾ `<BOS>` é–‹å§‹ï¼‰\n",
    "- ç›®æ¨™: æ¯æ¬¡é æ¸¬ä¸‹ä¸€å€‹ token\n",
    "- æ•ˆç‡: ä½ï¼ˆå¿…é ˆå¾ªåºç”Ÿæˆï¼‰\n",
    "\n",
    "### 6.2 è‡ªå›æ­¸ç”Ÿæˆæµç¨‹\n",
    "\n",
    "```\n",
    "Step 1: Input = [<BOS>]\n",
    "        Output = \"æˆ‘\"\n",
    "\n",
    "Step 2: Input = [<BOS>, \"æˆ‘\"]\n",
    "        Output = \"æ„›\"\n",
    "\n",
    "Step 3: Input = [<BOS>, \"æˆ‘\", \"æ„›\"]\n",
    "        Output = \"NLP\"\n",
    "\n",
    "Step 4: Input = [<BOS>, \"æˆ‘\", \"æ„›\", \"NLP\"]\n",
    "        Output = <EOS>  â†’ åœæ­¢ç”Ÿæˆ\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(\n",
    "    transformer: Transformer,\n",
    "    source_ids: np.ndarray,\n",
    "    max_len: int,\n",
    "    bos_token_id: int,\n",
    "    eos_token_id: int\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Greedy decoding for sequence generation\n",
    "    \n",
    "    Args:\n",
    "        transformer: Trained Transformer model\n",
    "        source_ids: Source sequence (source_seq_len,)\n",
    "        max_len: Maximum generation length\n",
    "        bos_token_id: Beginning-of-sequence token ID\n",
    "        eos_token_id: End-of-sequence token ID\n",
    "    \n",
    "    Returns:\n",
    "        generated_ids: Generated sequence (variable length)\n",
    "    \"\"\"\n",
    "    # Encode source once\n",
    "    encoder_output = transformer.encode(source_ids)\n",
    "    \n",
    "    # Initialize with <BOS>\n",
    "    generated_ids = [bos_token_id]\n",
    "    \n",
    "    for _ in range(max_len):\n",
    "        # Current target sequence\n",
    "        target_ids = np.array(generated_ids)\n",
    "        \n",
    "        # Create look-ahead mask\n",
    "        look_ahead_mask = create_look_ahead_mask(len(target_ids))\n",
    "        \n",
    "        # Decode\n",
    "        decoder_output = transformer.decode(target_ids, encoder_output, look_ahead_mask)\n",
    "        \n",
    "        # Project to vocabulary\n",
    "        logits = np.dot(decoder_output, transformer.output_projection)\n",
    "        \n",
    "        # Get the last token's logits and apply softmax\n",
    "        last_logits = logits[-1, :]\n",
    "        probs = np.exp(last_logits) / np.sum(np.exp(last_logits))\n",
    "        \n",
    "        # Greedy: select the token with highest probability\n",
    "        next_token_id = np.argmax(probs)\n",
    "        \n",
    "        # Append to sequence\n",
    "        generated_ids.append(next_token_id)\n",
    "        \n",
    "        # Stop if <EOS> generated\n",
    "        if next_token_id == eos_token_id:\n",
    "            break\n",
    "    \n",
    "    return np.array(generated_ids)\n",
    "\n",
    "\n",
    "# æ¨¡æ“¬ç”Ÿæˆæ¸¬è©¦\n",
    "source_ids = np.random.randint(0, 10000, size=8)\n",
    "BOS_TOKEN = 1\n",
    "EOS_TOKEN = 2\n",
    "MAX_LEN = 15\n",
    "\n",
    "print(\"é–‹å§‹è‡ªå›æ­¸ç”Ÿæˆ...\")\n",
    "generated = greedy_decode(transformer, source_ids, MAX_LEN, BOS_TOKEN, EOS_TOKEN)\n",
    "\n",
    "print(f\"\\nâœ… ç”Ÿæˆå®Œæˆ\")\n",
    "print(f\"Source é•·åº¦: {len(source_ids)}\")\n",
    "print(f\"ç”Ÿæˆåºåˆ—é•·åº¦: {len(generated)}\")\n",
    "print(f\"ç”Ÿæˆçš„ token IDs: {generated}\")\n",
    "print(f\"\\nèªªæ˜:\")\n",
    "print(f\"  â€¢ ç¬¬ä¸€å€‹ token æ˜¯ BOS ({BOS_TOKEN})\")\n",
    "print(f\"  â€¢ è‹¥é‡åˆ° EOS ({EOS_TOKEN}) å‰‡åœæ­¢ç”Ÿæˆ\")\n",
    "print(f\"  â€¢ æ¯æ­¥éƒ½åªé æ¸¬ä¸‹ä¸€å€‹ token (è‡ªå›æ­¸)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decoding-strategies",
   "metadata": {},
   "source": [
    "### 6.3 ä¸åŒè§£ç¢¼ç­–ç•¥\n",
    "\n",
    "| ç­–ç•¥ | èªªæ˜ | å„ªé» | ç¼ºé» |\n",
    "|------|------|------|------|\n",
    "| **Greedy** | æ¯æ­¥é¸æœ€é«˜æ©Ÿç‡ token | å¿«é€Ÿã€ç¢ºå®šæ€§ | å¯èƒ½é™·å…¥å±€éƒ¨æœ€å„ª |\n",
    "| **Beam Search** | ç¶­è­· top-k å€‹å€™é¸åºåˆ— | å“è³ªè¼ƒå¥½ | è¼ƒæ…¢ã€éœ€è¦æ›´å¤šè¨˜æ†¶é«” |\n",
    "| **Sampling** | æ ¹æ“šæ©Ÿç‡åˆ†ä½ˆéš¨æ©Ÿæ¡æ¨£ | å¤šæ¨£æ€§é«˜ | å¯èƒ½ä¸é€£è²« |\n",
    "| **Top-k Sampling** | åªå¾ top-k token ä¸­æ¡æ¨£ | å¹³è¡¡å“è³ªèˆ‡å¤šæ¨£æ€§ | éœ€èª¿æ•´ k å€¼ |\n",
    "| **Nucleus (Top-p)** | å¾ç´¯ç©æ©Ÿç‡é” p çš„ token ä¸­æ¡æ¨£ | è‡ªé©æ‡‰é¸æ“‡ç¯„åœ | éœ€èª¿æ•´ p å€¼ |\n",
    "\n",
    "**Beam Search ç¤ºæ„åœ–**:\n",
    "```\n",
    "Beam Size = 2\n",
    "\n",
    "Step 1:        [æˆ‘ (0.6)]     [ä½  (0.3)]\n",
    "               /     \\         /     \\\n",
    "Step 2:    [æ„›]    [å–œæ­¡]   [æ„›]   [è¨å­]\n",
    "           (0.4)   (0.3)   (0.2)   (0.15)\n",
    "           \n",
    "Keep top-2:  [æˆ‘, æ„›]  [æˆ‘, å–œæ­¡]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-7",
   "metadata": {},
   "source": [
    "## 7. ç¸½çµèˆ‡ç·´ç¿’ {#7-summary}\n",
    "\n",
    "### 7.1 æœ¬ç¯€é‡é»å›é¡§\n",
    "\n",
    "âœ… **Decoder çš„ä¸‰å¤§çµ„ä»¶**:\n",
    "1. **Masked Self-Attention**: ç¢ºä¿åªèƒ½çœ‹åˆ°éå»çš„ token\n",
    "2. **Cross-Attention**: èˆ‡ Encoder è¼¸å‡ºäº’å‹•ï¼Œç²å–æºåºåˆ—è³‡è¨Š\n",
    "3. **Feed-Forward Network**: éç·šæ€§è½‰æ›\n",
    "\n",
    "âœ… **é—œéµæ¦‚å¿µ**:\n",
    "- **Look-Ahead Mask**: ä¸‹ä¸‰è§’çŸ©é™£ï¼Œé˜²æ­¢çœ‹åˆ°æœªä¾†\n",
    "- **Cross-Attention**: Q ä¾†è‡ª Decoderï¼ŒK/V ä¾†è‡ª Encoder\n",
    "- **Autoregressive Generation**: é€æ­¥ç”Ÿæˆï¼Œæ¯æ¬¡é æ¸¬ä¸‹ä¸€å€‹ token\n",
    "\n",
    "âœ… **Encoder vs Decoder**:\n",
    "\n",
    "| ç‰¹æ€§ | Encoder | Decoder |\n",
    "|------|---------|----------|\n",
    "| æ³¨æ„åŠ›æ–¹å‘ | é›™å‘ | å–®å‘ (masked) |\n",
    "| Cross-Attention | âŒ | âœ… |\n",
    "| æ‡‰ç”¨ | ç†è§£ä»»å‹™ | ç”Ÿæˆä»»å‹™ |\n",
    "| ä»£è¡¨æ¨¡å‹ | BERT | GPT |\n",
    "\n",
    "### 7.2 å®Œæ•´ Transformer æ¶æ§‹ç¸½çµ\n",
    "\n",
    "**Encoder-Decoder æ¶æ§‹** (åŸå§‹ Transformer):\n",
    "- æ‡‰ç”¨: æ©Ÿå™¨ç¿»è­¯ã€æ–‡æœ¬æ‘˜è¦\n",
    "- ä»£è¡¨: T5, BART, mBART\n",
    "\n",
    "**Encoder-Only æ¶æ§‹**:\n",
    "- æ‡‰ç”¨: æ–‡æœ¬åˆ†é¡ã€NERã€å•ç­”\n",
    "- ä»£è¡¨: BERT, RoBERTa, ALBERT\n",
    "\n",
    "**Decoder-Only æ¶æ§‹**:\n",
    "- æ‡‰ç”¨: æ–‡æœ¬ç”Ÿæˆã€å°è©±ç³»çµ±\n",
    "- ä»£è¡¨: GPT, GPT-2, GPT-3, LLaMA\n",
    "\n",
    "### 7.3 å¯¦ä½œç·´ç¿’\n",
    "\n",
    "#### ç·´ç¿’ 1: è¦–è¦ºåŒ– Cross-Attention\n",
    "\n",
    "**ä»»å‹™**: ä½¿ç”¨çœŸå¯¦çš„ç¿»è­¯ç¯„ä¾‹ï¼Œè¦–è¦ºåŒ– Cross-Attention æ¬Šé‡ï¼Œè§€å¯Ÿå“ªäº› target token é—œæ³¨å“ªäº› source tokenã€‚\n",
    "\n",
    "**æç¤º**:\n",
    "```python\n",
    "source = \"I love natural language processing\"\n",
    "target = \"æˆ‘ æ„› è‡ªç„¶ èªè¨€ è™•ç†\"\n",
    "# ç¹ªè£½ heatmap é¡¯ç¤ºå°é½Šé—œä¿‚\n",
    "```\n",
    "\n",
    "#### ç·´ç¿’ 2: å¯¦ä½œ Beam Search\n",
    "\n",
    "**ä»»å‹™**: å¯¦ä½œ Beam Search è§£ç¢¼ç­–ç•¥ï¼Œä¸¦èˆ‡ Greedy Decoding æ¯”è¼ƒç”Ÿæˆå“è³ªã€‚\n",
    "\n",
    "**æç¤º**:\n",
    "```python\n",
    "def beam_search_decode(model, source, beam_size=5):\n",
    "    # ç¶­è­· top-k å€‹å€™é¸åºåˆ—\n",
    "    # æ¯æ­¥æ“´å±•æ‰€æœ‰å€™é¸ï¼Œä¿ç•™ç¸½æ©Ÿç‡æœ€é«˜çš„ k å€‹\n",
    "    pass\n",
    "```\n",
    "\n",
    "#### ç·´ç¿’ 3: å¯¦ä½œ Temperature Sampling\n",
    "\n",
    "**ä»»å‹™**: åœ¨ç”Ÿæˆæ™‚åŠ å…¥ temperature åƒæ•¸æ§åˆ¶éš¨æ©Ÿæ€§ã€‚\n",
    "\n",
    "**æç¤º**:\n",
    "```python\n",
    "# Temperature scaling\n",
    "logits = logits / temperature\n",
    "probs = softmax(logits)\n",
    "next_token = np.random.choice(vocab_size, p=probs)\n",
    "```\n",
    "\n",
    "**Temperature æ•ˆæœ**:\n",
    "- `T = 0.5`: æ›´ç¢ºå®šæ€§ï¼Œåå‘é«˜æ©Ÿç‡ token\n",
    "- `T = 1.0`: æ¨™æº– softmax\n",
    "- `T = 2.0`: æ›´éš¨æ©Ÿï¼Œåˆ†ä½ˆæ›´å‡å‹»\n",
    "\n",
    "### 7.4 å»¶ä¼¸é–±è®€\n",
    "\n",
    "1. **è«–æ–‡**:\n",
    "   - [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - ä»”ç´°é–±è®€ Decoder éƒ¨åˆ†\n",
    "   - [The Curious Case of Neural Text Degeneration](https://arxiv.org/abs/1904.09751) - Nucleus Sampling\n",
    "\n",
    "2. **å¯¦ä½œè³‡æº**:\n",
    "   - [Annotated Transformer - Decoder](http://nlp.seas.harvard.edu/annotated-transformer/)\n",
    "   - [Hugging Face - Generation Strategies](https://huggingface.co/docs/transformers/generation_strategies)\n",
    "\n",
    "3. **è¦–è¦ºåŒ–å·¥å…·**:\n",
    "   - [Seq2Seq Visualization](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ ä¸‹ä¸€ç¯€é å‘Š\n",
    "\n",
    "**CH07-06: Encoder vs Decoder vs Encoder-Decoder å°æ¯”**\n",
    "- ä¸‰ç¨®æ¶æ§‹çš„å„ªç¼ºé»åˆ†æ\n",
    "- ä¸åŒä»»å‹™çš„æœ€ä½³æ¶æ§‹é¸æ“‡\n",
    "- BERT vs GPT vs T5 å¯¦éš›å°æ¯”\n",
    "- æ¨¡å‹é¸æ“‡çš„å¯¦æˆ°å»ºè­°\n",
    "\n",
    "---\n",
    "\n",
    "**èª²ç¨‹å®Œæˆæ™‚é–“**: `____å¹´____æœˆ____æ—¥`  \n",
    "**å­¸ç¿’å¿ƒå¾—**: ___________________________________"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
