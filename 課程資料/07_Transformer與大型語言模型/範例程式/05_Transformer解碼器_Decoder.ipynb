{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# CH07-05: Transformer 解碼器 (Decoder)\n",
    "\n",
    "**課程時長**: 90 分鐘  \n",
    "**難度**: ⭐⭐⭐⭐⭐  \n",
    "**前置知識**: CH07-01, CH07-02, CH07-03, CH07-04  \n",
    "\n",
    "---\n",
    "\n",
    "## 📚 本節學習目標\n",
    "\n",
    "1. ✅ 理解 Decoder 與 Encoder 的差異\n",
    "2. ✅ 實作 Masked Self-Attention (Look-Ahead Mask)\n",
    "3. ✅ 實作 Cross-Attention (Encoder-Decoder Attention)\n",
    "4. ✅ 組合完整的 Encoder-Decoder 架構\n",
    "5. ✅ 理解自回歸生成 (Autoregressive Generation)\n",
    "\n",
    "---\n",
    "\n",
    "## 📖 目錄\n",
    "\n",
    "1. [Decoder 架構概覽](#1-decoder-overview)\n",
    "2. [Masked Self-Attention 實作](#2-masked-attention)\n",
    "3. [Cross-Attention 實作](#3-cross-attention)\n",
    "4. [完整 Decoder Layer 實作](#4-decoder-layer)\n",
    "5. [Encoder-Decoder 組合](#5-encoder-decoder)\n",
    "6. [自回歸生成與推論](#6-generation)\n",
    "7. [總結與練習](#7-summary)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1",
   "metadata": {},
   "source": [
    "## 1. Decoder 架構概覽 {#1-decoder-overview}\n",
    "\n",
    "### 1.1 Decoder vs Encoder 的關鍵差異\n",
    "\n",
    "| 特性 | Encoder | Decoder |\n",
    "|------|---------|----------|\n",
    "| Self-Attention | 雙向 (Bidirectional) | 單向 (Unidirectional) |\n",
    "| Masking | Padding Mask | Look-Ahead Mask + Padding Mask |\n",
    "| Cross-Attention | ❌ 無 | ✅ 有（與 Encoder 輸出互動）|\n",
    "| 子層數量 | 2 (MHA + FFN) | 3 (Masked MHA + Cross MHA + FFN) |\n",
    "| 應用場景 | 文本理解 | 文本生成 |\n",
    "\n",
    "### 1.2 Decoder Layer 架構\n",
    "\n",
    "```\n",
    "Input (Target Sequence)\n",
    "         ↓\n",
    "Masked Multi-Head Self-Attention  (看不到未來的 token)\n",
    "         ↓\n",
    "    Add & Norm\n",
    "         ↓\n",
    "Cross-Attention (與 Encoder 輸出互動)\n",
    "         ↓\n",
    "    Add & Norm\n",
    "         ↓\n",
    "Feed-Forward Network\n",
    "         ↓\n",
    "    Add & Norm\n",
    "         ↓\n",
    "       Output\n",
    "```\n",
    "\n",
    "### 1.3 為什麼需要 Masked Self-Attention?\n",
    "\n",
    "**問題**: 在生成任務中，模型不應該看到未來的 token\n",
    "\n",
    "**範例 (機器翻譯)**:\n",
    "```\n",
    "Source (Encoder):  I  love  NLP\n",
    "Target (Decoder):  我  愛   自然語言處理\n",
    "```\n",
    "\n",
    "當預測「愛」時:\n",
    "- ✅ 可以看到: 「我」（已生成的 token）\n",
    "- ❌ 不能看到: 「自然語言處理」（未來的 token）\n",
    "\n",
    "**Look-Ahead Mask** 確保每個位置只能看到自己和之前的 token。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入必要套件\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Optional, Tuple\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['font.sans-serif'] = ['Microsoft JhengHei', 'Arial Unicode MS']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(\"✅ 套件載入完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2",
   "metadata": {},
   "source": [
    "## 2. Masked Self-Attention 實作 {#2-masked-attention}\n",
    "\n",
    "### 2.1 Look-Ahead Mask 的數學定義\n",
    "\n",
    "對於長度為 $n$ 的序列，Look-Ahead Mask 是一個 $n \\times n$ 的下三角矩陣:\n",
    "\n",
    "$$\n",
    "\\text{Mask}_{ij} = \\begin{cases}\n",
    "1 & \\text{if } i \\geq j \\\\\n",
    "0 & \\text{if } i < j\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**範例 (序列長度 = 5)**:\n",
    "```\n",
    "     t0  t1  t2  t3  t4\n",
    "t0 [  1   0   0   0   0 ]  ← 位置 0 只能看到自己\n",
    "t1 [  1   1   0   0   0 ]  ← 位置 1 可以看到 0, 1\n",
    "t2 [  1   1   1   0   0 ]  ← 位置 2 可以看到 0, 1, 2\n",
    "t3 [  1   1   1   1   0 ]  ← 位置 3 可以看到 0, 1, 2, 3\n",
    "t4 [  1   1   1   1   1 ]  ← 位置 4 可以看到所有\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "look-ahead-mask",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(seq_len: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Create look-ahead mask for decoder self-attention\n",
    "    \n",
    "    Args:\n",
    "        seq_len: Sequence length\n",
    "    \n",
    "    Returns:\n",
    "        mask: Lower triangular matrix (seq_len, seq_len)\n",
    "              1 = can attend, 0 = cannot attend\n",
    "    \"\"\"\n",
    "    mask = np.tril(np.ones((seq_len, seq_len)))\n",
    "    return mask\n",
    "\n",
    "\n",
    "# 視覺化 Look-Ahead Mask\n",
    "seq_len = 8\n",
    "look_ahead_mask = create_look_ahead_mask(seq_len)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    look_ahead_mask, \n",
    "    annot=True, \n",
    "    fmt='.0f', \n",
    "    cmap='YlGnBu',\n",
    "    cbar_kws={'label': 'Can Attend'},\n",
    "    xticklabels=[f't{i}' for i in range(seq_len)],\n",
    "    yticklabels=[f't{i}' for i in range(seq_len)]\n",
    ")\n",
    "plt.title('Look-Ahead Mask (Decoder Self-Attention)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Key Position (被關注的位置)')\n",
    "plt.ylabel('Query Position (當前位置)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"說明:\")\n",
    "print(\"• 1 (藍色): 可以關注 (attend)\")\n",
    "print(\"• 0 (白色): 不能關注 (masked out)\")\n",
    "print(\"• 下三角矩陣確保每個位置只能看到自己和之前的 token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2-2",
   "metadata": {},
   "source": [
    "### 2.2 Masked Self-Attention 實作\n",
    "\n",
    "與 Encoder 的 Self-Attention 相比，唯一差異是添加 Look-Ahead Mask。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "masked-attention-func",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(\n",
    "    Q: np.ndarray, \n",
    "    K: np.ndarray, \n",
    "    V: np.ndarray, \n",
    "    mask: Optional[np.ndarray] = None\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Scaled Dot-Product Attention with optional masking\n",
    "    \n",
    "    Args:\n",
    "        Q: Query matrix (seq_len, d_k)\n",
    "        K: Key matrix (seq_len, d_k)\n",
    "        V: Value matrix (seq_len, d_v)\n",
    "        mask: Optional mask (seq_len, seq_len), 1=attend, 0=mask\n",
    "    \n",
    "    Returns:\n",
    "        output: Attention output (seq_len, d_v)\n",
    "        attention_weights: Attention weights (seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    d_k = K.shape[-1]\n",
    "    \n",
    "    # Compute attention scores\n",
    "    scores = np.matmul(Q, K.T) / np.sqrt(d_k)\n",
    "    \n",
    "    # Apply mask: set masked positions to large negative value\n",
    "    if mask is not None:\n",
    "        scores = np.where(mask == 0, -1e9, scores)\n",
    "    \n",
    "    # Apply softmax\n",
    "    attention_weights = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)\n",
    "    \n",
    "    # Compute weighted sum of values\n",
    "    output = np.matmul(attention_weights, V)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "\n",
    "# 測試 Masked Self-Attention\n",
    "seq_len = 5\n",
    "d_k = 64\n",
    "\n",
    "# 隨機生成 Q, K, V\n",
    "Q = K = V = np.random.randn(seq_len, d_k)\n",
    "\n",
    "# 建立 Look-Ahead Mask\n",
    "mask = create_look_ahead_mask(seq_len)\n",
    "\n",
    "# 計算 attention (有 mask vs 無 mask)\n",
    "output_masked, attn_masked = scaled_dot_product_attention(Q, K, V, mask)\n",
    "output_unmasked, attn_unmasked = scaled_dot_product_attention(Q, K, V, None)\n",
    "\n",
    "# 視覺化對比\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "sns.heatmap(attn_unmasked, annot=True, fmt='.2f', cmap='YlOrRd', \n",
    "            xticklabels=[f't{i}' for i in range(seq_len)],\n",
    "            yticklabels=[f't{i}' for i in range(seq_len)],\n",
    "            ax=axes[0], cbar_kws={'label': 'Attention Weight'})\n",
    "axes[0].set_title('Encoder Self-Attention (無 Mask)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Key Position')\n",
    "axes[0].set_ylabel('Query Position')\n",
    "\n",
    "sns.heatmap(attn_masked, annot=True, fmt='.2f', cmap='YlOrRd', \n",
    "            xticklabels=[f't{i}' for i in range(seq_len)],\n",
    "            yticklabels=[f't{i}' for i in range(seq_len)],\n",
    "            ax=axes[1], cbar_kws={'label': 'Attention Weight'})\n",
    "axes[1].set_title('Decoder Masked Self-Attention (有 Mask)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Key Position')\n",
    "axes[1].set_ylabel('Query Position')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n觀察:\")\n",
    "print(\"• 左圖: Encoder 可以看到所有位置（雙向注意力）\")\n",
    "print(\"• 右圖: Decoder 只能看到當前及之前位置（單向注意力）\")\n",
    "print(\"• 右上角三角區域的注意力權重為 0（被 mask 掉）\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3",
   "metadata": {},
   "source": [
    "## 3. Cross-Attention 實作 {#3-cross-attention}\n",
    "\n",
    "### 3.1 Cross-Attention 的作用\n",
    "\n",
    "**定義**: Decoder 使用 Encoder 的輸出來計算注意力\n",
    "\n",
    "**數學形式**:\n",
    "$$\n",
    "\\text{CrossAttention}(Q_{\\text{decoder}}, K_{\\text{encoder}}, V_{\\text{encoder}})\n",
    "$$\n",
    "\n",
    "**關鍵差異**:\n",
    "- **Self-Attention**: $Q, K, V$ 都來自同一序列\n",
    "- **Cross-Attention**: $Q$ 來自 Decoder，$K, V$ 來自 Encoder\n",
    "\n",
    "### 3.2 Cross-Attention 在機器翻譯中的應用\n",
    "\n",
    "```\n",
    "Encoder 輸入:  I     love   NLP    (Source)\n",
    "               ↓      ↓      ↓\n",
    "Encoder 輸出: [e1]   [e2]   [e3]   ← 作為 K, V\n",
    "                ↑      ↑      ↑\n",
    "                └──────┴──────┘\n",
    "                       ↓\n",
    "Decoder Query: [我]   [愛]   [?]    ← 作為 Q\n",
    "                       ↓\n",
    "Cross-Attention 計算每個 target token 與 source tokens 的相關性\n",
    "```\n",
    "\n",
    "**實例**: 當生成「愛」時\n",
    "- Query: 「愛」的表示向量\n",
    "- Keys: [\"I\", \"love\", \"NLP\"] 的表示向量\n",
    "- Attention 分數: 「愛」會高度關注 \"love\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-attention-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模擬 Cross-Attention 範例\n",
    "def cross_attention_demo():\n",
    "    \"\"\"\n",
    "    Demo: Visualize cross-attention between encoder and decoder\n",
    "    \"\"\"\n",
    "    # 序列長度\n",
    "    encoder_seq_len = 4  # Source: \"I love NLP .\"\n",
    "    decoder_seq_len = 5  # Target: \"我 愛 自然 語言 處理\"\n",
    "    d_k = 64\n",
    "    \n",
    "    # 模擬 Encoder 輸出 (作為 K, V)\n",
    "    encoder_output = np.random.randn(encoder_seq_len, d_k)\n",
    "    \n",
    "    # 模擬 Decoder 當前狀態 (作為 Q)\n",
    "    decoder_query = np.random.randn(decoder_seq_len, d_k)\n",
    "    \n",
    "    # Cross-Attention: Q from decoder, K,V from encoder\n",
    "    # 注意: K 和 V 的序列長度與 encoder 一致\n",
    "    K = V = encoder_output\n",
    "    Q = decoder_query\n",
    "    \n",
    "    # 計算 cross-attention scores\n",
    "    scores = np.matmul(Q, K.T) / np.sqrt(d_k)\n",
    "    attention_weights = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)\n",
    "    \n",
    "    # 視覺化\n",
    "    source_tokens = ['I', 'love', 'NLP', '.']\n",
    "    target_tokens = ['我', '愛', '自然', '語言', '處理']\n",
    "    \n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(\n",
    "        attention_weights, \n",
    "        annot=True, \n",
    "        fmt='.3f', \n",
    "        cmap='RdYlGn',\n",
    "        xticklabels=source_tokens,\n",
    "        yticklabels=target_tokens,\n",
    "        cbar_kws={'label': 'Attention Weight'}\n",
    "    )\n",
    "    plt.title('Cross-Attention: Decoder → Encoder', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Source (Encoder Keys/Values)', fontsize=12)\n",
    "    plt.ylabel('Target (Decoder Queries)', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"說明:\")\n",
    "    print(\"• 每一行: Decoder 中一個 token 對 Encoder 所有 tokens 的注意力分佈\")\n",
    "    print(\"• 例: '愛' 這一行顯示它對 ['I', 'love', 'NLP', '.'] 的注意力\")\n",
    "    print(\"• 理想情況: '愛' 會高度關注 'love'\")\n",
    "\n",
    "cross_attention_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3-2",
   "metadata": {},
   "source": [
    "### 3.3 Cross-Attention 實作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-attention-impl",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_attention(\n",
    "    Q_decoder: np.ndarray,\n",
    "    encoder_output: np.ndarray,\n",
    "    mask: Optional[np.ndarray] = None\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Cross-Attention between decoder and encoder\n",
    "    \n",
    "    Args:\n",
    "        Q_decoder: Decoder queries (decoder_seq_len, d_k)\n",
    "        encoder_output: Encoder output as K,V (encoder_seq_len, d_k)\n",
    "        mask: Optional padding mask for encoder (decoder_seq_len, encoder_seq_len)\n",
    "    \n",
    "    Returns:\n",
    "        output: Cross-attention output (decoder_seq_len, d_k)\n",
    "        attention_weights: (decoder_seq_len, encoder_seq_len)\n",
    "    \"\"\"\n",
    "    # Encoder output serves as both K and V\n",
    "    K = V = encoder_output\n",
    "    Q = Q_decoder\n",
    "    \n",
    "    d_k = K.shape[-1]\n",
    "    \n",
    "    # Compute attention scores\n",
    "    scores = np.matmul(Q, K.T) / np.sqrt(d_k)\n",
    "    \n",
    "    # Apply padding mask if provided\n",
    "    if mask is not None:\n",
    "        scores = np.where(mask == 0, -1e9, scores)\n",
    "    \n",
    "    # Softmax\n",
    "    attention_weights = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)\n",
    "    \n",
    "    # Weighted sum\n",
    "    output = np.matmul(attention_weights, V)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "\n",
    "# 測試 Cross-Attention\n",
    "encoder_seq_len = 6\n",
    "decoder_seq_len = 4\n",
    "d_k = 64\n",
    "\n",
    "encoder_output = np.random.randn(encoder_seq_len, d_k)\n",
    "decoder_query = np.random.randn(decoder_seq_len, d_k)\n",
    "\n",
    "output, attn_weights = cross_attention(decoder_query, encoder_output)\n",
    "\n",
    "print(\"✅ Cross-Attention 測試通過\")\n",
    "print(f\"Decoder Query 形狀: {decoder_query.shape}\")\n",
    "print(f\"Encoder Output 形狀: {encoder_output.shape}\")\n",
    "print(f\"Cross-Attention 輸出形狀: {output.shape}\")\n",
    "print(f\"Attention Weights 形狀: {attn_weights.shape}\")\n",
    "print(f\"\\n每個 decoder token 的注意力和應為 1.0:\")\n",
    "print(f\"  {np.sum(attn_weights, axis=1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4",
   "metadata": {},
   "source": [
    "## 4. 完整 Decoder Layer 實作 {#4-decoder-layer}\n",
    "\n",
    "### 4.1 Decoder Layer 的三個子層\n",
    "\n",
    "```\n",
    "Input\n",
    "  ↓\n",
    "① Masked Multi-Head Self-Attention\n",
    "  ↓\n",
    "Add & Norm\n",
    "  ↓\n",
    "② Cross-Attention (與 Encoder 輸出)\n",
    "  ↓\n",
    "Add & Norm\n",
    "  ↓\n",
    "③ Feed-Forward Network\n",
    "  ↓\n",
    "Add & Norm\n",
    "  ↓\n",
    "Output\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helper-components",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 輔助組件 (從 CH07-04 複製)\n",
    "\n",
    "class LayerNormalization:\n",
    "    \"\"\"Layer Normalization\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, eps: float = 1e-6):\n",
    "        self.gamma = np.ones(d_model)\n",
    "        self.beta = np.zeros(d_model)\n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        mean = np.mean(x, axis=-1, keepdims=True)\n",
    "        variance = np.var(x, axis=-1, keepdims=True)\n",
    "        x_norm = (x - mean) / np.sqrt(variance + self.eps)\n",
    "        return self.gamma * x_norm + self.beta\n",
    "\n",
    "\n",
    "class FeedForwardNetwork:\n",
    "    \"\"\"Position-wise Feed-Forward Network\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, d_ff: int):\n",
    "        self.W1 = np.random.randn(d_model, d_ff) / np.sqrt(d_model)\n",
    "        self.b1 = np.zeros(d_ff)\n",
    "        self.W2 = np.random.randn(d_ff, d_model) / np.sqrt(d_ff)\n",
    "        self.b2 = np.zeros(d_model)\n",
    "    \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        hidden = np.maximum(0, np.dot(x, self.W1) + self.b1)\n",
    "        output = np.dot(hidden, self.W2) + self.b2\n",
    "        return output\n",
    "\n",
    "\n",
    "class MultiHeadAttention:\n",
    "    \"\"\"Multi-Head Attention (可用於 Self-Attention 或 Cross-Attention)\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, num_heads: int):\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.W_q = np.random.randn(d_model, d_model) / np.sqrt(d_model)\n",
    "        self.W_k = np.random.randn(d_model, d_model) / np.sqrt(d_model)\n",
    "        self.W_v = np.random.randn(d_model, d_model) / np.sqrt(d_model)\n",
    "        self.W_o = np.random.randn(d_model, d_model) / np.sqrt(d_model)\n",
    "    \n",
    "    def split_heads(self, x: np.ndarray) -> np.ndarray:\n",
    "        seq_len = x.shape[0]\n",
    "        x = x.reshape(seq_len, self.num_heads, self.d_k)\n",
    "        return x.transpose(1, 0, 2)\n",
    "    \n",
    "    def forward(\n",
    "        self, \n",
    "        Q_input: np.ndarray,\n",
    "        K_input: np.ndarray,\n",
    "        V_input: np.ndarray,\n",
    "        mask: Optional[np.ndarray] = None\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            Q_input: Query input (seq_len_q, d_model)\n",
    "            K_input: Key input (seq_len_k, d_model)\n",
    "            V_input: Value input (seq_len_v, d_model)\n",
    "            mask: Optional mask (seq_len_q, seq_len_k)\n",
    "        \"\"\"\n",
    "        Q = np.dot(Q_input, self.W_q)\n",
    "        K = np.dot(K_input, self.W_k)\n",
    "        V = np.dot(V_input, self.W_v)\n",
    "        \n",
    "        Q = self.split_heads(Q)\n",
    "        K = self.split_heads(K)\n",
    "        V = self.split_heads(V)\n",
    "        \n",
    "        outputs = []\n",
    "        attention_weights_all = []\n",
    "        \n",
    "        for i in range(self.num_heads):\n",
    "            output, attn_weights = scaled_dot_product_attention(\n",
    "                Q[i], K[i], V[i], mask\n",
    "            )\n",
    "            outputs.append(output)\n",
    "            attention_weights_all.append(attn_weights)\n",
    "        \n",
    "        concat_output = np.concatenate(outputs, axis=-1)\n",
    "        final_output = np.dot(concat_output, self.W_o)\n",
    "        attention_weights = np.stack(attention_weights_all, axis=0)\n",
    "        \n",
    "        return final_output, attention_weights\n",
    "\n",
    "print(\"✅ 輔助組件載入完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decoder-layer",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer:\n",
    "    \"\"\"Complete Transformer Decoder Layer\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout: float = 0.1):\n",
    "        # ① Masked Multi-Head Self-Attention\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        \n",
    "        # ② Cross-Attention\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        \n",
    "        # ③ Feed-Forward Network\n",
    "        self.ffn = FeedForwardNetwork(d_model, d_ff)\n",
    "        \n",
    "        # Layer Normalization (3 個)\n",
    "        self.layernorm1 = LayerNormalization(d_model)\n",
    "        self.layernorm2 = LayerNormalization(d_model)\n",
    "        self.layernorm3 = LayerNormalization(d_model)\n",
    "        \n",
    "        self.dropout = dropout\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        x: np.ndarray,\n",
    "        encoder_output: np.ndarray,\n",
    "        look_ahead_mask: Optional[np.ndarray] = None,\n",
    "        padding_mask: Optional[np.ndarray] = None\n",
    "    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Decoder input (decoder_seq_len, d_model)\n",
    "            encoder_output: Encoder output (encoder_seq_len, d_model)\n",
    "            look_ahead_mask: Mask for self-attention (decoder_seq_len, decoder_seq_len)\n",
    "            padding_mask: Mask for cross-attention (decoder_seq_len, encoder_seq_len)\n",
    "        \n",
    "        Returns:\n",
    "            output: (decoder_seq_len, d_model)\n",
    "            self_attn_weights: (num_heads, decoder_seq_len, decoder_seq_len)\n",
    "            cross_attn_weights: (num_heads, decoder_seq_len, encoder_seq_len)\n",
    "        \"\"\"\n",
    "        # ① Masked Multi-Head Self-Attention\n",
    "        self_attn_output, self_attn_weights = self.self_attn.forward(\n",
    "            x, x, x, look_ahead_mask\n",
    "        )\n",
    "        x = self.layernorm1.forward(x + self_attn_output)\n",
    "        \n",
    "        # ② Cross-Attention\n",
    "        cross_attn_output, cross_attn_weights = self.cross_attn.forward(\n",
    "            Q_input=x,  # Query from decoder\n",
    "            K_input=encoder_output,  # Key from encoder\n",
    "            V_input=encoder_output,  # Value from encoder\n",
    "            mask=padding_mask\n",
    "        )\n",
    "        x = self.layernorm2.forward(x + cross_attn_output)\n",
    "        \n",
    "        # ③ Feed-Forward Network\n",
    "        ffn_output = self.ffn.forward(x)\n",
    "        x = self.layernorm3.forward(x + ffn_output)\n",
    "        \n",
    "        return x, self_attn_weights, cross_attn_weights\n",
    "\n",
    "\n",
    "# 測試 Decoder Layer\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "d_ff = 2048\n",
    "encoder_seq_len = 10\n",
    "decoder_seq_len = 8\n",
    "\n",
    "# 模擬輸入\n",
    "decoder_input = np.random.randn(decoder_seq_len, d_model)\n",
    "encoder_output = np.random.randn(encoder_seq_len, d_model)\n",
    "look_ahead_mask = create_look_ahead_mask(decoder_seq_len)\n",
    "\n",
    "# 建立並測試 Decoder Layer\n",
    "decoder_layer = TransformerDecoderLayer(d_model, num_heads, d_ff)\n",
    "output, self_attn, cross_attn = decoder_layer.forward(\n",
    "    decoder_input, encoder_output, look_ahead_mask\n",
    ")\n",
    "\n",
    "print(\"✅ Transformer Decoder Layer 測試通過\")\n",
    "print(f\"Decoder 輸入: {decoder_input.shape}\")\n",
    "print(f\"Encoder 輸出: {encoder_output.shape}\")\n",
    "print(f\"Decoder 輸出: {output.shape}\")\n",
    "print(f\"Self-Attention 權重: {self_attn.shape}\")\n",
    "print(f\"Cross-Attention 權重: {cross_attn.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-5",
   "metadata": {},
   "source": [
    "## 5. Encoder-Decoder 組合 {#5-encoder-decoder}\n",
    "\n",
    "### 5.1 完整 Transformer 架構\n",
    "\n",
    "```\n",
    "Source Input (Encoder)\n",
    "         ↓\n",
    "    Embedding + PE\n",
    "         ↓\n",
    "    Encoder Layer 1\n",
    "         ↓\n",
    "         ...\n",
    "         ↓\n",
    "    Encoder Layer N\n",
    "         ↓\n",
    "    Encoder Output ──────────────┐\n",
    "                                 │\n",
    "Target Input (Decoder)           │\n",
    "         ↓                       │\n",
    "    Embedding + PE               │\n",
    "         ↓                       │\n",
    "    Decoder Layer 1 ←────────────┘ (Cross-Attention)\n",
    "         ↓                       │\n",
    "         ...                    │\n",
    "         ↓                       │\n",
    "    Decoder Layer N ←────────────┘\n",
    "         ↓\n",
    "    Linear + Softmax\n",
    "         ↓\n",
    "    Output Probabilities\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "full-transformer",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer:\n",
    "    \"\"\"Complete Transformer with Encoder and Decoder\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        num_encoder_layers: int,\n",
    "        num_decoder_layers: int,\n",
    "        d_model: int,\n",
    "        num_heads: int,\n",
    "        d_ff: int,\n",
    "        source_vocab_size: int,\n",
    "        target_vocab_size: int,\n",
    "        max_seq_len: int,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Embeddings\n",
    "        self.source_embedding = np.random.randn(source_vocab_size, d_model) / np.sqrt(d_model)\n",
    "        self.target_embedding = np.random.randn(target_vocab_size, d_model) / np.sqrt(d_model)\n",
    "        self.positional_encoding = self.get_positional_encoding(max_seq_len, d_model)\n",
    "        \n",
    "        # Encoder stack\n",
    "        self.encoder_layers = [\n",
    "            self._create_encoder_layer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_encoder_layers)\n",
    "        ]\n",
    "        \n",
    "        # Decoder stack\n",
    "        self.decoder_layers = [\n",
    "            TransformerDecoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_decoder_layers)\n",
    "        ]\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_projection = np.random.randn(d_model, target_vocab_size) / np.sqrt(d_model)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_positional_encoding(max_seq_len: int, d_model: int) -> np.ndarray:\n",
    "        pos_encoding = np.zeros((max_seq_len, d_model))\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pos_encoding[pos, i] = np.sin(pos / (10000 ** (i / d_model)))\n",
    "                if i + 1 < d_model:\n",
    "                    pos_encoding[pos, i + 1] = np.cos(pos / (10000 ** (i / d_model)))\n",
    "        return pos_encoding\n",
    "    \n",
    "    def _create_encoder_layer(self, d_model, num_heads, d_ff, dropout):\n",
    "        \"\"\"簡化版 Encoder Layer (無 Cross-Attention)\"\"\"\n",
    "        class EncoderLayer:\n",
    "            def __init__(self):\n",
    "                self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "                self.ffn = FeedForwardNetwork(d_model, d_ff)\n",
    "                self.layernorm1 = LayerNormalization(d_model)\n",
    "                self.layernorm2 = LayerNormalization(d_model)\n",
    "            \n",
    "            def forward(self, x, mask=None):\n",
    "                attn_output, _ = self.self_attn.forward(x, x, x, mask)\n",
    "                x = self.layernorm1.forward(x + attn_output)\n",
    "                ffn_output = self.ffn.forward(x)\n",
    "                x = self.layernorm2.forward(x + ffn_output)\n",
    "                return x\n",
    "        \n",
    "        return EncoderLayer()\n",
    "    \n",
    "    def encode(self, source_ids: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Encode source sequence\"\"\"\n",
    "        seq_len = source_ids.shape[0]\n",
    "        \n",
    "        # Embedding + Positional Encoding\n",
    "        x = self.source_embedding[source_ids]\n",
    "        x = x + self.positional_encoding[:seq_len, :]\n",
    "        \n",
    "        # Pass through encoder layers\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            x = encoder_layer.forward(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def decode(\n",
    "        self,\n",
    "        target_ids: np.ndarray,\n",
    "        encoder_output: np.ndarray,\n",
    "        look_ahead_mask: Optional[np.ndarray] = None\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Decode target sequence\"\"\"\n",
    "        seq_len = target_ids.shape[0]\n",
    "        \n",
    "        # Embedding + Positional Encoding\n",
    "        x = self.target_embedding[target_ids]\n",
    "        x = x + self.positional_encoding[:seq_len, :]\n",
    "        \n",
    "        # Pass through decoder layers\n",
    "        for decoder_layer in self.decoder_layers:\n",
    "            x, _, _ = decoder_layer.forward(x, encoder_output, look_ahead_mask)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        source_ids: np.ndarray,\n",
    "        target_ids: np.ndarray\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Complete forward pass\n",
    "        \n",
    "        Args:\n",
    "            source_ids: Source token IDs (source_seq_len,)\n",
    "            target_ids: Target token IDs (target_seq_len,)\n",
    "        \n",
    "        Returns:\n",
    "            logits: Output logits (target_seq_len, target_vocab_size)\n",
    "        \"\"\"\n",
    "        # Encode\n",
    "        encoder_output = self.encode(source_ids)\n",
    "        \n",
    "        # Create look-ahead mask for decoder\n",
    "        target_seq_len = target_ids.shape[0]\n",
    "        look_ahead_mask = create_look_ahead_mask(target_seq_len)\n",
    "        \n",
    "        # Decode\n",
    "        decoder_output = self.decode(target_ids, encoder_output, look_ahead_mask)\n",
    "        \n",
    "        # Project to vocabulary\n",
    "        logits = np.dot(decoder_output, self.output_projection)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "# 測試完整 Transformer\n",
    "transformer = Transformer(\n",
    "    num_encoder_layers=6,\n",
    "    num_decoder_layers=6,\n",
    "    d_model=512,\n",
    "    num_heads=8,\n",
    "    d_ff=2048,\n",
    "    source_vocab_size=10000,\n",
    "    target_vocab_size=8000,\n",
    "    max_seq_len=100\n",
    ")\n",
    "\n",
    "# 模擬輸入\n",
    "source_ids = np.random.randint(0, 10000, size=12)\n",
    "target_ids = np.random.randint(0, 8000, size=10)\n",
    "\n",
    "# 前向傳播\n",
    "logits = transformer.forward(source_ids, target_ids)\n",
    "\n",
    "print(\"✅ 完整 Transformer 測試通過\")\n",
    "print(f\"Source 序列長度: {len(source_ids)}\")\n",
    "print(f\"Target 序列長度: {len(target_ids)}\")\n",
    "print(f\"輸出 logits 形狀: {logits.shape}\")\n",
    "print(f\"  → 每個 target token 有 {logits.shape[1]} 個詞彙的機率分數\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-6",
   "metadata": {},
   "source": [
    "## 6. 自回歸生成與推論 {#6-generation}\n",
    "\n",
    "### 6.1 訓練 vs 推論的差異\n",
    "\n",
    "**訓練時 (Teacher Forcing)**:\n",
    "- 輸入: 完整的 target 序列（已知所有 token）\n",
    "- 目標: 一次性預測所有位置的 token\n",
    "- 效率: 高（並行計算）\n",
    "\n",
    "**推論時 (Autoregressive Generation)**:\n",
    "- 輸入: 逐步生成的序列（從 `<BOS>` 開始）\n",
    "- 目標: 每次預測下一個 token\n",
    "- 效率: 低（必須循序生成）\n",
    "\n",
    "### 6.2 自回歸生成流程\n",
    "\n",
    "```\n",
    "Step 1: Input = [<BOS>]\n",
    "        Output = \"我\"\n",
    "\n",
    "Step 2: Input = [<BOS>, \"我\"]\n",
    "        Output = \"愛\"\n",
    "\n",
    "Step 3: Input = [<BOS>, \"我\", \"愛\"]\n",
    "        Output = \"NLP\"\n",
    "\n",
    "Step 4: Input = [<BOS>, \"我\", \"愛\", \"NLP\"]\n",
    "        Output = <EOS>  → 停止生成\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(\n",
    "    transformer: Transformer,\n",
    "    source_ids: np.ndarray,\n",
    "    max_len: int,\n",
    "    bos_token_id: int,\n",
    "    eos_token_id: int\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Greedy decoding for sequence generation\n",
    "    \n",
    "    Args:\n",
    "        transformer: Trained Transformer model\n",
    "        source_ids: Source sequence (source_seq_len,)\n",
    "        max_len: Maximum generation length\n",
    "        bos_token_id: Beginning-of-sequence token ID\n",
    "        eos_token_id: End-of-sequence token ID\n",
    "    \n",
    "    Returns:\n",
    "        generated_ids: Generated sequence (variable length)\n",
    "    \"\"\"\n",
    "    # Encode source once\n",
    "    encoder_output = transformer.encode(source_ids)\n",
    "    \n",
    "    # Initialize with <BOS>\n",
    "    generated_ids = [bos_token_id]\n",
    "    \n",
    "    for _ in range(max_len):\n",
    "        # Current target sequence\n",
    "        target_ids = np.array(generated_ids)\n",
    "        \n",
    "        # Create look-ahead mask\n",
    "        look_ahead_mask = create_look_ahead_mask(len(target_ids))\n",
    "        \n",
    "        # Decode\n",
    "        decoder_output = transformer.decode(target_ids, encoder_output, look_ahead_mask)\n",
    "        \n",
    "        # Project to vocabulary\n",
    "        logits = np.dot(decoder_output, transformer.output_projection)\n",
    "        \n",
    "        # Get the last token's logits and apply softmax\n",
    "        last_logits = logits[-1, :]\n",
    "        probs = np.exp(last_logits) / np.sum(np.exp(last_logits))\n",
    "        \n",
    "        # Greedy: select the token with highest probability\n",
    "        next_token_id = np.argmax(probs)\n",
    "        \n",
    "        # Append to sequence\n",
    "        generated_ids.append(next_token_id)\n",
    "        \n",
    "        # Stop if <EOS> generated\n",
    "        if next_token_id == eos_token_id:\n",
    "            break\n",
    "    \n",
    "    return np.array(generated_ids)\n",
    "\n",
    "\n",
    "# 模擬生成測試\n",
    "source_ids = np.random.randint(0, 10000, size=8)\n",
    "BOS_TOKEN = 1\n",
    "EOS_TOKEN = 2\n",
    "MAX_LEN = 15\n",
    "\n",
    "print(\"開始自回歸生成...\")\n",
    "generated = greedy_decode(transformer, source_ids, MAX_LEN, BOS_TOKEN, EOS_TOKEN)\n",
    "\n",
    "print(f\"\\n✅ 生成完成\")\n",
    "print(f\"Source 長度: {len(source_ids)}\")\n",
    "print(f\"生成序列長度: {len(generated)}\")\n",
    "print(f\"生成的 token IDs: {generated}\")\n",
    "print(f\"\\n說明:\")\n",
    "print(f\"  • 第一個 token 是 BOS ({BOS_TOKEN})\")\n",
    "print(f\"  • 若遇到 EOS ({EOS_TOKEN}) 則停止生成\")\n",
    "print(f\"  • 每步都只預測下一個 token (自回歸)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decoding-strategies",
   "metadata": {},
   "source": [
    "### 6.3 不同解碼策略\n",
    "\n",
    "| 策略 | 說明 | 優點 | 缺點 |\n",
    "|------|------|------|------|\n",
    "| **Greedy** | 每步選最高機率 token | 快速、確定性 | 可能陷入局部最優 |\n",
    "| **Beam Search** | 維護 top-k 個候選序列 | 品質較好 | 較慢、需要更多記憶體 |\n",
    "| **Sampling** | 根據機率分佈隨機採樣 | 多樣性高 | 可能不連貫 |\n",
    "| **Top-k Sampling** | 只從 top-k token 中採樣 | 平衡品質與多樣性 | 需調整 k 值 |\n",
    "| **Nucleus (Top-p)** | 從累積機率達 p 的 token 中採樣 | 自適應選擇範圍 | 需調整 p 值 |\n",
    "\n",
    "**Beam Search 示意圖**:\n",
    "```\n",
    "Beam Size = 2\n",
    "\n",
    "Step 1:        [我 (0.6)]     [你 (0.3)]\n",
    "               /     \\         /     \\\n",
    "Step 2:    [愛]    [喜歡]   [愛]   [討厭]\n",
    "           (0.4)   (0.3)   (0.2)   (0.15)\n",
    "           \n",
    "Keep top-2:  [我, 愛]  [我, 喜歡]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-7",
   "metadata": {},
   "source": [
    "## 7. 總結與練習 {#7-summary}\n",
    "\n",
    "### 7.1 本節重點回顧\n",
    "\n",
    "✅ **Decoder 的三大組件**:\n",
    "1. **Masked Self-Attention**: 確保只能看到過去的 token\n",
    "2. **Cross-Attention**: 與 Encoder 輸出互動，獲取源序列資訊\n",
    "3. **Feed-Forward Network**: 非線性轉換\n",
    "\n",
    "✅ **關鍵概念**:\n",
    "- **Look-Ahead Mask**: 下三角矩陣，防止看到未來\n",
    "- **Cross-Attention**: Q 來自 Decoder，K/V 來自 Encoder\n",
    "- **Autoregressive Generation**: 逐步生成，每次預測下一個 token\n",
    "\n",
    "✅ **Encoder vs Decoder**:\n",
    "\n",
    "| 特性 | Encoder | Decoder |\n",
    "|------|---------|----------|\n",
    "| 注意力方向 | 雙向 | 單向 (masked) |\n",
    "| Cross-Attention | ❌ | ✅ |\n",
    "| 應用 | 理解任務 | 生成任務 |\n",
    "| 代表模型 | BERT | GPT |\n",
    "\n",
    "### 7.2 完整 Transformer 架構總結\n",
    "\n",
    "**Encoder-Decoder 架構** (原始 Transformer):\n",
    "- 應用: 機器翻譯、文本摘要\n",
    "- 代表: T5, BART, mBART\n",
    "\n",
    "**Encoder-Only 架構**:\n",
    "- 應用: 文本分類、NER、問答\n",
    "- 代表: BERT, RoBERTa, ALBERT\n",
    "\n",
    "**Decoder-Only 架構**:\n",
    "- 應用: 文本生成、對話系統\n",
    "- 代表: GPT, GPT-2, GPT-3, LLaMA\n",
    "\n",
    "### 7.3 實作練習\n",
    "\n",
    "#### 練習 1: 視覺化 Cross-Attention\n",
    "\n",
    "**任務**: 使用真實的翻譯範例，視覺化 Cross-Attention 權重，觀察哪些 target token 關注哪些 source token。\n",
    "\n",
    "**提示**:\n",
    "```python\n",
    "source = \"I love natural language processing\"\n",
    "target = \"我 愛 自然 語言 處理\"\n",
    "# 繪製 heatmap 顯示對齊關係\n",
    "```\n",
    "\n",
    "#### 練習 2: 實作 Beam Search\n",
    "\n",
    "**任務**: 實作 Beam Search 解碼策略，並與 Greedy Decoding 比較生成品質。\n",
    "\n",
    "**提示**:\n",
    "```python\n",
    "def beam_search_decode(model, source, beam_size=5):\n",
    "    # 維護 top-k 個候選序列\n",
    "    # 每步擴展所有候選，保留總機率最高的 k 個\n",
    "    pass\n",
    "```\n",
    "\n",
    "#### 練習 3: 實作 Temperature Sampling\n",
    "\n",
    "**任務**: 在生成時加入 temperature 參數控制隨機性。\n",
    "\n",
    "**提示**:\n",
    "```python\n",
    "# Temperature scaling\n",
    "logits = logits / temperature\n",
    "probs = softmax(logits)\n",
    "next_token = np.random.choice(vocab_size, p=probs)\n",
    "```\n",
    "\n",
    "**Temperature 效果**:\n",
    "- `T = 0.5`: 更確定性，偏向高機率 token\n",
    "- `T = 1.0`: 標準 softmax\n",
    "- `T = 2.0`: 更隨機，分佈更均勻\n",
    "\n",
    "### 7.4 延伸閱讀\n",
    "\n",
    "1. **論文**:\n",
    "   - [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - 仔細閱讀 Decoder 部分\n",
    "   - [The Curious Case of Neural Text Degeneration](https://arxiv.org/abs/1904.09751) - Nucleus Sampling\n",
    "\n",
    "2. **實作資源**:\n",
    "   - [Annotated Transformer - Decoder](http://nlp.seas.harvard.edu/annotated-transformer/)\n",
    "   - [Hugging Face - Generation Strategies](https://huggingface.co/docs/transformers/generation_strategies)\n",
    "\n",
    "3. **視覺化工具**:\n",
    "   - [Seq2Seq Visualization](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 下一節預告\n",
    "\n",
    "**CH07-06: Encoder vs Decoder vs Encoder-Decoder 對比**\n",
    "- 三種架構的優缺點分析\n",
    "- 不同任務的最佳架構選擇\n",
    "- BERT vs GPT vs T5 實際對比\n",
    "- 模型選擇的實戰建議\n",
    "\n",
    "---\n",
    "\n",
    "**課程完成時間**: `____年____月____日`  \n",
    "**學習心得**: ___________________________________"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
