{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH07-02: åµŒå…¥å±¤ (Embeddings)\n",
    "\n",
    "**èª²ç¨‹ç›®æ¨™:**\n",
    "- ç†è§£è©åµŒå…¥ (Word Embeddings) çš„æ ¸å¿ƒæ¦‚å¿µ\n",
    "- æŒæ¡ Token Embeddings èˆ‡ Position Embeddings çš„å¯¦ä½œ\n",
    "- æ¯”è¼ƒ Word2Vec, GloVe èˆ‡ Transformer Embeddings çš„å·®ç•°\n",
    "- å¯¦æˆ°è¨“ç·´èˆ‡è©•ä¼°è©å‘é‡å“è³ª\n",
    "\n",
    "**å­¸ç¿’æ™‚é–“:** ç´„ 60 åˆ†é˜\n",
    "\n",
    "**å‰ç½®çŸ¥è­˜:**\n",
    "- è©å‘é‡åŸºç¤æ¦‚å¿µ (CH05-02)\n",
    "- Transformer æ¶æ§‹æ¦‚è¦½ (CH07-01)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š ç›®éŒ„\n",
    "\n",
    "1. [ç‚ºä»€éº¼éœ€è¦è©åµŒå…¥?](#1)\n",
    "2. [Token Embeddings å¯¦ä½œ](#2)\n",
    "3. [Positional Embeddings æ·±å…¥](#3)\n",
    "4. [Word2Vec vs Transformer Embeddings](#4)\n",
    "5. [å¯¦æˆ°: è¨“ç·´èˆ‡è©•ä¼°è©å‘é‡](#5)\n",
    "6. [é€²éšæŠ€å·§èˆ‡å„ªåŒ–](#6)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç’°å¢ƒè¨­å®šèˆ‡å¥—ä»¶å°å…¥\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# è¨­å®šä¸­æ–‡é¡¯ç¤º\n",
    "plt.rcParams['font.sans-serif'] = ['Microsoft JhengHei', 'SimHei', 'Arial Unicode MS']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# è¨­å®šéš¨æ©Ÿç¨®å­\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"âœ… ç’°å¢ƒè¨­å®šå®Œæˆ\")\n",
    "print(f\"NumPy ç‰ˆæœ¬: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "## 1. ç‚ºä»€éº¼éœ€è¦è©åµŒå…¥?\n",
    "\n",
    "### 1.1 å¾ One-Hot åˆ°è©åµŒå…¥çš„æ¼”é€²\n",
    "\n",
    "#### å•é¡Œ: å¦‚ä½•è¡¨ç¤ºè©?\n",
    "\n",
    "**æ–¹æ³• 1: One-Hot Encoding (ç¨ç†±ç·¨ç¢¼)**\n",
    "\n",
    "```python\n",
    "è©å½™è¡¨: [\"è²“\", \"ç‹—\", \"é³¥\"]\n",
    "\n",
    "\"è²“\" = [1, 0, 0]\n",
    "\"ç‹—\" = [0, 1, 0]\n",
    "\"é³¥\" = [0, 0, 1]\n",
    "```\n",
    "\n",
    "**ä¸‰å¤§å•é¡Œ:**\n",
    "1. âŒ **ç¶­åº¦ç½é›£**: è©å½™ 10 è¬ â†’ 10 è¬ç¶­å‘é‡!\n",
    "2. âŒ **ç¨€ç–æ€§**: 99.999% éƒ½æ˜¯ 0\n",
    "3. âŒ **ç„¡èªç¾©**: \"è²“\" å’Œ \"ç‹—\" åŒæ¨£é™é  (ä½™å¼¦ç›¸ä¼¼åº¦ = 0)\n",
    "\n",
    "---\n",
    "\n",
    "**æ–¹æ³• 2: è©åµŒå…¥ (Word Embeddings)**\n",
    "\n",
    "```python\n",
    "\"è²“\" = [0.8, -0.3, 0.5, ..., 0.2]  # 256 ç¶­ç¨ å¯†å‘é‡\n",
    "\"ç‹—\" = [0.7, -0.2, 0.6, ..., 0.3]  # ç›¸ä¼¼!\n",
    "\"é³¥\" = [0.3,  0.8, 0.1, ..., -0.5] # ä¸åŒ\n",
    "```\n",
    "\n",
    "**ä¸‰å¤§å„ªå‹¢:**\n",
    "1. âœ… **ä½ç¶­ç¨ å¯†**: é€šå¸¸ 256-1024 ç¶­\n",
    "2. âœ… **æ•æ‰èªç¾©**: ç›¸ä¼¼è©å‘é‡æ¥è¿‘\n",
    "3. âœ… **å¯é‹ç®—**: æ”¯æ´å‘é‡é‹ç®— (\"åœ‹ç‹\" - \"ç”·äºº\" + \"å¥³äºº\" â‰ˆ \"å¥³ç‹\")\n",
    "\n",
    "---\n",
    "\n",
    "### 1.2 Transformer ä¸­çš„é›™é‡åµŒå…¥\n",
    "\n",
    "Transformer ä½¿ç”¨**å…©ç¨®åµŒå…¥**çš„çµ„åˆ:\n",
    "\n",
    "$$\n",
    "\\text{Input} = \\text{Token Embedding} + \\text{Positional Embedding}\n",
    "$$\n",
    "\n",
    "1. **Token Embedding**: è¡¨ç¤ºè©çš„**èªç¾©**\n",
    "2. **Positional Embedding**: è¡¨ç¤ºè©çš„**ä½ç½®**\n",
    "\n",
    "**ç‚ºä»€éº¼éœ€è¦ä½ç½®åµŒå…¥?**\n",
    "\n",
    "- Self-Attention æ˜¯**ç„¡åºçš„** (permutation-invariant)\n",
    "- å¿…é ˆé¡¯å¼å‘ŠçŸ¥æ¨¡å‹è©çš„é †åº\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç¤ºç¯„: One-Hot vs è©åµŒå…¥\n",
    "vocab = [\"è²“\", \"ç‹—\", \"é³¥\", \"é­š\", \"å…”å­\"]\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 3  # ç°¡åŒ–ç‚º 3 ç¶­ä»¥ä¾¿è¦–è¦ºåŒ–\n",
    "\n",
    "# One-Hot ç·¨ç¢¼\n",
    "one_hot = np.eye(vocab_size)\n",
    "\n",
    "# æ¨¡æ“¬è©åµŒå…¥ (å¯¦éš›æ‡‰é€éè¨“ç·´ç²å¾—)\n",
    "# è¨­è¨ˆ: è²“ã€ç‹—æ¥è¿‘; é³¥ã€é­šæ¥è¿‘; å…”å­ä»‹æ–¼ä¸­é–“\n",
    "embeddings = np.array([\n",
    "    [0.8, 0.2, 0.1],   # è²“\n",
    "    [0.7, 0.3, 0.15],  # ç‹— (èˆ‡è²“ç›¸ä¼¼)\n",
    "    [0.1, 0.8, 0.7],   # é³¥\n",
    "    [0.15, 0.75, 0.8], # é­š (èˆ‡é³¥ç›¸ä¼¼)\n",
    "    [0.5, 0.5, 0.4],   # å…”å­ (ä¸­é–“)\n",
    "])\n",
    "\n",
    "# è¦–è¦ºåŒ–\n",
    "fig = plt.figure(figsize=(16, 6))\n",
    "\n",
    "# One-Hot ç·¨ç¢¼\n",
    "ax1 = fig.add_subplot(121)\n",
    "sns.heatmap(one_hot, annot=True, fmt='.0f', cmap='Greys', \n",
    "            xticklabels=[f'ç¶­åº¦{i+1}' for i in range(vocab_size)],\n",
    "            yticklabels=vocab, cbar_kws={'label': 'Value'})\n",
    "ax1.set_title('One-Hot ç·¨ç¢¼ (ç¨€ç–, ç„¡èªç¾©)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# è©åµŒå…¥\n",
    "ax2 = fig.add_subplot(122, projection='3d')\n",
    "colors = ['red', 'red', 'blue', 'blue', 'green']\n",
    "for i, word in enumerate(vocab):\n",
    "    ax2.scatter(embeddings[i, 0], embeddings[i, 1], embeddings[i, 2], \n",
    "                c=colors[i], s=200, alpha=0.6)\n",
    "    ax2.text(embeddings[i, 0], embeddings[i, 1], embeddings[i, 2], \n",
    "             word, fontsize=12, fontweight='bold')\n",
    "ax2.set_xlabel('Dimension 1', fontsize=11)\n",
    "ax2.set_ylabel('Dimension 2', fontsize=11)\n",
    "ax2.set_zlabel('Dimension 3', fontsize=11)\n",
    "ax2.set_title('è©åµŒå…¥ (ç¨ å¯†, æœ‰èªç¾©)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ” è§€å¯Ÿ:\")\n",
    "print(\"  - One-Hot: æ¯å€‹è©éƒ½æ˜¯æ­£äº¤å‘é‡ (ç„¡ç›¸ä¼¼åº¦)\")\n",
    "print(\"  - è©åµŒå…¥: ç›¸ä¼¼è© (è²“&ç‹—, é³¥&é­š) åœ¨ç©ºé–“ä¸­æ¥è¿‘\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "## 2. Token Embeddings å¯¦ä½œ\n",
    "\n",
    "### 2.1 åµŒå…¥å±¤ (Embedding Layer) åŸç†\n",
    "\n",
    "**åµŒå…¥å±¤æœ¬è³ª:** ä¸€å€‹**æŸ¥æ‰¾è¡¨** (Lookup Table)\n",
    "\n",
    "```python\n",
    "Embedding Matrix: (vocab_size, embedding_dim)\n",
    "\n",
    "              ç¶­åº¦ 1   ç¶­åº¦ 2   ...  ç¶­åº¦ d\n",
    "è© 0 (\"the\")   0.23    -0.45   ...   0.12\n",
    "è© 1 (\"cat\")   0.78     0.12   ...  -0.34\n",
    "è© 2 (\"sat\")  -0.23     0.89   ...   0.56\n",
    "...\n",
    "è© N          ...\n",
    "```\n",
    "\n",
    "**æŸ¥æ‰¾éç¨‹:**\n",
    "\n",
    "```python\n",
    "è¼¸å…¥ token ID: 1 (\"cat\")\n",
    "    â†“\n",
    "æŸ¥æ‰¾ Embedding Matrix ç¬¬ 1 è¡Œ\n",
    "    â†“\n",
    "è¼¸å‡º: [0.78, 0.12, ..., -0.34]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2.2 PyTorch é¢¨æ ¼å¯¦ä½œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¾é›¶å¯¦ä½œ Embedding Layer\n",
    "class SimpleEmbedding:\n",
    "    \"\"\"\n",
    "    ç°¡åŒ–ç‰ˆåµŒå…¥å±¤å¯¦ä½œ\n",
    "    \n",
    "    Args:\n",
    "        vocab_size: è©å½™è¡¨å¤§å°\n",
    "        embedding_dim: åµŒå…¥å‘é‡ç¶­åº¦\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # åˆå§‹åŒ–åµŒå…¥çŸ©é™£ (éš¨æ©Ÿåˆå§‹åŒ–, è¨“ç·´æ™‚æœƒæ›´æ–°)\n",
    "        self.weight = np.random.randn(vocab_size, embedding_dim) * 0.1\n",
    "        \n",
    "    def forward(self, token_ids):\n",
    "        \"\"\"\n",
    "        å‰å‘å‚³æ’­: æŸ¥è¡¨ç²å–åµŒå…¥å‘é‡\n",
    "        \n",
    "        Args:\n",
    "            token_ids: shape (batch_size, seq_len) æˆ– (seq_len,)\n",
    "            \n",
    "        Returns:\n",
    "            embeddings: shape (batch_size, seq_len, embedding_dim) æˆ– (seq_len, embedding_dim)\n",
    "        \"\"\"\n",
    "        return self.weight[token_ids]\n",
    "    \n",
    "    def __call__(self, token_ids):\n",
    "        return self.forward(token_ids)\n",
    "\n",
    "\n",
    "# æ¸¬è©¦\n",
    "vocab_size = 10000  # è©å½™è¡¨å¤§å°\n",
    "embedding_dim = 256  # BERT-base ä½¿ç”¨ 768, GPT-2 ä½¿ç”¨ 768, GPT-3 ä½¿ç”¨ 12288\n",
    "\n",
    "embedding_layer = SimpleEmbedding(vocab_size, embedding_dim)\n",
    "\n",
    "# æ¨¡æ“¬è¼¸å…¥: ä¸€å€‹å¥å­çš„ token IDs\n",
    "# ä¾‹å¦‚: \"The cat sat on the mat\" â†’ [20, 45, 123, 56, 20, 89]\n",
    "token_ids = np.array([20, 45, 123, 56, 20, 89])\n",
    "\n",
    "# ç²å–åµŒå…¥\n",
    "embeddings = embedding_layer(token_ids)\n",
    "\n",
    "print(f\"è¼¸å…¥ Token IDs å½¢ç‹€: {token_ids.shape}\")\n",
    "print(f\"è¼¸å‡º Embeddings å½¢ç‹€: {embeddings.shape}\")\n",
    "print(f\"\\nåµŒå…¥çŸ©é™£åƒæ•¸é‡: {vocab_size * embedding_dim:,} ({vocab_size:,} Ã— {embedding_dim})\")\n",
    "print(f\"\\nç¬¬ä¸€å€‹ token (ID=20) çš„åµŒå…¥å‘é‡ (å‰ 10 ç¶­):\")\n",
    "print(embeddings[0, :10])\n",
    "\n",
    "# é©—è­‰: ç›¸åŒ token çš„åµŒå…¥æ‡‰è©²ç›¸åŒ\n",
    "print(f\"\\nâœ… é©—è­‰: token[0] å’Œ token[4] åµŒå…¥æ˜¯å¦ç›¸åŒ? {np.allclose(embeddings[0], embeddings[4])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 åµŒå…¥å±¤çš„è¨“ç·´\n",
    "\n",
    "**é—œéµå•é¡Œ:** å¦‚ä½•å­¸ç¿’å¥½çš„åµŒå…¥?\n",
    "\n",
    "#### å…©ç¨®è¨“ç·´æ–¹å¼:\n",
    "\n",
    "1. **é è¨“ç·´ (Pre-trained)**\n",
    "   - åœ¨å¤§è¦æ¨¡èªæ–™ä¸Šé å…ˆè¨“ç·´ (Word2Vec, GloVe)\n",
    "   - é·ç§»åˆ°ä¸‹æ¸¸ä»»å‹™\n",
    "   - å„ªé»: åˆ©ç”¨æµ·é‡æ•¸æ“š, æ³›åŒ–èƒ½åŠ›å¼·\n",
    "\n",
    "2. **ç«¯åˆ°ç«¯è¨“ç·´ (End-to-End)**\n",
    "   - ä½œç‚ºæ¨¡å‹ä¸€éƒ¨åˆ†,éš¨ä»»å‹™ä¸€èµ·è¨“ç·´\n",
    "   - Transformer é€šå¸¸æ¡ç”¨æ­¤æ–¹å¼\n",
    "   - å„ªé»: é‡å°ç‰¹å®šä»»å‹™å„ªåŒ–\n",
    "\n",
    "---\n",
    "\n",
    "### 2.4 åµŒå…¥ç¶­åº¦çš„é¸æ“‡\n",
    "\n",
    "| æ¨¡å‹ | è©å½™é‡ | åµŒå…¥ç¶­åº¦ | åƒæ•¸é‡ |\n",
    "|------|--------|----------|--------|\n",
    "| Word2Vec | 100K | 300 | 30M |\n",
    "| BERT-base | 30K | 768 | 23M |\n",
    "| BERT-large | 30K | 1024 | 31M |\n",
    "| GPT-2 | 50K | 768 | 38M |\n",
    "| GPT-3 | 50K | 12288 | 614M |\n",
    "\n",
    "**æ¬Šè¡¡:**\n",
    "- ç¶­åº¦ â†‘ â†’ è¡¨é”èƒ½åŠ› â†‘, è¨ˆç®—æˆæœ¬ â†‘\n",
    "- ç¶­åº¦ â†“ â†’ è¨“ç·´é€Ÿåº¦ â†‘, å¯èƒ½æ¬ æ“¬åˆ\n",
    "\n",
    "**ç¶“é©—æ³•å‰‡:** 256-1024 ç¶­é©åˆå¤§å¤šæ•¸ä»»å‹™\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¦é©—: åµŒå…¥ç¶­åº¦å°åƒæ•¸é‡çš„å½±éŸ¿\n",
    "vocab_sizes = [10000, 30000, 50000, 100000]\n",
    "embedding_dims = [128, 256, 512, 768, 1024]\n",
    "\n",
    "# è¨ˆç®—åƒæ•¸é‡\n",
    "param_counts = np.array([[v * d for d in embedding_dims] for v in vocab_sizes])\n",
    "\n",
    "# è¦–è¦ºåŒ–\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "for i, vocab_size in enumerate(vocab_sizes):\n",
    "    ax.plot(embedding_dims, param_counts[i] / 1e6, 'o-', \n",
    "            label=f'Vocab: {vocab_size:,}', linewidth=2, markersize=8)\n",
    "\n",
    "ax.set_xlabel('Embedding Dimension', fontsize=12)\n",
    "ax.set_ylabel('Parameters (Millions)', fontsize=12)\n",
    "ax.set_title('åµŒå…¥å±¤åƒæ•¸é‡åˆ†æ', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ’¡ æ´å¯Ÿ:\")\n",
    "print(\"  - GPT-3 (50K vocab, 12288 dim) = 614M åƒæ•¸åƒ…åœ¨åµŒå…¥å±¤!\")\n",
    "print(\"  - åµŒå…¥å±¤åƒæ•¸é‡ = è©å½™é‡ Ã— åµŒå…¥ç¶­åº¦\")\n",
    "print(\"  - å¤§å‹æ¨¡å‹ä¸­,åµŒå…¥å±¤å¯å ç¸½åƒæ•¸é‡çš„ 5-20%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "## 3. Positional Embeddings æ·±å…¥\n",
    "\n",
    "### 3.1 ç‚ºä»€éº¼ Attention éœ€è¦ä½ç½®è¨Šæ¯?\n",
    "\n",
    "**å•é¡Œç¤ºç¯„:**\n",
    "\n",
    "```\n",
    "å¥å­ A: \"ç‹—å’¬äºº\"\n",
    "å¥å­ B: \"äººå’¬ç‹—\"  â† é †åºä¸åŒ,èªç¾©å®Œå…¨ç›¸å!\n",
    "```\n",
    "\n",
    "ä½† Self-Attention çš„è¨ˆç®—æ˜¯:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V\n",
    "$$\n",
    "\n",
    "é€™å€‹å…¬å¼å°è©çš„é †åº**ä¸æ•æ„Ÿ**!\n",
    "\n",
    "```python\n",
    "Attention([\"ç‹—\", \"å’¬\", \"äºº\"]) == Attention([\"äºº\", \"å’¬\", \"ç‹—\"])  # å®Œå…¨ç›¸åŒ!\n",
    "```\n",
    "\n",
    "**è§£æ±ºæ–¹æ¡ˆ:** åŠ å…¥ä½ç½®ç·¨ç¢¼\n",
    "\n",
    "---\n",
    "\n",
    "### 3.2 å…©ç¨®ä½ç½®ç·¨ç¢¼æ–¹å¼\n",
    "\n",
    "#### æ–¹æ³• 1: æ­£å¼¦ä½ç½®ç·¨ç¢¼ (Sinusoidal) - Transformer åŸè«–æ–‡\n",
    "\n",
    "$$\n",
    "\\text{PE}_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d}}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{PE}_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d}}\\right)\n",
    "$$\n",
    "\n",
    "**å„ªé»:**\n",
    "- âœ… ä¸éœ€è¨“ç·´ (å›ºå®šå‡½æ•¸)\n",
    "- âœ… å¯è™•ç†ä»»æ„é•·åº¦åºåˆ—\n",
    "- âœ… ç›¸å°ä½ç½®å¯è¨ˆç®—\n",
    "\n",
    "---\n",
    "\n",
    "#### æ–¹æ³• 2: å¯å­¸ç¿’ä½ç½®ç·¨ç¢¼ (Learned) - BERT, GPT\n",
    "\n",
    "```python\n",
    "position_embeddings = Embedding(max_seq_len, embedding_dim)\n",
    "```\n",
    "\n",
    "**å„ªé»:**\n",
    "- âœ… å¯é‡å°ä»»å‹™å„ªåŒ–\n",
    "- âœ… å¯¦ä½œç°¡å–®\n",
    "\n",
    "**ç¼ºé»:**\n",
    "- âŒ éœ€è¦è¨­å®šæœ€å¤§é•·åº¦ (max_seq_len)\n",
    "- âŒ ç„¡æ³•ç›´æ¥è™•ç†è¶…é•·åºåˆ—\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®Œæ•´å¯¦ä½œå…©ç¨®ä½ç½®ç·¨ç¢¼\n",
    "\n",
    "class SinusoidalPositionalEncoding:\n",
    "    \"\"\"æ­£å¼¦ä½ç½®ç·¨ç¢¼ (Transformer åŸè«–æ–‡)\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # é è¨ˆç®—ä½ç½®ç·¨ç¢¼\n",
    "        pe = np.zeros((max_len, d_model))\n",
    "        position = np.arange(0, max_len).reshape(-1, 1)\n",
    "        \n",
    "        # è¨ˆç®—é™¤æ•¸é …\n",
    "        div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
    "        \n",
    "        # Sin for even indices\n",
    "        pe[:, 0::2] = np.sin(position * div_term)\n",
    "        # Cos for odd indices\n",
    "        pe[:, 1::2] = np.cos(position * div_term)\n",
    "        \n",
    "        self.pe = pe\n",
    "    \n",
    "    def __call__(self, seq_len):\n",
    "        \"\"\"è¿”å›åºåˆ—é•·åº¦ç‚º seq_len çš„ä½ç½®ç·¨ç¢¼\"\"\"\n",
    "        return self.pe[:seq_len]\n",
    "\n",
    "\n",
    "class LearnedPositionalEncoding:\n",
    "    \"\"\"å¯å­¸ç¿’ä½ç½®ç·¨ç¢¼ (BERT/GPT)\"\"\"\n",
    "    \n",
    "    def __init__(self, max_len, d_model):\n",
    "        self.max_len = max_len\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # éš¨æ©Ÿåˆå§‹åŒ– (è¨“ç·´æ™‚æœƒæ›´æ–°)\n",
    "        self.pe = np.random.randn(max_len, d_model) * 0.02\n",
    "    \n",
    "    def __call__(self, seq_len):\n",
    "        \"\"\"è¿”å›åºåˆ—é•·åº¦ç‚º seq_len çš„ä½ç½®ç·¨ç¢¼\"\"\"\n",
    "        if seq_len > self.max_len:\n",
    "            raise ValueError(f\"åºåˆ—é•·åº¦ {seq_len} è¶…éæœ€å¤§é•·åº¦ {self.max_len}\")\n",
    "        return self.pe[:seq_len]\n",
    "\n",
    "\n",
    "# æ¸¬è©¦èˆ‡æ¯”è¼ƒ\n",
    "d_model = 512\n",
    "max_len = 100\n",
    "\n",
    "sin_pe = SinusoidalPositionalEncoding(d_model, max_len)\n",
    "learned_pe = LearnedPositionalEncoding(max_len, d_model)\n",
    "\n",
    "# ç²å–ä½ç½®ç·¨ç¢¼\n",
    "sin_encoding = sin_pe(50)\n",
    "learned_encoding = learned_pe(50)\n",
    "\n",
    "# è¦–è¦ºåŒ–æ¯”è¼ƒ\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# æ­£å¼¦ç·¨ç¢¼\n",
    "im1 = axes[0].imshow(sin_encoding.T, cmap='RdBu', aspect='auto', vmin=-1, vmax=1)\n",
    "axes[0].set_xlabel('Position', fontsize=12)\n",
    "axes[0].set_ylabel('Dimension', fontsize=12)\n",
    "axes[0].set_title('æ­£å¼¦ä½ç½®ç·¨ç¢¼ (Sinusoidal)\\nå›ºå®šã€å¯å¤–æ¨', fontsize=13, fontweight='bold')\n",
    "plt.colorbar(im1, ax=axes[0], fraction=0.046, pad=0.04)\n",
    "\n",
    "# å¯å­¸ç¿’ç·¨ç¢¼\n",
    "im2 = axes[1].imshow(learned_encoding.T, cmap='RdBu', aspect='auto', vmin=-1, vmax=1)\n",
    "axes[1].set_xlabel('Position', fontsize=12)\n",
    "axes[1].set_ylabel('Dimension', fontsize=12)\n",
    "axes[1].set_title('å¯å­¸ç¿’ä½ç½®ç·¨ç¢¼ (Learned)\\nå¯è¨“ç·´ã€å›ºå®šé•·åº¦', fontsize=13, fontweight='bold')\n",
    "plt.colorbar(im2, ax=axes[1], fraction=0.046, pad=0.04)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ“Š ç‰¹æ€§å°æ¯”:\")\n",
    "print(\"\\næ­£å¼¦ç·¨ç¢¼:\")\n",
    "print(\"  - è¦å¾‹çš„æ³¢å½¢æ¨¡å¼ (ä½ç¶­é«˜é », é«˜ç¶­ä½é »)\")\n",
    "print(\"  - å¯è™•ç†ä»»æ„é•·åº¦\")\n",
    "print(\"  - ä½¿ç”¨: Transformer åŸè«–æ–‡, T5\")\n",
    "print(\"\\nå¯å­¸ç¿’ç·¨ç¢¼:\")\n",
    "print(\"  - éš¨æ©Ÿåˆå§‹åŒ–,éœ€è¨“ç·´\")\n",
    "print(\"  - å—é™æ–¼ max_len\")\n",
    "print(\"  - ä½¿ç”¨: BERT, GPT, GPT-2, GPT-3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 ä½ç½®ç·¨ç¢¼çš„æ•¸å­¸ç›´è¦º\n",
    "\n",
    "#### ç‚ºä»€éº¼æ­£å¼¦å‡½æ•¸æœ‰æ•ˆ?\n",
    "\n",
    "**é—œéµæ€§è³ª:** ç›¸å°ä½ç½®å¯ä»¥ç·šæ€§è¡¨ç¤º\n",
    "\n",
    "$$\n",
    "\\text{PE}(pos + k) = f(\\text{PE}(pos), k)\n",
    "$$\n",
    "\n",
    "åˆ©ç”¨ä¸‰è§’æ†ç­‰å¼:\n",
    "\n",
    "$$\n",
    "\\sin(\\alpha + \\beta) = \\sin\\alpha\\cos\\beta + \\cos\\alpha\\sin\\beta\n",
    "$$\n",
    "\n",
    "æ¨¡å‹å¯ä»¥å­¸ç¿’ã€Œå‘å‰/å‘å¾Œ k å€‹ä½ç½®ã€çš„é—œä¿‚!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¦–è¦ºåŒ–: ä½ç½®ç·¨ç¢¼çš„ç›¸å°ä½ç½®ç‰¹æ€§\n",
    "d_model = 128\n",
    "pe = SinusoidalPositionalEncoding(d_model, max_len=200)\n",
    "encodings = pe(200)\n",
    "\n",
    "# è¨ˆç®—ä¸åŒä½ç½®å°ä¹‹é–“çš„ä½™å¼¦ç›¸ä¼¼åº¦\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "# é¸æ“‡å¹¾å€‹åƒè€ƒä½ç½®\n",
    "reference_positions = [0, 50, 100, 150]\n",
    "similarity_matrix = np.zeros((len(reference_positions), 200))\n",
    "\n",
    "for i, ref_pos in enumerate(reference_positions):\n",
    "    for target_pos in range(200):\n",
    "        similarity_matrix[i, target_pos] = cosine_similarity(\n",
    "            encodings[ref_pos], encodings[target_pos]\n",
    "        )\n",
    "\n",
    "# è¦–è¦ºåŒ–\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "for i, ref_pos in enumerate(reference_positions):\n",
    "    plt.plot(similarity_matrix[i], label=f'åƒè€ƒä½ç½® {ref_pos}', linewidth=2)\n",
    "\n",
    "plt.xlabel('Target Position', fontsize=12)\n",
    "plt.ylabel('Cosine Similarity', fontsize=12)\n",
    "plt.title('ä½ç½®ç·¨ç¢¼çš„ä½™å¼¦ç›¸ä¼¼åº¦åˆ†æ\\n(ç›¸ä¼¼åº¦éš¨è·é›¢éæ¸›)', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ” è§€å¯Ÿ:\")\n",
    "print(\"  - ç›¸é„°ä½ç½®ç›¸ä¼¼åº¦é«˜ (å³°å€¼åœ¨è‡ªå·±ä½ç½®)\")\n",
    "print(\"  - è·é›¢è¶Šé ,ç›¸ä¼¼åº¦è¶Šä½\")\n",
    "print(\"  - å‘ˆç¾é€±æœŸæ€§æ¨¡å¼ (æ­£å¼¦å‡½æ•¸ç‰¹æ€§)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "## 4. Word2Vec vs Transformer Embeddings\n",
    "\n",
    "### 4.1 è©åµŒå…¥æ¼”é€²å²\n",
    "\n",
    "```\n",
    "2013 â†’ Word2Vec (Mikolov et al.)\n",
    "       - CBOW, Skip-gram\n",
    "       - éœæ…‹åµŒå…¥ (æ¯å€‹è©åªæœ‰ä¸€å€‹å‘é‡)\n",
    "\n",
    "2014 â†’ GloVe (Pennington et al.)\n",
    "       - å…¨å±€è©-è©å…±ç¾çŸ©é™£\n",
    "       - éœæ…‹åµŒå…¥\n",
    "\n",
    "2017 â†’ ELMo (Peters et al.)\n",
    "       - é›™å‘ LSTM\n",
    "       - ä¸Šä¸‹æ–‡ç›¸é—œåµŒå…¥ (é–‹å§‹!)\n",
    "\n",
    "2018 â†’ BERT (Devlin et al.)\n",
    "       - Transformer Encoder\n",
    "       - å¼·å¤§çš„ä¸Šä¸‹æ–‡åµŒå…¥\n",
    "\n",
    "2019+ â†’ GPT-2/3, RoBERTa, T5, ...\n",
    "        - Transformer çµ±æ²» NLP\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4.2 éœæ…‹ vs ä¸Šä¸‹æ–‡åµŒå…¥\n",
    "\n",
    "#### å•é¡Œ: ä¸€è©å¤šç¾©\n",
    "\n",
    "```python\n",
    "å¥å­ 1: \"æˆ‘å»éŠ€è¡Œå­˜éŒ¢\"      â† \"éŠ€è¡Œ\" = é‡‘èæ©Ÿæ§‹\n",
    "å¥å­ 2: \"æˆ‘åœ¨æ²³éŠ€è¡Œé‡£é­š\"    â† \"éŠ€è¡Œ\" = æ²³å²¸\n",
    "```\n",
    "\n",
    "**Word2Vec/GloVe (éœæ…‹):**\n",
    "```python\n",
    "\"éŠ€è¡Œ\" â†’ [0.23, -0.45, 0.67, ...]  # æ°¸é ç›¸åŒ!\n",
    "```\n",
    "\n",
    "**BERT/GPT (ä¸Šä¸‹æ–‡):**\n",
    "```python\n",
    "\"éŠ€è¡Œ\" in \"æˆ‘å»éŠ€è¡Œå­˜éŒ¢\"   â†’ [0.78, 0.12, -0.23, ...]\n",
    "\"éŠ€è¡Œ\" in \"æˆ‘åœ¨æ²³éŠ€è¡Œé‡£é­š\" â†’ [-0.34, 0.89, 0.45, ...]  # ä¸åŒ!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4.3 è©³ç´°å°æ¯”\n",
    "\n",
    "| ç‰¹æ€§ | Word2Vec/GloVe | Transformer (BERT/GPT) |\n",
    "|------|----------------|------------------------|\n",
    "| **åµŒå…¥é¡å‹** | éœæ…‹ | ä¸Šä¸‹æ–‡ç›¸é—œ |\n",
    "| **è¨“ç·´ç›®æ¨™** | é æ¸¬é„°è¿‘è© | Masked LM / CLM |\n",
    "| **åƒæ•¸é‡** | ~100M | 100M - 175B |\n",
    "| **è¨“ç·´æ•¸æ“š** | Wikipedia | Internet-scale |\n",
    "| **ä¸€è©å¤šç¾©** | âŒ ç„¡æ³•è™•ç† | âœ… å®Œç¾è™•ç† |\n",
    "| **è¨“ç·´æˆæœ¬** | ä½ (CPU å¯è¨“ç·´) | é«˜ (éœ€å¤§é‡ GPU) |\n",
    "| **é©ç”¨å ´æ™¯** | ç°¡å–®åˆ†é¡ã€èšé¡ | å¹¾ä¹æ‰€æœ‰ NLP ä»»å‹™ |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¨¡æ“¬: éœæ…‹ vs ä¸Šä¸‹æ–‡åµŒå…¥\n",
    "\n",
    "# éœæ…‹åµŒå…¥ (Word2Vec é¢¨æ ¼)\n",
    "word2vec_bank = np.array([0.5, 0.3, -0.2, 0.8])  # \"éŠ€è¡Œ\" æ°¸é ç›¸åŒ\n",
    "\n",
    "# ä¸Šä¸‹æ–‡åµŒå…¥ (BERT é¢¨æ ¼ - ç°¡åŒ–æ¨¡æ“¬)\n",
    "def contextual_embedding(word, context):\n",
    "    \"\"\"\n",
    "    æ¨¡æ“¬ä¸Šä¸‹æ–‡ç›¸é—œåµŒå…¥\n",
    "    (å¯¦éš› BERT æœƒé€šé Transformer è¨ˆç®—)\n",
    "    \"\"\"\n",
    "    base = np.array([0.5, 0.3, -0.2, 0.8])\n",
    "    \n",
    "    if \"å­˜éŒ¢\" in context or \"é‡‘è\" in context:\n",
    "        # é‡‘èèªå¢ƒ\n",
    "        context_shift = np.array([0.3, -0.1, 0.2, 0.0])\n",
    "    elif \"æ²³\" in context or \"é‡£é­š\" in context:\n",
    "        # è‡ªç„¶èªå¢ƒ\n",
    "        context_shift = np.array([-0.3, 0.4, 0.1, -0.3])\n",
    "    else:\n",
    "        context_shift = np.zeros(4)\n",
    "    \n",
    "    return base + context_shift\n",
    "\n",
    "# å…©å€‹ä¸åŒèªå¢ƒ\n",
    "context1 = \"æˆ‘å»éŠ€è¡Œå­˜éŒ¢\"\n",
    "context2 = \"æˆ‘åœ¨æ²³éŠ€è¡Œé‡£é­š\"\n",
    "\n",
    "bert_bank_1 = contextual_embedding(\"éŠ€è¡Œ\", context1)\n",
    "bert_bank_2 = contextual_embedding(\"éŠ€è¡Œ\", context2)\n",
    "\n",
    "# è¦–è¦ºåŒ–\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# éœæ…‹åµŒå…¥\n",
    "axes[0].bar(['ç¶­åº¦1', 'ç¶­åº¦2', 'ç¶­åº¦3', 'ç¶­åº¦4'], word2vec_bank, color='steelblue', alpha=0.7)\n",
    "axes[0].axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "axes[0].set_ylim(-1, 1)\n",
    "axes[0].set_title('Word2Vec: éœæ…‹åµŒå…¥\\n\"éŠ€è¡Œ\" æ°¸é ç›¸åŒ', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Value', fontsize=11)\n",
    "\n",
    "# ä¸Šä¸‹æ–‡åµŒå…¥\n",
    "x = np.arange(4)\n",
    "width = 0.35\n",
    "axes[1].bar(x - width/2, bert_bank_1, width, label='\"æˆ‘å»éŠ€è¡Œå­˜éŒ¢\"', alpha=0.7)\n",
    "axes[1].bar(x + width/2, bert_bank_2, width, label='\"æˆ‘åœ¨æ²³éŠ€è¡Œé‡£é­š\"', alpha=0.7)\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(['ç¶­åº¦1', 'ç¶­åº¦2', 'ç¶­åº¦3', 'ç¶­åº¦4'])\n",
    "axes[1].axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "axes[1].set_ylim(-1, 1)\n",
    "axes[1].set_title('BERT: ä¸Šä¸‹æ–‡åµŒå…¥\\n\"éŠ€è¡Œ\" éš¨èªå¢ƒæ”¹è®Š', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Value', fontsize=11)\n",
    "axes[1].legend(fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# è¨ˆç®—ç›¸ä¼¼åº¦\n",
    "cos_sim = cosine_similarity(bert_bank_1, bert_bank_2)\n",
    "print(f\"\\nğŸ“ å…©ç¨®èªå¢ƒä¸‹ 'éŠ€è¡Œ' çš„ä½™å¼¦ç›¸ä¼¼åº¦: {cos_sim:.3f}\")\n",
    "print(\"   (< 1.0 è¡¨ç¤ºä¸å®Œå…¨ç›¸åŒ,æˆåŠŸæ•æ‰èªå¢ƒå·®ç•°!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "## 5. å¯¦æˆ°: è¨“ç·´èˆ‡è©•ä¼°è©å‘é‡\n",
    "\n",
    "### 5.1 ä½¿ç”¨ Gensim è¨“ç·´ Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨“ç·´ç°¡å–®çš„ Word2Vec æ¨¡å‹\n",
    "try:\n",
    "    from gensim.models import Word2Vec\n",
    "    GENSIM_AVAILABLE = True\n",
    "except ImportError:\n",
    "    GENSIM_AVAILABLE = False\n",
    "    print(\"âš ï¸  Gensim æœªå®‰è£,è·³éæ­¤éƒ¨åˆ†\")\n",
    "    print(\"   å®‰è£: pip install gensim\")\n",
    "\n",
    "if GENSIM_AVAILABLE:\n",
    "    # æº–å‚™è¨“ç·´èªæ–™ (ç°¡åŒ–ç¤ºä¾‹)\n",
    "    sentences = [\n",
    "        ['åœ‹ç‹', 'çµ±æ²»', 'åœ‹å®¶'],\n",
    "        ['å¥³ç‹', 'çµ±æ²»', 'ç‹åœ‹'],\n",
    "        ['ç”·äºº', 'æ˜¯', 'äººé¡'],\n",
    "        ['å¥³äºº', 'æ˜¯', 'äººé¡'],\n",
    "        ['åœ‹ç‹', 'æ˜¯', 'ç”·äºº'],\n",
    "        ['å¥³ç‹', 'æ˜¯', 'å¥³äºº'],\n",
    "        ['è²“', 'æŠ“', 'è€é¼ '],\n",
    "        ['ç‹—', 'è¿½', 'è²“'],\n",
    "        ['é³¥', 'åœ¨', 'å¤©ç©º', 'é£›'],\n",
    "        ['é­š', 'åœ¨', 'æ°´', 'è£¡', 'æ¸¸'],\n",
    "    ] * 100  # é‡è¤‡ä»¥å¢åŠ è¨“ç·´æ•¸æ“š\n",
    "    \n",
    "    # è¨“ç·´ Word2Vec\n",
    "    print(\"ğŸš€ è¨“ç·´ Word2Vec æ¨¡å‹...\")\n",
    "    model = Word2Vec(\n",
    "        sentences=sentences,\n",
    "        vector_size=50,     # åµŒå…¥ç¶­åº¦\n",
    "        window=3,           # ä¸Šä¸‹æ–‡çª—å£\n",
    "        min_count=1,        # æœ€å°è©é »\n",
    "        workers=4,          # ä¸¦è¡Œç·šç¨‹\n",
    "        sg=1,               # Skip-gram (1) or CBOW (0)\n",
    "        epochs=50\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… è¨“ç·´å®Œæˆ!\")\n",
    "    print(f\"\\nè©å½™é‡: {len(model.wv)}\")\n",
    "    print(f\"åµŒå…¥ç¶­åº¦: {model.wv.vector_size}\")\n",
    "    \n",
    "    # æ¸¬è©¦è©å‘é‡\n",
    "    print(\"\\nğŸ” è©å‘é‡ç›¸ä¼¼åº¦æ¸¬è©¦:\")\n",
    "    test_pairs = [\n",
    "        ('åœ‹ç‹', 'å¥³ç‹'),\n",
    "        ('ç”·äºº', 'å¥³äºº'),\n",
    "        ('è²“', 'ç‹—'),\n",
    "        ('é³¥', 'é­š'),\n",
    "        ('åœ‹ç‹', 'è²“'),  # ä¸ç›¸é—œ\n",
    "    ]\n",
    "    \n",
    "    for word1, word2 in test_pairs:\n",
    "        try:\n",
    "            sim = model.wv.similarity(word1, word2)\n",
    "            print(f\"  {word1:4s} â†” {word2:4s}: {sim:.3f}\")\n",
    "        except KeyError:\n",
    "            print(f\"  {word1} æˆ– {word2} ä¸åœ¨è©å½™è¡¨ä¸­\")\n",
    "    \n",
    "    # ç¶“å…¸é¡æ¯”ä»»å‹™: åœ‹ç‹ - ç”·äºº + å¥³äºº â‰ˆ å¥³ç‹\n",
    "    print(\"\\nğŸ‘‘ ç¶“å…¸é¡æ¯”: åœ‹ç‹ - ç”·äºº + å¥³äºº = ?\")\n",
    "    try:\n",
    "        result = model.wv.most_similar(\n",
    "            positive=['å¥³äºº', 'åœ‹ç‹'], \n",
    "            negative=['ç”·äºº'], \n",
    "            topn=3\n",
    "        )\n",
    "        for word, score in result:\n",
    "            print(f\"  {word}: {score:.3f}\")\n",
    "    except:\n",
    "        print(\"  èªæ–™å¤ªå°,ç„¡æ³•å®Œæˆé¡æ¯”\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 è©å‘é‡å“è³ªè©•ä¼°\n",
    "\n",
    "#### ä¸‰ç¨®è©•ä¼°æ–¹å¼:\n",
    "\n",
    "1. **å…§åœ¨è©•ä¼° (Intrinsic)**\n",
    "   - è©ç›¸ä¼¼åº¦ä»»å‹™\n",
    "   - è©é¡æ¯”ä»»å‹™\n",
    "   - å¿«é€Ÿ,ä½†èˆ‡ä¸‹æ¸¸ä»»å‹™ç›¸é—œæ€§ä½\n",
    "\n",
    "2. **å¤–åœ¨è©•ä¼° (Extrinsic)**\n",
    "   - åœ¨å¯¦éš›ä»»å‹™ä¸Šæ¸¬è©¦ (åˆ†é¡ã€NER ç­‰)\n",
    "   - æ…¢,ä½†æœ€æº–ç¢º\n",
    "\n",
    "3. **è¦–è¦ºåŒ–è©•ä¼°**\n",
    "   - t-SNE / PCA é™ç¶­å¯è¦–åŒ–\n",
    "   - ç›´è§€,ä½†ä¸»è§€\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¦–è¦ºåŒ–è©å‘é‡ (ä½¿ç”¨ t-SNE é™ç¶­)\n",
    "if GENSIM_AVAILABLE:\n",
    "    # ç²å–æ‰€æœ‰è©å‘é‡\n",
    "    words = list(model.wv.index_to_key)\n",
    "    vectors = np.array([model.wv[word] for word in words])\n",
    "    \n",
    "    # t-SNE é™ç¶­åˆ° 2D\n",
    "    print(\"ğŸ¨ ä½¿ç”¨ t-SNE é™ç¶­åˆ° 2D...\")\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=min(5, len(words)-1))\n",
    "    vectors_2d = tsne.fit_transform(vectors)\n",
    "    \n",
    "    # è¦–è¦ºåŒ–\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # ç¹ªè£½æ‰€æœ‰è©\n",
    "    plt.scatter(vectors_2d[:, 0], vectors_2d[:, 1], alpha=0.5, s=100)\n",
    "    \n",
    "    # æ¨™è¨»è©\n",
    "    for i, word in enumerate(words):\n",
    "        plt.annotate(word, \n",
    "                    xy=(vectors_2d[i, 0], vectors_2d[i, 1]),\n",
    "                    xytext=(5, 2),\n",
    "                    textcoords='offset points',\n",
    "                    fontsize=11,\n",
    "                    fontweight='bold')\n",
    "    \n",
    "    plt.xlabel('t-SNE Dimension 1', fontsize=12)\n",
    "    plt.ylabel('t-SNE Dimension 2', fontsize=12)\n",
    "    plt.title('Word2Vec è©å‘é‡ t-SNE è¦–è¦ºåŒ–\\n(ç›¸ä¼¼è©æ‡‰è©²èšé›†åœ¨ä¸€èµ·)', \n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nğŸ’¡ è§€å¯Ÿ:\")\n",
    "    print(\"  - ç›¸ä¼¼è© (å¦‚ 'åœ‹ç‹' å’Œ 'å¥³ç‹') æ‡‰è©²é è¿‘\")\n",
    "    print(\"  - ä¸åŒèªç¾©é¡åˆ¥ (å¦‚å‹•ç‰© vs äººç‰©) æ‡‰è©²åˆ†é›¢\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a>\n",
    "## 6. é€²éšæŠ€å·§èˆ‡å„ªåŒ–\n",
    "\n",
    "### 6.1 åµŒå…¥å±¤çš„åˆå§‹åŒ–ç­–ç•¥\n",
    "\n",
    "#### å¸¸è¦‹åˆå§‹åŒ–æ–¹æ³•:\n",
    "\n",
    "1. **éš¨æ©Ÿåˆå§‹åŒ–**\n",
    "   ```python\n",
    "   embeddings = np.random.randn(vocab_size, d_model) * 0.01\n",
    "   ```\n",
    "   - ç°¡å–®,ä½†éœ€è¦æ›´å¤šè¨“ç·´æ™‚é–“\n",
    "\n",
    "2. **Xavier/Glorot åˆå§‹åŒ–**\n",
    "   ```python\n",
    "   limit = np.sqrt(6 / (vocab_size + d_model))\n",
    "   embeddings = np.random.uniform(-limit, limit, (vocab_size, d_model))\n",
    "   ```\n",
    "   - ä¿æŒæ¢¯åº¦ç©©å®š\n",
    "\n",
    "3. **é è¨“ç·´åµŒå…¥**\n",
    "   ```python\n",
    "   embeddings = load_pretrained_embeddings(\"glove.6B.300d.txt\")\n",
    "   ```\n",
    "   - é·ç§»å­¸ç¿’,å†·å•Ÿå‹•å¿«\n",
    "\n",
    "---\n",
    "\n",
    "### 6.2 åµŒå…¥å±¤çš„æ­£å‰‡åŒ–\n",
    "\n",
    "#### Dropout on Embeddings\n",
    "\n",
    "```python\n",
    "embeddings = embedding_layer(token_ids)\n",
    "embeddings = dropout(embeddings, p=0.1)  # éš¨æ©Ÿä¸Ÿæ£„ 10%\n",
    "```\n",
    "\n",
    "**ä½œç”¨:** é˜²æ­¢éæ“¬åˆ,æå‡æ³›åŒ–èƒ½åŠ›\n",
    "\n",
    "---\n",
    "\n",
    "### 6.3 å­è©åµŒå…¥ (Subword Embeddings)\n",
    "\n",
    "**å•é¡Œ:** ç½•è¦‹è©ã€æ–°è©ã€æ‹¼å¯«éŒ¯èª¤?\n",
    "\n",
    "**è§£æ±ºæ–¹æ¡ˆ:** ä½¿ç”¨å­è©å–®å…ƒ (Subword Units)\n",
    "\n",
    "#### ä¸‰ç¨®ä¸»æµæ–¹æ³•:\n",
    "\n",
    "1. **Byte Pair Encoding (BPE)** - GPTç³»åˆ—\n",
    "   ```\n",
    "   \"tokenization\" â†’ [\"token\", \"ization\"]\n",
    "   ```\n",
    "\n",
    "2. **WordPiece** - BERT\n",
    "   ```\n",
    "   \"tokenization\" â†’ [\"token\", \"##ization\"]\n",
    "   ```\n",
    "\n",
    "3. **SentencePiece** - T5, LLaMA\n",
    "   ```\n",
    "   \"tokenization\" â†’ [\"â–token\", \"ization\"]\n",
    "   ```\n",
    "\n",
    "**å„ªé»:**\n",
    "- âœ… è™•ç†æœªç™»éŒ„è© (OOV)\n",
    "- âœ… æ¸›å°è©å½™è¡¨å¤§å°\n",
    "- âœ… å¤šèªè¨€å‹å¥½\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç¤ºç¯„: ç°¡å–®çš„ BPE åˆ†è©\n",
    "try:\n",
    "    from transformers import GPT2Tokenizer\n",
    "    TRANSFORMERS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TRANSFORMERS_AVAILABLE = False\n",
    "    print(\"âš ï¸  Transformers æœªå®‰è£,è·³éæ­¤ç¤ºç¯„\")\n",
    "    print(\"   å®‰è£: pip install transformers\")\n",
    "\n",
    "if TRANSFORMERS_AVAILABLE:\n",
    "    # è¼‰å…¥ GPT-2 Tokenizer (ä½¿ç”¨ BPE)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    \n",
    "    # æ¸¬è©¦å¥å­\n",
    "    sentences = [\n",
    "        \"tokenization\",\n",
    "        \"unbelievable\",\n",
    "        \"Transformer\",\n",
    "        \"COVID-19\",  # æ–°è©\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nğŸ”¤ BPE å­è©åˆ†è©ç¤ºç¯„ (GPT-2):\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for sent in sentences:\n",
    "        tokens = tokenizer.tokenize(sent)\n",
    "        token_ids = tokenizer.encode(sent)\n",
    "        print(f\"\\nåŸæ–‡: {sent}\")\n",
    "        print(f\"  â”œâ”€ Tokens: {tokens}\")\n",
    "        print(f\"  â””â”€ IDs:    {token_ids}\")\n",
    "    \n",
    "    print(\"\\nğŸ’¡ è§€å¯Ÿ:\")\n",
    "    print(\"  - å¸¸è¦‹è©ä¿æŒå®Œæ•´ (å¦‚ 'Transformer')\")\n",
    "    print(\"  - ç½•è¦‹è©åˆ†è§£ç‚ºå­è© (å¦‚ 'token' + 'ization')\")\n",
    "    print(\"  - æ–°è©ä¹Ÿèƒ½è™•ç† (å¦‚ 'COVID-19')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“š æœ¬èª²ç¸½çµ\n",
    "\n",
    "### æ ¸å¿ƒè¦é»å›é¡§:\n",
    "\n",
    "1. **è©åµŒå…¥åŸºç¤:**\n",
    "   - One-Hot â†’ è©åµŒå…¥çš„æ¼”é€²\n",
    "   - ä½ç¶­ç¨ å¯†å‘é‡è¡¨ç¤ºèªç¾©\n",
    "   - åµŒå…¥å±¤ = å¯è¨“ç·´çš„æŸ¥æ‰¾è¡¨\n",
    "\n",
    "2. **Transformer é›™é‡åµŒå…¥:**\n",
    "   - Token Embeddings: è¡¨ç¤ºè©çš„èªç¾©\n",
    "   - Positional Embeddings: è¡¨ç¤ºè©çš„ä½ç½®\n",
    "   - è¼¸å…¥ = Token Emb + Position Emb\n",
    "\n",
    "3. **ä½ç½®ç·¨ç¢¼å…©ç¨®æ–¹å¼:**\n",
    "   - æ­£å¼¦ç·¨ç¢¼: å›ºå®šã€å¯å¤–æ¨ (Transformer åŸè«–æ–‡)\n",
    "   - å¯å­¸ç¿’ç·¨ç¢¼: å¯è¨“ç·´ã€å›ºå®šé•·åº¦ (BERT, GPT)\n",
    "\n",
    "4. **éœæ…‹ vs ä¸Šä¸‹æ–‡åµŒå…¥:**\n",
    "   - Word2Vec/GloVe: éœæ…‹,ç„¡æ³•è™•ç†ä¸€è©å¤šç¾©\n",
    "   - BERT/GPT: ä¸Šä¸‹æ–‡ç›¸é—œ,æ•æ‰èªå¢ƒ\n",
    "\n",
    "5. **é€²éšæŠ€å·§:**\n",
    "   - åˆå§‹åŒ–ç­–ç•¥ (éš¨æ©Ÿã€Xavierã€é è¨“ç·´)\n",
    "   - æ­£å‰‡åŒ– (Dropout)\n",
    "   - å­è©åµŒå…¥ (BPE, WordPiece, SentencePiece)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ ä¸‹ç¯€é å‘Š\n",
    "\n",
    "**CH07-03: æ³¨æ„åŠ›æ©Ÿåˆ¶ (Attention Mechanism)**\n",
    "\n",
    "æˆ‘å€‘å°‡æ·±å…¥æ¢è¨:\n",
    "- Self-Attention æ•¸å­¸æ¨å°\n",
    "- Multi-Head Attention å¯¦ä½œ\n",
    "- Scaled Dot-Product Attention\n",
    "- Attention å¯è¦–åŒ–èˆ‡è§£é‡‹æ€§\n",
    "- å¯¦æˆ°: å¾é›¶å¯¦ä½œå®Œæ•´ Attention å±¤\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“– å»¶ä¼¸é–±è®€\n",
    "\n",
    "1. **è«–æ–‡:**\n",
    "   - [Efficient Estimation of Word Representations](https://arxiv.org/abs/1301.3781) (Word2Vec)\n",
    "   - [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf)\n",
    "   - [Deep Contextualized Word Representations](https://arxiv.org/abs/1802.05365) (ELMo)\n",
    "\n",
    "2. **æ•™å­¸è³‡æº:**\n",
    "   - [Word Embeddings Explained](https://www.tensorflow.org/text/guide/word_embeddings)\n",
    "   - [The Illustrated Word2Vec](http://jalammar.github.io/illustrated-word2vec/)\n",
    "\n",
    "3. **å¯¦ä½œæ•™å­¸:**\n",
    "   - [Gensim Word2Vec Tutorial](https://radimrehurek.com/gensim/models/word2vec.html)\n",
    "   - [Hugging Face Tokenizers](https://huggingface.co/docs/tokenizers/index)\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ™‹ èª²å¾Œç·´ç¿’\n",
    "\n",
    "1. ä½¿ç”¨è‡ªå·±çš„èªæ–™è¨“ç·´ Word2Vec æ¨¡å‹\n",
    "2. æ¯”è¼ƒä¸åŒåˆå§‹åŒ–ç­–ç•¥å°è¨“ç·´çš„å½±éŸ¿\n",
    "3. å¯¦ä½œä¸€å€‹ç°¡å–®çš„ BPE åˆ†è©å™¨\n",
    "\n",
    "---\n",
    "\n",
    "**èª²ç¨‹è³‡è¨Š:**\n",
    "- **ä½œè€…:** iSpan NLP Team\n",
    "- **ç‰ˆæœ¬:** v1.0\n",
    "- **æœ€å¾Œæ›´æ–°:** 2025-10-17\n",
    "- **æˆæ¬Š:** MIT License (åƒ…ä¾›æ•™å­¸ä½¿ç”¨)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
