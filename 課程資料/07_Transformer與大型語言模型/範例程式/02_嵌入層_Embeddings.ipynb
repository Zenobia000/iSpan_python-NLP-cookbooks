{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH07-02: 嵌入層 (Embeddings)\n",
    "\n",
    "**課程目標:**\n",
    "- 理解詞嵌入 (Word Embeddings) 的核心概念\n",
    "- 掌握 Token Embeddings 與 Position Embeddings 的實作\n",
    "- 比較 Word2Vec, GloVe 與 Transformer Embeddings 的差異\n",
    "- 實戰訓練與評估詞向量品質\n",
    "\n",
    "**學習時間:** 約 60 分鐘\n",
    "\n",
    "**前置知識:**\n",
    "- 詞向量基礎概念 (CH05-02)\n",
    "- Transformer 架構概覽 (CH07-01)\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 目錄\n",
    "\n",
    "1. [為什麼需要詞嵌入?](#1)\n",
    "2. [Token Embeddings 實作](#2)\n",
    "3. [Positional Embeddings 深入](#3)\n",
    "4. [Word2Vec vs Transformer Embeddings](#4)\n",
    "5. [實戰: 訓練與評估詞向量](#5)\n",
    "6. [進階技巧與優化](#6)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 環境設定與套件導入\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 設定中文顯示\n",
    "plt.rcParams['font.sans-serif'] = ['Microsoft JhengHei', 'SimHei', 'Arial Unicode MS']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 設定隨機種子\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"✅ 環境設定完成\")\n",
    "print(f\"NumPy 版本: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "## 1. 為什麼需要詞嵌入?\n",
    "\n",
    "### 1.1 從 One-Hot 到詞嵌入的演進\n",
    "\n",
    "#### 問題: 如何表示詞?\n",
    "\n",
    "**方法 1: One-Hot Encoding (獨熱編碼)**\n",
    "\n",
    "```python\n",
    "詞彙表: [\"貓\", \"狗\", \"鳥\"]\n",
    "\n",
    "\"貓\" = [1, 0, 0]\n",
    "\"狗\" = [0, 1, 0]\n",
    "\"鳥\" = [0, 0, 1]\n",
    "```\n",
    "\n",
    "**三大問題:**\n",
    "1. ❌ **維度災難**: 詞彙 10 萬 → 10 萬維向量!\n",
    "2. ❌ **稀疏性**: 99.999% 都是 0\n",
    "3. ❌ **無語義**: \"貓\" 和 \"狗\" 同樣遙遠 (余弦相似度 = 0)\n",
    "\n",
    "---\n",
    "\n",
    "**方法 2: 詞嵌入 (Word Embeddings)**\n",
    "\n",
    "```python\n",
    "\"貓\" = [0.8, -0.3, 0.5, ..., 0.2]  # 256 維稠密向量\n",
    "\"狗\" = [0.7, -0.2, 0.6, ..., 0.3]  # 相似!\n",
    "\"鳥\" = [0.3,  0.8, 0.1, ..., -0.5] # 不同\n",
    "```\n",
    "\n",
    "**三大優勢:**\n",
    "1. ✅ **低維稠密**: 通常 256-1024 維\n",
    "2. ✅ **捕捉語義**: 相似詞向量接近\n",
    "3. ✅ **可運算**: 支援向量運算 (\"國王\" - \"男人\" + \"女人\" ≈ \"女王\")\n",
    "\n",
    "---\n",
    "\n",
    "### 1.2 Transformer 中的雙重嵌入\n",
    "\n",
    "Transformer 使用**兩種嵌入**的組合:\n",
    "\n",
    "$$\n",
    "\\text{Input} = \\text{Token Embedding} + \\text{Positional Embedding}\n",
    "$$\n",
    "\n",
    "1. **Token Embedding**: 表示詞的**語義**\n",
    "2. **Positional Embedding**: 表示詞的**位置**\n",
    "\n",
    "**為什麼需要位置嵌入?**\n",
    "\n",
    "- Self-Attention 是**無序的** (permutation-invariant)\n",
    "- 必須顯式告知模型詞的順序\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示範: One-Hot vs 詞嵌入\n",
    "vocab = [\"貓\", \"狗\", \"鳥\", \"魚\", \"兔子\"]\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 3  # 簡化為 3 維以便視覺化\n",
    "\n",
    "# One-Hot 編碼\n",
    "one_hot = np.eye(vocab_size)\n",
    "\n",
    "# 模擬詞嵌入 (實際應透過訓練獲得)\n",
    "# 設計: 貓、狗接近; 鳥、魚接近; 兔子介於中間\n",
    "embeddings = np.array([\n",
    "    [0.8, 0.2, 0.1],   # 貓\n",
    "    [0.7, 0.3, 0.15],  # 狗 (與貓相似)\n",
    "    [0.1, 0.8, 0.7],   # 鳥\n",
    "    [0.15, 0.75, 0.8], # 魚 (與鳥相似)\n",
    "    [0.5, 0.5, 0.4],   # 兔子 (中間)\n",
    "])\n",
    "\n",
    "# 視覺化\n",
    "fig = plt.figure(figsize=(16, 6))\n",
    "\n",
    "# One-Hot 編碼\n",
    "ax1 = fig.add_subplot(121)\n",
    "sns.heatmap(one_hot, annot=True, fmt='.0f', cmap='Greys', \n",
    "            xticklabels=[f'維度{i+1}' for i in range(vocab_size)],\n",
    "            yticklabels=vocab, cbar_kws={'label': 'Value'})\n",
    "ax1.set_title('One-Hot 編碼 (稀疏, 無語義)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 詞嵌入\n",
    "ax2 = fig.add_subplot(122, projection='3d')\n",
    "colors = ['red', 'red', 'blue', 'blue', 'green']\n",
    "for i, word in enumerate(vocab):\n",
    "    ax2.scatter(embeddings[i, 0], embeddings[i, 1], embeddings[i, 2], \n",
    "                c=colors[i], s=200, alpha=0.6)\n",
    "    ax2.text(embeddings[i, 0], embeddings[i, 1], embeddings[i, 2], \n",
    "             word, fontsize=12, fontweight='bold')\n",
    "ax2.set_xlabel('Dimension 1', fontsize=11)\n",
    "ax2.set_ylabel('Dimension 2', fontsize=11)\n",
    "ax2.set_zlabel('Dimension 3', fontsize=11)\n",
    "ax2.set_title('詞嵌入 (稠密, 有語義)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n🔍 觀察:\")\n",
    "print(\"  - One-Hot: 每個詞都是正交向量 (無相似度)\")\n",
    "print(\"  - 詞嵌入: 相似詞 (貓&狗, 鳥&魚) 在空間中接近\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "## 2. Token Embeddings 實作\n",
    "\n",
    "### 2.1 嵌入層 (Embedding Layer) 原理\n",
    "\n",
    "**嵌入層本質:** 一個**查找表** (Lookup Table)\n",
    "\n",
    "```python\n",
    "Embedding Matrix: (vocab_size, embedding_dim)\n",
    "\n",
    "              維度 1   維度 2   ...  維度 d\n",
    "詞 0 (\"the\")   0.23    -0.45   ...   0.12\n",
    "詞 1 (\"cat\")   0.78     0.12   ...  -0.34\n",
    "詞 2 (\"sat\")  -0.23     0.89   ...   0.56\n",
    "...\n",
    "詞 N          ...\n",
    "```\n",
    "\n",
    "**查找過程:**\n",
    "\n",
    "```python\n",
    "輸入 token ID: 1 (\"cat\")\n",
    "    ↓\n",
    "查找 Embedding Matrix 第 1 行\n",
    "    ↓\n",
    "輸出: [0.78, 0.12, ..., -0.34]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2.2 PyTorch 風格實作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 從零實作 Embedding Layer\n",
    "class SimpleEmbedding:\n",
    "    \"\"\"\n",
    "    簡化版嵌入層實作\n",
    "    \n",
    "    Args:\n",
    "        vocab_size: 詞彙表大小\n",
    "        embedding_dim: 嵌入向量維度\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # 初始化嵌入矩陣 (隨機初始化, 訓練時會更新)\n",
    "        self.weight = np.random.randn(vocab_size, embedding_dim) * 0.1\n",
    "        \n",
    "    def forward(self, token_ids):\n",
    "        \"\"\"\n",
    "        前向傳播: 查表獲取嵌入向量\n",
    "        \n",
    "        Args:\n",
    "            token_ids: shape (batch_size, seq_len) 或 (seq_len,)\n",
    "            \n",
    "        Returns:\n",
    "            embeddings: shape (batch_size, seq_len, embedding_dim) 或 (seq_len, embedding_dim)\n",
    "        \"\"\"\n",
    "        return self.weight[token_ids]\n",
    "    \n",
    "    def __call__(self, token_ids):\n",
    "        return self.forward(token_ids)\n",
    "\n",
    "\n",
    "# 測試\n",
    "vocab_size = 10000  # 詞彙表大小\n",
    "embedding_dim = 256  # BERT-base 使用 768, GPT-2 使用 768, GPT-3 使用 12288\n",
    "\n",
    "embedding_layer = SimpleEmbedding(vocab_size, embedding_dim)\n",
    "\n",
    "# 模擬輸入: 一個句子的 token IDs\n",
    "# 例如: \"The cat sat on the mat\" → [20, 45, 123, 56, 20, 89]\n",
    "token_ids = np.array([20, 45, 123, 56, 20, 89])\n",
    "\n",
    "# 獲取嵌入\n",
    "embeddings = embedding_layer(token_ids)\n",
    "\n",
    "print(f\"輸入 Token IDs 形狀: {token_ids.shape}\")\n",
    "print(f\"輸出 Embeddings 形狀: {embeddings.shape}\")\n",
    "print(f\"\\n嵌入矩陣參數量: {vocab_size * embedding_dim:,} ({vocab_size:,} × {embedding_dim})\")\n",
    "print(f\"\\n第一個 token (ID=20) 的嵌入向量 (前 10 維):\")\n",
    "print(embeddings[0, :10])\n",
    "\n",
    "# 驗證: 相同 token 的嵌入應該相同\n",
    "print(f\"\\n✅ 驗證: token[0] 和 token[4] 嵌入是否相同? {np.allclose(embeddings[0], embeddings[4])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 嵌入層的訓練\n",
    "\n",
    "**關鍵問題:** 如何學習好的嵌入?\n",
    "\n",
    "#### 兩種訓練方式:\n",
    "\n",
    "1. **預訓練 (Pre-trained)**\n",
    "   - 在大規模語料上預先訓練 (Word2Vec, GloVe)\n",
    "   - 遷移到下游任務\n",
    "   - 優點: 利用海量數據, 泛化能力強\n",
    "\n",
    "2. **端到端訓練 (End-to-End)**\n",
    "   - 作為模型一部分,隨任務一起訓練\n",
    "   - Transformer 通常採用此方式\n",
    "   - 優點: 針對特定任務優化\n",
    "\n",
    "---\n",
    "\n",
    "### 2.4 嵌入維度的選擇\n",
    "\n",
    "| 模型 | 詞彙量 | 嵌入維度 | 參數量 |\n",
    "|------|--------|----------|--------|\n",
    "| Word2Vec | 100K | 300 | 30M |\n",
    "| BERT-base | 30K | 768 | 23M |\n",
    "| BERT-large | 30K | 1024 | 31M |\n",
    "| GPT-2 | 50K | 768 | 38M |\n",
    "| GPT-3 | 50K | 12288 | 614M |\n",
    "\n",
    "**權衡:**\n",
    "- 維度 ↑ → 表達能力 ↑, 計算成本 ↑\n",
    "- 維度 ↓ → 訓練速度 ↑, 可能欠擬合\n",
    "\n",
    "**經驗法則:** 256-1024 維適合大多數任務\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 實驗: 嵌入維度對參數量的影響\n",
    "vocab_sizes = [10000, 30000, 50000, 100000]\n",
    "embedding_dims = [128, 256, 512, 768, 1024]\n",
    "\n",
    "# 計算參數量\n",
    "param_counts = np.array([[v * d for d in embedding_dims] for v in vocab_sizes])\n",
    "\n",
    "# 視覺化\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "for i, vocab_size in enumerate(vocab_sizes):\n",
    "    ax.plot(embedding_dims, param_counts[i] / 1e6, 'o-', \n",
    "            label=f'Vocab: {vocab_size:,}', linewidth=2, markersize=8)\n",
    "\n",
    "ax.set_xlabel('Embedding Dimension', fontsize=12)\n",
    "ax.set_ylabel('Parameters (Millions)', fontsize=12)\n",
    "ax.set_title('嵌入層參數量分析', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n💡 洞察:\")\n",
    "print(\"  - GPT-3 (50K vocab, 12288 dim) = 614M 參數僅在嵌入層!\")\n",
    "print(\"  - 嵌入層參數量 = 詞彙量 × 嵌入維度\")\n",
    "print(\"  - 大型模型中,嵌入層可占總參數量的 5-20%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "## 3. Positional Embeddings 深入\n",
    "\n",
    "### 3.1 為什麼 Attention 需要位置訊息?\n",
    "\n",
    "**問題示範:**\n",
    "\n",
    "```\n",
    "句子 A: \"狗咬人\"\n",
    "句子 B: \"人咬狗\"  ← 順序不同,語義完全相反!\n",
    "```\n",
    "\n",
    "但 Self-Attention 的計算是:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V\n",
    "$$\n",
    "\n",
    "這個公式對詞的順序**不敏感**!\n",
    "\n",
    "```python\n",
    "Attention([\"狗\", \"咬\", \"人\"]) == Attention([\"人\", \"咬\", \"狗\"])  # 完全相同!\n",
    "```\n",
    "\n",
    "**解決方案:** 加入位置編碼\n",
    "\n",
    "---\n",
    "\n",
    "### 3.2 兩種位置編碼方式\n",
    "\n",
    "#### 方法 1: 正弦位置編碼 (Sinusoidal) - Transformer 原論文\n",
    "\n",
    "$$\n",
    "\\text{PE}_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d}}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{PE}_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d}}\\right)\n",
    "$$\n",
    "\n",
    "**優點:**\n",
    "- ✅ 不需訓練 (固定函數)\n",
    "- ✅ 可處理任意長度序列\n",
    "- ✅ 相對位置可計算\n",
    "\n",
    "---\n",
    "\n",
    "#### 方法 2: 可學習位置編碼 (Learned) - BERT, GPT\n",
    "\n",
    "```python\n",
    "position_embeddings = Embedding(max_seq_len, embedding_dim)\n",
    "```\n",
    "\n",
    "**優點:**\n",
    "- ✅ 可針對任務優化\n",
    "- ✅ 實作簡單\n",
    "\n",
    "**缺點:**\n",
    "- ❌ 需要設定最大長度 (max_seq_len)\n",
    "- ❌ 無法直接處理超長序列\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 完整實作兩種位置編碼\n",
    "\n",
    "class SinusoidalPositionalEncoding:\n",
    "    \"\"\"正弦位置編碼 (Transformer 原論文)\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # 預計算位置編碼\n",
    "        pe = np.zeros((max_len, d_model))\n",
    "        position = np.arange(0, max_len).reshape(-1, 1)\n",
    "        \n",
    "        # 計算除數項\n",
    "        div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
    "        \n",
    "        # Sin for even indices\n",
    "        pe[:, 0::2] = np.sin(position * div_term)\n",
    "        # Cos for odd indices\n",
    "        pe[:, 1::2] = np.cos(position * div_term)\n",
    "        \n",
    "        self.pe = pe\n",
    "    \n",
    "    def __call__(self, seq_len):\n",
    "        \"\"\"返回序列長度為 seq_len 的位置編碼\"\"\"\n",
    "        return self.pe[:seq_len]\n",
    "\n",
    "\n",
    "class LearnedPositionalEncoding:\n",
    "    \"\"\"可學習位置編碼 (BERT/GPT)\"\"\"\n",
    "    \n",
    "    def __init__(self, max_len, d_model):\n",
    "        self.max_len = max_len\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # 隨機初始化 (訓練時會更新)\n",
    "        self.pe = np.random.randn(max_len, d_model) * 0.02\n",
    "    \n",
    "    def __call__(self, seq_len):\n",
    "        \"\"\"返回序列長度為 seq_len 的位置編碼\"\"\"\n",
    "        if seq_len > self.max_len:\n",
    "            raise ValueError(f\"序列長度 {seq_len} 超過最大長度 {self.max_len}\")\n",
    "        return self.pe[:seq_len]\n",
    "\n",
    "\n",
    "# 測試與比較\n",
    "d_model = 512\n",
    "max_len = 100\n",
    "\n",
    "sin_pe = SinusoidalPositionalEncoding(d_model, max_len)\n",
    "learned_pe = LearnedPositionalEncoding(max_len, d_model)\n",
    "\n",
    "# 獲取位置編碼\n",
    "sin_encoding = sin_pe(50)\n",
    "learned_encoding = learned_pe(50)\n",
    "\n",
    "# 視覺化比較\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# 正弦編碼\n",
    "im1 = axes[0].imshow(sin_encoding.T, cmap='RdBu', aspect='auto', vmin=-1, vmax=1)\n",
    "axes[0].set_xlabel('Position', fontsize=12)\n",
    "axes[0].set_ylabel('Dimension', fontsize=12)\n",
    "axes[0].set_title('正弦位置編碼 (Sinusoidal)\\n固定、可外推', fontsize=13, fontweight='bold')\n",
    "plt.colorbar(im1, ax=axes[0], fraction=0.046, pad=0.04)\n",
    "\n",
    "# 可學習編碼\n",
    "im2 = axes[1].imshow(learned_encoding.T, cmap='RdBu', aspect='auto', vmin=-1, vmax=1)\n",
    "axes[1].set_xlabel('Position', fontsize=12)\n",
    "axes[1].set_ylabel('Dimension', fontsize=12)\n",
    "axes[1].set_title('可學習位置編碼 (Learned)\\n可訓練、固定長度', fontsize=13, fontweight='bold')\n",
    "plt.colorbar(im2, ax=axes[1], fraction=0.046, pad=0.04)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n📊 特性對比:\")\n",
    "print(\"\\n正弦編碼:\")\n",
    "print(\"  - 規律的波形模式 (低維高頻, 高維低頻)\")\n",
    "print(\"  - 可處理任意長度\")\n",
    "print(\"  - 使用: Transformer 原論文, T5\")\n",
    "print(\"\\n可學習編碼:\")\n",
    "print(\"  - 隨機初始化,需訓練\")\n",
    "print(\"  - 受限於 max_len\")\n",
    "print(\"  - 使用: BERT, GPT, GPT-2, GPT-3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 位置編碼的數學直覺\n",
    "\n",
    "#### 為什麼正弦函數有效?\n",
    "\n",
    "**關鍵性質:** 相對位置可以線性表示\n",
    "\n",
    "$$\n",
    "\\text{PE}(pos + k) = f(\\text{PE}(pos), k)\n",
    "$$\n",
    "\n",
    "利用三角恆等式:\n",
    "\n",
    "$$\n",
    "\\sin(\\alpha + \\beta) = \\sin\\alpha\\cos\\beta + \\cos\\alpha\\sin\\beta\n",
    "$$\n",
    "\n",
    "模型可以學習「向前/向後 k 個位置」的關係!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化: 位置編碼的相對位置特性\n",
    "d_model = 128\n",
    "pe = SinusoidalPositionalEncoding(d_model, max_len=200)\n",
    "encodings = pe(200)\n",
    "\n",
    "# 計算不同位置對之間的余弦相似度\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "# 選擇幾個參考位置\n",
    "reference_positions = [0, 50, 100, 150]\n",
    "similarity_matrix = np.zeros((len(reference_positions), 200))\n",
    "\n",
    "for i, ref_pos in enumerate(reference_positions):\n",
    "    for target_pos in range(200):\n",
    "        similarity_matrix[i, target_pos] = cosine_similarity(\n",
    "            encodings[ref_pos], encodings[target_pos]\n",
    "        )\n",
    "\n",
    "# 視覺化\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "for i, ref_pos in enumerate(reference_positions):\n",
    "    plt.plot(similarity_matrix[i], label=f'參考位置 {ref_pos}', linewidth=2)\n",
    "\n",
    "plt.xlabel('Target Position', fontsize=12)\n",
    "plt.ylabel('Cosine Similarity', fontsize=12)\n",
    "plt.title('位置編碼的余弦相似度分析\\n(相似度隨距離遞減)', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n🔍 觀察:\")\n",
    "print(\"  - 相鄰位置相似度高 (峰值在自己位置)\")\n",
    "print(\"  - 距離越遠,相似度越低\")\n",
    "print(\"  - 呈現週期性模式 (正弦函數特性)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "## 4. Word2Vec vs Transformer Embeddings\n",
    "\n",
    "### 4.1 詞嵌入演進史\n",
    "\n",
    "```\n",
    "2013 → Word2Vec (Mikolov et al.)\n",
    "       - CBOW, Skip-gram\n",
    "       - 靜態嵌入 (每個詞只有一個向量)\n",
    "\n",
    "2014 → GloVe (Pennington et al.)\n",
    "       - 全局詞-詞共現矩陣\n",
    "       - 靜態嵌入\n",
    "\n",
    "2017 → ELMo (Peters et al.)\n",
    "       - 雙向 LSTM\n",
    "       - 上下文相關嵌入 (開始!)\n",
    "\n",
    "2018 → BERT (Devlin et al.)\n",
    "       - Transformer Encoder\n",
    "       - 強大的上下文嵌入\n",
    "\n",
    "2019+ → GPT-2/3, RoBERTa, T5, ...\n",
    "        - Transformer 統治 NLP\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4.2 靜態 vs 上下文嵌入\n",
    "\n",
    "#### 問題: 一詞多義\n",
    "\n",
    "```python\n",
    "句子 1: \"我去銀行存錢\"      ← \"銀行\" = 金融機構\n",
    "句子 2: \"我在河銀行釣魚\"    ← \"銀行\" = 河岸\n",
    "```\n",
    "\n",
    "**Word2Vec/GloVe (靜態):**\n",
    "```python\n",
    "\"銀行\" → [0.23, -0.45, 0.67, ...]  # 永遠相同!\n",
    "```\n",
    "\n",
    "**BERT/GPT (上下文):**\n",
    "```python\n",
    "\"銀行\" in \"我去銀行存錢\"   → [0.78, 0.12, -0.23, ...]\n",
    "\"銀行\" in \"我在河銀行釣魚\" → [-0.34, 0.89, 0.45, ...]  # 不同!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4.3 詳細對比\n",
    "\n",
    "| 特性 | Word2Vec/GloVe | Transformer (BERT/GPT) |\n",
    "|------|----------------|------------------------|\n",
    "| **嵌入類型** | 靜態 | 上下文相關 |\n",
    "| **訓練目標** | 預測鄰近詞 | Masked LM / CLM |\n",
    "| **參數量** | ~100M | 100M - 175B |\n",
    "| **訓練數據** | Wikipedia | Internet-scale |\n",
    "| **一詞多義** | ❌ 無法處理 | ✅ 完美處理 |\n",
    "| **訓練成本** | 低 (CPU 可訓練) | 高 (需大量 GPU) |\n",
    "| **適用場景** | 簡單分類、聚類 | 幾乎所有 NLP 任務 |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模擬: 靜態 vs 上下文嵌入\n",
    "\n",
    "# 靜態嵌入 (Word2Vec 風格)\n",
    "word2vec_bank = np.array([0.5, 0.3, -0.2, 0.8])  # \"銀行\" 永遠相同\n",
    "\n",
    "# 上下文嵌入 (BERT 風格 - 簡化模擬)\n",
    "def contextual_embedding(word, context):\n",
    "    \"\"\"\n",
    "    模擬上下文相關嵌入\n",
    "    (實際 BERT 會通過 Transformer 計算)\n",
    "    \"\"\"\n",
    "    base = np.array([0.5, 0.3, -0.2, 0.8])\n",
    "    \n",
    "    if \"存錢\" in context or \"金融\" in context:\n",
    "        # 金融語境\n",
    "        context_shift = np.array([0.3, -0.1, 0.2, 0.0])\n",
    "    elif \"河\" in context or \"釣魚\" in context:\n",
    "        # 自然語境\n",
    "        context_shift = np.array([-0.3, 0.4, 0.1, -0.3])\n",
    "    else:\n",
    "        context_shift = np.zeros(4)\n",
    "    \n",
    "    return base + context_shift\n",
    "\n",
    "# 兩個不同語境\n",
    "context1 = \"我去銀行存錢\"\n",
    "context2 = \"我在河銀行釣魚\"\n",
    "\n",
    "bert_bank_1 = contextual_embedding(\"銀行\", context1)\n",
    "bert_bank_2 = contextual_embedding(\"銀行\", context2)\n",
    "\n",
    "# 視覺化\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 靜態嵌入\n",
    "axes[0].bar(['維度1', '維度2', '維度3', '維度4'], word2vec_bank, color='steelblue', alpha=0.7)\n",
    "axes[0].axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "axes[0].set_ylim(-1, 1)\n",
    "axes[0].set_title('Word2Vec: 靜態嵌入\\n\"銀行\" 永遠相同', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Value', fontsize=11)\n",
    "\n",
    "# 上下文嵌入\n",
    "x = np.arange(4)\n",
    "width = 0.35\n",
    "axes[1].bar(x - width/2, bert_bank_1, width, label='\"我去銀行存錢\"', alpha=0.7)\n",
    "axes[1].bar(x + width/2, bert_bank_2, width, label='\"我在河銀行釣魚\"', alpha=0.7)\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(['維度1', '維度2', '維度3', '維度4'])\n",
    "axes[1].axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "axes[1].set_ylim(-1, 1)\n",
    "axes[1].set_title('BERT: 上下文嵌入\\n\"銀行\" 隨語境改變', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Value', fontsize=11)\n",
    "axes[1].legend(fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 計算相似度\n",
    "cos_sim = cosine_similarity(bert_bank_1, bert_bank_2)\n",
    "print(f\"\\n📐 兩種語境下 '銀行' 的余弦相似度: {cos_sim:.3f}\")\n",
    "print(\"   (< 1.0 表示不完全相同,成功捕捉語境差異!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "## 5. 實戰: 訓練與評估詞向量\n",
    "\n",
    "### 5.1 使用 Gensim 訓練 Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練簡單的 Word2Vec 模型\n",
    "try:\n",
    "    from gensim.models import Word2Vec\n",
    "    GENSIM_AVAILABLE = True\n",
    "except ImportError:\n",
    "    GENSIM_AVAILABLE = False\n",
    "    print(\"⚠️  Gensim 未安裝,跳過此部分\")\n",
    "    print(\"   安裝: pip install gensim\")\n",
    "\n",
    "if GENSIM_AVAILABLE:\n",
    "    # 準備訓練語料 (簡化示例)\n",
    "    sentences = [\n",
    "        ['國王', '統治', '國家'],\n",
    "        ['女王', '統治', '王國'],\n",
    "        ['男人', '是', '人類'],\n",
    "        ['女人', '是', '人類'],\n",
    "        ['國王', '是', '男人'],\n",
    "        ['女王', '是', '女人'],\n",
    "        ['貓', '抓', '老鼠'],\n",
    "        ['狗', '追', '貓'],\n",
    "        ['鳥', '在', '天空', '飛'],\n",
    "        ['魚', '在', '水', '裡', '游'],\n",
    "    ] * 100  # 重複以增加訓練數據\n",
    "    \n",
    "    # 訓練 Word2Vec\n",
    "    print(\"🚀 訓練 Word2Vec 模型...\")\n",
    "    model = Word2Vec(\n",
    "        sentences=sentences,\n",
    "        vector_size=50,     # 嵌入維度\n",
    "        window=3,           # 上下文窗口\n",
    "        min_count=1,        # 最小詞頻\n",
    "        workers=4,          # 並行線程\n",
    "        sg=1,               # Skip-gram (1) or CBOW (0)\n",
    "        epochs=50\n",
    "    )\n",
    "    \n",
    "    print(\"✅ 訓練完成!\")\n",
    "    print(f\"\\n詞彙量: {len(model.wv)}\")\n",
    "    print(f\"嵌入維度: {model.wv.vector_size}\")\n",
    "    \n",
    "    # 測試詞向量\n",
    "    print(\"\\n🔍 詞向量相似度測試:\")\n",
    "    test_pairs = [\n",
    "        ('國王', '女王'),\n",
    "        ('男人', '女人'),\n",
    "        ('貓', '狗'),\n",
    "        ('鳥', '魚'),\n",
    "        ('國王', '貓'),  # 不相關\n",
    "    ]\n",
    "    \n",
    "    for word1, word2 in test_pairs:\n",
    "        try:\n",
    "            sim = model.wv.similarity(word1, word2)\n",
    "            print(f\"  {word1:4s} ↔ {word2:4s}: {sim:.3f}\")\n",
    "        except KeyError:\n",
    "            print(f\"  {word1} 或 {word2} 不在詞彙表中\")\n",
    "    \n",
    "    # 經典類比任務: 國王 - 男人 + 女人 ≈ 女王\n",
    "    print(\"\\n👑 經典類比: 國王 - 男人 + 女人 = ?\")\n",
    "    try:\n",
    "        result = model.wv.most_similar(\n",
    "            positive=['女人', '國王'], \n",
    "            negative=['男人'], \n",
    "            topn=3\n",
    "        )\n",
    "        for word, score in result:\n",
    "            print(f\"  {word}: {score:.3f}\")\n",
    "    except:\n",
    "        print(\"  語料太小,無法完成類比\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 詞向量品質評估\n",
    "\n",
    "#### 三種評估方式:\n",
    "\n",
    "1. **內在評估 (Intrinsic)**\n",
    "   - 詞相似度任務\n",
    "   - 詞類比任務\n",
    "   - 快速,但與下游任務相關性低\n",
    "\n",
    "2. **外在評估 (Extrinsic)**\n",
    "   - 在實際任務上測試 (分類、NER 等)\n",
    "   - 慢,但最準確\n",
    "\n",
    "3. **視覺化評估**\n",
    "   - t-SNE / PCA 降維可視化\n",
    "   - 直觀,但主觀\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化詞向量 (使用 t-SNE 降維)\n",
    "if GENSIM_AVAILABLE:\n",
    "    # 獲取所有詞向量\n",
    "    words = list(model.wv.index_to_key)\n",
    "    vectors = np.array([model.wv[word] for word in words])\n",
    "    \n",
    "    # t-SNE 降維到 2D\n",
    "    print(\"🎨 使用 t-SNE 降維到 2D...\")\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=min(5, len(words)-1))\n",
    "    vectors_2d = tsne.fit_transform(vectors)\n",
    "    \n",
    "    # 視覺化\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # 繪製所有詞\n",
    "    plt.scatter(vectors_2d[:, 0], vectors_2d[:, 1], alpha=0.5, s=100)\n",
    "    \n",
    "    # 標註詞\n",
    "    for i, word in enumerate(words):\n",
    "        plt.annotate(word, \n",
    "                    xy=(vectors_2d[i, 0], vectors_2d[i, 1]),\n",
    "                    xytext=(5, 2),\n",
    "                    textcoords='offset points',\n",
    "                    fontsize=11,\n",
    "                    fontweight='bold')\n",
    "    \n",
    "    plt.xlabel('t-SNE Dimension 1', fontsize=12)\n",
    "    plt.ylabel('t-SNE Dimension 2', fontsize=12)\n",
    "    plt.title('Word2Vec 詞向量 t-SNE 視覺化\\n(相似詞應該聚集在一起)', \n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n💡 觀察:\")\n",
    "    print(\"  - 相似詞 (如 '國王' 和 '女王') 應該靠近\")\n",
    "    print(\"  - 不同語義類別 (如動物 vs 人物) 應該分離\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a>\n",
    "## 6. 進階技巧與優化\n",
    "\n",
    "### 6.1 嵌入層的初始化策略\n",
    "\n",
    "#### 常見初始化方法:\n",
    "\n",
    "1. **隨機初始化**\n",
    "   ```python\n",
    "   embeddings = np.random.randn(vocab_size, d_model) * 0.01\n",
    "   ```\n",
    "   - 簡單,但需要更多訓練時間\n",
    "\n",
    "2. **Xavier/Glorot 初始化**\n",
    "   ```python\n",
    "   limit = np.sqrt(6 / (vocab_size + d_model))\n",
    "   embeddings = np.random.uniform(-limit, limit, (vocab_size, d_model))\n",
    "   ```\n",
    "   - 保持梯度穩定\n",
    "\n",
    "3. **預訓練嵌入**\n",
    "   ```python\n",
    "   embeddings = load_pretrained_embeddings(\"glove.6B.300d.txt\")\n",
    "   ```\n",
    "   - 遷移學習,冷啟動快\n",
    "\n",
    "---\n",
    "\n",
    "### 6.2 嵌入層的正則化\n",
    "\n",
    "#### Dropout on Embeddings\n",
    "\n",
    "```python\n",
    "embeddings = embedding_layer(token_ids)\n",
    "embeddings = dropout(embeddings, p=0.1)  # 隨機丟棄 10%\n",
    "```\n",
    "\n",
    "**作用:** 防止過擬合,提升泛化能力\n",
    "\n",
    "---\n",
    "\n",
    "### 6.3 子詞嵌入 (Subword Embeddings)\n",
    "\n",
    "**問題:** 罕見詞、新詞、拼寫錯誤?\n",
    "\n",
    "**解決方案:** 使用子詞單元 (Subword Units)\n",
    "\n",
    "#### 三種主流方法:\n",
    "\n",
    "1. **Byte Pair Encoding (BPE)** - GPT系列\n",
    "   ```\n",
    "   \"tokenization\" → [\"token\", \"ization\"]\n",
    "   ```\n",
    "\n",
    "2. **WordPiece** - BERT\n",
    "   ```\n",
    "   \"tokenization\" → [\"token\", \"##ization\"]\n",
    "   ```\n",
    "\n",
    "3. **SentencePiece** - T5, LLaMA\n",
    "   ```\n",
    "   \"tokenization\" → [\"▁token\", \"ization\"]\n",
    "   ```\n",
    "\n",
    "**優點:**\n",
    "- ✅ 處理未登錄詞 (OOV)\n",
    "- ✅ 減小詞彙表大小\n",
    "- ✅ 多語言友好\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示範: 簡單的 BPE 分詞\n",
    "try:\n",
    "    from transformers import GPT2Tokenizer\n",
    "    TRANSFORMERS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TRANSFORMERS_AVAILABLE = False\n",
    "    print(\"⚠️  Transformers 未安裝,跳過此示範\")\n",
    "    print(\"   安裝: pip install transformers\")\n",
    "\n",
    "if TRANSFORMERS_AVAILABLE:\n",
    "    # 載入 GPT-2 Tokenizer (使用 BPE)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    \n",
    "    # 測試句子\n",
    "    sentences = [\n",
    "        \"tokenization\",\n",
    "        \"unbelievable\",\n",
    "        \"Transformer\",\n",
    "        \"COVID-19\",  # 新詞\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n🔤 BPE 子詞分詞示範 (GPT-2):\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for sent in sentences:\n",
    "        tokens = tokenizer.tokenize(sent)\n",
    "        token_ids = tokenizer.encode(sent)\n",
    "        print(f\"\\n原文: {sent}\")\n",
    "        print(f\"  ├─ Tokens: {tokens}\")\n",
    "        print(f\"  └─ IDs:    {token_ids}\")\n",
    "    \n",
    "    print(\"\\n💡 觀察:\")\n",
    "    print(\"  - 常見詞保持完整 (如 'Transformer')\")\n",
    "    print(\"  - 罕見詞分解為子詞 (如 'token' + 'ization')\")\n",
    "    print(\"  - 新詞也能處理 (如 'COVID-19')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 📚 本課總結\n",
    "\n",
    "### 核心要點回顧:\n",
    "\n",
    "1. **詞嵌入基礎:**\n",
    "   - One-Hot → 詞嵌入的演進\n",
    "   - 低維稠密向量表示語義\n",
    "   - 嵌入層 = 可訓練的查找表\n",
    "\n",
    "2. **Transformer 雙重嵌入:**\n",
    "   - Token Embeddings: 表示詞的語義\n",
    "   - Positional Embeddings: 表示詞的位置\n",
    "   - 輸入 = Token Emb + Position Emb\n",
    "\n",
    "3. **位置編碼兩種方式:**\n",
    "   - 正弦編碼: 固定、可外推 (Transformer 原論文)\n",
    "   - 可學習編碼: 可訓練、固定長度 (BERT, GPT)\n",
    "\n",
    "4. **靜態 vs 上下文嵌入:**\n",
    "   - Word2Vec/GloVe: 靜態,無法處理一詞多義\n",
    "   - BERT/GPT: 上下文相關,捕捉語境\n",
    "\n",
    "5. **進階技巧:**\n",
    "   - 初始化策略 (隨機、Xavier、預訓練)\n",
    "   - 正則化 (Dropout)\n",
    "   - 子詞嵌入 (BPE, WordPiece, SentencePiece)\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 下節預告\n",
    "\n",
    "**CH07-03: 注意力機制 (Attention Mechanism)**\n",
    "\n",
    "我們將深入探討:\n",
    "- Self-Attention 數學推導\n",
    "- Multi-Head Attention 實作\n",
    "- Scaled Dot-Product Attention\n",
    "- Attention 可視化與解釋性\n",
    "- 實戰: 從零實作完整 Attention 層\n",
    "\n",
    "---\n",
    "\n",
    "## 📖 延伸閱讀\n",
    "\n",
    "1. **論文:**\n",
    "   - [Efficient Estimation of Word Representations](https://arxiv.org/abs/1301.3781) (Word2Vec)\n",
    "   - [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf)\n",
    "   - [Deep Contextualized Word Representations](https://arxiv.org/abs/1802.05365) (ELMo)\n",
    "\n",
    "2. **教學資源:**\n",
    "   - [Word Embeddings Explained](https://www.tensorflow.org/text/guide/word_embeddings)\n",
    "   - [The Illustrated Word2Vec](http://jalammar.github.io/illustrated-word2vec/)\n",
    "\n",
    "3. **實作教學:**\n",
    "   - [Gensim Word2Vec Tutorial](https://radimrehurek.com/gensim/models/word2vec.html)\n",
    "   - [Hugging Face Tokenizers](https://huggingface.co/docs/tokenizers/index)\n",
    "\n",
    "---\n",
    "\n",
    "### 🙋 課後練習\n",
    "\n",
    "1. 使用自己的語料訓練 Word2Vec 模型\n",
    "2. 比較不同初始化策略對訓練的影響\n",
    "3. 實作一個簡單的 BPE 分詞器\n",
    "\n",
    "---\n",
    "\n",
    "**課程資訊:**\n",
    "- **作者:** iSpan NLP Team\n",
    "- **版本:** v1.0\n",
    "- **最後更新:** 2025-10-17\n",
    "- **授權:** MIT License (僅供教學使用)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
