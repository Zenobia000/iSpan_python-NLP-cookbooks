{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# CH07-04: Transformer ç·¨ç¢¼å™¨ (Encoder)\n",
    "\n",
    "**èª²ç¨‹æ™‚é•·**: 90 åˆ†é˜  \n",
    "**é›£åº¦**: â­â­â­â­  \n",
    "**å‰ç½®çŸ¥è­˜**: CH07-01, CH07-02, CH07-03  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š æœ¬ç¯€å­¸ç¿’ç›®æ¨™\n",
    "\n",
    "1. âœ… ç†è§£ Transformer Encoder çš„å®Œæ•´æ¶æ§‹\n",
    "2. âœ… å¯¦ä½œå¤šå±¤ Encoder å †ç–Š (Stacking)\n",
    "3. âœ… ä½¿ç”¨ Encoder é€²è¡Œæ–‡æœ¬åˆ†é¡ä»»å‹™\n",
    "4. âœ… ç†è§£ BERT æ¨¡å‹çš„æ¶æ§‹è¨­è¨ˆ\n",
    "5. âœ… æŒæ¡ Encoder-Only æ¨¡å‹çš„æ‡‰ç”¨å ´æ™¯\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“– ç›®éŒ„\n",
    "\n",
    "1. [Encoder Layer æ¶æ§‹å›é¡§](#1-encoder-layer)\n",
    "2. [å¤šå±¤ Encoder å †ç–Š](#2-multi-layer-encoder)\n",
    "3. [å®Œæ•´ Transformer Encoder å¯¦ä½œ](#3-full-encoder)\n",
    "4. [æ–‡æœ¬åˆ†é¡å¯¦æˆ°](#4-text-classification)\n",
    "5. [BERT æ¶æ§‹è§£æ](#5-bert-architecture)\n",
    "6. [ç¸½çµèˆ‡ç·´ç¿’](#6-summary)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1",
   "metadata": {},
   "source": [
    "## 1. Encoder Layer æ¶æ§‹å›é¡§ {#1-encoder-layer}\n",
    "\n",
    "### 1.1 å–®å±¤ Encoder çš„çµ„æˆ\n",
    "\n",
    "å›é¡§ CH07-03ï¼Œä¸€å€‹ Encoder Layer åŒ…å«ï¼š\n",
    "\n",
    "```\n",
    "Input\n",
    "  â†“\n",
    "Multi-Head Self-Attention\n",
    "  â†“\n",
    "Add & Norm (Residual Connection + Layer Normalization)\n",
    "  â†“\n",
    "Feed-Forward Network (2-layer MLP)\n",
    "  â†“\n",
    "Add & Norm\n",
    "  â†“\n",
    "Output\n",
    "```\n",
    "\n",
    "### 1.2 é—œéµçµ„ä»¶æ•¸å­¸å®šç¾©\n",
    "\n",
    "**Multi-Head Attention**:\n",
    "$$\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O\n",
    "$$\n",
    "\n",
    "**Feed-Forward Network**:\n",
    "$$\n",
    "\\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2\n",
    "$$\n",
    "\n",
    "**Layer Normalization**:\n",
    "$$\n",
    "\\text{LayerNorm}(x) = \\gamma \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¼‰å…¥å¿…è¦å¥—ä»¶\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Optional, Tuple\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['font.sans-serif'] = ['Microsoft JhengHei', 'Arial Unicode MS']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(\"âœ… å¥—ä»¶è¼‰å…¥å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2",
   "metadata": {},
   "source": [
    "## 2. å¤šå±¤ Encoder å †ç–Š {#2-multi-layer-encoder}\n",
    "\n",
    "### 2.1 ç‚ºä»€éº¼è¦å †ç–Šå¤šå±¤ï¼Ÿ\n",
    "\n",
    "**å–®å±¤ Encoder**:\n",
    "- åªèƒ½æ•æ‰æ·ºå±¤çš„èªç¾©é—œä¿‚\n",
    "- ç„¡æ³•å­¸ç¿’è¤‡é›œçš„å±¤æ¬¡åŒ–ç‰¹å¾µ\n",
    "\n",
    "**å¤šå±¤ Encoder (N=6 in original paper)**:\n",
    "- ä½å±¤ï¼šæ•æ‰èªæ³•çµæ§‹ï¼ˆè©æ€§ã€å¥æ³•ï¼‰\n",
    "- ä¸­å±¤ï¼šæ•æ‰èªç¾©é—œä¿‚ï¼ˆåŒç¾©è©ã€ä¸Šä¸‹ä½è©ï¼‰\n",
    "- é«˜å±¤ï¼šæ•æ‰æŠ½è±¡æ¦‚å¿µï¼ˆæƒ…æ„Ÿã€ä¸»é¡Œï¼‰\n",
    "\n",
    "### 2.2 å †ç–Šæ¶æ§‹\n",
    "\n",
    "```\n",
    "Input Embeddings + Positional Encoding\n",
    "           â†“\n",
    "    Encoder Layer 1\n",
    "           â†“\n",
    "    Encoder Layer 2\n",
    "           â†“\n",
    "         ...\n",
    "           â†“\n",
    "    Encoder Layer N\n",
    "           â†“\n",
    "   Final Representations\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "layer-norm",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization:\n",
    "    \"\"\"Layer Normalization implementation\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, eps: float = 1e-6):\n",
    "        self.gamma = np.ones(d_model)   # scale parameter\n",
    "        self.beta = np.zeros(d_model)   # shift parameter\n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: shape (batch_size, seq_len, d_model) or (seq_len, d_model)\n",
    "        \n",
    "        Returns:\n",
    "            Normalized output with same shape as input\n",
    "        \"\"\"\n",
    "        # Compute mean and variance along last dimension (d_model)\n",
    "        mean = np.mean(x, axis=-1, keepdims=True)\n",
    "        variance = np.var(x, axis=-1, keepdims=True)\n",
    "        \n",
    "        # Normalize\n",
    "        x_norm = (x - mean) / np.sqrt(variance + self.eps)\n",
    "        \n",
    "        # Scale and shift\n",
    "        return self.gamma * x_norm + self.beta\n",
    "\n",
    "\n",
    "# æ¸¬è©¦ Layer Normalization\n",
    "d_model = 512\n",
    "x = np.random.randn(10, d_model)  # 10 tokens, 512 dimensions\n",
    "\n",
    "layer_norm = LayerNormalization(d_model)\n",
    "x_normalized = layer_norm.forward(x)\n",
    "\n",
    "print(f\"åŸå§‹è¼¸å…¥ - Mean: {x.mean():.4f}, Std: {x.std():.4f}\")\n",
    "print(f\"æ¨™æº–åŒ–å¾Œ - Mean: {x_normalized.mean():.4f}, Std: {x_normalized.std():.4f}\")\n",
    "print(f\"æ¯å€‹ token çš„ mean æ˜¯å¦æ¥è¿‘ 0: {np.allclose(x_normalized.mean(axis=-1), 0, atol=1e-5)}\")\n",
    "print(f\"æ¯å€‹ token çš„ std æ˜¯å¦æ¥è¿‘ 1: {np.allclose(x_normalized.std(axis=-1), 1, atol=1e-1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feed-forward",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetwork:\n",
    "    \"\"\"Position-wise Feed-Forward Network\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, d_ff: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: Model dimension (e.g., 512)\n",
    "            d_ff: Hidden layer dimension (typically 4 * d_model = 2048)\n",
    "        \"\"\"\n",
    "        # Xavier initialization\n",
    "        self.W1 = np.random.randn(d_model, d_ff) / np.sqrt(d_model)\n",
    "        self.b1 = np.zeros(d_ff)\n",
    "        self.W2 = np.random.randn(d_ff, d_model) / np.sqrt(d_ff)\n",
    "        self.b2 = np.zeros(d_model)\n",
    "    \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        FFN(x) = max(0, xW1 + b1)W2 + b2\n",
    "        \n",
    "        Args:\n",
    "            x: shape (seq_len, d_model)\n",
    "        \n",
    "        Returns:\n",
    "            output: shape (seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # First linear transformation + ReLU\n",
    "        hidden = np.maximum(0, np.dot(x, self.W1) + self.b1)\n",
    "        \n",
    "        # Second linear transformation\n",
    "        output = np.dot(hidden, self.W2) + self.b2\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "# æ¸¬è©¦ Feed-Forward Network\n",
    "d_model = 512\n",
    "d_ff = 2048\n",
    "seq_len = 10\n",
    "\n",
    "x = np.random.randn(seq_len, d_model)\n",
    "ffn = FeedForwardNetwork(d_model, d_ff)\n",
    "output = ffn.forward(x)\n",
    "\n",
    "print(f\"è¼¸å…¥å½¢ç‹€: {x.shape}\")\n",
    "print(f\"è¼¸å‡ºå½¢ç‹€: {output.shape}\")\n",
    "print(f\"åƒæ•¸é‡: W1={d_model * d_ff:,}, W2={d_ff * d_model:,}, ç¸½è¨ˆ={2 * d_model * d_ff:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3",
   "metadata": {},
   "source": [
    "## 3. å®Œæ•´ Transformer Encoder å¯¦ä½œ {#3-full-encoder}\n",
    "\n",
    "### 3.1 å–®å±¤ Encoder Layer å®Œæ•´å¯¦ä½œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multi-head-attention",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(\n",
    "    Q: np.ndarray, \n",
    "    K: np.ndarray, \n",
    "    V: np.ndarray, \n",
    "    mask: Optional[np.ndarray] = None\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Scaled Dot-Product Attention (from CH07-03)\n",
    "    \n",
    "    Args:\n",
    "        Q: Query matrix (seq_len, d_k)\n",
    "        K: Key matrix (seq_len, d_k)\n",
    "        V: Value matrix (seq_len, d_v)\n",
    "        mask: Optional mask (seq_len, seq_len)\n",
    "    \n",
    "    Returns:\n",
    "        output: Attention output (seq_len, d_v)\n",
    "        attention_weights: Attention weights (seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    d_k = K.shape[-1]\n",
    "    \n",
    "    # Compute attention scores\n",
    "    scores = np.matmul(Q, K.T) / np.sqrt(d_k)\n",
    "    \n",
    "    # Apply mask if provided\n",
    "    if mask is not None:\n",
    "        scores = np.where(mask == 0, -1e9, scores)\n",
    "    \n",
    "    # Apply softmax\n",
    "    attention_weights = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)\n",
    "    \n",
    "    # Compute weighted sum of values\n",
    "    output = np.matmul(attention_weights, V)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "\n",
    "class MultiHeadAttention:\n",
    "    \"\"\"Multi-Head Attention implementation\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, num_heads: int):\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        # Weight matrices for Q, K, V for all heads (combined)\n",
    "        self.W_q = np.random.randn(d_model, d_model) / np.sqrt(d_model)\n",
    "        self.W_k = np.random.randn(d_model, d_model) / np.sqrt(d_model)\n",
    "        self.W_v = np.random.randn(d_model, d_model) / np.sqrt(d_model)\n",
    "        \n",
    "        # Output projection\n",
    "        self.W_o = np.random.randn(d_model, d_model) / np.sqrt(d_model)\n",
    "    \n",
    "    def split_heads(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Split the last dimension into (num_heads, d_k)\n",
    "        \n",
    "        Args:\n",
    "            x: shape (seq_len, d_model)\n",
    "        \n",
    "        Returns:\n",
    "            shape (num_heads, seq_len, d_k)\n",
    "        \"\"\"\n",
    "        seq_len = x.shape[0]\n",
    "        # Reshape to (seq_len, num_heads, d_k)\n",
    "        x = x.reshape(seq_len, self.num_heads, self.d_k)\n",
    "        # Transpose to (num_heads, seq_len, d_k)\n",
    "        return x.transpose(1, 0, 2)\n",
    "    \n",
    "    def forward(\n",
    "        self, \n",
    "        x: np.ndarray, \n",
    "        mask: Optional[np.ndarray] = None\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input (seq_len, d_model)\n",
    "            mask: Optional mask (seq_len, seq_len)\n",
    "        \n",
    "        Returns:\n",
    "            output: (seq_len, d_model)\n",
    "            attention_weights: (num_heads, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        seq_len = x.shape[0]\n",
    "        \n",
    "        # Linear projections\n",
    "        Q = np.dot(x, self.W_q)  # (seq_len, d_model)\n",
    "        K = np.dot(x, self.W_k)\n",
    "        V = np.dot(x, self.W_v)\n",
    "        \n",
    "        # Split into multiple heads\n",
    "        Q = self.split_heads(Q)  # (num_heads, seq_len, d_k)\n",
    "        K = self.split_heads(K)\n",
    "        V = self.split_heads(V)\n",
    "        \n",
    "        # Apply attention for each head\n",
    "        outputs = []\n",
    "        attention_weights_all = []\n",
    "        \n",
    "        for i in range(self.num_heads):\n",
    "            output, attn_weights = scaled_dot_product_attention(\n",
    "                Q[i], K[i], V[i], mask\n",
    "            )\n",
    "            outputs.append(output)\n",
    "            attention_weights_all.append(attn_weights)\n",
    "        \n",
    "        # Concatenate all heads\n",
    "        # outputs: list of (seq_len, d_k) -> (seq_len, d_model)\n",
    "        concat_output = np.concatenate(outputs, axis=-1)\n",
    "        \n",
    "        # Final linear projection\n",
    "        final_output = np.dot(concat_output, self.W_o)\n",
    "        \n",
    "        # Stack attention weights\n",
    "        attention_weights = np.stack(attention_weights_all, axis=0)\n",
    "        \n",
    "        return final_output, attention_weights\n",
    "\n",
    "\n",
    "# æ¸¬è©¦ Multi-Head Attention\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "seq_len = 10\n",
    "\n",
    "x = np.random.randn(seq_len, d_model)\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "output, attn_weights = mha.forward(x)\n",
    "\n",
    "print(f\"è¼¸å…¥å½¢ç‹€: {x.shape}\")\n",
    "print(f\"è¼¸å‡ºå½¢ç‹€: {output.shape}\")\n",
    "print(f\"æ³¨æ„åŠ›æ¬Šé‡å½¢ç‹€: {attn_weights.shape}\")\n",
    "print(f\"æ¯å€‹ head çš„ç¶­åº¦ d_k: {mha.d_k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "encoder-layer",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer:\n",
    "    \"\"\"Complete Transformer Encoder Layer\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        d_model: int, \n",
    "        num_heads: int, \n",
    "        d_ff: int, \n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = FeedForwardNetwork(d_model, d_ff)\n",
    "        \n",
    "        self.layernorm1 = LayerNormalization(d_model)\n",
    "        self.layernorm2 = LayerNormalization(d_model)\n",
    "        \n",
    "        self.dropout = dropout\n",
    "    \n",
    "    def forward(\n",
    "        self, \n",
    "        x: np.ndarray, \n",
    "        mask: Optional[np.ndarray] = None\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input (seq_len, d_model)\n",
    "            mask: Optional padding mask (seq_len, seq_len)\n",
    "        \n",
    "        Returns:\n",
    "            output: (seq_len, d_model)\n",
    "            attention_weights: (num_heads, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        # Multi-Head Self-Attention + Residual + LayerNorm\n",
    "        attn_output, attn_weights = self.mha.forward(x, mask)\n",
    "        x = self.layernorm1.forward(x + attn_output)  # Residual connection\n",
    "        \n",
    "        # Feed-Forward Network + Residual + LayerNorm\n",
    "        ffn_output = self.ffn.forward(x)\n",
    "        x = self.layernorm2.forward(x + ffn_output)\n",
    "        \n",
    "        return x, attn_weights\n",
    "\n",
    "\n",
    "# æ¸¬è©¦å–®å±¤ Encoder Layer\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "d_ff = 2048\n",
    "seq_len = 10\n",
    "\n",
    "x = np.random.randn(seq_len, d_model)\n",
    "encoder_layer = TransformerEncoderLayer(d_model, num_heads, d_ff)\n",
    "output, attn_weights = encoder_layer.forward(x)\n",
    "\n",
    "print(\"âœ… å–®å±¤ Transformer Encoder Layer æ¸¬è©¦é€šé\")\n",
    "print(f\"è¼¸å…¥å½¢ç‹€: {x.shape}\")\n",
    "print(f\"è¼¸å‡ºå½¢ç‹€: {output.shape}\")\n",
    "print(f\"æ³¨æ„åŠ›æ¬Šé‡å½¢ç‹€: {attn_weights.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3-2",
   "metadata": {},
   "source": [
    "### 3.2 å¤šå±¤ Encoder å †ç–Šå¯¦ä½œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transformer-encoder",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder:\n",
    "    \"\"\"Complete Transformer Encoder with multiple layers\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers: int,\n",
    "        d_model: int,\n",
    "        num_heads: int,\n",
    "        d_ff: int,\n",
    "        vocab_size: int,\n",
    "        max_seq_len: int,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Embedding layers\n",
    "        self.token_embedding = np.random.randn(vocab_size, d_model) / np.sqrt(d_model)\n",
    "        self.positional_encoding = self.get_positional_encoding(max_seq_len, d_model)\n",
    "        \n",
    "        # Stack of encoder layers\n",
    "        self.encoder_layers = [\n",
    "            TransformerEncoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_positional_encoding(max_seq_len: int, d_model: int) -> np.ndarray:\n",
    "        \"\"\"Generate positional encoding\"\"\"\n",
    "        pos_encoding = np.zeros((max_seq_len, d_model))\n",
    "        \n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pos_encoding[pos, i] = np.sin(pos / (10000 ** (i / d_model)))\n",
    "                if i + 1 < d_model:\n",
    "                    pos_encoding[pos, i + 1] = np.cos(pos / (10000 ** (i / d_model)))\n",
    "        \n",
    "        return pos_encoding\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        token_ids: np.ndarray,\n",
    "        mask: Optional[np.ndarray] = None\n",
    "    ) -> Tuple[np.ndarray, list]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_ids: Token IDs (seq_len,)\n",
    "            mask: Optional padding mask (seq_len, seq_len)\n",
    "        \n",
    "        Returns:\n",
    "            output: Final encoder output (seq_len, d_model)\n",
    "            all_attention_weights: List of attention weights from each layer\n",
    "        \"\"\"\n",
    "        seq_len = token_ids.shape[0]\n",
    "        \n",
    "        # Token embedding + Positional encoding\n",
    "        x = self.token_embedding[token_ids]  # (seq_len, d_model)\n",
    "        x = x + self.positional_encoding[:seq_len, :]  # Add positional encoding\n",
    "        \n",
    "        # Pass through all encoder layers\n",
    "        all_attention_weights = []\n",
    "        \n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            x, attn_weights = encoder_layer.forward(x, mask)\n",
    "            all_attention_weights.append(attn_weights)\n",
    "        \n",
    "        return x, all_attention_weights\n",
    "\n",
    "\n",
    "# æ¸¬è©¦å®Œæ•´ Transformer Encoder\n",
    "num_layers = 6\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "d_ff = 2048\n",
    "vocab_size = 10000\n",
    "max_seq_len = 100\n",
    "seq_len = 10\n",
    "\n",
    "# éš¨æ©Ÿç”Ÿæˆ token IDs\n",
    "token_ids = np.random.randint(0, vocab_size, size=seq_len)\n",
    "\n",
    "# å»ºç«‹ Transformer Encoder\n",
    "encoder = TransformerEncoder(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    d_ff=d_ff,\n",
    "    vocab_size=vocab_size,\n",
    "    max_seq_len=max_seq_len\n",
    ")\n",
    "\n",
    "# å‰å‘å‚³æ’­\n",
    "output, all_attn_weights = encoder.forward(token_ids)\n",
    "\n",
    "print(\"âœ… å®Œæ•´ Transformer Encoder æ¸¬è©¦é€šé\")\n",
    "print(f\"è¼¸å…¥ token IDs: {token_ids.shape}\")\n",
    "print(f\"è¼¸å‡ºå½¢ç‹€: {output.shape}\")\n",
    "print(f\"Encoder å±¤æ•¸: {len(all_attn_weights)}\")\n",
    "print(f\"æ¯å±¤æ³¨æ„åŠ›æ¬Šé‡å½¢ç‹€: {all_attn_weights[0].shape}\")\n",
    "\n",
    "# è¨ˆç®—åƒæ•¸é‡\n",
    "embedding_params = vocab_size * d_model\n",
    "layer_params = (\n",
    "    4 * d_model * d_model +  # Q, K, V, O projections\n",
    "    2 * d_model * d_ff +      # FFN\n",
    "    4 * d_model               # LayerNorm gamma, beta (x2)\n",
    ")\n",
    "total_params = embedding_params + num_layers * layer_params\n",
    "\n",
    "print(f\"\\nğŸ“Š åƒæ•¸é‡çµ±è¨ˆ:\")\n",
    "print(f\"  Embedding: {embedding_params:,}\")\n",
    "print(f\"  æ¯å±¤ Encoder: {layer_params:,}\")\n",
    "print(f\"  ç¸½åƒæ•¸é‡: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4",
   "metadata": {},
   "source": [
    "## 4. æ–‡æœ¬åˆ†é¡å¯¦æˆ° {#4-text-classification}\n",
    "\n",
    "### 4.1 ä»»å‹™èªªæ˜\n",
    "\n",
    "ä½¿ç”¨ Transformer Encoder é€²è¡Œæƒ…æ„Ÿåˆ†é¡ï¼ˆPositive/Negativeï¼‰\n",
    "\n",
    "**æ¶æ§‹**:\n",
    "```\n",
    "Input Tokens\n",
    "     â†“\n",
    "Transformer Encoder (6 layers)\n",
    "     â†“\n",
    "[CLS] Token Representation\n",
    "     â†“\n",
    "Classification Head (Linear + Softmax)\n",
    "     â†“\n",
    "Positive / Negative\n",
    "```\n",
    "\n",
    "### 4.2 ä½¿ç”¨ Keras å¯¦ä½œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "keras-encoder",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    print(f\"âœ… TensorFlow {tf.__version__} è¼‰å…¥æˆåŠŸ\")\n",
    "    \n",
    "    class TransformerBlock(layers.Layer):\n",
    "        \"\"\"Transformer Encoder Block using Keras\"\"\"\n",
    "        \n",
    "        def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "            super().__init__()\n",
    "            self.att = layers.MultiHeadAttention(\n",
    "                num_heads=num_heads, \n",
    "                key_dim=d_model // num_heads\n",
    "            )\n",
    "            self.ffn = keras.Sequential([\n",
    "                layers.Dense(d_ff, activation='relu'),\n",
    "                layers.Dense(d_model)\n",
    "            ])\n",
    "            \n",
    "            self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "            self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "            \n",
    "            self.dropout1 = layers.Dropout(dropout)\n",
    "            self.dropout2 = layers.Dropout(dropout)\n",
    "        \n",
    "        def call(self, inputs, training=False):\n",
    "            # Multi-Head Self-Attention\n",
    "            attn_output = self.att(inputs, inputs)\n",
    "            attn_output = self.dropout1(attn_output, training=training)\n",
    "            out1 = self.layernorm1(inputs + attn_output)\n",
    "            \n",
    "            # Feed-Forward Network\n",
    "            ffn_output = self.ffn(out1)\n",
    "            ffn_output = self.dropout2(ffn_output, training=training)\n",
    "            out2 = self.layernorm2(out1 + ffn_output)\n",
    "            \n",
    "            return out2\n",
    "    \n",
    "    \n",
    "    class PositionalEncoding(layers.Layer):\n",
    "        \"\"\"Positional Encoding Layer\"\"\"\n",
    "        \n",
    "        def __init__(self, max_seq_len, d_model):\n",
    "            super().__init__()\n",
    "            self.pos_encoding = self.get_positional_encoding(max_seq_len, d_model)\n",
    "        \n",
    "        def get_positional_encoding(self, max_seq_len, d_model):\n",
    "            pos = np.arange(max_seq_len)[:, np.newaxis]\n",
    "            i = np.arange(d_model)[np.newaxis, :]\n",
    "            \n",
    "            angle_rates = 1 / np.power(10000, (2 * (i // 2)) / d_model)\n",
    "            angle_rads = pos * angle_rates\n",
    "            \n",
    "            # Apply sin to even indices\n",
    "            angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "            # Apply cos to odd indices\n",
    "            angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "            \n",
    "            return tf.cast(angle_rads[np.newaxis, ...], dtype=tf.float32)\n",
    "        \n",
    "        def call(self, inputs):\n",
    "            seq_len = tf.shape(inputs)[1]\n",
    "            return inputs + self.pos_encoding[:, :seq_len, :]\n",
    "    \n",
    "    \n",
    "    def build_transformer_classifier(\n",
    "        vocab_size=10000,\n",
    "        max_seq_len=100,\n",
    "        d_model=128,\n",
    "        num_heads=8,\n",
    "        d_ff=512,\n",
    "        num_layers=2,\n",
    "        num_classes=2,\n",
    "        dropout=0.1\n",
    "    ):\n",
    "        \"\"\"Build Transformer-based text classifier\"\"\"\n",
    "        \n",
    "        # Input\n",
    "        inputs = layers.Input(shape=(max_seq_len,), dtype=tf.int32)\n",
    "        \n",
    "        # Embedding\n",
    "        x = layers.Embedding(vocab_size, d_model)(inputs)\n",
    "        x = x * tf.math.sqrt(tf.cast(d_model, tf.float32))  # Scale embeddings\n",
    "        \n",
    "        # Positional Encoding\n",
    "        x = PositionalEncoding(max_seq_len, d_model)(x)\n",
    "        \n",
    "        # Encoder layers\n",
    "        for _ in range(num_layers):\n",
    "            x = TransformerBlock(d_model, num_heads, d_ff, dropout)(x)\n",
    "        \n",
    "        # Global average pooling (alternative to [CLS] token)\n",
    "        x = layers.GlobalAveragePooling1D()(x)\n",
    "        \n",
    "        # Classification head\n",
    "        x = layers.Dropout(dropout)(x)\n",
    "        outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "        \n",
    "        model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "        return model\n",
    "    \n",
    "    \n",
    "    # å»ºç«‹æ¨¡å‹\n",
    "    model = build_transformer_classifier(\n",
    "        vocab_size=10000,\n",
    "        max_seq_len=100,\n",
    "        d_model=128,\n",
    "        num_heads=8,\n",
    "        d_ff=512,\n",
    "        num_layers=2,\n",
    "        num_classes=2\n",
    "    )\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    print(\"\\nâœ… Transformer åˆ†é¡å™¨å»ºç«‹æˆåŠŸ\")\n",
    "    model.summary()\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"âš ï¸ TensorFlow æœªå®‰è£ï¼Œè·³é Keras å¯¦ä½œç¯„ä¾‹\")\n",
    "    print(\"   å®‰è£æŒ‡ä»¤: pip install tensorflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demo-data",
   "metadata": {},
   "source": [
    "### 4.3 æ¸¬è©¦åˆ†é¡å™¨\n",
    "\n",
    "ä½¿ç”¨å°‘é‡æ¨¡æ“¬æ•¸æ“šæ¸¬è©¦æ¨¡å‹æ˜¯å¦èƒ½æ­£å¸¸é‹è¡Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-classifier",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # ç”Ÿæˆæ¨¡æ“¬æ•¸æ“š\n",
    "    num_samples = 100\n",
    "    max_seq_len = 100\n",
    "    vocab_size = 10000\n",
    "    \n",
    "    X_train = np.random.randint(0, vocab_size, size=(num_samples, max_seq_len))\n",
    "    y_train = np.random.randint(0, 2, size=num_samples)\n",
    "    \n",
    "    print(\"è¨“ç·´æ•¸æ“šå½¢ç‹€:\")\n",
    "    print(f\"  X_train: {X_train.shape}\")\n",
    "    print(f\"  y_train: {y_train.shape}\")\n",
    "    \n",
    "    # è¨“ç·´æ¨¡å‹ï¼ˆåƒ… 1 å€‹ epoch ç”¨æ–¼æ¸¬è©¦ï¼‰\n",
    "    print(\"\\né–‹å§‹è¨“ç·´...\")\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        batch_size=16,\n",
    "        epochs=1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # æ¸¬è©¦é æ¸¬\n",
    "    test_input = X_train[:5]\n",
    "    predictions = model.predict(test_input)\n",
    "    \n",
    "    print(\"\\nâœ… é æ¸¬æ¸¬è©¦:\")\n",
    "    for i in range(5):\n",
    "        pred_class = np.argmax(predictions[i])\n",
    "        confidence = predictions[i][pred_class]\n",
    "        print(f\"  æ¨£æœ¬ {i+1}: é¡åˆ¥ {pred_class}, ä¿¡å¿ƒåº¦ {confidence:.2%}\")\n",
    "    \n",
    "except NameError:\n",
    "    print(\"âš ï¸ æ¨¡å‹æœªå»ºç«‹ï¼Œè·³éæ¸¬è©¦\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-5",
   "metadata": {},
   "source": [
    "## 5. BERT æ¶æ§‹è§£æ {#5-bert-architecture}\n",
    "\n",
    "### 5.1 BERT = Encoder-Only Transformer\n",
    "\n",
    "**BERT (Bidirectional Encoder Representations from Transformers)**:\n",
    "- åªä½¿ç”¨ Transformer çš„ Encoder éƒ¨åˆ†\n",
    "- 12 å±¤ï¼ˆBERT-Baseï¼‰æˆ– 24 å±¤ï¼ˆBERT-Largeï¼‰\n",
    "- ä½¿ç”¨ç‰¹æ®Š token: `[CLS]`, `[SEP]`, `[MASK]`\n",
    "\n",
    "### 5.2 BERT vs åŸå§‹ Transformer Encoder\n",
    "\n",
    "| ç‰¹æ€§ | åŸå§‹ Transformer Encoder | BERT |\n",
    "|------|-------------------------|------|\n",
    "| å±¤æ•¸ | 6 å±¤ | 12 å±¤ (Base) / 24 å±¤ (Large) |\n",
    "| éš±è—ç¶­åº¦ | 512 | 768 (Base) / 1024 (Large) |\n",
    "| æ³¨æ„åŠ›é ­æ•¸ | 8 | 12 (Base) / 16 (Large) |\n",
    "| åƒæ•¸é‡ | ~65M | 110M (Base) / 340M (Large) |\n",
    "| è¨“ç·´ä»»å‹™ | åºåˆ—è½‰åºåˆ— | MLM + NSP |\n",
    "| æ‡‰ç”¨å ´æ™¯ | æ©Ÿå™¨ç¿»è­¯ | æ–‡æœ¬ç†è§£ |\n",
    "\n",
    "### 5.3 BERT çš„å…©å€‹è¨“ç·´ä»»å‹™\n",
    "\n",
    "**1. Masked Language Model (MLM)**:\n",
    "```\n",
    "åŸå§‹: The cat sat on the mat\n",
    "é®è”½: The [MASK] sat on the [MASK]\n",
    "é æ¸¬: cat, mat\n",
    "```\n",
    "\n",
    "**2. Next Sentence Prediction (NSP)**:\n",
    "```\n",
    "å¥å­ A: I love NLP.\n",
    "å¥å­ B: It is very interesting.\n",
    "æ¨™ç±¤: IsNext (1) or NotNext (0)\n",
    "```\n",
    "\n",
    "### 5.4 BERT çš„è¼¸å…¥è¡¨ç¤º\n",
    "\n",
    "```\n",
    "Input = Token Embeddings + Segment Embeddings + Position Embeddings\n",
    "```\n",
    "\n",
    "**ç¯„ä¾‹**:\n",
    "```\n",
    "Token:    [CLS] I love NLP [SEP] It is great [SEP]\n",
    "Segment:    A   A   A   A    A     B  B   B     B\n",
    "Position:   0   1   2   3    4     5  6   7     8\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bert-visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¦–è¦ºåŒ– BERT è¼¸å…¥è¡¨ç¤º\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "tokens = ['[CLS]', 'I', 'love', 'NLP', '[SEP]', 'It', 'is', 'great', '[SEP]']\n",
    "segment_ids = [0, 0, 0, 0, 0, 1, 1, 1, 1]\n",
    "position_ids = list(range(len(tokens)))\n",
    "\n",
    "# Token Embeddings\n",
    "token_embeds = np.random.randn(len(tokens), 8)\n",
    "sns.heatmap(token_embeds.T, annot=False, cmap='RdBu_r', \n",
    "            xticklabels=tokens, yticklabels=False, ax=axes[0], cbar=True)\n",
    "axes[0].set_title('Token Embeddings', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Tokens')\n",
    "\n",
    "# Segment Embeddings\n",
    "segment_embeds = np.array([segment_ids] * 8)\n",
    "sns.heatmap(segment_embeds, annot=False, cmap='coolwarm', \n",
    "            xticklabels=tokens, yticklabels=False, ax=axes[1], cbar=True)\n",
    "axes[1].set_title('Segment Embeddings', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Tokens')\n",
    "\n",
    "# Position Embeddings\n",
    "pos_embeds = np.array([position_ids] * 8)\n",
    "sns.heatmap(pos_embeds, annot=False, cmap='viridis', \n",
    "            xticklabels=tokens, yticklabels=False, ax=axes[2], cbar=True)\n",
    "axes[2].set_title('Position Embeddings', fontsize=14, fontweight='bold')\n",
    "axes[2].set_xlabel('Tokens')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nèªªæ˜:\")\n",
    "print(\"â€¢ Token Embeddings: æ¯å€‹ token çš„èªç¾©å‘é‡\")\n",
    "print(\"â€¢ Segment Embeddings: å€åˆ†ä¸åŒå¥å­ (å¥å­ A=0, å¥å­ B=1)\")\n",
    "print(\"â€¢ Position Embeddings: æ¨™è¨˜ token åœ¨åºåˆ—ä¸­çš„ä½ç½®\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bert-applications",
   "metadata": {},
   "source": [
    "### 5.5 BERT çš„ä¸‹æ¸¸ä»»å‹™å¾®èª¿\n",
    "\n",
    "**å¸¸è¦‹æ‡‰ç”¨**:\n",
    "\n",
    "1. **æ–‡æœ¬åˆ†é¡**: ä½¿ç”¨ `[CLS]` token çš„è¼¸å‡º\n",
    "2. **å‘½åå¯¦é«”è­˜åˆ¥ (NER)**: ä½¿ç”¨æ¯å€‹ token çš„è¼¸å‡º\n",
    "3. **å•ç­”ç³»çµ±**: é æ¸¬ç­”æ¡ˆçš„èµ·å§‹å’ŒçµæŸä½ç½®\n",
    "4. **æ–‡æœ¬ç›¸ä¼¼åº¦**: è¨ˆç®—å…©å€‹å¥å­çš„ `[CLS]` å‘é‡ç›¸ä¼¼åº¦\n",
    "\n",
    "**å¾®èª¿æ¶æ§‹**:\n",
    "```\n",
    "BERT Encoder (é è¨“ç·´)\n",
    "      â†“\n",
    "[CLS] è¡¨ç¤ºå‘é‡\n",
    "      â†“\n",
    "ä»»å‹™ç‰¹å®šå±¤ (åˆ†é¡å™¨/å›æ­¸å™¨)\n",
    "      â†“\n",
    "    è¼¸å‡º\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-6",
   "metadata": {},
   "source": [
    "## 6. ç¸½çµèˆ‡ç·´ç¿’ {#6-summary}\n",
    "\n",
    "### 6.1 æœ¬ç¯€é‡é»å›é¡§\n",
    "\n",
    "âœ… **æ¶æ§‹ç†è§£**:\n",
    "- Transformer Encoder = å¤šå±¤å †ç–Šçš„ Encoder Layer\n",
    "- æ¯å±¤åŒ…å«: Multi-Head Attention + FFN + Residual + LayerNorm\n",
    "\n",
    "âœ… **å¯¦ä½œèƒ½åŠ›**:\n",
    "- å¾é›¶å¯¦ä½œ LayerNormalization, FeedForwardNetwork\n",
    "- çµ„åˆå®Œæ•´çš„ TransformerEncoderLayer\n",
    "- å †ç–Šå¤šå±¤ Encoder å»ºç«‹å®Œæ•´æ¨¡å‹\n",
    "\n",
    "âœ… **æ‡‰ç”¨å ´æ™¯**:\n",
    "- æ–‡æœ¬åˆ†é¡: ä½¿ç”¨ Global Pooling æˆ– [CLS] token\n",
    "- BERT: Encoder-Only æ¶æ§‹çš„ä»£è¡¨ä½œ\n",
    "\n",
    "âœ… **BERT ç‰¹è‰²**:\n",
    "- ä½¿ç”¨ MLM + NSP é è¨“ç·´\n",
    "- ä¸‰ç¨® Embeddings: Token + Segment + Position\n",
    "- å¯å¾®èª¿è‡³å„ç¨® NLP ä»»å‹™\n",
    "\n",
    "### 6.2 é—œéµåƒæ•¸å°æ¯”\n",
    "\n",
    "| æ¨¡å‹ | å±¤æ•¸ | d_model | num_heads | åƒæ•¸é‡ |\n",
    "|------|------|---------|-----------|--------|\n",
    "| Transformer-Base | 6 | 512 | 8 | ~65M |\n",
    "| BERT-Base | 12 | 768 | 12 | 110M |\n",
    "| BERT-Large | 24 | 1024 | 16 | 340M |\n",
    "| GPT-2 | 12-48 | 768-1600 | 12-25 | 117M-1.5B |\n",
    "\n",
    "### 6.3 å¯¦ä½œç·´ç¿’\n",
    "\n",
    "#### ç·´ç¿’ 1: å¯è¦–åŒ–å±¤ç´šç‰¹å¾µ\n",
    "\n",
    "**ä»»å‹™**: æå–ä¸¦å¯è¦–åŒ– Encoder æ¯ä¸€å±¤çš„æ³¨æ„åŠ›æ¬Šé‡ï¼Œè§€å¯Ÿä¸åŒå±¤å­¸åˆ°çš„æ¨¡å¼ã€‚\n",
    "\n",
    "**æç¤º**:\n",
    "```python\n",
    "# ä½¿ç”¨ all_attention_weights[layer_idx][head_idx]\n",
    "# ç¹ªè£½ç†±åœ–è§€å¯Ÿæ³¨æ„åŠ›åˆ†ä½ˆ\n",
    "```\n",
    "\n",
    "#### ç·´ç¿’ 2: å¯¦ä½œ [CLS] Token æ©Ÿåˆ¶\n",
    "\n",
    "**ä»»å‹™**: ä¿®æ”¹ TransformerEncoderï¼Œåœ¨è¼¸å…¥åºåˆ—å‰æ·»åŠ  `[CLS]` tokenï¼Œä¸¦åœ¨åˆ†é¡æ™‚åªä½¿ç”¨ `[CLS]` çš„è¼¸å‡ºè€Œé Global Poolingã€‚\n",
    "\n",
    "**æç¤º**:\n",
    "```python\n",
    "# åœ¨ token_ids å‰æ’å…¥ CLS_TOKEN_ID\n",
    "# æœ€çµ‚è¼¸å‡ºå– output[0, :] (ç¬¬ä¸€å€‹ token)\n",
    "```\n",
    "\n",
    "#### ç·´ç¿’ 3: å¯¦ä½œ Masked Language Model\n",
    "\n",
    "**ä»»å‹™**: å¯¦ä½œç°¡å–®çš„ MLM è¨“ç·´ï¼Œéš¨æ©Ÿé®è”½ 15% çš„ tokenï¼Œè¨“ç·´æ¨¡å‹é æ¸¬è¢«é®è”½çš„è©ã€‚\n",
    "\n",
    "**æç¤º**:\n",
    "```python\n",
    "def create_masked_lm_data(tokens, mask_prob=0.15):\n",
    "    # éš¨æ©Ÿé¸æ“‡ token é€²è¡Œé®è”½\n",
    "    # è¿”å› masked_tokens, original_tokens, mask_positions\n",
    "    pass\n",
    "```\n",
    "\n",
    "### 6.4 å»¶ä¼¸é–±è®€\n",
    "\n",
    "1. **è«–æ–‡**:\n",
    "   - [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - åŸå§‹ Transformer\n",
    "   - [BERT: Pre-training of Deep Bidirectional Transformers](https://arxiv.org/abs/1810.04805)\n",
    "\n",
    "2. **å¯¦ä½œè³‡æº**:\n",
    "   - [Hugging Face Transformers](https://huggingface.co/docs/transformers)\n",
    "   - [Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)\n",
    "\n",
    "3. **è¦–è¦ºåŒ–å·¥å…·**:\n",
    "   - [BertViz](https://github.com/jessevig/bertviz) - BERT æ³¨æ„åŠ›è¦–è¦ºåŒ–\n",
    "   - [Tensor2Tensor Visualization](https://github.com/tensorflow/tensor2tensor)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ ä¸‹ä¸€ç¯€é å‘Š\n",
    "\n",
    "**CH07-05: Transformer è§£ç¢¼å™¨ (Decoder)**\n",
    "- Decoder çš„æ¶æ§‹èˆ‡ Encoder çš„å·®ç•°\n",
    "- Masked Self-Attention çš„å¯¦ä½œ\n",
    "- Cross-Attention æ©Ÿåˆ¶\n",
    "- å®Œæ•´çš„ Encoder-Decoder æ¶æ§‹\n",
    "- æ©Ÿå™¨ç¿»è­¯å¯¦æˆ°\n",
    "\n",
    "---\n",
    "\n",
    "**èª²ç¨‹å®Œæˆæ™‚é–“**: `____å¹´____æœˆ____æ—¥`  \n",
    "**å­¸ç¿’å¿ƒå¾—**: ___________________________________"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
