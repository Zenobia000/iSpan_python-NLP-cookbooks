{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# CH07-04: Transformer 編碼器 (Encoder)\n",
    "\n",
    "**課程時長**: 90 分鐘  \n",
    "**難度**: ⭐⭐⭐⭐  \n",
    "**前置知識**: CH07-01, CH07-02, CH07-03  \n",
    "\n",
    "---\n",
    "\n",
    "## 📚 本節學習目標\n",
    "\n",
    "1. ✅ 理解 Transformer Encoder 的完整架構\n",
    "2. ✅ 實作多層 Encoder 堆疊 (Stacking)\n",
    "3. ✅ 使用 Encoder 進行文本分類任務\n",
    "4. ✅ 理解 BERT 模型的架構設計\n",
    "5. ✅ 掌握 Encoder-Only 模型的應用場景\n",
    "\n",
    "---\n",
    "\n",
    "## 📖 目錄\n",
    "\n",
    "1. [Encoder Layer 架構回顧](#1-encoder-layer)\n",
    "2. [多層 Encoder 堆疊](#2-multi-layer-encoder)\n",
    "3. [完整 Transformer Encoder 實作](#3-full-encoder)\n",
    "4. [文本分類實戰](#4-text-classification)\n",
    "5. [BERT 架構解析](#5-bert-architecture)\n",
    "6. [總結與練習](#6-summary)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1",
   "metadata": {},
   "source": [
    "## 1. Encoder Layer 架構回顧 {#1-encoder-layer}\n",
    "\n",
    "### 1.1 單層 Encoder 的組成\n",
    "\n",
    "回顧 CH07-03，一個 Encoder Layer 包含：\n",
    "\n",
    "```\n",
    "Input\n",
    "  ↓\n",
    "Multi-Head Self-Attention\n",
    "  ↓\n",
    "Add & Norm (Residual Connection + Layer Normalization)\n",
    "  ↓\n",
    "Feed-Forward Network (2-layer MLP)\n",
    "  ↓\n",
    "Add & Norm\n",
    "  ↓\n",
    "Output\n",
    "```\n",
    "\n",
    "### 1.2 關鍵組件數學定義\n",
    "\n",
    "**Multi-Head Attention**:\n",
    "$$\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O\n",
    "$$\n",
    "\n",
    "**Feed-Forward Network**:\n",
    "$$\n",
    "\\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2\n",
    "$$\n",
    "\n",
    "**Layer Normalization**:\n",
    "$$\n",
    "\\text{LayerNorm}(x) = \\gamma \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入必要套件\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Optional, Tuple\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['font.sans-serif'] = ['Microsoft JhengHei', 'Arial Unicode MS']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(\"✅ 套件載入完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2",
   "metadata": {},
   "source": [
    "## 2. 多層 Encoder 堆疊 {#2-multi-layer-encoder}\n",
    "\n",
    "### 2.1 為什麼要堆疊多層？\n",
    "\n",
    "**單層 Encoder**:\n",
    "- 只能捕捉淺層的語義關係\n",
    "- 無法學習複雜的層次化特徵\n",
    "\n",
    "**多層 Encoder (N=6 in original paper)**:\n",
    "- 低層：捕捉語法結構（詞性、句法）\n",
    "- 中層：捕捉語義關係（同義詞、上下位詞）\n",
    "- 高層：捕捉抽象概念（情感、主題）\n",
    "\n",
    "### 2.2 堆疊架構\n",
    "\n",
    "```\n",
    "Input Embeddings + Positional Encoding\n",
    "           ↓\n",
    "    Encoder Layer 1\n",
    "           ↓\n",
    "    Encoder Layer 2\n",
    "           ↓\n",
    "         ...\n",
    "           ↓\n",
    "    Encoder Layer N\n",
    "           ↓\n",
    "   Final Representations\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "layer-norm",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization:\n",
    "    \"\"\"Layer Normalization implementation\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, eps: float = 1e-6):\n",
    "        self.gamma = np.ones(d_model)   # scale parameter\n",
    "        self.beta = np.zeros(d_model)   # shift parameter\n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: shape (batch_size, seq_len, d_model) or (seq_len, d_model)\n",
    "        \n",
    "        Returns:\n",
    "            Normalized output with same shape as input\n",
    "        \"\"\"\n",
    "        # Compute mean and variance along last dimension (d_model)\n",
    "        mean = np.mean(x, axis=-1, keepdims=True)\n",
    "        variance = np.var(x, axis=-1, keepdims=True)\n",
    "        \n",
    "        # Normalize\n",
    "        x_norm = (x - mean) / np.sqrt(variance + self.eps)\n",
    "        \n",
    "        # Scale and shift\n",
    "        return self.gamma * x_norm + self.beta\n",
    "\n",
    "\n",
    "# 測試 Layer Normalization\n",
    "d_model = 512\n",
    "x = np.random.randn(10, d_model)  # 10 tokens, 512 dimensions\n",
    "\n",
    "layer_norm = LayerNormalization(d_model)\n",
    "x_normalized = layer_norm.forward(x)\n",
    "\n",
    "print(f\"原始輸入 - Mean: {x.mean():.4f}, Std: {x.std():.4f}\")\n",
    "print(f\"標準化後 - Mean: {x_normalized.mean():.4f}, Std: {x_normalized.std():.4f}\")\n",
    "print(f\"每個 token 的 mean 是否接近 0: {np.allclose(x_normalized.mean(axis=-1), 0, atol=1e-5)}\")\n",
    "print(f\"每個 token 的 std 是否接近 1: {np.allclose(x_normalized.std(axis=-1), 1, atol=1e-1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feed-forward",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetwork:\n",
    "    \"\"\"Position-wise Feed-Forward Network\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, d_ff: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: Model dimension (e.g., 512)\n",
    "            d_ff: Hidden layer dimension (typically 4 * d_model = 2048)\n",
    "        \"\"\"\n",
    "        # Xavier initialization\n",
    "        self.W1 = np.random.randn(d_model, d_ff) / np.sqrt(d_model)\n",
    "        self.b1 = np.zeros(d_ff)\n",
    "        self.W2 = np.random.randn(d_ff, d_model) / np.sqrt(d_ff)\n",
    "        self.b2 = np.zeros(d_model)\n",
    "    \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        FFN(x) = max(0, xW1 + b1)W2 + b2\n",
    "        \n",
    "        Args:\n",
    "            x: shape (seq_len, d_model)\n",
    "        \n",
    "        Returns:\n",
    "            output: shape (seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # First linear transformation + ReLU\n",
    "        hidden = np.maximum(0, np.dot(x, self.W1) + self.b1)\n",
    "        \n",
    "        # Second linear transformation\n",
    "        output = np.dot(hidden, self.W2) + self.b2\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "# 測試 Feed-Forward Network\n",
    "d_model = 512\n",
    "d_ff = 2048\n",
    "seq_len = 10\n",
    "\n",
    "x = np.random.randn(seq_len, d_model)\n",
    "ffn = FeedForwardNetwork(d_model, d_ff)\n",
    "output = ffn.forward(x)\n",
    "\n",
    "print(f\"輸入形狀: {x.shape}\")\n",
    "print(f\"輸出形狀: {output.shape}\")\n",
    "print(f\"參數量: W1={d_model * d_ff:,}, W2={d_ff * d_model:,}, 總計={2 * d_model * d_ff:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3",
   "metadata": {},
   "source": [
    "## 3. 完整 Transformer Encoder 實作 {#3-full-encoder}\n",
    "\n",
    "### 3.1 單層 Encoder Layer 完整實作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multi-head-attention",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(\n",
    "    Q: np.ndarray, \n",
    "    K: np.ndarray, \n",
    "    V: np.ndarray, \n",
    "    mask: Optional[np.ndarray] = None\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Scaled Dot-Product Attention (from CH07-03)\n",
    "    \n",
    "    Args:\n",
    "        Q: Query matrix (seq_len, d_k)\n",
    "        K: Key matrix (seq_len, d_k)\n",
    "        V: Value matrix (seq_len, d_v)\n",
    "        mask: Optional mask (seq_len, seq_len)\n",
    "    \n",
    "    Returns:\n",
    "        output: Attention output (seq_len, d_v)\n",
    "        attention_weights: Attention weights (seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    d_k = K.shape[-1]\n",
    "    \n",
    "    # Compute attention scores\n",
    "    scores = np.matmul(Q, K.T) / np.sqrt(d_k)\n",
    "    \n",
    "    # Apply mask if provided\n",
    "    if mask is not None:\n",
    "        scores = np.where(mask == 0, -1e9, scores)\n",
    "    \n",
    "    # Apply softmax\n",
    "    attention_weights = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)\n",
    "    \n",
    "    # Compute weighted sum of values\n",
    "    output = np.matmul(attention_weights, V)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "\n",
    "class MultiHeadAttention:\n",
    "    \"\"\"Multi-Head Attention implementation\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, num_heads: int):\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        # Weight matrices for Q, K, V for all heads (combined)\n",
    "        self.W_q = np.random.randn(d_model, d_model) / np.sqrt(d_model)\n",
    "        self.W_k = np.random.randn(d_model, d_model) / np.sqrt(d_model)\n",
    "        self.W_v = np.random.randn(d_model, d_model) / np.sqrt(d_model)\n",
    "        \n",
    "        # Output projection\n",
    "        self.W_o = np.random.randn(d_model, d_model) / np.sqrt(d_model)\n",
    "    \n",
    "    def split_heads(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Split the last dimension into (num_heads, d_k)\n",
    "        \n",
    "        Args:\n",
    "            x: shape (seq_len, d_model)\n",
    "        \n",
    "        Returns:\n",
    "            shape (num_heads, seq_len, d_k)\n",
    "        \"\"\"\n",
    "        seq_len = x.shape[0]\n",
    "        # Reshape to (seq_len, num_heads, d_k)\n",
    "        x = x.reshape(seq_len, self.num_heads, self.d_k)\n",
    "        # Transpose to (num_heads, seq_len, d_k)\n",
    "        return x.transpose(1, 0, 2)\n",
    "    \n",
    "    def forward(\n",
    "        self, \n",
    "        x: np.ndarray, \n",
    "        mask: Optional[np.ndarray] = None\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input (seq_len, d_model)\n",
    "            mask: Optional mask (seq_len, seq_len)\n",
    "        \n",
    "        Returns:\n",
    "            output: (seq_len, d_model)\n",
    "            attention_weights: (num_heads, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        seq_len = x.shape[0]\n",
    "        \n",
    "        # Linear projections\n",
    "        Q = np.dot(x, self.W_q)  # (seq_len, d_model)\n",
    "        K = np.dot(x, self.W_k)\n",
    "        V = np.dot(x, self.W_v)\n",
    "        \n",
    "        # Split into multiple heads\n",
    "        Q = self.split_heads(Q)  # (num_heads, seq_len, d_k)\n",
    "        K = self.split_heads(K)\n",
    "        V = self.split_heads(V)\n",
    "        \n",
    "        # Apply attention for each head\n",
    "        outputs = []\n",
    "        attention_weights_all = []\n",
    "        \n",
    "        for i in range(self.num_heads):\n",
    "            output, attn_weights = scaled_dot_product_attention(\n",
    "                Q[i], K[i], V[i], mask\n",
    "            )\n",
    "            outputs.append(output)\n",
    "            attention_weights_all.append(attn_weights)\n",
    "        \n",
    "        # Concatenate all heads\n",
    "        # outputs: list of (seq_len, d_k) -> (seq_len, d_model)\n",
    "        concat_output = np.concatenate(outputs, axis=-1)\n",
    "        \n",
    "        # Final linear projection\n",
    "        final_output = np.dot(concat_output, self.W_o)\n",
    "        \n",
    "        # Stack attention weights\n",
    "        attention_weights = np.stack(attention_weights_all, axis=0)\n",
    "        \n",
    "        return final_output, attention_weights\n",
    "\n",
    "\n",
    "# 測試 Multi-Head Attention\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "seq_len = 10\n",
    "\n",
    "x = np.random.randn(seq_len, d_model)\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "output, attn_weights = mha.forward(x)\n",
    "\n",
    "print(f\"輸入形狀: {x.shape}\")\n",
    "print(f\"輸出形狀: {output.shape}\")\n",
    "print(f\"注意力權重形狀: {attn_weights.shape}\")\n",
    "print(f\"每個 head 的維度 d_k: {mha.d_k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "encoder-layer",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer:\n",
    "    \"\"\"Complete Transformer Encoder Layer\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        d_model: int, \n",
    "        num_heads: int, \n",
    "        d_ff: int, \n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = FeedForwardNetwork(d_model, d_ff)\n",
    "        \n",
    "        self.layernorm1 = LayerNormalization(d_model)\n",
    "        self.layernorm2 = LayerNormalization(d_model)\n",
    "        \n",
    "        self.dropout = dropout\n",
    "    \n",
    "    def forward(\n",
    "        self, \n",
    "        x: np.ndarray, \n",
    "        mask: Optional[np.ndarray] = None\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input (seq_len, d_model)\n",
    "            mask: Optional padding mask (seq_len, seq_len)\n",
    "        \n",
    "        Returns:\n",
    "            output: (seq_len, d_model)\n",
    "            attention_weights: (num_heads, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        # Multi-Head Self-Attention + Residual + LayerNorm\n",
    "        attn_output, attn_weights = self.mha.forward(x, mask)\n",
    "        x = self.layernorm1.forward(x + attn_output)  # Residual connection\n",
    "        \n",
    "        # Feed-Forward Network + Residual + LayerNorm\n",
    "        ffn_output = self.ffn.forward(x)\n",
    "        x = self.layernorm2.forward(x + ffn_output)\n",
    "        \n",
    "        return x, attn_weights\n",
    "\n",
    "\n",
    "# 測試單層 Encoder Layer\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "d_ff = 2048\n",
    "seq_len = 10\n",
    "\n",
    "x = np.random.randn(seq_len, d_model)\n",
    "encoder_layer = TransformerEncoderLayer(d_model, num_heads, d_ff)\n",
    "output, attn_weights = encoder_layer.forward(x)\n",
    "\n",
    "print(\"✅ 單層 Transformer Encoder Layer 測試通過\")\n",
    "print(f\"輸入形狀: {x.shape}\")\n",
    "print(f\"輸出形狀: {output.shape}\")\n",
    "print(f\"注意力權重形狀: {attn_weights.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3-2",
   "metadata": {},
   "source": [
    "### 3.2 多層 Encoder 堆疊實作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transformer-encoder",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder:\n",
    "    \"\"\"Complete Transformer Encoder with multiple layers\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers: int,\n",
    "        d_model: int,\n",
    "        num_heads: int,\n",
    "        d_ff: int,\n",
    "        vocab_size: int,\n",
    "        max_seq_len: int,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Embedding layers\n",
    "        self.token_embedding = np.random.randn(vocab_size, d_model) / np.sqrt(d_model)\n",
    "        self.positional_encoding = self.get_positional_encoding(max_seq_len, d_model)\n",
    "        \n",
    "        # Stack of encoder layers\n",
    "        self.encoder_layers = [\n",
    "            TransformerEncoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_positional_encoding(max_seq_len: int, d_model: int) -> np.ndarray:\n",
    "        \"\"\"Generate positional encoding\"\"\"\n",
    "        pos_encoding = np.zeros((max_seq_len, d_model))\n",
    "        \n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pos_encoding[pos, i] = np.sin(pos / (10000 ** (i / d_model)))\n",
    "                if i + 1 < d_model:\n",
    "                    pos_encoding[pos, i + 1] = np.cos(pos / (10000 ** (i / d_model)))\n",
    "        \n",
    "        return pos_encoding\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        token_ids: np.ndarray,\n",
    "        mask: Optional[np.ndarray] = None\n",
    "    ) -> Tuple[np.ndarray, list]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_ids: Token IDs (seq_len,)\n",
    "            mask: Optional padding mask (seq_len, seq_len)\n",
    "        \n",
    "        Returns:\n",
    "            output: Final encoder output (seq_len, d_model)\n",
    "            all_attention_weights: List of attention weights from each layer\n",
    "        \"\"\"\n",
    "        seq_len = token_ids.shape[0]\n",
    "        \n",
    "        # Token embedding + Positional encoding\n",
    "        x = self.token_embedding[token_ids]  # (seq_len, d_model)\n",
    "        x = x + self.positional_encoding[:seq_len, :]  # Add positional encoding\n",
    "        \n",
    "        # Pass through all encoder layers\n",
    "        all_attention_weights = []\n",
    "        \n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            x, attn_weights = encoder_layer.forward(x, mask)\n",
    "            all_attention_weights.append(attn_weights)\n",
    "        \n",
    "        return x, all_attention_weights\n",
    "\n",
    "\n",
    "# 測試完整 Transformer Encoder\n",
    "num_layers = 6\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "d_ff = 2048\n",
    "vocab_size = 10000\n",
    "max_seq_len = 100\n",
    "seq_len = 10\n",
    "\n",
    "# 隨機生成 token IDs\n",
    "token_ids = np.random.randint(0, vocab_size, size=seq_len)\n",
    "\n",
    "# 建立 Transformer Encoder\n",
    "encoder = TransformerEncoder(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    d_ff=d_ff,\n",
    "    vocab_size=vocab_size,\n",
    "    max_seq_len=max_seq_len\n",
    ")\n",
    "\n",
    "# 前向傳播\n",
    "output, all_attn_weights = encoder.forward(token_ids)\n",
    "\n",
    "print(\"✅ 完整 Transformer Encoder 測試通過\")\n",
    "print(f\"輸入 token IDs: {token_ids.shape}\")\n",
    "print(f\"輸出形狀: {output.shape}\")\n",
    "print(f\"Encoder 層數: {len(all_attn_weights)}\")\n",
    "print(f\"每層注意力權重形狀: {all_attn_weights[0].shape}\")\n",
    "\n",
    "# 計算參數量\n",
    "embedding_params = vocab_size * d_model\n",
    "layer_params = (\n",
    "    4 * d_model * d_model +  # Q, K, V, O projections\n",
    "    2 * d_model * d_ff +      # FFN\n",
    "    4 * d_model               # LayerNorm gamma, beta (x2)\n",
    ")\n",
    "total_params = embedding_params + num_layers * layer_params\n",
    "\n",
    "print(f\"\\n📊 參數量統計:\")\n",
    "print(f\"  Embedding: {embedding_params:,}\")\n",
    "print(f\"  每層 Encoder: {layer_params:,}\")\n",
    "print(f\"  總參數量: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4",
   "metadata": {},
   "source": [
    "## 4. 文本分類實戰 {#4-text-classification}\n",
    "\n",
    "### 4.1 任務說明\n",
    "\n",
    "使用 Transformer Encoder 進行情感分類（Positive/Negative）\n",
    "\n",
    "**架構**:\n",
    "```\n",
    "Input Tokens\n",
    "     ↓\n",
    "Transformer Encoder (6 layers)\n",
    "     ↓\n",
    "[CLS] Token Representation\n",
    "     ↓\n",
    "Classification Head (Linear + Softmax)\n",
    "     ↓\n",
    "Positive / Negative\n",
    "```\n",
    "\n",
    "### 4.2 使用 Keras 實作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "keras-encoder",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    print(f\"✅ TensorFlow {tf.__version__} 載入成功\")\n",
    "    \n",
    "    class TransformerBlock(layers.Layer):\n",
    "        \"\"\"Transformer Encoder Block using Keras\"\"\"\n",
    "        \n",
    "        def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "            super().__init__()\n",
    "            self.att = layers.MultiHeadAttention(\n",
    "                num_heads=num_heads, \n",
    "                key_dim=d_model // num_heads\n",
    "            )\n",
    "            self.ffn = keras.Sequential([\n",
    "                layers.Dense(d_ff, activation='relu'),\n",
    "                layers.Dense(d_model)\n",
    "            ])\n",
    "            \n",
    "            self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "            self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "            \n",
    "            self.dropout1 = layers.Dropout(dropout)\n",
    "            self.dropout2 = layers.Dropout(dropout)\n",
    "        \n",
    "        def call(self, inputs, training=False):\n",
    "            # Multi-Head Self-Attention\n",
    "            attn_output = self.att(inputs, inputs)\n",
    "            attn_output = self.dropout1(attn_output, training=training)\n",
    "            out1 = self.layernorm1(inputs + attn_output)\n",
    "            \n",
    "            # Feed-Forward Network\n",
    "            ffn_output = self.ffn(out1)\n",
    "            ffn_output = self.dropout2(ffn_output, training=training)\n",
    "            out2 = self.layernorm2(out1 + ffn_output)\n",
    "            \n",
    "            return out2\n",
    "    \n",
    "    \n",
    "    class PositionalEncoding(layers.Layer):\n",
    "        \"\"\"Positional Encoding Layer\"\"\"\n",
    "        \n",
    "        def __init__(self, max_seq_len, d_model):\n",
    "            super().__init__()\n",
    "            self.pos_encoding = self.get_positional_encoding(max_seq_len, d_model)\n",
    "        \n",
    "        def get_positional_encoding(self, max_seq_len, d_model):\n",
    "            pos = np.arange(max_seq_len)[:, np.newaxis]\n",
    "            i = np.arange(d_model)[np.newaxis, :]\n",
    "            \n",
    "            angle_rates = 1 / np.power(10000, (2 * (i // 2)) / d_model)\n",
    "            angle_rads = pos * angle_rates\n",
    "            \n",
    "            # Apply sin to even indices\n",
    "            angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "            # Apply cos to odd indices\n",
    "            angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "            \n",
    "            return tf.cast(angle_rads[np.newaxis, ...], dtype=tf.float32)\n",
    "        \n",
    "        def call(self, inputs):\n",
    "            seq_len = tf.shape(inputs)[1]\n",
    "            return inputs + self.pos_encoding[:, :seq_len, :]\n",
    "    \n",
    "    \n",
    "    def build_transformer_classifier(\n",
    "        vocab_size=10000,\n",
    "        max_seq_len=100,\n",
    "        d_model=128,\n",
    "        num_heads=8,\n",
    "        d_ff=512,\n",
    "        num_layers=2,\n",
    "        num_classes=2,\n",
    "        dropout=0.1\n",
    "    ):\n",
    "        \"\"\"Build Transformer-based text classifier\"\"\"\n",
    "        \n",
    "        # Input\n",
    "        inputs = layers.Input(shape=(max_seq_len,), dtype=tf.int32)\n",
    "        \n",
    "        # Embedding\n",
    "        x = layers.Embedding(vocab_size, d_model)(inputs)\n",
    "        x = x * tf.math.sqrt(tf.cast(d_model, tf.float32))  # Scale embeddings\n",
    "        \n",
    "        # Positional Encoding\n",
    "        x = PositionalEncoding(max_seq_len, d_model)(x)\n",
    "        \n",
    "        # Encoder layers\n",
    "        for _ in range(num_layers):\n",
    "            x = TransformerBlock(d_model, num_heads, d_ff, dropout)(x)\n",
    "        \n",
    "        # Global average pooling (alternative to [CLS] token)\n",
    "        x = layers.GlobalAveragePooling1D()(x)\n",
    "        \n",
    "        # Classification head\n",
    "        x = layers.Dropout(dropout)(x)\n",
    "        outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "        \n",
    "        model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "        return model\n",
    "    \n",
    "    \n",
    "    # 建立模型\n",
    "    model = build_transformer_classifier(\n",
    "        vocab_size=10000,\n",
    "        max_seq_len=100,\n",
    "        d_model=128,\n",
    "        num_heads=8,\n",
    "        d_ff=512,\n",
    "        num_layers=2,\n",
    "        num_classes=2\n",
    "    )\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    print(\"\\n✅ Transformer 分類器建立成功\")\n",
    "    model.summary()\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"⚠️ TensorFlow 未安裝，跳過 Keras 實作範例\")\n",
    "    print(\"   安裝指令: pip install tensorflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demo-data",
   "metadata": {},
   "source": [
    "### 4.3 測試分類器\n",
    "\n",
    "使用少量模擬數據測試模型是否能正常運行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-classifier",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # 生成模擬數據\n",
    "    num_samples = 100\n",
    "    max_seq_len = 100\n",
    "    vocab_size = 10000\n",
    "    \n",
    "    X_train = np.random.randint(0, vocab_size, size=(num_samples, max_seq_len))\n",
    "    y_train = np.random.randint(0, 2, size=num_samples)\n",
    "    \n",
    "    print(\"訓練數據形狀:\")\n",
    "    print(f\"  X_train: {X_train.shape}\")\n",
    "    print(f\"  y_train: {y_train.shape}\")\n",
    "    \n",
    "    # 訓練模型（僅 1 個 epoch 用於測試）\n",
    "    print(\"\\n開始訓練...\")\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        batch_size=16,\n",
    "        epochs=1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # 測試預測\n",
    "    test_input = X_train[:5]\n",
    "    predictions = model.predict(test_input)\n",
    "    \n",
    "    print(\"\\n✅ 預測測試:\")\n",
    "    for i in range(5):\n",
    "        pred_class = np.argmax(predictions[i])\n",
    "        confidence = predictions[i][pred_class]\n",
    "        print(f\"  樣本 {i+1}: 類別 {pred_class}, 信心度 {confidence:.2%}\")\n",
    "    \n",
    "except NameError:\n",
    "    print(\"⚠️ 模型未建立，跳過測試\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-5",
   "metadata": {},
   "source": [
    "## 5. BERT 架構解析 {#5-bert-architecture}\n",
    "\n",
    "### 5.1 BERT = Encoder-Only Transformer\n",
    "\n",
    "**BERT (Bidirectional Encoder Representations from Transformers)**:\n",
    "- 只使用 Transformer 的 Encoder 部分\n",
    "- 12 層（BERT-Base）或 24 層（BERT-Large）\n",
    "- 使用特殊 token: `[CLS]`, `[SEP]`, `[MASK]`\n",
    "\n",
    "### 5.2 BERT vs 原始 Transformer Encoder\n",
    "\n",
    "| 特性 | 原始 Transformer Encoder | BERT |\n",
    "|------|-------------------------|------|\n",
    "| 層數 | 6 層 | 12 層 (Base) / 24 層 (Large) |\n",
    "| 隱藏維度 | 512 | 768 (Base) / 1024 (Large) |\n",
    "| 注意力頭數 | 8 | 12 (Base) / 16 (Large) |\n",
    "| 參數量 | ~65M | 110M (Base) / 340M (Large) |\n",
    "| 訓練任務 | 序列轉序列 | MLM + NSP |\n",
    "| 應用場景 | 機器翻譯 | 文本理解 |\n",
    "\n",
    "### 5.3 BERT 的兩個訓練任務\n",
    "\n",
    "**1. Masked Language Model (MLM)**:\n",
    "```\n",
    "原始: The cat sat on the mat\n",
    "遮蔽: The [MASK] sat on the [MASK]\n",
    "預測: cat, mat\n",
    "```\n",
    "\n",
    "**2. Next Sentence Prediction (NSP)**:\n",
    "```\n",
    "句子 A: I love NLP.\n",
    "句子 B: It is very interesting.\n",
    "標籤: IsNext (1) or NotNext (0)\n",
    "```\n",
    "\n",
    "### 5.4 BERT 的輸入表示\n",
    "\n",
    "```\n",
    "Input = Token Embeddings + Segment Embeddings + Position Embeddings\n",
    "```\n",
    "\n",
    "**範例**:\n",
    "```\n",
    "Token:    [CLS] I love NLP [SEP] It is great [SEP]\n",
    "Segment:    A   A   A   A    A     B  B   B     B\n",
    "Position:   0   1   2   3    4     5  6   7     8\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bert-visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化 BERT 輸入表示\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "tokens = ['[CLS]', 'I', 'love', 'NLP', '[SEP]', 'It', 'is', 'great', '[SEP]']\n",
    "segment_ids = [0, 0, 0, 0, 0, 1, 1, 1, 1]\n",
    "position_ids = list(range(len(tokens)))\n",
    "\n",
    "# Token Embeddings\n",
    "token_embeds = np.random.randn(len(tokens), 8)\n",
    "sns.heatmap(token_embeds.T, annot=False, cmap='RdBu_r', \n",
    "            xticklabels=tokens, yticklabels=False, ax=axes[0], cbar=True)\n",
    "axes[0].set_title('Token Embeddings', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Tokens')\n",
    "\n",
    "# Segment Embeddings\n",
    "segment_embeds = np.array([segment_ids] * 8)\n",
    "sns.heatmap(segment_embeds, annot=False, cmap='coolwarm', \n",
    "            xticklabels=tokens, yticklabels=False, ax=axes[1], cbar=True)\n",
    "axes[1].set_title('Segment Embeddings', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Tokens')\n",
    "\n",
    "# Position Embeddings\n",
    "pos_embeds = np.array([position_ids] * 8)\n",
    "sns.heatmap(pos_embeds, annot=False, cmap='viridis', \n",
    "            xticklabels=tokens, yticklabels=False, ax=axes[2], cbar=True)\n",
    "axes[2].set_title('Position Embeddings', fontsize=14, fontweight='bold')\n",
    "axes[2].set_xlabel('Tokens')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n說明:\")\n",
    "print(\"• Token Embeddings: 每個 token 的語義向量\")\n",
    "print(\"• Segment Embeddings: 區分不同句子 (句子 A=0, 句子 B=1)\")\n",
    "print(\"• Position Embeddings: 標記 token 在序列中的位置\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bert-applications",
   "metadata": {},
   "source": [
    "### 5.5 BERT 的下游任務微調\n",
    "\n",
    "**常見應用**:\n",
    "\n",
    "1. **文本分類**: 使用 `[CLS]` token 的輸出\n",
    "2. **命名實體識別 (NER)**: 使用每個 token 的輸出\n",
    "3. **問答系統**: 預測答案的起始和結束位置\n",
    "4. **文本相似度**: 計算兩個句子的 `[CLS]` 向量相似度\n",
    "\n",
    "**微調架構**:\n",
    "```\n",
    "BERT Encoder (預訓練)\n",
    "      ↓\n",
    "[CLS] 表示向量\n",
    "      ↓\n",
    "任務特定層 (分類器/回歸器)\n",
    "      ↓\n",
    "    輸出\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-6",
   "metadata": {},
   "source": [
    "## 6. 總結與練習 {#6-summary}\n",
    "\n",
    "### 6.1 本節重點回顧\n",
    "\n",
    "✅ **架構理解**:\n",
    "- Transformer Encoder = 多層堆疊的 Encoder Layer\n",
    "- 每層包含: Multi-Head Attention + FFN + Residual + LayerNorm\n",
    "\n",
    "✅ **實作能力**:\n",
    "- 從零實作 LayerNormalization, FeedForwardNetwork\n",
    "- 組合完整的 TransformerEncoderLayer\n",
    "- 堆疊多層 Encoder 建立完整模型\n",
    "\n",
    "✅ **應用場景**:\n",
    "- 文本分類: 使用 Global Pooling 或 [CLS] token\n",
    "- BERT: Encoder-Only 架構的代表作\n",
    "\n",
    "✅ **BERT 特色**:\n",
    "- 使用 MLM + NSP 預訓練\n",
    "- 三種 Embeddings: Token + Segment + Position\n",
    "- 可微調至各種 NLP 任務\n",
    "\n",
    "### 6.2 關鍵參數對比\n",
    "\n",
    "| 模型 | 層數 | d_model | num_heads | 參數量 |\n",
    "|------|------|---------|-----------|--------|\n",
    "| Transformer-Base | 6 | 512 | 8 | ~65M |\n",
    "| BERT-Base | 12 | 768 | 12 | 110M |\n",
    "| BERT-Large | 24 | 1024 | 16 | 340M |\n",
    "| GPT-2 | 12-48 | 768-1600 | 12-25 | 117M-1.5B |\n",
    "\n",
    "### 6.3 實作練習\n",
    "\n",
    "#### 練習 1: 可視化層級特徵\n",
    "\n",
    "**任務**: 提取並可視化 Encoder 每一層的注意力權重，觀察不同層學到的模式。\n",
    "\n",
    "**提示**:\n",
    "```python\n",
    "# 使用 all_attention_weights[layer_idx][head_idx]\n",
    "# 繪製熱圖觀察注意力分佈\n",
    "```\n",
    "\n",
    "#### 練習 2: 實作 [CLS] Token 機制\n",
    "\n",
    "**任務**: 修改 TransformerEncoder，在輸入序列前添加 `[CLS]` token，並在分類時只使用 `[CLS]` 的輸出而非 Global Pooling。\n",
    "\n",
    "**提示**:\n",
    "```python\n",
    "# 在 token_ids 前插入 CLS_TOKEN_ID\n",
    "# 最終輸出取 output[0, :] (第一個 token)\n",
    "```\n",
    "\n",
    "#### 練習 3: 實作 Masked Language Model\n",
    "\n",
    "**任務**: 實作簡單的 MLM 訓練，隨機遮蔽 15% 的 token，訓練模型預測被遮蔽的詞。\n",
    "\n",
    "**提示**:\n",
    "```python\n",
    "def create_masked_lm_data(tokens, mask_prob=0.15):\n",
    "    # 隨機選擇 token 進行遮蔽\n",
    "    # 返回 masked_tokens, original_tokens, mask_positions\n",
    "    pass\n",
    "```\n",
    "\n",
    "### 6.4 延伸閱讀\n",
    "\n",
    "1. **論文**:\n",
    "   - [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - 原始 Transformer\n",
    "   - [BERT: Pre-training of Deep Bidirectional Transformers](https://arxiv.org/abs/1810.04805)\n",
    "\n",
    "2. **實作資源**:\n",
    "   - [Hugging Face Transformers](https://huggingface.co/docs/transformers)\n",
    "   - [Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)\n",
    "\n",
    "3. **視覺化工具**:\n",
    "   - [BertViz](https://github.com/jessevig/bertviz) - BERT 注意力視覺化\n",
    "   - [Tensor2Tensor Visualization](https://github.com/tensorflow/tensor2tensor)\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 下一節預告\n",
    "\n",
    "**CH07-05: Transformer 解碼器 (Decoder)**\n",
    "- Decoder 的架構與 Encoder 的差異\n",
    "- Masked Self-Attention 的實作\n",
    "- Cross-Attention 機制\n",
    "- 完整的 Encoder-Decoder 架構\n",
    "- 機器翻譯實戰\n",
    "\n",
    "---\n",
    "\n",
    "**課程完成時間**: `____年____月____日`  \n",
    "**學習心得**: ___________________________________"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
