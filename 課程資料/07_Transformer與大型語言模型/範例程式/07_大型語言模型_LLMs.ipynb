{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# CH07-07: 大型語言模型 (Large Language Models)\n",
    "\n",
    "**課程時長**: 90 分鐘  \n",
    "**難度**: ⭐⭐⭐⭐  \n",
    "**前置知識**: CH07-01 至 CH07-06  \n",
    "\n",
    "---\n",
    "\n",
    "## 📚 本節學習目標\n",
    "\n",
    "1. ✅ 理解 LLM 的定義與規模定律 (Scaling Laws)\n",
    "2. ✅ 掌握 Instruction Tuning 與 RLHF 原理\n",
    "3. ✅ 學會 Prompt Engineering 核心技巧\n",
    "4. ✅ 理解 In-Context Learning 的工作機制\n",
    "5. ✅ 認識 LLM 的能力邊界與倫理議題\n",
    "\n",
    "---\n",
    "\n",
    "## 📖 目錄\n",
    "\n",
    "1. [什麼是大型語言模型](#1-what-is-llm)\n",
    "2. [規模定律與湧現能力](#2-scaling-laws)\n",
    "3. [從預訓練到對齊](#3-alignment)\n",
    "4. [Prompt Engineering 實戰](#4-prompt-engineering)\n",
    "5. [In-Context Learning 原理](#5-in-context-learning)\n",
    "6. [LLM 的局限性與未來](#6-limitations)\n",
    "7. [總結](#7-summary)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1",
   "metadata": {},
   "source": [
    "## 1. 什麼是大型語言模型 {#1-what-is-llm}\n",
    "\n",
    "### 1.1 定義\n",
    "\n",
    "**Large Language Model (LLM)**:\n",
    "> 擁有數十億至數兆參數的 Transformer 模型，在大規模文本語料上預訓練，展現出強大的語言理解與生成能力。\n",
    "\n",
    "**關鍵特徵**:\n",
    "- **規模**: 參數量通常 > 1B (10億)\n",
    "- **架構**: 主要基於 Decoder-Only Transformer\n",
    "- **訓練數據**: 數百 GB 至 TB 級文本\n",
    "- **能力**: Few-shot learning, In-context learning, Chain-of-thought reasoning\n",
    "\n",
    "### 1.2 LLM 時間軸\n",
    "\n",
    "```\n",
    "2018 ─ GPT (117M)          └─ 證明預訓練有效性\n",
    "       BERT (340M)\n",
    "\n",
    "2019 ─ GPT-2 (1.5B)        └─ 首個 10億+ 參數模型\n",
    "       T5 (11B)\n",
    "\n",
    "2020 ─ GPT-3 (175B)        └─ Few-shot learning 突破\n",
    "\n",
    "2021 ─ Codex (12B)         └─ 專精程式碼\n",
    "       DALL-E (12B)\n",
    "\n",
    "2022 ─ ChatGPT             └─ RLHF + Instruction Tuning\n",
    "       InstructGPT\n",
    "\n",
    "2023 ─ GPT-4 (~1.8T?)      └─ Multimodal LLM\n",
    "       LLaMA (7B-65B)      └─ 開源高效 LLM\n",
    "       Mistral (7B)        └─ MoE 架構\n",
    "\n",
    "2024 ─ Gemini 1.5 Pro      └─ 超長上下文 (1M tokens)\n",
    "       Claude 3            └─ 優秀推理能力\n",
    "```\n",
    "\n",
    "### 1.3 參數規模對比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入必要套件\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['font.sans-serif'] = ['Microsoft JhengHei', 'Arial Unicode MS']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(\"✅ 套件載入完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-params-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化模型參數規模演進\n",
    "\n",
    "models_data = {\n",
    "    'Model': [\n",
    "        'BERT-Large', 'GPT', 'GPT-2', 'T5-11B', 'GPT-3', \n",
    "        'LLaMA-65B', 'PaLM', 'GPT-4 (估計)'\n",
    "    ],\n",
    "    'Parameters (B)': [0.34, 0.117, 1.5, 11, 175, 65, 540, 1800],\n",
    "    'Year': [2018, 2018, 2019, 2019, 2020, 2023, 2022, 2023],\n",
    "    'Type': ['Encoder', 'Decoder', 'Decoder', 'Enc-Dec', 'Decoder', \n",
    "             'Decoder', 'Decoder', 'Decoder']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(models_data)\n",
    "\n",
    "# 繪圖\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# 左圖: 對數尺度的參數增長\n",
    "colors = {'Encoder': 'skyblue', 'Decoder': 'lightcoral', 'Enc-Dec': 'lightgreen'}\n",
    "for model_type in df['Type'].unique():\n",
    "    subset = df[df['Type'] == model_type]\n",
    "    axes[0].scatter(\n",
    "        subset['Year'], \n",
    "        subset['Parameters (B)'], \n",
    "        s=200, \n",
    "        alpha=0.7,\n",
    "        label=model_type,\n",
    "        color=colors[model_type]\n",
    "    )\n",
    "    \n",
    "    for _, row in subset.iterrows():\n",
    "        axes[0].text(\n",
    "            row['Year'], \n",
    "            row['Parameters (B)'] * 1.3, \n",
    "            row['Model'], \n",
    "            ha='center', \n",
    "            fontsize=9\n",
    "        )\n",
    "\n",
    "axes[0].set_yscale('log')\n",
    "axes[0].set_xlabel('發布年份', fontsize=12)\n",
    "axes[0].set_ylabel('參數量 (Billions, 對數尺度)', fontsize=12)\n",
    "axes[0].set_title('LLM 參數規模演進 (2018-2023)', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 右圖: 參數量條形圖\n",
    "df_sorted = df.sort_values('Parameters (B)')\n",
    "bar_colors = [colors[t] for t in df_sorted['Type']]\n",
    "\n",
    "axes[1].barh(df_sorted['Model'], df_sorted['Parameters (B)'], color=bar_colors, alpha=0.7)\n",
    "axes[1].set_xlabel('參數量 (Billions)', fontsize=12)\n",
    "axes[1].set_title('模型參數量對比', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xscale('log')\n",
    "axes[1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "for i, (model, params) in enumerate(zip(df_sorted['Model'], df_sorted['Parameters (B)'])):\n",
    "    axes[1].text(params * 1.2, i, f'{params}B', va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n關鍵觀察:\")\n",
    "print(f\"• 2018-2023 年間，最大模型參數增長了 {1800/0.117:.0f}x 倍\")\n",
    "print(\"• Decoder-Only 架構主導 LLM 發展\")\n",
    "print(\"• GPT-4 參數量約為 GPT-3 的 10 倍\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2",
   "metadata": {},
   "source": [
    "## 2. 規模定律與湧現能力 {#2-scaling-laws}\n",
    "\n",
    "### 2.1 Scaling Laws (規模定律)\n",
    "\n",
    "**核心發現** (Kaplan et al., 2020):\n",
    "> 語言模型的效能與三個因素呈現冪律關係 (Power Law):\n",
    "> 1. 模型參數量 (N)\n",
    "> 2. 訓練數據量 (D)\n",
    "> 3. 計算量 (C)\n",
    "\n",
    "**數學形式**:\n",
    "$$\n",
    "\\text{Loss}(N) \\propto N^{-\\alpha}\n",
    "$$\n",
    "\n",
    "**實際意義**:\n",
    "- 10x 參數 → 顯著的效能提升\n",
    "- 但效能提升速度遞減 (邊際效應)\n",
    "\n",
    "### 2.2 湧現能力 (Emergent Abilities)\n",
    "\n",
    "**定義**:\n",
    "> 當模型規模超過某個臨界點時，突然展現出小模型不具備的能力。\n",
    "\n",
    "**典型湧現能力**:\n",
    "\n",
    "1. **算術推理**\n",
    "   - < 10B: 幾乎無法解決\n",
    "   - 10B-100B: 開始出現能力\n",
    "   - > 100B: 接近人類水平\n",
    "\n",
    "2. **多步推理 (Chain-of-Thought)**\n",
    "   ```\n",
    "   問題: \"Roger 有 5 顆網球。他又買了 2 罐網球，每罐 3 顆。他現在有幾顆網球?\"\n",
    "   \n",
    "   小模型 (<10B): \"7 顆\" ❌\n",
    "   \n",
    "   大模型 (>100B):\n",
    "   \"讓我們一步步思考:\n",
    "   1. Roger 原本有 5 顆網球\n",
    "   2. 他買了 2 罐，每罐 3 顆，所以買了 2 × 3 = 6 顆\n",
    "   3. 總共有 5 + 6 = 11 顆網球\" ✅\n",
    "   ```\n",
    "\n",
    "3. **Few-Shot Learning**\n",
    "   - 從少量範例中學習新任務\n",
    "   - 無需梯度更新\n",
    "\n",
    "4. **Code Generation**\n",
    "   - 從自然語言生成可執行程式碼\n",
    "\n",
    "### 2.3 視覺化湧現能力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emergent-abilities-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模擬湧現能力曲線\n",
    "\n",
    "def sigmoid_emergence(x, threshold, steepness):\n",
    "    \"\"\"模擬湧現能力的 sigmoid 曲線\"\"\"\n",
    "    return 1 / (1 + np.exp(-steepness * (x - threshold)))\n",
    "\n",
    "# 參數規模 (對數尺度)\n",
    "params = np.logspace(8, 12, 100)  # 100M to 1T\n",
    "\n",
    "# 不同能力的湧現點\n",
    "abilities = {\n",
    "    '基礎語言能力': (9.5, 2),\n",
    "    'Few-Shot Learning': (10.5, 3),\n",
    "    '算術推理': (11.0, 4),\n",
    "    'Chain-of-Thought': (11.5, 5),\n",
    "    '複雜推理': (11.8, 6)\n",
    "}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "for ability, (threshold, steepness) in abilities.items():\n",
    "    performance = sigmoid_emergence(np.log10(params), threshold, steepness)\n",
    "    ax.plot(params, performance * 100, label=ability, linewidth=2.5)\n",
    "\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('模型參數量', fontsize=13)\n",
    "ax.set_ylabel('任務表現 (%)', fontsize=13)\n",
    "ax.set_title('LLM 湧現能力: 不同規模下的表現', fontsize=15, fontweight='bold')\n",
    "ax.legend(fontsize=11, loc='upper left')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 標記重要里程碑\n",
    "milestones = [\n",
    "    (1e9, 'GPT-2\\n1.5B'),\n",
    "    (1e11, 'GPT-3\\n175B'),\n",
    "    (5e11, 'PaLM\\n540B')\n",
    "]\n",
    "\n",
    "for param, label in milestones:\n",
    "    ax.axvline(param, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax.text(param, 95, label, ha='center', fontsize=9, \n",
    "            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "ax.set_xticks([1e8, 1e9, 1e10, 1e11, 1e12])\n",
    "ax.set_xticklabels(['100M', '1B', '10B', '100B', '1T'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n湧現能力的特點:\")\n",
    "print(\"• 非線性: 能力突然爆發，而非平滑增長\")\n",
    "print(\"• 閾值效應: 存在明確的臨界規模\")\n",
    "print(\"• 不可預測: 難以預先知道何時會湧現新能力\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3",
   "metadata": {},
   "source": [
    "## 3. 從預訓練到對齊 {#3-alignment}\n",
    "\n",
    "### 3.1 LLM 訓練的三個階段\n",
    "\n",
    "```\n",
    "階段 1: 預訓練 (Pre-training)\n",
    "        ↓\n",
    "   學到語言結構與知識\n",
    "        ↓\n",
    "階段 2: 指令微調 (Instruction Tuning)\n",
    "        ↓\n",
    "   學會遵循指令\n",
    "        ↓\n",
    "階段 3: 人類反饋強化學習 (RLHF)\n",
    "        ↓\n",
    "   對齊人類價值觀\n",
    "```\n",
    "\n",
    "### 3.2 階段 1: 預訓練 (Pre-training)\n",
    "\n",
    "**目標**: 學習語言的統計結構與世界知識\n",
    "\n",
    "**訓練任務**: Causal Language Modeling\n",
    "```python\n",
    "Input:  \"The capital of France is\"\n",
    "Target: \"Paris\"\n",
    "\n",
    "Loss = -log P(\"Paris\" | \"The capital of France is\")\n",
    "```\n",
    "\n",
    "**訓練數據**:\n",
    "- Common Crawl (網頁)\n",
    "- Books (書籍)\n",
    "- Wikipedia (百科)\n",
    "- GitHub (程式碼)\n",
    "- Reddit, StackOverflow (論壇)\n",
    "\n",
    "**GPT-3 數據組成**:\n",
    "- Common Crawl: 60%\n",
    "- WebText2: 22%\n",
    "- Books: 16%\n",
    "- Wikipedia: 3%\n",
    "\n",
    "**挑戰**:\n",
    "- 計算成本極高 (數百萬美元)\n",
    "- 需要數千個 GPU/TPU\n",
    "- 訓練時間: 數週至數月\n",
    "\n",
    "### 3.3 階段 2: 指令微調 (Instruction Tuning)\n",
    "\n",
    "**目標**: 讓模型理解並遵循人類指令\n",
    "\n",
    "**資料格式**:\n",
    "```python\n",
    "# 範例 1: 問答\n",
    "Instruction: \"What is the capital of France?\"\n",
    "Response: \"The capital of France is Paris.\"\n",
    "\n",
    "# 範例 2: 翻譯\n",
    "Instruction: \"Translate to French: Hello, how are you?\"\n",
    "Response: \"Bonjour, comment allez-vous?\"\n",
    "\n",
    "# 範例 3: 程式碼\n",
    "Instruction: \"Write a Python function to calculate factorial\"\n",
    "Response: \"def factorial(n):\\n    if n == 0: return 1\\n    return n * factorial(n-1)\"\n",
    "```\n",
    "\n",
    "**資料集**:\n",
    "- **FLAN** (Google): 1800+ 任務\n",
    "- **T0** (BigScience): 多樣化 prompts\n",
    "- **Self-Instruct** (Stanford): 使用 GPT-3 生成指令數據\n",
    "\n",
    "**效果**:\n",
    "- 顯著提升遵循指令的能力\n",
    "- 改善 zero-shot 表現\n",
    "- 減少有害輸出\n",
    "\n",
    "### 3.4 階段 3: RLHF (Reinforcement Learning from Human Feedback)\n",
    "\n",
    "**核心思想**: 用人類偏好訓練獎勵模型，再用 RL 優化 LLM\n",
    "\n",
    "**流程**:\n",
    "\n",
    "```\n",
    "Step 1: 收集人類偏好數據\n",
    "   Prompt: \"寫一首關於春天的詩\"\n",
    "   Response A: [詩歌 A]\n",
    "   Response B: [詩歌 B]\n",
    "   人類標註: A > B  (A 更好)\n",
    "\n",
    "Step 2: 訓練獎勵模型 (Reward Model)\n",
    "   輸入: (Prompt, Response)\n",
    "   輸出: Reward Score (預測人類滿意度)\n",
    "\n",
    "Step 3: 用 PPO 演算法優化 LLM\n",
    "   目標: 最大化獎勵模型的分數\n",
    "   約束: 不要偏離原始模型太遠 (KL divergence penalty)\n",
    "```\n",
    "\n",
    "**數學形式**:\n",
    "$$\n",
    "\\text{Objective} = \\mathbb{E}_{x \\sim D, y \\sim \\pi_\\theta}[r(x, y)] - \\beta \\cdot D_{KL}(\\pi_\\theta || \\pi_{\\text{ref}})\n",
    "$$\n",
    "\n",
    "其中:\n",
    "- $r(x, y)$: 獎勵模型分數\n",
    "- $\\pi_\\theta$: 當前策略 (LLM)\n",
    "- $\\pi_{\\text{ref}}$: 參考策略 (原始模型)\n",
    "- $\\beta$: KL 懲罰係數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rlhf-process-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化 RLHF 流程\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "\n",
    "# 定義流程圖節點\n",
    "steps = [\n",
    "    (0.5, 0.9, \"預訓練 LLM\", 'lightblue'),\n",
    "    (0.5, 0.75, \"Instruction Tuning\", 'lightgreen'),\n",
    "    (0.2, 0.55, \"收集偏好數據\\n(A vs B)\", 'lightyellow'),\n",
    "    (0.5, 0.55, \"訓練獎勵模型\\n(Reward Model)\", 'lightcoral'),\n",
    "    (0.8, 0.55, \"生成多個回應\", 'lavender'),\n",
    "    (0.5, 0.35, \"PPO 強化學習優化\", 'lightgreen'),\n",
    "    (0.5, 0.15, \"對齊後的 LLM\\n(ChatGPT)\", 'gold')\n",
    "]\n",
    "\n",
    "for x, y, label, color in steps:\n",
    "    ax.add_patch(plt.Rectangle(\n",
    "        (x-0.12, y-0.05), 0.24, 0.08, \n",
    "        facecolor=color, edgecolor='black', linewidth=2\n",
    "    ))\n",
    "    ax.text(x, y, label, ha='center', va='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "# 繪製箭頭\n",
    "arrows = [\n",
    "    ((0.5, 0.82), (0.5, 0.75)),\n",
    "    ((0.5, 0.70), (0.2, 0.60)),\n",
    "    ((0.5, 0.70), (0.5, 0.60)),\n",
    "    ((0.5, 0.70), (0.8, 0.60)),\n",
    "    ((0.2, 0.50), (0.5, 0.40)),\n",
    "    ((0.5, 0.50), (0.5, 0.40)),\n",
    "    ((0.8, 0.50), (0.5, 0.40)),\n",
    "    ((0.5, 0.30), (0.5, 0.20))\n",
    "]\n",
    "\n",
    "for start, end in arrows:\n",
    "    ax.annotate('', xy=end, xytext=start,\n",
    "               arrowprops=dict(arrowstyle='->', lw=2, color='darkblue'))\n",
    "\n",
    "# 添加說明文字\n",
    "annotations = [\n",
    "    (0.15, 0.60, \"人類評分員\\n比較回應\", 9),\n",
    "    (0.65, 0.60, \"LLM 生成\\n候選回應\", 9),\n",
    "    (0.75, 0.35, \"最大化\\n獎勵分數\", 9, 'green'),\n",
    "    (0.25, 0.35, \"學習\\n人類偏好\", 9, 'red')\n",
    "]\n",
    "\n",
    "for x, y, text, size, *color in annotations:\n",
    "    c = color[0] if color else 'black'\n",
    "    ax.text(x, y, text, ha='center', fontsize=size, style='italic', color=c,\n",
    "           bbox=dict(boxstyle='round', facecolor='white', alpha=0.7))\n",
    "\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.axis('off')\n",
    "ax.set_title('RLHF 完整流程: 從預訓練到對齊', fontsize=16, fontweight='bold', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nRLHF 的重要性:\")\n",
    "print(\"• 使 LLM 更安全、更有用、更誠實\")\n",
    "print(\"• 減少有害、偏見、錯誤的輸出\")\n",
    "print(\"• ChatGPT 的成功關鍵之一\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4",
   "metadata": {},
   "source": [
    "## 4. Prompt Engineering 實戰 {#4-prompt-engineering}\n",
    "\n",
    "### 4.1 什麼是 Prompt Engineering?\n",
    "\n",
    "**定義**:\n",
    "> 設計和優化輸入文本 (prompt)，以引導 LLM 產生期望的輸出。\n",
    "\n",
    "**為什麼重要?**\n",
    "- LLM 對 prompt 措辭極度敏感\n",
    "- 好的 prompt 可以大幅提升表現\n",
    "- 無需微調，成本低廉\n",
    "\n",
    "### 4.2 核心技巧\n",
    "\n",
    "#### 技巧 1: Zero-Shot Prompting\n",
    "\n",
    "**基礎版**:\n",
    "```\n",
    "Prompt: \"Classify the sentiment: I love this product!\"\n",
    "Output: \"Positive\"\n",
    "```\n",
    "\n",
    "**改進版** (更明確的指令):\n",
    "```\n",
    "Prompt: \"Classify the sentiment of the following text as Positive, Negative, or Neutral.\\n\\nText: I love this product!\\nSentiment:\"\n",
    "Output: \"Positive\"\n",
    "```\n",
    "\n",
    "#### 技巧 2: Few-Shot Prompting\n",
    "\n",
    "提供範例來引導模型:\n",
    "\n",
    "```\n",
    "Classify sentiment as Positive, Negative, or Neutral.\n",
    "\n",
    "Text: \"This movie was amazing!\"\n",
    "Sentiment: Positive\n",
    "\n",
    "Text: \"I hated this book.\"\n",
    "Sentiment: Negative\n",
    "\n",
    "Text: \"The weather is okay.\"\n",
    "Sentiment: Neutral\n",
    "\n",
    "Text: \"Best purchase ever!\"\n",
    "Sentiment:\n",
    "```\n",
    "\n",
    "#### 技巧 3: Chain-of-Thought (CoT) Prompting\n",
    "\n",
    "**標準 Prompt** (表現差):\n",
    "```\n",
    "Q: Roger 有 5 顆網球。他又買了 2 罐，每罐 3 顆。他現在有幾顆?\n",
    "A: 7\n",
    "```\n",
    "\n",
    "**CoT Prompt** (表現好):\n",
    "```\n",
    "Q: Roger 有 5 顆網球。他又買了 2 罐，每罐 3 顆。他現在有幾顆?\n",
    "A: Let's think step by step.\n",
    "1. Roger 原本有 5 顆網球\n",
    "2. 他買了 2 罐，每罐 3 顆，所以是 2 × 3 = 6 顆\n",
    "3. 總共 5 + 6 = 11 顆\n",
    "答案是 11 顆。\n",
    "```\n",
    "\n",
    "**神奇的一句話**: `\"Let's think step by step.\"`\n",
    "\n",
    "#### 技巧 4: Self-Consistency\n",
    "\n",
    "生成多個推理路徑，取多數決:\n",
    "\n",
    "```\n",
    "同一問題生成 5 次 (temperature > 0):\n",
    "路徑 1: ... 答案是 11\n",
    "路徑 2: ... 答案是 11\n",
    "路徑 3: ... 答案是 7  (錯誤)\n",
    "路徑 4: ... 答案是 11\n",
    "路徑 5: ... 答案是 11\n",
    "\n",
    "最終答案: 11 (多數決)\n",
    "```\n",
    "\n",
    "#### 技巧 5: Role Prompting\n",
    "\n",
    "```\n",
    "差: \"寫一篇關於氣候變遷的文章\"\n",
    "\n",
    "好: \"你是一位專精氣候科學的資深教授。請用淺顯易懂的方式，向高中生解釋氣候變遷的原因與影響。\"\n",
    "```\n",
    "\n",
    "### 4.3 Prompt 設計原則"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prompt-principles",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 展示 Prompt Engineering 的最佳實踐\n",
    "\n",
    "principles = {\n",
    "    '原則': [\n",
    "        '1. 明確性',\n",
    "        '2. 具體性',\n",
    "        '3. 範例導向',\n",
    "        '4. 結構化輸出',\n",
    "        '5. 迭代優化'\n",
    "    ],\n",
    "    '差的 Prompt': [\n",
    "        '\"分類這個\"',\n",
    "        '\"寫點東西\"',\n",
    "        '沒有範例',\n",
    "        '\"告訴我結果\"',\n",
    "        '一次就想完美'\n",
    "    ],\n",
    "    '好的 Prompt': [\n",
    "        '\"將以下文本分類為 Positive/Negative\"',\n",
    "        '\"用 3 段落介紹 Transformer，每段 100 字\"',\n",
    "        '提供 2-3 個範例',\n",
    "        '\"以 JSON 格式輸出: {\\\"sentiment\\\": ...}\"',\n",
    "        '測試多個版本，選最佳'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_principles = pd.DataFrame(principles)\n",
    "\n",
    "# 使用 HTML 顯示表格\n",
    "print(\"=\" * 80)\n",
    "print(\"Prompt Engineering 設計原則\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "for _, row in df_principles.iterrows():\n",
    "    print(f\"{row['原則']:20s}\")\n",
    "    print(f\"  ❌ 差: {row['差的 Prompt']}\")\n",
    "    print(f\"  ✅ 好: {row['好的 Prompt']}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n核心建議:\")\n",
    "print(\"• 把 LLM 當作一個聰明但字面意義的助手\")\n",
    "print(\"• 提供上下文、範例、格式要求\")\n",
    "print(\"• 迭代測試，記錄有效的 prompts\")\n",
    "print(\"• 使用 temperature=0 保證穩定輸出\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-5",
   "metadata": {},
   "source": [
    "## 5. In-Context Learning 原理 {#5-in-context-learning}\n",
    "\n",
    "### 5.1 什麼是 In-Context Learning?\n",
    "\n",
    "**定義**:\n",
    "> LLM 能夠在推論時從 prompt 中的範例學習，執行新任務，無需更新參數。\n",
    "\n",
    "**神奇之處**:\n",
    "- 不需要梯度下降\n",
    "- 不需要微調\n",
    "- 只需要在 prompt 中提供範例\n",
    "\n",
    "**範例**:\n",
    "```python\n",
    "# 模型從未見過這個任務，但能立即學會\n",
    "Prompt = \"\"\"\n",
    "將句子轉換為過去式:\n",
    "\n",
    "I walk to school → I walked to school\n",
    "She eats an apple → She ate an apple\n",
    "They play soccer → They played soccer\n",
    "He runs fast →\n",
    "\"\"\"\n",
    "\n",
    "Output: \"He ran fast\"\n",
    "```\n",
    "\n",
    "### 5.2 工作機制假說\n",
    "\n",
    "**假說 1: 隱式微調**\n",
    "- LLM 內部執行了類似梯度下降的過程\n",
    "- 前向傳播中調整內部表示\n",
    "\n",
    "**假說 2: 模式匹配**\n",
    "- 訓練時見過類似的 few-shot 模式\n",
    "- 識別並複製該模式\n",
    "\n",
    "**假說 3: 貝葉斯推斷**\n",
    "- LLM 維護多個假說\n",
    "- 根據範例更新假說的後驗機率\n",
    "\n",
    "**最新研究**:\n",
    "- Transformer 可以實現梯度下降 (Von Oswald et al., 2023)\n",
    "- In-context learning ≈ 隱式參數更新\n",
    "\n",
    "### 5.3 影響 In-Context Learning 的因素\n",
    "\n",
    "**1. 範例數量**:\n",
    "- 0-shot < 1-shot < few-shot (通常 3-5 個最佳)\n",
    "- 但超過某個數量後邊際效益遞減\n",
    "\n",
    "**2. 範例品質**:\n",
    "- 多樣性 > 相似性\n",
    "- 代表性範例 > 極端案例\n",
    "\n",
    "**3. 範例順序**:\n",
    "- 重要! 順序會影響結果\n",
    "- 建議: 隨機化測試多種順序\n",
    "\n",
    "**4. 模型規模**:\n",
    "- < 10B: In-context learning 能力弱\n",
    "- > 100B: 顯著提升"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "icl-performance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模擬 In-Context Learning 效能與範例數量的關係\n",
    "\n",
    "def icl_performance(num_examples, model_size):\n",
    "    \"\"\"模擬 ICL 表現隨範例數增長\"\"\"\n",
    "    # 使用對數函數模擬邊際遞減\n",
    "    base_performance = 30 * (model_size / 100)  # 模型規模基準\n",
    "    improvement = 30 * np.log1p(num_examples) * (model_size / 100)\n",
    "    noise = np.random.normal(0, 2)\n",
    "    return min(95, base_performance + improvement + noise)\n",
    "\n",
    "num_examples_range = range(0, 11)\n",
    "model_sizes = [10, 100, 500]  # 10B, 100B, 500B\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "for size in model_sizes:\n",
    "    performances = [icl_performance(n, size) for n in num_examples_range]\n",
    "    ax.plot(num_examples_range, performances, marker='o', linewidth=2.5, \n",
    "            label=f'{size}B 參數', markersize=8)\n",
    "\n",
    "ax.set_xlabel('範例數量 (Few-Shot)', fontsize=13)\n",
    "ax.set_ylabel('任務準確率 (%)', fontsize=13)\n",
    "ax.set_title('In-Context Learning: 範例數量 vs 表現', fontsize=15, fontweight='bold')\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xticks(range(0, 11))\n",
    "\n",
    "# 標註關鍵區域\n",
    "ax.axvspan(-0.5, 0.5, alpha=0.1, color='red', label='Zero-Shot')\n",
    "ax.axvspan(0.5, 1.5, alpha=0.1, color='orange', label='One-Shot')\n",
    "ax.axvspan(1.5, 10.5, alpha=0.1, color='green', label='Few-Shot')\n",
    "\n",
    "ax.text(0, 25, 'Zero-Shot', ha='center', fontsize=10, style='italic')\n",
    "ax.text(1, 25, 'One-Shot', ha='center', fontsize=10, style='italic')\n",
    "ax.text(5, 25, 'Few-Shot', ha='center', fontsize=10, style='italic')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n關鍵觀察:\")\n",
    "print(\"• 大模型從 0 個範例就有不錯表現\")\n",
    "print(\"• 3-5 個範例通常是最佳平衡點\")\n",
    "print(\"• 小模型 (<10B) 幾乎無 in-context learning 能力\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-6",
   "metadata": {},
   "source": [
    "## 6. LLM 的局限性與未來 {#6-limitations}\n",
    "\n",
    "### 6.1 核心局限\n",
    "\n",
    "#### 1. 幻覺 (Hallucination)\n",
    "\n",
    "**定義**: 生成聽起來合理但實際錯誤的內容\n",
    "\n",
    "**範例**:\n",
    "```\n",
    "Q: \"誰發明了電燈泡?\"\n",
    "A: \"湯瑪斯·愛迪生於 1879 年發明了電燈泡。\" ✅ 正確\n",
    "\n",
    "Q: \"愛迪生在哪所大學獲得博士學位?\"\n",
    "A: \"愛迪生在耶魯大學獲得物理學博士學位。\" ❌ 編造 (他沒有大學學位)\n",
    "```\n",
    "\n",
    "**原因**:\n",
    "- 訓練目標是預測下一個詞，而非保證真實性\n",
    "- 知識截止日期後的資訊無法獲得\n",
    "- 訓練數據可能包含錯誤資訊\n",
    "\n",
    "**緩解方法**:\n",
    "- 檢索增強生成 (RAG)\n",
    "- 引用來源\n",
    "- 人工審核關鍵輸出\n",
    "\n",
    "#### 2. 上下文長度限制\n",
    "\n",
    "| 模型 | 上下文長度 | 可處理文本量 |\n",
    "|------|-----------|-------------|\n",
    "| GPT-3 | 2,048 tokens | ~1,500 英文單字 |\n",
    "| GPT-3.5 | 4,096 tokens | ~3,000 英文單字 |\n",
    "| GPT-4 | 8,192 / 32,768 | ~6,000 / 24,000 單字 |\n",
    "| Claude 3 | 200,000 tokens | ~150,000 單字 |\n",
    "| Gemini 1.5 | 1,000,000 tokens | ~750,000 單字 |\n",
    "\n",
    "**挑戰**:\n",
    "- 無法處理超長文檔\n",
    "- 計算複雜度 O(n²)\n",
    "\n",
    "#### 3. 推理能力限制\n",
    "\n",
    "**擅長**:\n",
    "- 模式識別\n",
    "- 簡單推理\n",
    "- 常識應用\n",
    "\n",
    "**不擅長**:\n",
    "- 複雜數學證明\n",
    "- 多步邏輯推理\n",
    "- 需要精確計算的任務\n",
    "\n",
    "**範例 (失敗案例)**:\n",
    "```\n",
    "Q: \"計算 4,567 × 8,923\"\n",
    "A: \"40,759,241\"  ❌ (正確答案: 40,755,241)\n",
    "```\n",
    "\n",
    "#### 4. 訓練成本與環境影響\n",
    "\n",
    "**GPT-3 訓練成本估計**:\n",
    "- 電力: ~1,287 MWh\n",
    "- 碳排放: ~552 噸 CO₂\n",
    "- 金錢: $4.6M USD\n",
    "\n",
    "#### 5. 偏見與安全性\n",
    "\n",
    "**問題來源**:\n",
    "- 訓練數據包含社會偏見\n",
    "- 可能生成有害內容\n",
    "- 隱私洩露風險\n",
    "\n",
    "**應對措施**:\n",
    "- RLHF 對齊\n",
    "- 內容過濾\n",
    "- 持續監控與改進\n",
    "\n",
    "### 6.2 未來趨勢\n",
    "\n",
    "**1. 多模態 LLM**\n",
    "- GPT-4, Gemini: 文本 + 圖像\n",
    "- 未來: 音訊 + 視頻 + 3D\n",
    "\n",
    "**2. 效率提升**\n",
    "- Mixture of Experts (MoE)\n",
    "- 量化 (Quantization)\n",
    "- 稀疏激活\n",
    "\n",
    "**3. 更長上下文**\n",
    "- 突破 1M tokens\n",
    "- 高效注意力機制\n",
    "\n",
    "**4. 工具使用 (Tool Use)**\n",
    "- 呼叫外部 API\n",
    "- 執行程式碼\n",
    "- 檢索即時資訊\n",
    "\n",
    "**5. 多智能體系統**\n",
    "- 多個 LLM 協作\n",
    "- 專精不同領域\n",
    "- 自我修正與辯論"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "future-trends-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化 LLM 發展趨勢\n",
    "\n",
    "trends = {\n",
    "    '趨勢': [\n",
    "        '模型規模',\n",
    "        '上下文長度',\n",
    "        '多模態能力',\n",
    "        '推理能力',\n",
    "        '效率 (成本)',\n",
    "        '安全性對齊'\n",
    "    ],\n",
    "    '2023': [85, 60, 50, 55, 40, 65],\n",
    "    '2024 (預測)': [90, 80, 75, 70, 60, 80],\n",
    "    '2025 (預測)': [92, 90, 85, 80, 75, 90]\n",
    "}\n",
    "\n",
    "df_trends = pd.DataFrame(trends)\n",
    "df_trends = df_trends.set_index('趨勢')\n",
    "\n",
    "# 繪製雷達圖\n",
    "categories = df_trends.index.tolist()\n",
    "N = len(categories)\n",
    "\n",
    "angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "angles += angles[:1]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
    "\n",
    "for year in df_trends.columns:\n",
    "    values = df_trends[year].tolist()\n",
    "    values += values[:1]\n",
    "    ax.plot(angles, values, 'o-', linewidth=2, label=year, markersize=8)\n",
    "    ax.fill(angles, values, alpha=0.15)\n",
    "\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(categories, fontsize=11)\n",
    "ax.set_ylim(0, 100)\n",
    "ax.set_yticks([20, 40, 60, 80, 100])\n",
    "ax.set_yticklabels(['20', '40', '60', '80', '100'], fontsize=9)\n",
    "ax.set_title('LLM 能力發展趨勢 (雷達圖)', fontsize=15, fontweight='bold', pad=30)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=11)\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n趨勢解讀:\")\n",
    "print(\"• 上下文長度: 快速增長 (1M+ tokens 成為可能)\")\n",
    "print(\"• 多模態: 從單純文本擴展到圖像、音訊、視頻\")\n",
    "print(\"• 效率: 透過 MoE, 量化等技術大幅降低成本\")\n",
    "print(\"• 安全性: 更強的對齊與過濾機制\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-7",
   "metadata": {},
   "source": [
    "## 7. 總結 {#7-summary}\n",
    "\n",
    "### 7.1 關鍵要點回顧\n",
    "\n",
    "✅ **LLM 的核心**:\n",
    "- 定義: > 1B 參數的 Transformer 模型\n",
    "- 架構: 主要為 Decoder-Only\n",
    "- 能力: Few-shot learning, Chain-of-thought, In-context learning\n",
    "\n",
    "✅ **規模定律**:\n",
    "- 效能 ∝ 參數量^(-α)\n",
    "- 湧現能力在特定規模突然出現\n",
    "- 但邊際效應遞減\n",
    "\n",
    "✅ **訓練三階段**:\n",
    "1. 預訓練: 學習語言與知識\n",
    "2. Instruction Tuning: 學會遵循指令\n",
    "3. RLHF: 對齊人類價值觀\n",
    "\n",
    "✅ **Prompt Engineering**:\n",
    "- 明確、具體、結構化\n",
    "- Few-shot > One-shot > Zero-shot\n",
    "- Chain-of-Thought 提升推理\n",
    "- 迭代優化是關鍵\n",
    "\n",
    "✅ **In-Context Learning**:\n",
    "- 無需梯度更新的學習\n",
    "- 3-5 個範例最佳\n",
    "- 需要 > 10B 參數才有效\n",
    "\n",
    "✅ **局限性**:\n",
    "- 幻覺 (編造事實)\n",
    "- 上下文長度限制\n",
    "- 推理能力有限\n",
    "- 訓練成本高\n",
    "- 偏見與安全問題\n",
    "\n",
    "### 7.2 實用建議\n",
    "\n",
    "**使用 LLM 的最佳實踐**:\n",
    "\n",
    "1. **選擇合適模型**:\n",
    "   - 簡單任務: GPT-3.5, Claude Instant\n",
    "   - 複雜任務: GPT-4, Claude 3\n",
    "   - 預算有限: 開源 LLaMA, Mistral\n",
    "\n",
    "2. **設計有效 Prompt**:\n",
    "   - 從簡單開始，逐步優化\n",
    "   - 記錄有效的 prompts\n",
    "   - 使用 prompt 模板庫\n",
    "\n",
    "3. **處理輸出**:\n",
    "   - 驗證關鍵事實\n",
    "   - 使用多次生成 + 投票\n",
    "   - 結合檢索系統 (RAG)\n",
    "\n",
    "4. **成本控制**:\n",
    "   - 快取常用結果\n",
    "   - 使用較小模型處理簡單任務\n",
    "   - 優化 prompt 長度\n",
    "\n",
    "### 7.3 延伸閱讀\n",
    "\n",
    "**論文**:\n",
    "1. [Language Models are Few-Shot Learners (GPT-3)](https://arxiv.org/abs/2005.14165)\n",
    "2. [Training language models to follow instructions (InstructGPT)](https://arxiv.org/abs/2203.02155)\n",
    "3. [Chain-of-Thought Prompting](https://arxiv.org/abs/2201.11903)\n",
    "4. [Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361)\n",
    "\n",
    "**資源**:\n",
    "- [OpenAI Prompt Engineering Guide](https://platform.openai.com/docs/guides/prompt-engineering)\n",
    "- [Anthropic Claude Documentation](https://docs.anthropic.com/)\n",
    "- [Awesome LLM (GitHub)](https://github.com/Hannibal046/Awesome-LLM)\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 下一節預告\n",
    "\n",
    "**CH07-08: LLM 實際應用案例**\n",
    "- RAG (檢索增強生成) 實作\n",
    "- LangChain 框架使用\n",
    "- Agent 系統設計\n",
    "- 使用 Hugging Face API 呼叫開源 LLM\n",
    "- 生產環境部署考量\n",
    "\n",
    "---\n",
    "\n",
    "**課程完成時間**: `____年____月____日`  \n",
    "**學習心得**: ___________________________________"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
