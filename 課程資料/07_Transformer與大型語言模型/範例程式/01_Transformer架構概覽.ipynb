{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH07-01: Transformer æ¶æ§‹æ¦‚è¦½\n",
    "\n",
    "**èª²ç¨‹ç›®æ¨™:**\n",
    "- ç†è§£ Transformer çš„èª•ç”ŸèƒŒæ™¯èˆ‡é©å‘½æ€§æ„ç¾©\n",
    "- æŒæ¡ Transformer çš„æ•´é«”æ¶æ§‹èˆ‡æ ¸å¿ƒçµ„ä»¶\n",
    "- äº†è§£ Self-Attention æ©Ÿåˆ¶çš„ç›´è§€æ¦‚å¿µ\n",
    "- èªè­˜ Transformer åœ¨ NLP é ˜åŸŸçš„é‡è¦æ‡‰ç”¨\n",
    "\n",
    "**å­¸ç¿’æ™‚é–“:** ç´„ 90 åˆ†é˜\n",
    "\n",
    "**å‰ç½®çŸ¥è­˜:**\n",
    "- åŸºç¤ç¥ç¶“ç¶²è·¯ (MLP)\n",
    "- åºåˆ—æ¨¡å‹ (RNN/LSTM)\n",
    "- è©å‘é‡ (Word Embeddings)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š ç›®éŒ„\n",
    "\n",
    "1. [ç‚ºä»€éº¼éœ€è¦ Transformer?](#1)\n",
    "2. [Transformer æ¶æ§‹å…¨è²Œ](#2)\n",
    "3. [æ ¸å¿ƒçµ„ä»¶ä»‹ç´¹](#3)\n",
    "4. [Attention æ©Ÿåˆ¶ç›´è§€ç†è§£](#4)\n",
    "5. [Transformer çš„å„ªå‹¢èˆ‡æ‡‰ç”¨](#5)\n",
    "6. [å¯¦æˆ°ç·´ç¿’](#6)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç’°å¢ƒè¨­å®šèˆ‡å¥—ä»¶å°å…¥\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# è¨­å®šä¸­æ–‡é¡¯ç¤º\n",
    "plt.rcParams['font.sans-serif'] = ['Microsoft JhengHei', 'SimHei', 'Arial Unicode MS']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# è¨­å®šé¡¯ç¤ºé¢¨æ ¼\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"âœ… ç’°å¢ƒè¨­å®šå®Œæˆ\")\n",
    "print(f\"NumPy ç‰ˆæœ¬: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "## 1. ç‚ºä»€éº¼éœ€è¦ Transformer?\n",
    "\n",
    "### 1.1 å‚³çµ±åºåˆ—æ¨¡å‹çš„æŒ‘æˆ°\n",
    "\n",
    "åœ¨ Transformer å‡ºç¾ä¹‹å‰,NLP é ˜åŸŸä¸»è¦ä½¿ç”¨ **RNN** å’Œ **LSTM** è™•ç†åºåˆ—æ•¸æ“šã€‚\n",
    "\n",
    "#### RNN/LSTM çš„ä¸‰å¤§é™åˆ¶:\n",
    "\n",
    "1. **ç„¡æ³•ä¸¦è¡ŒåŒ– (Sequential Processing)**\n",
    "   - å¿…é ˆæŒ‰é †åºé€æ­¥è™•ç†,ç„¡æ³•åˆ©ç”¨ GPU ä¸¦è¡ŒåŠ é€Ÿ\n",
    "   - è¨“ç·´é€Ÿåº¦æ…¢,é›£ä»¥æ“´å±•åˆ°å¤§å‹æ•¸æ“šé›†\n",
    "\n",
    "2. **é•·è·é›¢ä¾è³´å•é¡Œ (Long-Range Dependencies)**\n",
    "   - å„˜ç®¡ LSTM ä½¿ç”¨é–€æ§æ©Ÿåˆ¶,ä½†è™•ç†è¶…é•·åºåˆ—ä»åŠ›ä¸å¾å¿ƒ\n",
    "   - æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸å•é¡Œ\n",
    "\n",
    "3. **å›ºå®šä¸Šä¸‹æ–‡çª—å£ (Fixed Context Window)**\n",
    "   - éš±è—ç‹€æ…‹æœ‰é™,é›£ä»¥æ•æ‰å…¨å±€è¨Šæ¯\n",
    "   - å®¹æ˜“éºå¿˜æ—©æœŸé‡è¦è¨Šæ¯\n",
    "\n",
    "---\n",
    "\n",
    "### 1.2 Transformer çš„é©å‘½æ€§çªç ´\n",
    "\n",
    "**è«–æ–‡:** *Attention is All You Need* (Vaswani et al., 2017)\n",
    "\n",
    "**æ ¸å¿ƒæ€æƒ³:** å®Œå…¨æ‹‹æ£„å¾ªç’°çµæ§‹,ç´”ç²¹ä½¿ç”¨ **Self-Attention æ©Ÿåˆ¶**!\n",
    "\n",
    "#### Transformer çš„ä¸‰å¤§å„ªå‹¢:\n",
    "\n",
    "1. âœ… **å®Œå…¨ä¸¦è¡ŒåŒ–** - æ‰€æœ‰ä½ç½®åŒæ™‚è¨ˆç®—,è¨“ç·´é€Ÿåº¦å¿« 10-100 å€\n",
    "2. âœ… **å…¨å±€è¦–é‡** - æ¯å€‹è©å¯ä»¥ç›´æ¥é—œæ³¨åºåˆ—ä¸­çš„ä»»ä½•å…¶ä»–è©\n",
    "3. âœ… **å¯æ“´å±•æ€§** - å¯è¼•é¬†æ“´å±•åˆ°æ•¸åå„„åƒæ•¸ (GPT-3: 175B)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¦–è¦ºåŒ–: RNN vs Transformer è¨“ç·´é€Ÿåº¦å°æ¯”\n",
    "sequence_lengths = [50, 100, 200, 500, 1000]\n",
    "rnn_time = [1, 4, 16, 100, 400]  # æ¨¡æ“¬è¨“ç·´æ™‚é–“ (ç§’)\n",
    "transformer_time = [2, 3, 4, 6, 10]  # ä¸¦è¡ŒåŒ–å„ªå‹¢\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(sequence_lengths, rnn_time, 'o-', label='RNN/LSTM (Sequential)', linewidth=2, markersize=8)\n",
    "plt.plot(sequence_lengths, transformer_time, 's-', label='Transformer (Parallel)', linewidth=2, markersize=8)\n",
    "plt.xlabel('Sequence Length', fontsize=12)\n",
    "plt.ylabel('Training Time (seconds)', fontsize=12)\n",
    "plt.title('RNN vs Transformer è¨“ç·´é€Ÿåº¦å°æ¯” (æ¨¡æ“¬)', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ“Š è§€å¯Ÿ:\")\n",
    "print(\"  - RNN: åºåˆ—è¶Šé•·,è¨“ç·´æ™‚é–“æŒ‡æ•¸å¢é•·\")\n",
    "print(\"  - Transformer: åºåˆ—é•·åº¦å½±éŸ¿å°,å¯ä¸¦è¡Œè™•ç†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "## 2. Transformer æ¶æ§‹å…¨è²Œ\n",
    "\n",
    "### 2.1 æ•´é«”æ¶æ§‹\n",
    "\n",
    "Transformer æ¡ç”¨ç¶“å…¸çš„ **Encoder-Decoder** çµæ§‹:\n",
    "\n",
    "```\n",
    "è¼¸å…¥åºåˆ— (Input)\n",
    "    â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   Encoder (ç·¨ç¢¼å™¨)        â”‚  â† ç†è§£è¼¸å…¥èªç¾©\n",
    "â”‚   - N å±¤å †ç–Š              â”‚\n",
    "â”‚   - Self-Attention       â”‚\n",
    "â”‚   - Feed Forward         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "    â†“ (Context Vector)\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   Decoder (è§£ç¢¼å™¨)        â”‚  â† ç”Ÿæˆè¼¸å‡ºåºåˆ—\n",
    "â”‚   - N å±¤å †ç–Š              â”‚\n",
    "â”‚   - Masked Self-Attentionâ”‚\n",
    "â”‚   - Cross-Attention      â”‚\n",
    "â”‚   - Feed Forward         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "    â†“\n",
    "è¼¸å‡ºåºåˆ— (Output)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2.2 æ ¸å¿ƒçµ„ä»¶è§£æ\n",
    "\n",
    "#### 2.2.1 Encoder å±¤çµæ§‹\n",
    "\n",
    "æ¯å€‹ Encoder å±¤åŒ…å«å…©å€‹å­å±¤:\n",
    "\n",
    "1. **Multi-Head Self-Attention**\n",
    "   - è®“æ¯å€‹è©é—œæ³¨åºåˆ—ä¸­çš„æ‰€æœ‰è©\n",
    "   - å¤šé ­æ©Ÿåˆ¶æ•æ‰ä¸åŒé—œä¿‚ (èªæ³•ã€èªç¾©ç­‰)\n",
    "\n",
    "2. **Position-wise Feed-Forward Network**\n",
    "   - å…©å±¤å…¨é€£æ¥ç¶²è·¯\n",
    "   - å°æ¯å€‹ä½ç½®ç¨ç«‹è™•ç†\n",
    "\n",
    "**æ¯å€‹å­å±¤å¾Œéƒ½æœ‰:**\n",
    "- Residual Connection (æ®˜å·®é€£æ¥)\n",
    "- Layer Normalization (å±¤æ­¸ä¸€åŒ–)\n",
    "\n",
    "---\n",
    "\n",
    "#### 2.2.2 Decoder å±¤çµæ§‹\n",
    "\n",
    "æ¯å€‹ Decoder å±¤åŒ…å«ä¸‰å€‹å­å±¤:\n",
    "\n",
    "1. **Masked Multi-Head Self-Attention**\n",
    "   - é˜²æ­¢çœ‹åˆ°æœªä¾†çš„è© (è‡ªå›æ­¸ç”Ÿæˆ)\n",
    "\n",
    "2. **Cross-Attention (Encoder-Decoder Attention)**\n",
    "   - Decoder é—œæ³¨ Encoder çš„è¼¸å‡º\n",
    "   - é€£æ¥è¼¸å…¥èˆ‡è¼¸å‡ºçš„æ©‹æ¨‘\n",
    "\n",
    "3. **Position-wise Feed-Forward Network**\n",
    "   - èˆ‡ Encoder ç›¸åŒ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¦–è¦ºåŒ–: Transformer æ¶æ§‹å±¤ç´šçµæ§‹\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "ax.set_xlim(0, 10)\n",
    "ax.set_ylim(0, 12)\n",
    "ax.axis('off')\n",
    "\n",
    "# Encoder å€å¡Š\n",
    "encoder_rect = mpatches.Rectangle((0.5, 6), 4, 5, \n",
    "                                   linewidth=2, edgecolor='blue', facecolor='lightblue', alpha=0.3)\n",
    "ax.add_patch(encoder_rect)\n",
    "ax.text(2.5, 10.5, 'ENCODER', ha='center', fontsize=16, fontweight='bold', color='darkblue')\n",
    "\n",
    "# Encoder å­å±¤\n",
    "ax.text(2.5, 9.5, 'Multi-Head\\nSelf-Attention', ha='center', fontsize=11, \n",
    "        bbox=dict(boxstyle='round', facecolor='white', edgecolor='blue'))\n",
    "ax.text(2.5, 8, 'Add & Norm', ha='center', fontsize=9, style='italic')\n",
    "ax.text(2.5, 7.2, 'Feed Forward\\nNetwork', ha='center', fontsize=11,\n",
    "        bbox=dict(boxstyle='round', facecolor='white', edgecolor='blue'))\n",
    "ax.text(2.5, 6.4, 'Add & Norm', ha='center', fontsize=9, style='italic')\n",
    "\n",
    "# Decoder å€å¡Š\n",
    "decoder_rect = mpatches.Rectangle((5.5, 6), 4, 5, \n",
    "                                   linewidth=2, edgecolor='green', facecolor='lightgreen', alpha=0.3)\n",
    "ax.add_patch(decoder_rect)\n",
    "ax.text(7.5, 10.5, 'DECODER', ha='center', fontsize=16, fontweight='bold', color='darkgreen')\n",
    "\n",
    "# Decoder å­å±¤\n",
    "ax.text(7.5, 9.7, 'Masked\\nSelf-Attention', ha='center', fontsize=10,\n",
    "        bbox=dict(boxstyle='round', facecolor='white', edgecolor='green'))\n",
    "ax.text(7.5, 9, 'Add & Norm', ha='center', fontsize=9, style='italic')\n",
    "ax.text(7.5, 8.2, 'Cross-Attention', ha='center', fontsize=10,\n",
    "        bbox=dict(boxstyle='round', facecolor='white', edgecolor='green'))\n",
    "ax.text(7.5, 7.5, 'Add & Norm', ha='center', fontsize=9, style='italic')\n",
    "ax.text(7.5, 6.7, 'Feed Forward', ha='center', fontsize=10,\n",
    "        bbox=dict(boxstyle='round', facecolor='white', edgecolor='green'))\n",
    "\n",
    "# è¼¸å…¥èˆ‡è¼¸å‡º\n",
    "ax.text(2.5, 5.3, 'Input Embedding\\n+\\nPositional Encoding', ha='center', fontsize=10,\n",
    "        bbox=dict(boxstyle='round', facecolor='lightyellow'))\n",
    "ax.text(7.5, 5.3, 'Output Embedding\\n+\\nPositional Encoding', ha='center', fontsize=10,\n",
    "        bbox=dict(boxstyle='round', facecolor='lightyellow'))\n",
    "\n",
    "# ç®­é ­\n",
    "ax.arrow(2.5, 5.8, 0, 0.15, head_width=0.2, head_length=0.1, fc='black', ec='black')\n",
    "ax.arrow(7.5, 5.8, 0, 0.15, head_width=0.2, head_length=0.1, fc='black', ec='black')\n",
    "ax.arrow(4.5, 8.5, 0.9, 0, head_width=0.15, head_length=0.1, fc='red', ec='red', linestyle='--')\n",
    "ax.text(5, 8.8, 'Context', ha='center', fontsize=9, color='red')\n",
    "\n",
    "# æ¨™é¡Œ\n",
    "ax.text(5, 11.5, 'Transformer æ¶æ§‹å…¨è²Œ', ha='center', fontsize=18, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ” æ¶æ§‹é‡é»:\")\n",
    "print(\"  1. Encoder: å°‡è¼¸å…¥ç·¨ç¢¼ç‚ºèªç¾©è¡¨ç¤º\")\n",
    "print(\"  2. Decoder: åŸºæ–¼ç·¨ç¢¼ç”Ÿæˆè¼¸å‡ºåºåˆ—\")\n",
    "print(\"  3. Cross-Attention: Encoder èˆ‡ Decoder çš„æ©‹æ¨‘\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "## 3. æ ¸å¿ƒçµ„ä»¶æ·±å…¥\n",
    "\n",
    "### 3.1 ä½ç½®ç·¨ç¢¼ (Positional Encoding)\n",
    "\n",
    "**å•é¡Œ:** Attention æ©Ÿåˆ¶æ˜¯**ç„¡åºçš„** (permutation-invariant),ç„¡æ³•å€åˆ†è©çš„é †åº!\n",
    "\n",
    "**è§£æ±ºæ–¹æ¡ˆ:** åœ¨è©å‘é‡ä¸­åŠ å…¥ä½ç½®è¨Šæ¯\n",
    "\n",
    "#### æ­£å¼¦ä½ç½®ç·¨ç¢¼å…¬å¼:\n",
    "\n",
    "$$\n",
    "\\text{PE}_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{PE}_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)\n",
    "$$\n",
    "\n",
    "å…¶ä¸­:\n",
    "- `pos`: è©åœ¨åºåˆ—ä¸­çš„ä½ç½® (0, 1, 2, ...)\n",
    "- `i`: ç¶­åº¦ç´¢å¼• (0, 1, 2, ..., d_model/2)\n",
    "- `d_model`: è©å‘é‡ç¶­åº¦ (è«–æ–‡ä¸­ç‚º 512)\n",
    "\n",
    "**å„ªé»:**\n",
    "- ä¸åŒä½ç½®æœ‰ç¨ç‰¹çš„ç·¨ç¢¼\n",
    "- ç›¸å°ä½ç½®é—œä¿‚å¯å­¸ç¿’\n",
    "- å¯è™•ç†ä»»æ„é•·åº¦åºåˆ—\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¦ä½œä¸¦è¦–è¦ºåŒ–ä½ç½®ç·¨ç¢¼\n",
    "def get_positional_encoding(max_seq_len, d_model):\n",
    "    \"\"\"\n",
    "    ç”Ÿæˆæ­£å¼¦ä½ç½®ç·¨ç¢¼\n",
    "    \n",
    "    Args:\n",
    "        max_seq_len: æœ€å¤§åºåˆ—é•·åº¦\n",
    "        d_model: è©å‘é‡ç¶­åº¦\n",
    "        \n",
    "    Returns:\n",
    "        pos_encoding: shape (max_seq_len, d_model)\n",
    "    \"\"\"\n",
    "    pos_encoding = np.zeros((max_seq_len, d_model))\n",
    "    \n",
    "    for pos in range(max_seq_len):\n",
    "        for i in range(0, d_model, 2):\n",
    "            # Sin for even indices\n",
    "            pos_encoding[pos, i] = np.sin(pos / (10000 ** (i / d_model)))\n",
    "            \n",
    "            # Cos for odd indices\n",
    "            if i + 1 < d_model:\n",
    "                pos_encoding[pos, i + 1] = np.cos(pos / (10000 ** (i / d_model)))\n",
    "    \n",
    "    return pos_encoding\n",
    "\n",
    "# ç”Ÿæˆä½ç½®ç·¨ç¢¼\n",
    "max_len = 100\n",
    "d_model = 512\n",
    "pos_enc = get_positional_encoding(max_len, d_model)\n",
    "\n",
    "print(f\"ä½ç½®ç·¨ç¢¼çŸ©é™£å½¢ç‹€: {pos_enc.shape}\")\n",
    "print(f\"å‰ 3 å€‹ä½ç½®çš„å‰ 8 ç¶­ç·¨ç¢¼:\\n{pos_enc[:3, :8]}\")\n",
    "\n",
    "# è¦–è¦ºåŒ–ä½ç½®ç·¨ç¢¼\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(pos_enc[:50, :128].T, cmap='RdBu', aspect='auto')\n",
    "plt.colorbar(label='Encoding Value')\n",
    "plt.xlabel('Position in Sequence', fontsize=12)\n",
    "plt.ylabel('Embedding Dimension', fontsize=12)\n",
    "plt.title('æ­£å¼¦ä½ç½®ç·¨ç¢¼è¦–è¦ºåŒ– (å‰ 50 ä½ç½®, å‰ 128 ç¶­åº¦)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ¨ è§€å¯Ÿ:\")\n",
    "print(\"  - æ¯å€‹ä½ç½®æœ‰ç¨ç‰¹çš„ç·¨ç¢¼æ¨¡å¼\")\n",
    "print(\"  - ä½ç¶­åº¦è®ŠåŒ–å¿« (é«˜é »),é«˜ç¶­åº¦è®ŠåŒ–æ…¢ (ä½é »)\")\n",
    "print(\"  - ç›¸é„°ä½ç½®ç·¨ç¢¼ç›¸ä¼¼,ä½†ä¸å®Œå…¨ç›¸åŒ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 æ®˜å·®é€£æ¥èˆ‡å±¤æ­¸ä¸€åŒ–\n",
    "\n",
    "#### 3.2.1 æ®˜å·®é€£æ¥ (Residual Connection)\n",
    "\n",
    "$$\n",
    "\\text{Output} = \\text{SubLayer}(x) + x\n",
    "$$\n",
    "\n",
    "**ä½œç”¨:**\n",
    "- ç·©è§£æ¢¯åº¦æ¶ˆå¤±å•é¡Œ\n",
    "- è®“è¨Šæ¯æ›´å®¹æ˜“å‚³é\n",
    "- åŠ é€Ÿè¨“ç·´æ”¶æ–‚\n",
    "\n",
    "---\n",
    "\n",
    "#### 3.2.2 å±¤æ­¸ä¸€åŒ– (Layer Normalization)\n",
    "\n",
    "$$\n",
    "\\text{LayerNorm}(x) = \\gamma \\cdot \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta\n",
    "$$\n",
    "\n",
    "**ä½œç”¨:**\n",
    "- ç©©å®šè¨“ç·´éç¨‹\n",
    "- åŠ é€Ÿæ”¶æ–‚\n",
    "- æ¸›å°‘å…§éƒ¨å”è®Šé‡åç§»\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "## 4. Attention æ©Ÿåˆ¶ç›´è§€ç†è§£\n",
    "\n",
    "### 4.1 ä»€éº¼æ˜¯ Attention?\n",
    "\n",
    "**æ ¸å¿ƒæ€æƒ³:** è®“æ¨¡å‹\"é—œæ³¨\"æœ€ç›¸é—œçš„éƒ¨åˆ†\n",
    "\n",
    "#### äººé¡é–±è®€çš„ä¾‹å­:\n",
    "\n",
    "å¥å­: *\"The **animal** didn't cross the street because **it** was too tired.\"*\n",
    "\n",
    "å•é¡Œ: \"it\" æŒ‡çš„æ˜¯ä»€éº¼?\n",
    "\n",
    "äººé¡æœƒè‡ªå‹•é—œæ³¨:\n",
    "- \"it\" â† **é«˜åº¦é—œæ³¨** â† \"animal\"\n",
    "- \"it\" â† ä½åº¦é—œæ³¨ â† \"street\"\n",
    "\n",
    "**é€™å°±æ˜¯ Attention!**\n",
    "\n",
    "---\n",
    "\n",
    "### 4.2 Self-Attention æ©Ÿåˆ¶\n",
    "\n",
    "**ä¸‰å€‹é—œéµçŸ©é™£:**\n",
    "\n",
    "1. **Query (Q)**: \"æˆ‘æƒ³æŸ¥è©¢ä»€éº¼?\"\n",
    "2. **Key (K)**: \"æˆ‘æ˜¯ä»€éº¼?\"\n",
    "3. **Value (V)**: \"æˆ‘æœ‰ä»€éº¼è¨Šæ¯?\"\n",
    "\n",
    "#### Attention è¨ˆç®—å…¬å¼:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V\n",
    "$$\n",
    "\n",
    "**æ­¥é©Ÿè§£æ:**\n",
    "1. è¨ˆç®— Query èˆ‡ Key çš„ç›¸ä¼¼åº¦: $QK^T$\n",
    "2. ç¸®æ”¾ (é¿å…æ•¸å€¼éå¤§): $\\frac{QK^T}{\\sqrt{d_k}}$\n",
    "3. Softmax æ­¸ä¸€åŒ–ç‚ºæ¦‚ç‡åˆ†å¸ƒ\n",
    "4. åŠ æ¬Šçµ„åˆ Value: $\\text{softmax}(...) \\cdot V$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç°¡å–®çš„ Self-Attention å¯¦ä½œç¤ºä¾‹\n",
    "def simple_self_attention(X, d_k):\n",
    "    \"\"\"\n",
    "    ç°¡åŒ–ç‰ˆ Self-Attention æ©Ÿåˆ¶\n",
    "    \n",
    "    Args:\n",
    "        X: è¼¸å…¥çŸ©é™£ (seq_len, d_model)\n",
    "        d_k: Key ç¶­åº¦ (ç”¨æ–¼ç¸®æ”¾)\n",
    "        \n",
    "    Returns:\n",
    "        output: Attention è¼¸å‡º\n",
    "        attention_weights: Attention æ¬Šé‡çŸ©é™£\n",
    "    \"\"\"\n",
    "    # ç°¡åŒ–: ç›´æ¥ä½¿ç”¨ X ä½œç‚º Q, K, V (å¯¦éš›æ‡‰ç”¨éœ€æŠ•å½±çŸ©é™£)\n",
    "    Q = K = V = X\n",
    "    \n",
    "    # Step 1: è¨ˆç®—ç›¸ä¼¼åº¦ (Dot Product)\n",
    "    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n",
    "    \n",
    "    # Step 2: Softmax æ­¸ä¸€åŒ–\n",
    "    attention_weights = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)\n",
    "    \n",
    "    # Step 3: åŠ æ¬Šçµ„åˆ Value\n",
    "    output = np.dot(attention_weights, V)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "# æ¸¬è©¦ç¯„ä¾‹\n",
    "np.random.seed(42)\n",
    "seq_len = 5\n",
    "d_model = 8\n",
    "d_k = d_model\n",
    "\n",
    "# æ¨¡æ“¬è¼¸å…¥åºåˆ—\n",
    "X = np.random.randn(seq_len, d_model)\n",
    "\n",
    "# è¨ˆç®— Attention\n",
    "output, attn_weights = simple_self_attention(X, d_k)\n",
    "\n",
    "print(f\"è¼¸å…¥å½¢ç‹€: {X.shape}\")\n",
    "print(f\"è¼¸å‡ºå½¢ç‹€: {output.shape}\")\n",
    "print(f\"Attention æ¬Šé‡å½¢ç‹€: {attn_weights.shape}\")\n",
    "print(f\"\\nAttention æ¬Šé‡çŸ©é™£:\\n{attn_weights}\")\n",
    "\n",
    "# è¦–è¦ºåŒ– Attention æ¬Šé‡\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(attn_weights, annot=True, fmt='.2f', cmap='YlOrRd', \n",
    "            xticklabels=[f'ä½ç½® {i+1}' for i in range(seq_len)],\n",
    "            yticklabels=[f'ä½ç½® {i+1}' for i in range(seq_len)],\n",
    "            cbar_kws={'label': 'Attention Score'})\n",
    "plt.xlabel('Key (è¢«é—œæ³¨çš„ä½ç½®)', fontsize=12)\n",
    "plt.ylabel('Query (æŸ¥è©¢ä½ç½®)', fontsize=12)\n",
    "plt.title('Self-Attention æ¬Šé‡çŸ©é™£è¦–è¦ºåŒ–', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ’¡ è§£è®€:\")\n",
    "print(\"  - æ¯ä¸€è¡Œç¸½å’Œç‚º 1 (Softmax ç‰¹æ€§)\")\n",
    "print(\"  - é¡è‰²è¶Šæ·±è¡¨ç¤ºé—œæ³¨åº¦è¶Šé«˜\")\n",
    "print(\"  - å°è§’ç·šå€¼è¼ƒé«˜: è‡ªå·±é—œæ³¨è‡ªå·±\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Multi-Head Attention\n",
    "\n",
    "**æ ¸å¿ƒæ€æƒ³:** ä½¿ç”¨å¤šå€‹ Attention \"é ­\",å„è‡ªå­¸ç¿’ä¸åŒé—œä¿‚\n",
    "\n",
    "#### ç‚ºä»€éº¼éœ€è¦å¤šé ­?\n",
    "\n",
    "ä¸€å€‹å¥å­ä¸­å¯èƒ½æœ‰å¤šç¨®é—œä¿‚:\n",
    "- **èªæ³•é—œä¿‚:** ä¸»è©-å‹•è©-å—è©\n",
    "- **èªç¾©é—œä¿‚:** è¿‘ç¾©è©ã€åç¾©è©\n",
    "- **æŒ‡ä»£é—œä¿‚:** ä»£åè©æŒ‡æ¶‰\n",
    "\n",
    "**å¤šé ­è®“æ¨¡å‹åŒæ™‚å­¸ç¿’é€™äº›ä¸åŒé—œä¿‚!**\n",
    "\n",
    "#### å¤šé ­ Attention å…¬å¼:\n",
    "\n",
    "$$\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h) W^O\n",
    "$$\n",
    "\n",
    "å…¶ä¸­æ¯å€‹é ­:\n",
    "\n",
    "$$\n",
    "\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n",
    "$$\n",
    "\n",
    "**è«–æ–‡è¨­å®š:** 8 å€‹é ­,æ¯å€‹é ­ç¶­åº¦ 64 (ç¸½ç¶­åº¦ 512)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "## 5. Transformer çš„å„ªå‹¢èˆ‡æ‡‰ç”¨\n",
    "\n",
    "### 5.1 Transformer ä¸‰å¤§å„ªå‹¢\n",
    "\n",
    "| ç‰¹æ€§ | RNN/LSTM | Transformer |\n",
    "|------|----------|-------------|\n",
    "| **ä¸¦è¡ŒåŒ–** | âŒ åªèƒ½ä¸²è¡Œè™•ç† | âœ… å®Œå…¨ä¸¦è¡Œ |\n",
    "| **é•·è·é›¢ä¾è³´** | âš ï¸ å—é™æ–¼éš±è—ç‹€æ…‹ | âœ… ç›´æ¥é€£æ¥æ‰€æœ‰ä½ç½® |\n",
    "| **è¨“ç·´é€Ÿåº¦** | ğŸŒ æ…¢ | ğŸš€ å¿« 10-100 å€ |\n",
    "| **å¯æ“´å±•æ€§** | âš ï¸ é›£ä»¥æ“´å±• | âœ… å¯æ“´å±•åˆ°æ•¸åå„„åƒæ•¸ |\n",
    "| **å…¨å±€è¦–é‡** | âŒ æœ‰é™ä¸Šä¸‹æ–‡çª—å£ | âœ… å…¨åºåˆ—è¦–é‡ |\n",
    "\n",
    "---\n",
    "\n",
    "### 5.2 Transformer ä¸‰å¤§æ¨¡å‹å®¶æ—\n",
    "\n",
    "#### 5.2.1 Encoder-Only (BERT ç³»åˆ—)\n",
    "\n",
    "**ä»£è¡¨:** BERT, RoBERTa, ALBERT\n",
    "\n",
    "**ç‰¹é»:**\n",
    "- é›™å‘ Attention (å¯çœ‹åˆ°æ•´å€‹å¥å­)\n",
    "- é©åˆç†è§£ä»»å‹™\n",
    "\n",
    "**æ‡‰ç”¨:**\n",
    "- æ–‡æœ¬åˆ†é¡\n",
    "- å‘½åå¯¦é«”è­˜åˆ¥ (NER)\n",
    "- å•ç­”ç³»çµ±\n",
    "\n",
    "---\n",
    "\n",
    "#### 5.2.2 Decoder-Only (GPT ç³»åˆ—)\n",
    "\n",
    "**ä»£è¡¨:** GPT-2, GPT-3, GPT-4, Claude\n",
    "\n",
    "**ç‰¹é»:**\n",
    "- å–®å‘ Attention (åªçœ‹å‰æ–‡)\n",
    "- è‡ªå›æ­¸ç”Ÿæˆ\n",
    "\n",
    "**æ‡‰ç”¨:**\n",
    "- æ–‡æœ¬ç”Ÿæˆ\n",
    "- å°è©±ç³»çµ±\n",
    "- ç¨‹å¼ç¢¼ç”Ÿæˆ\n",
    "\n",
    "---\n",
    "\n",
    "#### 5.2.3 Encoder-Decoder (T5 ç³»åˆ—)\n",
    "\n",
    "**ä»£è¡¨:** T5, BART, mT5\n",
    "\n",
    "**ç‰¹é»:**\n",
    "- å®Œæ•´ Transformer æ¶æ§‹\n",
    "- è¼¸å…¥â†’è¼¸å‡ºè½‰æ›\n",
    "\n",
    "**æ‡‰ç”¨:**\n",
    "- æ©Ÿå™¨ç¿»è­¯\n",
    "- æ–‡æœ¬æ‘˜è¦\n",
    "- å•ç­”ç”Ÿæˆ\n",
    "\n",
    "---\n",
    "\n",
    "### 5.3 Transformer çš„å½±éŸ¿åŠ›\n",
    "\n",
    "**2017-2024 NLP ç™¼å±•å²:**\n",
    "\n",
    "```\n",
    "2017 â†’ Transformer è«–æ–‡ç™¼è¡¨ (\"Attention is All You Need\")\n",
    "2018 â†’ BERT æ©«ç©ºå‡ºä¸– (åˆ·æ–° 11 é … NLP ç´€éŒ„)\n",
    "2019 â†’ GPT-2 å±•ç¤ºå¼·å¤§ç”Ÿæˆèƒ½åŠ›\n",
    "2020 â†’ GPT-3 (175B åƒæ•¸) é©šè±”ä¸–ç•Œ\n",
    "2022 â†’ ChatGPT å¼•çˆ† AI ç†±æ½®\n",
    "2023 â†’ GPT-4, Claude, LLaMA ç™¾èŠ±é½Šæ”¾\n",
    "2024 â†’ Transformer æ‡‰ç”¨å»¶ä¼¸è‡³è¦–è¦ºã€å¤šæ¨¡æ…‹\n",
    "```\n",
    "\n",
    "**çµè«–:** Transformer å¾¹åº•æ”¹è®Šäº† AI é ˜åŸŸ!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a>\n",
    "## 6. å¯¦æˆ°ç·´ç¿’\n",
    "\n",
    "### ç·´ç¿’ 1: ä½ç½®ç·¨ç¢¼å¯¦é©—\n",
    "\n",
    "ä¿®æ”¹ä½ç½®ç·¨ç¢¼å‡½æ•¸,å˜—è©¦ä¸åŒçš„ç·¨ç¢¼æ–¹å¼,è§€å¯Ÿè¦–è¦ºåŒ–å·®ç•°ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç·´ç¿’ 1: å¯¦ä½œå¯å­¸ç¿’çš„ä½ç½®ç·¨ç¢¼\n",
    "def learned_positional_encoding(max_seq_len, d_model):\n",
    "    \"\"\"\n",
    "    å¯å­¸ç¿’çš„ä½ç½®ç·¨ç¢¼ (éš¨æ©Ÿåˆå§‹åŒ–,å¯¦éš›è¨“ç·´æ™‚æœƒæ›´æ–°)\n",
    "    \"\"\"\n",
    "    # TODO: ä½¿ç”¨éš¨æ©Ÿåˆå§‹åŒ–æ›¿ä»£æ­£å¼¦å‡½æ•¸\n",
    "    return np.random.randn(max_seq_len, d_model) * 0.1\n",
    "\n",
    "# æ¯”è¼ƒå…©ç¨®ç·¨ç¢¼\n",
    "sinusoidal_pe = get_positional_encoding(50, 128)\n",
    "learned_pe = learned_positional_encoding(50, 128)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# æ­£å¼¦ç·¨ç¢¼\n",
    "im1 = axes[0].imshow(sinusoidal_pe.T, cmap='RdBu', aspect='auto')\n",
    "axes[0].set_title('æ­£å¼¦ä½ç½®ç·¨ç¢¼ (å›ºå®š)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Position')\n",
    "axes[0].set_ylabel('Dimension')\n",
    "plt.colorbar(im1, ax=axes[0])\n",
    "\n",
    "# å¯å­¸ç¿’ç·¨ç¢¼\n",
    "im2 = axes[1].imshow(learned_pe.T, cmap='RdBu', aspect='auto')\n",
    "axes[1].set_title('å¯å­¸ç¿’ä½ç½®ç·¨ç¢¼ (åˆå§‹åŒ–)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Position')\n",
    "axes[1].set_ylabel('Dimension')\n",
    "plt.colorbar(im2, ax=axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ’¡ æ€è€ƒ:\")\n",
    "print(\"  1. å…©ç¨®ç·¨ç¢¼çš„è¦–è¦ºåŒ–æœ‰ä»€éº¼å·®ç•°?\")\n",
    "print(\"  2. å“ªä¸€ç¨®æ›´é©åˆè™•ç†é•·åºåˆ—?\")\n",
    "print(\"  3. GPT ç³»åˆ—ä½¿ç”¨å“ªä¸€ç¨®ç·¨ç¢¼?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ç·´ç¿’ 2: Attention æ¬Šé‡åˆ†æ\n",
    "\n",
    "åˆ†æä¸€å€‹å¯¦éš›å¥å­çš„ Attention æ¨¡å¼ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç·´ç¿’ 2: åˆ†æçœŸå¯¦å¥å­çš„ Attention\n",
    "sentence = \"The cat sat on the mat\".split()\n",
    "seq_len = len(sentence)\n",
    "\n",
    "# æ¨¡æ“¬ Attention æ¬Šé‡ (å¯¦éš›éœ€è¦è¨“ç·´å¥½çš„æ¨¡å‹)\n",
    "np.random.seed(123)\n",
    "attn = np.random.rand(seq_len, seq_len)\n",
    "attn = attn / attn.sum(axis=1, keepdims=True)  # æ­¸ä¸€åŒ–\n",
    "\n",
    "# è¦–è¦ºåŒ–\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(attn, annot=True, fmt='.2f', cmap='Blues',\n",
    "            xticklabels=sentence,\n",
    "            yticklabels=sentence,\n",
    "            cbar_kws={'label': 'Attention Score'})\n",
    "plt.xlabel('è¢«é—œæ³¨çš„è© (Key)', fontsize=12)\n",
    "plt.ylabel('æŸ¥è©¢è© (Query)', fontsize=12)\n",
    "plt.title('å¥å­: \"The cat sat on the mat\" çš„ Attention æ¨¡å¼', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ” åˆ†æ:\")\n",
    "print(f\"  - 'cat' æœ€é—œæ³¨å“ªäº›è©? {sentence[np.argmax(attn[1])]}\")\n",
    "print(f\"  - 'sat' æœ€é—œæ³¨å“ªäº›è©? {sentence[np.argmax(attn[2])]}\")\n",
    "print(\"\\nğŸ’­ æ€è€ƒ: å¯¦éš›è¨“ç·´çš„æ¨¡å‹æœƒå­¸åˆ°ä»€éº¼æ¨£çš„ Attention æ¨¡å¼?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“š æœ¬èª²ç¸½çµ\n",
    "\n",
    "### æ ¸å¿ƒè¦é»å›é¡§:\n",
    "\n",
    "1. **Transformer é©å‘½æ€§å‰µæ–°:**\n",
    "   - å®Œå…¨æ‹‹æ£„ RNN,ç´”ç²¹ä½¿ç”¨ Self-Attention\n",
    "   - ä¸¦è¡ŒåŒ–è¨“ç·´,é€Ÿåº¦æå‡ 10-100 å€\n",
    "   - å¯æ“´å±•åˆ°æ•¸åå„„åƒæ•¸çš„å¤§å‹èªè¨€æ¨¡å‹\n",
    "\n",
    "2. **æ¶æ§‹çµ„ä»¶:**\n",
    "   - Encoder-Decoder é›™å¡”çµæ§‹\n",
    "   - Multi-Head Self-Attention æ ¸å¿ƒæ©Ÿåˆ¶\n",
    "   - ä½ç½®ç·¨ç¢¼è§£æ±ºè©åºå•é¡Œ\n",
    "   - æ®˜å·®é€£æ¥èˆ‡å±¤æ­¸ä¸€åŒ–ç©©å®šè¨“ç·´\n",
    "\n",
    "3. **Attention æ©Ÿåˆ¶:**\n",
    "   - Query, Key, Value ä¸‰è¦ç´ \n",
    "   - Softmax æ­¸ä¸€åŒ–æ¬Šé‡\n",
    "   - å¤šé ­å­¸ç¿’ä¸åŒé—œä¿‚\n",
    "\n",
    "4. **ä¸‰å¤§æ¨¡å‹å®¶æ—:**\n",
    "   - Encoder-Only (BERT): ç†è§£ä»»å‹™\n",
    "   - Decoder-Only (GPT): ç”Ÿæˆä»»å‹™\n",
    "   - Encoder-Decoder (T5): è½‰æ›ä»»å‹™\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ ä¸‹ç¯€é å‘Š\n",
    "\n",
    "**CH07-02: åµŒå…¥å±¤ (Embeddings)**\n",
    "\n",
    "æˆ‘å€‘å°‡æ·±å…¥æ¢è¨:\n",
    "- Token Embeddings è©åµŒå…¥\n",
    "- ä½ç½®ç·¨ç¢¼çš„æ•¸å­¸åŸç†\n",
    "- Word2Vec, GloVe vs Transformer Embeddings\n",
    "- å¯¦æˆ°: è¨“ç·´è‡ªå·±çš„è©å‘é‡\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“– å»¶ä¼¸é–±è®€\n",
    "\n",
    "1. **è«–æ–‡:**\n",
    "   - [Attention is All You Need](https://arxiv.org/abs/1706.03762) (Vaswani et al., 2017)\n",
    "   - [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/) (Jay Alammar)\n",
    "\n",
    "2. **æ•™å­¸è³‡æº:**\n",
    "   - [Stanford CS224N Lecture on Transformers](https://web.stanford.edu/class/cs224n/)\n",
    "   - [Hugging Face Transformers Course](https://huggingface.co/learn/nlp-course)\n",
    "\n",
    "3. **å¯¦ä½œæ•™å­¸:**\n",
    "   - [Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)\n",
    "   - [The Transformer Family 2.0](https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/)\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ™‹ å•é¡Œè¨è«–\n",
    "\n",
    "æœ‰ä»»ä½•å•é¡Œå—?æ­¡è¿åœ¨è¨è«–å€æå•!\n",
    "\n",
    "---\n",
    "\n",
    "**èª²ç¨‹è³‡è¨Š:**\n",
    "- **ä½œè€…:** iSpan NLP Team\n",
    "- **ç‰ˆæœ¬:** v1.0\n",
    "- **æœ€å¾Œæ›´æ–°:** 2025-10-17\n",
    "- **æˆæ¬Š:** MIT License (åƒ…ä¾›æ•™å­¸ä½¿ç”¨)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
