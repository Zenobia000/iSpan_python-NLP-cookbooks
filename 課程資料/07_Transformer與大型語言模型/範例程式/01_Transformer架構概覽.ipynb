{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH07-01: Transformer 架構概覽\n",
    "\n",
    "**課程目標:**\n",
    "- 理解 Transformer 的誕生背景與革命性意義\n",
    "- 掌握 Transformer 的整體架構與核心組件\n",
    "- 了解 Self-Attention 機制的直觀概念\n",
    "- 認識 Transformer 在 NLP 領域的重要應用\n",
    "\n",
    "**學習時間:** 約 90 分鐘\n",
    "\n",
    "**前置知識:**\n",
    "- 基礎神經網路 (MLP)\n",
    "- 序列模型 (RNN/LSTM)\n",
    "- 詞向量 (Word Embeddings)\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 目錄\n",
    "\n",
    "1. [為什麼需要 Transformer?](#1)\n",
    "2. [Transformer 架構全貌](#2)\n",
    "3. [核心組件介紹](#3)\n",
    "4. [Attention 機制直觀理解](#4)\n",
    "5. [Transformer 的優勢與應用](#5)\n",
    "6. [實戰練習](#6)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 環境設定與套件導入\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# 設定中文顯示\n",
    "plt.rcParams['font.sans-serif'] = ['Microsoft JhengHei', 'SimHei', 'Arial Unicode MS']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 設定顯示風格\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"✅ 環境設定完成\")\n",
    "print(f\"NumPy 版本: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "## 1. 為什麼需要 Transformer?\n",
    "\n",
    "### 1.1 傳統序列模型的挑戰\n",
    "\n",
    "在 Transformer 出現之前,NLP 領域主要使用 **RNN** 和 **LSTM** 處理序列數據。\n",
    "\n",
    "#### RNN/LSTM 的三大限制:\n",
    "\n",
    "1. **無法並行化 (Sequential Processing)**\n",
    "   - 必須按順序逐步處理,無法利用 GPU 並行加速\n",
    "   - 訓練速度慢,難以擴展到大型數據集\n",
    "\n",
    "2. **長距離依賴問題 (Long-Range Dependencies)**\n",
    "   - 儘管 LSTM 使用門控機制,但處理超長序列仍力不從心\n",
    "   - 梯度消失/爆炸問題\n",
    "\n",
    "3. **固定上下文窗口 (Fixed Context Window)**\n",
    "   - 隱藏狀態有限,難以捕捉全局訊息\n",
    "   - 容易遺忘早期重要訊息\n",
    "\n",
    "---\n",
    "\n",
    "### 1.2 Transformer 的革命性突破\n",
    "\n",
    "**論文:** *Attention is All You Need* (Vaswani et al., 2017)\n",
    "\n",
    "**核心思想:** 完全拋棄循環結構,純粹使用 **Self-Attention 機制**!\n",
    "\n",
    "#### Transformer 的三大優勢:\n",
    "\n",
    "1. ✅ **完全並行化** - 所有位置同時計算,訓練速度快 10-100 倍\n",
    "2. ✅ **全局視野** - 每個詞可以直接關注序列中的任何其他詞\n",
    "3. ✅ **可擴展性** - 可輕鬆擴展到數十億參數 (GPT-3: 175B)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化: RNN vs Transformer 訓練速度對比\n",
    "sequence_lengths = [50, 100, 200, 500, 1000]\n",
    "rnn_time = [1, 4, 16, 100, 400]  # 模擬訓練時間 (秒)\n",
    "transformer_time = [2, 3, 4, 6, 10]  # 並行化優勢\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(sequence_lengths, rnn_time, 'o-', label='RNN/LSTM (Sequential)', linewidth=2, markersize=8)\n",
    "plt.plot(sequence_lengths, transformer_time, 's-', label='Transformer (Parallel)', linewidth=2, markersize=8)\n",
    "plt.xlabel('Sequence Length', fontsize=12)\n",
    "plt.ylabel('Training Time (seconds)', fontsize=12)\n",
    "plt.title('RNN vs Transformer 訓練速度對比 (模擬)', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"📊 觀察:\")\n",
    "print(\"  - RNN: 序列越長,訓練時間指數增長\")\n",
    "print(\"  - Transformer: 序列長度影響小,可並行處理\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "## 2. Transformer 架構全貌\n",
    "\n",
    "### 2.1 整體架構\n",
    "\n",
    "Transformer 採用經典的 **Encoder-Decoder** 結構:\n",
    "\n",
    "```\n",
    "輸入序列 (Input)\n",
    "    ↓\n",
    "┌─────────────────────────┐\n",
    "│   Encoder (編碼器)        │  ← 理解輸入語義\n",
    "│   - N 層堆疊              │\n",
    "│   - Self-Attention       │\n",
    "│   - Feed Forward         │\n",
    "└─────────────────────────┘\n",
    "    ↓ (Context Vector)\n",
    "┌─────────────────────────┐\n",
    "│   Decoder (解碼器)        │  ← 生成輸出序列\n",
    "│   - N 層堆疊              │\n",
    "│   - Masked Self-Attention│\n",
    "│   - Cross-Attention      │\n",
    "│   - Feed Forward         │\n",
    "└─────────────────────────┘\n",
    "    ↓\n",
    "輸出序列 (Output)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2.2 核心組件解析\n",
    "\n",
    "#### 2.2.1 Encoder 層結構\n",
    "\n",
    "每個 Encoder 層包含兩個子層:\n",
    "\n",
    "1. **Multi-Head Self-Attention**\n",
    "   - 讓每個詞關注序列中的所有詞\n",
    "   - 多頭機制捕捉不同關係 (語法、語義等)\n",
    "\n",
    "2. **Position-wise Feed-Forward Network**\n",
    "   - 兩層全連接網路\n",
    "   - 對每個位置獨立處理\n",
    "\n",
    "**每個子層後都有:**\n",
    "- Residual Connection (殘差連接)\n",
    "- Layer Normalization (層歸一化)\n",
    "\n",
    "---\n",
    "\n",
    "#### 2.2.2 Decoder 層結構\n",
    "\n",
    "每個 Decoder 層包含三個子層:\n",
    "\n",
    "1. **Masked Multi-Head Self-Attention**\n",
    "   - 防止看到未來的詞 (自回歸生成)\n",
    "\n",
    "2. **Cross-Attention (Encoder-Decoder Attention)**\n",
    "   - Decoder 關注 Encoder 的輸出\n",
    "   - 連接輸入與輸出的橋樑\n",
    "\n",
    "3. **Position-wise Feed-Forward Network**\n",
    "   - 與 Encoder 相同\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化: Transformer 架構層級結構\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "ax.set_xlim(0, 10)\n",
    "ax.set_ylim(0, 12)\n",
    "ax.axis('off')\n",
    "\n",
    "# Encoder 區塊\n",
    "encoder_rect = mpatches.Rectangle((0.5, 6), 4, 5, \n",
    "                                   linewidth=2, edgecolor='blue', facecolor='lightblue', alpha=0.3)\n",
    "ax.add_patch(encoder_rect)\n",
    "ax.text(2.5, 10.5, 'ENCODER', ha='center', fontsize=16, fontweight='bold', color='darkblue')\n",
    "\n",
    "# Encoder 子層\n",
    "ax.text(2.5, 9.5, 'Multi-Head\\nSelf-Attention', ha='center', fontsize=11, \n",
    "        bbox=dict(boxstyle='round', facecolor='white', edgecolor='blue'))\n",
    "ax.text(2.5, 8, 'Add & Norm', ha='center', fontsize=9, style='italic')\n",
    "ax.text(2.5, 7.2, 'Feed Forward\\nNetwork', ha='center', fontsize=11,\n",
    "        bbox=dict(boxstyle='round', facecolor='white', edgecolor='blue'))\n",
    "ax.text(2.5, 6.4, 'Add & Norm', ha='center', fontsize=9, style='italic')\n",
    "\n",
    "# Decoder 區塊\n",
    "decoder_rect = mpatches.Rectangle((5.5, 6), 4, 5, \n",
    "                                   linewidth=2, edgecolor='green', facecolor='lightgreen', alpha=0.3)\n",
    "ax.add_patch(decoder_rect)\n",
    "ax.text(7.5, 10.5, 'DECODER', ha='center', fontsize=16, fontweight='bold', color='darkgreen')\n",
    "\n",
    "# Decoder 子層\n",
    "ax.text(7.5, 9.7, 'Masked\\nSelf-Attention', ha='center', fontsize=10,\n",
    "        bbox=dict(boxstyle='round', facecolor='white', edgecolor='green'))\n",
    "ax.text(7.5, 9, 'Add & Norm', ha='center', fontsize=9, style='italic')\n",
    "ax.text(7.5, 8.2, 'Cross-Attention', ha='center', fontsize=10,\n",
    "        bbox=dict(boxstyle='round', facecolor='white', edgecolor='green'))\n",
    "ax.text(7.5, 7.5, 'Add & Norm', ha='center', fontsize=9, style='italic')\n",
    "ax.text(7.5, 6.7, 'Feed Forward', ha='center', fontsize=10,\n",
    "        bbox=dict(boxstyle='round', facecolor='white', edgecolor='green'))\n",
    "\n",
    "# 輸入與輸出\n",
    "ax.text(2.5, 5.3, 'Input Embedding\\n+\\nPositional Encoding', ha='center', fontsize=10,\n",
    "        bbox=dict(boxstyle='round', facecolor='lightyellow'))\n",
    "ax.text(7.5, 5.3, 'Output Embedding\\n+\\nPositional Encoding', ha='center', fontsize=10,\n",
    "        bbox=dict(boxstyle='round', facecolor='lightyellow'))\n",
    "\n",
    "# 箭頭\n",
    "ax.arrow(2.5, 5.8, 0, 0.15, head_width=0.2, head_length=0.1, fc='black', ec='black')\n",
    "ax.arrow(7.5, 5.8, 0, 0.15, head_width=0.2, head_length=0.1, fc='black', ec='black')\n",
    "ax.arrow(4.5, 8.5, 0.9, 0, head_width=0.15, head_length=0.1, fc='red', ec='red', linestyle='--')\n",
    "ax.text(5, 8.8, 'Context', ha='center', fontsize=9, color='red')\n",
    "\n",
    "# 標題\n",
    "ax.text(5, 11.5, 'Transformer 架構全貌', ha='center', fontsize=18, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n🔍 架構重點:\")\n",
    "print(\"  1. Encoder: 將輸入編碼為語義表示\")\n",
    "print(\"  2. Decoder: 基於編碼生成輸出序列\")\n",
    "print(\"  3. Cross-Attention: Encoder 與 Decoder 的橋樑\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "## 3. 核心組件深入\n",
    "\n",
    "### 3.1 位置編碼 (Positional Encoding)\n",
    "\n",
    "**問題:** Attention 機制是**無序的** (permutation-invariant),無法區分詞的順序!\n",
    "\n",
    "**解決方案:** 在詞向量中加入位置訊息\n",
    "\n",
    "#### 正弦位置編碼公式:\n",
    "\n",
    "$$\n",
    "\\text{PE}_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{PE}_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)\n",
    "$$\n",
    "\n",
    "其中:\n",
    "- `pos`: 詞在序列中的位置 (0, 1, 2, ...)\n",
    "- `i`: 維度索引 (0, 1, 2, ..., d_model/2)\n",
    "- `d_model`: 詞向量維度 (論文中為 512)\n",
    "\n",
    "**優點:**\n",
    "- 不同位置有獨特的編碼\n",
    "- 相對位置關係可學習\n",
    "- 可處理任意長度序列\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 實作並視覺化位置編碼\n",
    "def get_positional_encoding(max_seq_len, d_model):\n",
    "    \"\"\"\n",
    "    生成正弦位置編碼\n",
    "    \n",
    "    Args:\n",
    "        max_seq_len: 最大序列長度\n",
    "        d_model: 詞向量維度\n",
    "        \n",
    "    Returns:\n",
    "        pos_encoding: shape (max_seq_len, d_model)\n",
    "    \"\"\"\n",
    "    pos_encoding = np.zeros((max_seq_len, d_model))\n",
    "    \n",
    "    for pos in range(max_seq_len):\n",
    "        for i in range(0, d_model, 2):\n",
    "            # Sin for even indices\n",
    "            pos_encoding[pos, i] = np.sin(pos / (10000 ** (i / d_model)))\n",
    "            \n",
    "            # Cos for odd indices\n",
    "            if i + 1 < d_model:\n",
    "                pos_encoding[pos, i + 1] = np.cos(pos / (10000 ** (i / d_model)))\n",
    "    \n",
    "    return pos_encoding\n",
    "\n",
    "# 生成位置編碼\n",
    "max_len = 100\n",
    "d_model = 512\n",
    "pos_enc = get_positional_encoding(max_len, d_model)\n",
    "\n",
    "print(f\"位置編碼矩陣形狀: {pos_enc.shape}\")\n",
    "print(f\"前 3 個位置的前 8 維編碼:\\n{pos_enc[:3, :8]}\")\n",
    "\n",
    "# 視覺化位置編碼\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(pos_enc[:50, :128].T, cmap='RdBu', aspect='auto')\n",
    "plt.colorbar(label='Encoding Value')\n",
    "plt.xlabel('Position in Sequence', fontsize=12)\n",
    "plt.ylabel('Embedding Dimension', fontsize=12)\n",
    "plt.title('正弦位置編碼視覺化 (前 50 位置, 前 128 維度)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n🎨 觀察:\")\n",
    "print(\"  - 每個位置有獨特的編碼模式\")\n",
    "print(\"  - 低維度變化快 (高頻),高維度變化慢 (低頻)\")\n",
    "print(\"  - 相鄰位置編碼相似,但不完全相同\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 殘差連接與層歸一化\n",
    "\n",
    "#### 3.2.1 殘差連接 (Residual Connection)\n",
    "\n",
    "$$\n",
    "\\text{Output} = \\text{SubLayer}(x) + x\n",
    "$$\n",
    "\n",
    "**作用:**\n",
    "- 緩解梯度消失問題\n",
    "- 讓訊息更容易傳遞\n",
    "- 加速訓練收斂\n",
    "\n",
    "---\n",
    "\n",
    "#### 3.2.2 層歸一化 (Layer Normalization)\n",
    "\n",
    "$$\n",
    "\\text{LayerNorm}(x) = \\gamma \\cdot \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta\n",
    "$$\n",
    "\n",
    "**作用:**\n",
    "- 穩定訓練過程\n",
    "- 加速收斂\n",
    "- 減少內部協變量偏移\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "## 4. Attention 機制直觀理解\n",
    "\n",
    "### 4.1 什麼是 Attention?\n",
    "\n",
    "**核心思想:** 讓模型\"關注\"最相關的部分\n",
    "\n",
    "#### 人類閱讀的例子:\n",
    "\n",
    "句子: *\"The **animal** didn't cross the street because **it** was too tired.\"*\n",
    "\n",
    "問題: \"it\" 指的是什麼?\n",
    "\n",
    "人類會自動關注:\n",
    "- \"it\" ← **高度關注** ← \"animal\"\n",
    "- \"it\" ← 低度關注 ← \"street\"\n",
    "\n",
    "**這就是 Attention!**\n",
    "\n",
    "---\n",
    "\n",
    "### 4.2 Self-Attention 機制\n",
    "\n",
    "**三個關鍵矩陣:**\n",
    "\n",
    "1. **Query (Q)**: \"我想查詢什麼?\"\n",
    "2. **Key (K)**: \"我是什麼?\"\n",
    "3. **Value (V)**: \"我有什麼訊息?\"\n",
    "\n",
    "#### Attention 計算公式:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V\n",
    "$$\n",
    "\n",
    "**步驟解析:**\n",
    "1. 計算 Query 與 Key 的相似度: $QK^T$\n",
    "2. 縮放 (避免數值過大): $\\frac{QK^T}{\\sqrt{d_k}}$\n",
    "3. Softmax 歸一化為概率分布\n",
    "4. 加權組合 Value: $\\text{softmax}(...) \\cdot V$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 簡單的 Self-Attention 實作示例\n",
    "def simple_self_attention(X, d_k):\n",
    "    \"\"\"\n",
    "    簡化版 Self-Attention 機制\n",
    "    \n",
    "    Args:\n",
    "        X: 輸入矩陣 (seq_len, d_model)\n",
    "        d_k: Key 維度 (用於縮放)\n",
    "        \n",
    "    Returns:\n",
    "        output: Attention 輸出\n",
    "        attention_weights: Attention 權重矩陣\n",
    "    \"\"\"\n",
    "    # 簡化: 直接使用 X 作為 Q, K, V (實際應用需投影矩陣)\n",
    "    Q = K = V = X\n",
    "    \n",
    "    # Step 1: 計算相似度 (Dot Product)\n",
    "    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n",
    "    \n",
    "    # Step 2: Softmax 歸一化\n",
    "    attention_weights = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)\n",
    "    \n",
    "    # Step 3: 加權組合 Value\n",
    "    output = np.dot(attention_weights, V)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "# 測試範例\n",
    "np.random.seed(42)\n",
    "seq_len = 5\n",
    "d_model = 8\n",
    "d_k = d_model\n",
    "\n",
    "# 模擬輸入序列\n",
    "X = np.random.randn(seq_len, d_model)\n",
    "\n",
    "# 計算 Attention\n",
    "output, attn_weights = simple_self_attention(X, d_k)\n",
    "\n",
    "print(f\"輸入形狀: {X.shape}\")\n",
    "print(f\"輸出形狀: {output.shape}\")\n",
    "print(f\"Attention 權重形狀: {attn_weights.shape}\")\n",
    "print(f\"\\nAttention 權重矩陣:\\n{attn_weights}\")\n",
    "\n",
    "# 視覺化 Attention 權重\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(attn_weights, annot=True, fmt='.2f', cmap='YlOrRd', \n",
    "            xticklabels=[f'位置 {i+1}' for i in range(seq_len)],\n",
    "            yticklabels=[f'位置 {i+1}' for i in range(seq_len)],\n",
    "            cbar_kws={'label': 'Attention Score'})\n",
    "plt.xlabel('Key (被關注的位置)', fontsize=12)\n",
    "plt.ylabel('Query (查詢位置)', fontsize=12)\n",
    "plt.title('Self-Attention 權重矩陣視覺化', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n💡 解讀:\")\n",
    "print(\"  - 每一行總和為 1 (Softmax 特性)\")\n",
    "print(\"  - 顏色越深表示關注度越高\")\n",
    "print(\"  - 對角線值較高: 自己關注自己\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Multi-Head Attention\n",
    "\n",
    "**核心思想:** 使用多個 Attention \"頭\",各自學習不同關係\n",
    "\n",
    "#### 為什麼需要多頭?\n",
    "\n",
    "一個句子中可能有多種關係:\n",
    "- **語法關係:** 主詞-動詞-受詞\n",
    "- **語義關係:** 近義詞、反義詞\n",
    "- **指代關係:** 代名詞指涉\n",
    "\n",
    "**多頭讓模型同時學習這些不同關係!**\n",
    "\n",
    "#### 多頭 Attention 公式:\n",
    "\n",
    "$$\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h) W^O\n",
    "$$\n",
    "\n",
    "其中每個頭:\n",
    "\n",
    "$$\n",
    "\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n",
    "$$\n",
    "\n",
    "**論文設定:** 8 個頭,每個頭維度 64 (總維度 512)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "## 5. Transformer 的優勢與應用\n",
    "\n",
    "### 5.1 Transformer 三大優勢\n",
    "\n",
    "| 特性 | RNN/LSTM | Transformer |\n",
    "|------|----------|-------------|\n",
    "| **並行化** | ❌ 只能串行處理 | ✅ 完全並行 |\n",
    "| **長距離依賴** | ⚠️ 受限於隱藏狀態 | ✅ 直接連接所有位置 |\n",
    "| **訓練速度** | 🐌 慢 | 🚀 快 10-100 倍 |\n",
    "| **可擴展性** | ⚠️ 難以擴展 | ✅ 可擴展到數十億參數 |\n",
    "| **全局視野** | ❌ 有限上下文窗口 | ✅ 全序列視野 |\n",
    "\n",
    "---\n",
    "\n",
    "### 5.2 Transformer 三大模型家族\n",
    "\n",
    "#### 5.2.1 Encoder-Only (BERT 系列)\n",
    "\n",
    "**代表:** BERT, RoBERTa, ALBERT\n",
    "\n",
    "**特點:**\n",
    "- 雙向 Attention (可看到整個句子)\n",
    "- 適合理解任務\n",
    "\n",
    "**應用:**\n",
    "- 文本分類\n",
    "- 命名實體識別 (NER)\n",
    "- 問答系統\n",
    "\n",
    "---\n",
    "\n",
    "#### 5.2.2 Decoder-Only (GPT 系列)\n",
    "\n",
    "**代表:** GPT-2, GPT-3, GPT-4, Claude\n",
    "\n",
    "**特點:**\n",
    "- 單向 Attention (只看前文)\n",
    "- 自回歸生成\n",
    "\n",
    "**應用:**\n",
    "- 文本生成\n",
    "- 對話系統\n",
    "- 程式碼生成\n",
    "\n",
    "---\n",
    "\n",
    "#### 5.2.3 Encoder-Decoder (T5 系列)\n",
    "\n",
    "**代表:** T5, BART, mT5\n",
    "\n",
    "**特點:**\n",
    "- 完整 Transformer 架構\n",
    "- 輸入→輸出轉換\n",
    "\n",
    "**應用:**\n",
    "- 機器翻譯\n",
    "- 文本摘要\n",
    "- 問答生成\n",
    "\n",
    "---\n",
    "\n",
    "### 5.3 Transformer 的影響力\n",
    "\n",
    "**2017-2024 NLP 發展史:**\n",
    "\n",
    "```\n",
    "2017 → Transformer 論文發表 (\"Attention is All You Need\")\n",
    "2018 → BERT 橫空出世 (刷新 11 項 NLP 紀錄)\n",
    "2019 → GPT-2 展示強大生成能力\n",
    "2020 → GPT-3 (175B 參數) 驚豔世界\n",
    "2022 → ChatGPT 引爆 AI 熱潮\n",
    "2023 → GPT-4, Claude, LLaMA 百花齊放\n",
    "2024 → Transformer 應用延伸至視覺、多模態\n",
    "```\n",
    "\n",
    "**結論:** Transformer 徹底改變了 AI 領域!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a>\n",
    "## 6. 實戰練習\n",
    "\n",
    "### 練習 1: 位置編碼實驗\n",
    "\n",
    "修改位置編碼函數,嘗試不同的編碼方式,觀察視覺化差異。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 練習 1: 實作可學習的位置編碼\n",
    "def learned_positional_encoding(max_seq_len, d_model):\n",
    "    \"\"\"\n",
    "    可學習的位置編碼 (隨機初始化,實際訓練時會更新)\n",
    "    \"\"\"\n",
    "    # TODO: 使用隨機初始化替代正弦函數\n",
    "    return np.random.randn(max_seq_len, d_model) * 0.1\n",
    "\n",
    "# 比較兩種編碼\n",
    "sinusoidal_pe = get_positional_encoding(50, 128)\n",
    "learned_pe = learned_positional_encoding(50, 128)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# 正弦編碼\n",
    "im1 = axes[0].imshow(sinusoidal_pe.T, cmap='RdBu', aspect='auto')\n",
    "axes[0].set_title('正弦位置編碼 (固定)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Position')\n",
    "axes[0].set_ylabel('Dimension')\n",
    "plt.colorbar(im1, ax=axes[0])\n",
    "\n",
    "# 可學習編碼\n",
    "im2 = axes[1].imshow(learned_pe.T, cmap='RdBu', aspect='auto')\n",
    "axes[1].set_title('可學習位置編碼 (初始化)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Position')\n",
    "axes[1].set_ylabel('Dimension')\n",
    "plt.colorbar(im2, ax=axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"💡 思考:\")\n",
    "print(\"  1. 兩種編碼的視覺化有什麼差異?\")\n",
    "print(\"  2. 哪一種更適合處理長序列?\")\n",
    "print(\"  3. GPT 系列使用哪一種編碼?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習 2: Attention 權重分析\n",
    "\n",
    "分析一個實際句子的 Attention 模式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 練習 2: 分析真實句子的 Attention\n",
    "sentence = \"The cat sat on the mat\".split()\n",
    "seq_len = len(sentence)\n",
    "\n",
    "# 模擬 Attention 權重 (實際需要訓練好的模型)\n",
    "np.random.seed(123)\n",
    "attn = np.random.rand(seq_len, seq_len)\n",
    "attn = attn / attn.sum(axis=1, keepdims=True)  # 歸一化\n",
    "\n",
    "# 視覺化\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(attn, annot=True, fmt='.2f', cmap='Blues',\n",
    "            xticklabels=sentence,\n",
    "            yticklabels=sentence,\n",
    "            cbar_kws={'label': 'Attention Score'})\n",
    "plt.xlabel('被關注的詞 (Key)', fontsize=12)\n",
    "plt.ylabel('查詢詞 (Query)', fontsize=12)\n",
    "plt.title('句子: \"The cat sat on the mat\" 的 Attention 模式', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n🔍 分析:\")\n",
    "print(f\"  - 'cat' 最關注哪些詞? {sentence[np.argmax(attn[1])]}\")\n",
    "print(f\"  - 'sat' 最關注哪些詞? {sentence[np.argmax(attn[2])]}\")\n",
    "print(\"\\n💭 思考: 實際訓練的模型會學到什麼樣的 Attention 模式?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 📚 本課總結\n",
    "\n",
    "### 核心要點回顧:\n",
    "\n",
    "1. **Transformer 革命性創新:**\n",
    "   - 完全拋棄 RNN,純粹使用 Self-Attention\n",
    "   - 並行化訓練,速度提升 10-100 倍\n",
    "   - 可擴展到數十億參數的大型語言模型\n",
    "\n",
    "2. **架構組件:**\n",
    "   - Encoder-Decoder 雙塔結構\n",
    "   - Multi-Head Self-Attention 核心機制\n",
    "   - 位置編碼解決詞序問題\n",
    "   - 殘差連接與層歸一化穩定訓練\n",
    "\n",
    "3. **Attention 機制:**\n",
    "   - Query, Key, Value 三要素\n",
    "   - Softmax 歸一化權重\n",
    "   - 多頭學習不同關係\n",
    "\n",
    "4. **三大模型家族:**\n",
    "   - Encoder-Only (BERT): 理解任務\n",
    "   - Decoder-Only (GPT): 生成任務\n",
    "   - Encoder-Decoder (T5): 轉換任務\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 下節預告\n",
    "\n",
    "**CH07-02: 嵌入層 (Embeddings)**\n",
    "\n",
    "我們將深入探討:\n",
    "- Token Embeddings 詞嵌入\n",
    "- 位置編碼的數學原理\n",
    "- Word2Vec, GloVe vs Transformer Embeddings\n",
    "- 實戰: 訓練自己的詞向量\n",
    "\n",
    "---\n",
    "\n",
    "## 📖 延伸閱讀\n",
    "\n",
    "1. **論文:**\n",
    "   - [Attention is All You Need](https://arxiv.org/abs/1706.03762) (Vaswani et al., 2017)\n",
    "   - [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/) (Jay Alammar)\n",
    "\n",
    "2. **教學資源:**\n",
    "   - [Stanford CS224N Lecture on Transformers](https://web.stanford.edu/class/cs224n/)\n",
    "   - [Hugging Face Transformers Course](https://huggingface.co/learn/nlp-course)\n",
    "\n",
    "3. **實作教學:**\n",
    "   - [Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)\n",
    "   - [The Transformer Family 2.0](https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/)\n",
    "\n",
    "---\n",
    "\n",
    "### 🙋 問題討論\n",
    "\n",
    "有任何問題嗎?歡迎在討論區提問!\n",
    "\n",
    "---\n",
    "\n",
    "**課程資訊:**\n",
    "- **作者:** iSpan NLP Team\n",
    "- **版本:** v1.0\n",
    "- **最後更新:** 2025-10-17\n",
    "- **授權:** MIT License (僅供教學使用)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
