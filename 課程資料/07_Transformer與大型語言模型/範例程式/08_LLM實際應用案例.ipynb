{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# CH07-08: LLM 實際應用案例\n",
    "\n",
    "**課程時長**: 90 分鐘  \n",
    "**難度**: ⭐⭐⭐⭐⭐  \n",
    "**前置知識**: CH07-01 至 CH07-07  \n",
    "\n",
    "---\n",
    "\n",
    "## 📚 本節學習目標\n",
    "\n",
    "1. ✅ 使用 Hugging Face Transformers 載入與使用 LLM\n",
    "2. ✅ 實作檢索增強生成 (RAG) 系統\n",
    "3. ✅ 掌握 LangChain 框架核心功能\n",
    "4. ✅ 設計 LLM Agent 系統\n",
    "5. ✅ 理解生產環境部署考量\n",
    "\n",
    "---\n",
    "\n",
    "## 📖 目錄\n",
    "\n",
    "1. [使用 Hugging Face 載入 LLM](#1-huggingface)\n",
    "2. [文本生成實戰](#2-text-generation)\n",
    "3. [檢索增強生成 (RAG)](#3-rag)\n",
    "4. [LangChain 框架入門](#4-langchain)\n",
    "5. [Agent 系統設計](#5-agents)\n",
    "6. [生產環境部署](#6-deployment)\n",
    "7. [總結](#7-summary)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1",
   "metadata": {},
   "source": [
    "## 1. 使用 Hugging Face 載入 LLM {#1-huggingface}\n",
    "\n",
    "### 1.1 Hugging Face Hub 簡介\n",
    "\n",
    "**Hugging Face** 是最大的開源 AI 社群:\n",
    "- 200,000+ 預訓練模型\n",
    "- 50,000+ 數據集\n",
    "- 統一的 API 介面\n",
    "\n",
    "**熱門開源 LLM**:\n",
    "- **LLaMA 2** (Meta): 7B, 13B, 70B\n",
    "- **Mistral** (Mistral AI): 7B, 8x7B (MoE)\n",
    "- **Falcon** (TII): 7B, 40B, 180B\n",
    "- **MPT** (MosaicML): 7B, 30B\n",
    "- **Bloom** (BigScience): 176B\n",
    "\n",
    "### 1.2 安裝必要套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-packages",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安裝必要套件 (取消註解以執行)\n",
    "# !pip install transformers>=4.35.0\n",
    "# !pip install torch>=2.0.0\n",
    "# !pip install accelerate>=0.20.0\n",
    "# !pip install bitsandbytes>=0.40.0  # 用於量化\n",
    "# !pip install sentence-transformers>=2.2.0  # 用於 embeddings\n",
    "# !pip install faiss-cpu>=1.7.4  # 用於向量檢索\n",
    "# !pip install langchain>=0.0.300  # LangChain 框架\n",
    "\n",
    "print(\"✅ 套件安裝指令已準備好\")\n",
    "print(\"\\n注意: 實際執行前請確認環境，建議使用虛擬環境\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入基礎套件\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✅ 基礎套件載入完成\")\n",
    "\n",
    "# 嘗試載入 Transformers (可能未安裝)\n",
    "try:\n",
    "    from transformers import (\n",
    "        AutoModelForCausalLM,\n",
    "        AutoTokenizer,\n",
    "        pipeline,\n",
    "        BitsAndBytesConfig\n",
    "    )\n",
    "    import torch\n",
    "    \n",
    "    print(f\"✅ Transformers {transformers.__version__} 載入成功\")\n",
    "    print(f\"✅ PyTorch {torch.__version__} 載入成功\")\n",
    "    print(f\"✅ CUDA 可用: {torch.cuda.is_available()}\")\n",
    "    \n",
    "    HF_AVAILABLE = True\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(\"⚠️ Transformers 或 PyTorch 未安裝\")\n",
    "    print(\"   請執行: pip install transformers torch\")\n",
    "    HF_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1-2",
   "metadata": {},
   "source": [
    "### 1.3 載入小型 LLM (GPT-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-gpt2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if HF_AVAILABLE:\n",
    "    print(\"載入 GPT-2 Small (124M 參數)...\")\n",
    "    \n",
    "    # 載入 tokenizer 和模型\n",
    "    model_name = \"gpt2\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    \n",
    "    # 移到 GPU (如果可用)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = model.to(device)\n",
    "    \n",
    "    print(f\"✅ GPT-2 載入完成 (Device: {device})\")\n",
    "    print(f\"✅ Tokenizer 詞彙量: {len(tokenizer)}\")\n",
    "    print(f\"✅ 模型參數量: {model.num_parameters():,}\")\n",
    "    \n",
    "    # 測試生成\n",
    "    prompt = \"Artificial intelligence is\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=30,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.7,\n",
    "        do_sample=True\n",
    "    )\n",
    "    \n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(\"\\n生成範例:\")\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Output: {generated_text}\")\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️ 請先安裝 transformers 和 torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2",
   "metadata": {},
   "source": [
    "## 2. 文本生成實戰 {#2-text-generation}\n",
    "\n",
    "### 2.1 生成參數詳解\n",
    "\n",
    "**關鍵參數**:\n",
    "\n",
    "| 參數 | 說明 | 推薦值 |\n",
    "|------|------|--------|\n",
    "| **max_new_tokens** | 生成的最大 token 數 | 50-500 |\n",
    "| **temperature** | 隨機性控制 (越高越隨機) | 0.7-1.0 (創意), 0.1-0.3 (事實) |\n",
    "| **top_k** | 只從 top-k 個 token 中採樣 | 50 |\n",
    "| **top_p** | Nucleus sampling (累積機率) | 0.9-0.95 |\n",
    "| **num_beams** | Beam search 寬度 | 1 (greedy), 4-5 (品質優先) |\n",
    "| **do_sample** | 是否隨機採樣 | True (多樣), False (確定性) |\n",
    "| **repetition_penalty** | 重複懲罰 | 1.2-1.5 |\n",
    "\n",
    "### 2.2 不同生成策略對比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generation-strategies",
   "metadata": {},
   "outputs": [],
   "source": [
    "if HF_AVAILABLE:\n",
    "    prompt = \"The future of artificial intelligence will\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    strategies = [\n",
    "        {\n",
    "            'name': 'Greedy (確定性)',\n",
    "            'params': {\n",
    "                'max_new_tokens': 40,\n",
    "                'do_sample': False\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'name': 'Sampling (隨機)',\n",
    "            'params': {\n",
    "                'max_new_tokens': 40,\n",
    "                'do_sample': True,\n",
    "                'temperature': 1.0\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'name': 'Top-k Sampling',\n",
    "            'params': {\n",
    "                'max_new_tokens': 40,\n",
    "                'do_sample': True,\n",
    "                'top_k': 50,\n",
    "                'temperature': 0.8\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'name': 'Nucleus (Top-p) Sampling',\n",
    "            'params': {\n",
    "                'max_new_tokens': 40,\n",
    "                'do_sample': True,\n",
    "                'top_p': 0.9,\n",
    "                'temperature': 0.8\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'name': 'Beam Search',\n",
    "            'params': {\n",
    "                'max_new_tokens': 40,\n",
    "                'num_beams': 5,\n",
    "                'early_stopping': True\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Prompt: '{prompt}'\")\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "    \n",
    "    for strategy in strategies:\n",
    "        outputs = model.generate(**inputs, **strategy['params'])\n",
    "        text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        print(f\"【{strategy['name']}】\")\n",
    "        print(f\"{text}\")\n",
    "        print()\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\n觀察:\")\n",
    "    print(\"• Greedy: 每次都生成相同結果\")\n",
    "    print(\"• Sampling: 每次結果不同,更有創意\")\n",
    "    print(\"• Top-k/Top-p: 平衡品質與多樣性\")\n",
    "    print(\"• Beam Search: 品質最高但缺乏多樣性\")\n",
    "\n",
    "else:\n",
    "    print(\"⚠️ 需要安裝 transformers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "use-pipeline",
   "metadata": {},
   "source": [
    "### 2.3 使用 Pipeline 簡化操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pipeline-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "if HF_AVAILABLE:\n",
    "    # 建立文本生成 pipeline\n",
    "    generator = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model_name,\n",
    "        device=0 if torch.cuda.is_available() else -1  # 0=GPU, -1=CPU\n",
    "    )\n",
    "    \n",
    "    # 生成文本\n",
    "    prompt = \"Once upon a time in a magical forest,\"\n",
    "    \n",
    "    results = generator(\n",
    "        prompt,\n",
    "        max_new_tokens=60,\n",
    "        num_return_sequences=3,  # 生成 3 個版本\n",
    "        temperature=0.8,\n",
    "        top_p=0.95,\n",
    "        do_sample=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Prompt: '{prompt}'\\n\")\n",
    "    print(\"生成的 3 個版本:\\n\")\n",
    "    \n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"版本 {i}:\")\n",
    "        print(result['generated_text'])\n",
    "        print()\n",
    "\n",
    "else:\n",
    "    print(\"⚠️ 需要安裝 transformers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3",
   "metadata": {},
   "source": [
    "## 3. 檢索增強生成 (RAG) {#3-rag}\n",
    "\n",
    "### 3.1 RAG 原理\n",
    "\n",
    "**問題**: LLM 的知識是靜態的,無法獲取最新資訊\n",
    "\n",
    "**解決方案**: Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "```\n",
    "用戶問題\n",
    "    ↓\n",
    "① 檢索相關文檔 (Retrieval)\n",
    "    ↓\n",
    "    向量數據庫搜尋\n",
    "    ↓\n",
    "② 整合上下文 (Augmentation)\n",
    "    ↓\n",
    "    問題 + 檢索到的文檔 → Prompt\n",
    "    ↓\n",
    "③ LLM 生成答案 (Generation)\n",
    "    ↓\n",
    "   最終答案\n",
    "```\n",
    "\n",
    "**優勢**:\n",
    "- ✅ 獲取最新資訊\n",
    "- ✅ 減少幻覺\n",
    "- ✅ 可追溯來源\n",
    "- ✅ 無需重新訓練\n",
    "\n",
    "### 3.2 實作簡單 RAG 系統"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rag-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模擬知識庫\n",
    "knowledge_base = [\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"text\": \"Transformer 是一種基於自注意力機制的神經網路架構,由 Vaswani 等人於 2017 年提出。它徹底改變了 NLP 領域。\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 2,\n",
    "        \"text\": \"BERT 是 Google 在 2018 年發布的預訓練語言模型,使用 Transformer Encoder 架構,在多個 NLP 任務上取得突破性成果。\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 3,\n",
    "        \"text\": \"GPT-3 是 OpenAI 於 2020 年發布的大型語言模型,擁有 175B 參數,展現了強大的 few-shot learning 能力。\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 4,\n",
    "        \"text\": \"LLaMA 是 Meta 在 2023 年開源的高效 LLM 系列,參數從 7B 到 65B,在相同參數量下表現優於其他開源模型。\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 5,\n",
    "        \"text\": \"RAG (檢索增強生成) 結合了檢索系統和生成模型,通過檢索相關文檔來增強 LLM 的回答,減少幻覺問題。\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"✅ 知識庫建立完成\")\n",
    "print(f\"   共 {len(knowledge_base)} 筆文檔\")\n",
    "\n",
    "# 簡單的關鍵字檢索 (實際應用中應使用向量檢索)\n",
    "def simple_retrieve(query, knowledge_base, top_k=2):\n",
    "    \"\"\"簡單的關鍵字匹配檢索\"\"\"\n",
    "    scores = []\n",
    "    query_lower = query.lower()\n",
    "    \n",
    "    for doc in knowledge_base:\n",
    "        # 計算匹配分數 (簡化版)\n",
    "        text_lower = doc['text'].lower()\n",
    "        score = sum(1 for word in query_lower.split() if word in text_lower)\n",
    "        scores.append((doc, score))\n",
    "    \n",
    "    # 排序並返回 top-k\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    return [doc for doc, score in scores[:top_k]]\n",
    "\n",
    "# 測試檢索\n",
    "test_query = \"什麼是 Transformer?\"\n",
    "retrieved_docs = simple_retrieve(test_query, knowledge_base)\n",
    "\n",
    "print(f\"\\n查詢: '{test_query}'\")\n",
    "print(\"\\n檢索到的文檔:\")\n",
    "for doc in retrieved_docs:\n",
    "    print(f\"  [{doc['id']}] {doc['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rag-qa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG 問答函數\n",
    "def rag_qa(query, knowledge_base, model, tokenizer, top_k=2):\n",
    "    \"\"\"\n",
    "    RAG 問答系統\n",
    "    \n",
    "    Args:\n",
    "        query: 用戶問題\n",
    "        knowledge_base: 知識庫\n",
    "        model: LLM 模型\n",
    "        tokenizer: Tokenizer\n",
    "        top_k: 檢索文檔數量\n",
    "    \n",
    "    Returns:\n",
    "        答案文本\n",
    "    \"\"\"\n",
    "    # 1. 檢索相關文檔\n",
    "    docs = simple_retrieve(query, knowledge_base, top_k)\n",
    "    \n",
    "    # 2. 構建 prompt\n",
    "    context = \"\\n\".join([f\"- {doc['text']}\" for doc in docs])\n",
    "    \n",
    "    prompt = f\"\"\"根據以下資訊回答問題:\n",
    "\n",
    "{context}\n",
    "\n",
    "問題: {query}\n",
    "答案:\"\"\"\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"構建的 Prompt:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(prompt)\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 3. 生成答案\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        temperature=0.3,  # 降低隨機性,更事實性\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # 提取答案部分\n",
    "    answer = answer.split(\"答案:\")[-1].strip()\n",
    "    \n",
    "    return answer, docs\n",
    "\n",
    "# 測試 RAG 系統\n",
    "if HF_AVAILABLE:\n",
    "    queries = [\n",
    "        \"什麼是 Transformer?\",\n",
    "        \"GPT-3 有多少參數?\",\n",
    "        \"BERT 是什麼時候發布的?\"\n",
    "    ]\n",
    "    \n",
    "    for query in queries:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(f\"問題: {query}\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        answer, sources = rag_qa(query, knowledge_base, model, tokenizer)\n",
    "        \n",
    "        print(f\"\\n答案: {answer}\")\n",
    "        print(f\"\\n來源:\")\n",
    "        for doc in sources:\n",
    "            print(f\"  [{doc['id']}] {doc['text'][:50]}...\")\n",
    "\n",
    "else:\n",
    "    print(\"⚠️ 需要載入 LLM 模型\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vector-search",
   "metadata": {},
   "source": [
    "### 3.3 使用向量檢索 (進階)\n",
    "\n",
    "**實際 RAG 系統應使用語義相似度檢索**:\n",
    "\n",
    "```python\n",
    "# 使用 Sentence Transformers + FAISS\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "# 1. 載入 embedding 模型\n",
    "embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# 2. 將文檔轉換為向量\n",
    "doc_embeddings = embed_model.encode([doc['text'] for doc in knowledge_base])\n",
    "\n",
    "# 3. 建立 FAISS 索引\n",
    "index = faiss.IndexFlatL2(doc_embeddings.shape[1])\n",
    "index.add(doc_embeddings)\n",
    "\n",
    "# 4. 檢索\n",
    "query_embedding = embed_model.encode([query])\n",
    "distances, indices = index.search(query_embedding, k=2)\n",
    "```\n",
    "\n",
    "**向量檢索 vs 關鍵字檢索**:\n",
    "\n",
    "| 特性 | 關鍵字檢索 | 向量檢索 |\n",
    "|------|-----------|----------|\n",
    "| 精確度 | 低 | 高 |\n",
    "| 語義理解 | ❌ | ✅ |\n",
    "| 同義詞 | ❌ | ✅ |\n",
    "| 速度 | 快 | 需索引加速 |\n",
    "| 實作複雜度 | 低 | 中 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4",
   "metadata": {},
   "source": [
    "## 4. LangChain 框架入門 {#4-langchain}\n",
    "\n",
    "### 4.1 LangChain 簡介\n",
    "\n",
    "**LangChain** 是構建 LLM 應用的框架:\n",
    "- 統一的 LLM 介面\n",
    "- 鏈式組合 (Chain)\n",
    "- 記憶管理 (Memory)\n",
    "- Agent 系統\n",
    "\n",
    "**核心概念**:\n",
    "\n",
    "```\n",
    "LangChain\n",
    "├── Models (LLM, Chat Model, Embeddings)\n",
    "├── Prompts (PromptTemplate, ChatPromptTemplate)\n",
    "├── Chains (LLMChain, SequentialChain, ...)\n",
    "├── Memory (ConversationBufferMemory, ...)\n",
    "├── Agents (ReAct, Plan-and-Execute, ...)\n",
    "└── Tools (Search, Calculator, API calls, ...)\n",
    "```\n",
    "\n",
    "### 4.2 LangChain 基礎範例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "langchain-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain 範例 (示範程式碼,需安裝 langchain)\n",
    "\n",
    "langchain_code = '''\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# 1. 包裝 HuggingFace 模型為 LangChain LLM\n",
    "llm = HuggingFacePipeline(pipeline=generator)\n",
    "\n",
    "# 2. 建立 Prompt Template\n",
    "template = \"\"\"你是一個友善的 AI 助手。\n",
    "\n",
    "對話歷史:\n",
    "{history}\n",
    "\n",
    "用戶: {input}\n",
    "助手:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"history\", \"input\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "# 3. 建立 Memory\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"history\",\n",
    "    input_key=\"input\"\n",
    ")\n",
    "\n",
    "# 4. 建立 Chain\n",
    "conversation = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 5. 多輪對話\n",
    "print(conversation.predict(input=\"你好,你是誰?\"))\n",
    "print(conversation.predict(input=\"你能做什麼?\"))\n",
    "print(conversation.predict(input=\"剛才我問了什麼?\"))  # 測試記憶\n",
    "'''\n",
    "\n",
    "print(\"LangChain 對話系統範例程式碼:\")\n",
    "print(\"=\" * 80)\n",
    "print(langchain_code)\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n說明:\")\n",
    "print(\"• LLMChain: 組合 LLM + Prompt + Memory\")\n",
    "print(\"• ConversationBufferMemory: 記住完整對話歷史\")\n",
    "print(\"• PromptTemplate: 動態填充變數到 prompt\")\n",
    "print(\"• 自動處理多輪對話的上下文傳遞\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-5",
   "metadata": {},
   "source": [
    "## 5. Agent 系統設計 {#5-agents}\n",
    "\n",
    "### 5.1 什麼是 Agent?\n",
    "\n",
    "**定義**:\n",
    "> LLM Agent 是能夠自主決策、使用工具、執行多步驟任務的智能體。\n",
    "\n",
    "**核心組件**:\n",
    "\n",
    "```\n",
    "Agent\n",
    "├── LLM (決策大腦)\n",
    "├── Tools (可用工具)\n",
    "│   ├── Search (搜尋)\n",
    "│   ├── Calculator (計算)\n",
    "│   ├── Database (數據庫)\n",
    "│   └── API Calls (API 呼叫)\n",
    "├── Memory (記憶)\n",
    "└── Executor (執行器)\n",
    "```\n",
    "\n",
    "### 5.2 ReAct Agent 架構\n",
    "\n",
    "**ReAct = Reasoning + Acting**\n",
    "\n",
    "**流程**:\n",
    "```\n",
    "1. Thought: \"我需要搜尋最新的股價\"\n",
    "2. Action: Search(\"AAPL stock price\")\n",
    "3. Observation: \"Apple 股價 $180.25\"\n",
    "4. Thought: \"我現在知道股價了,可以回答\"\n",
    "5. Final Answer: \"Apple 當前股價為 $180.25\"\n",
    "```\n",
    "\n",
    "**範例 Prompt**:\n",
    "```\n",
    "你有以下工具:\n",
    "- Search: 搜尋網路資訊\n",
    "- Calculator: 執行數學計算\n",
    "\n",
    "使用以下格式:\n",
    "\n",
    "Question: 用戶問題\n",
    "Thought: 你的思考過程\n",
    "Action: 工具名稱[工具輸入]\n",
    "Observation: 工具輸出\n",
    "... (重複 Thought/Action/Observation)\n",
    "Thought: 我現在知道最終答案了\n",
    "Final Answer: 最終答案\n",
    "\n",
    "Question: Apple 股價是多少?去年同期相比漲了多少?\n",
    "```\n",
    "\n",
    "### 5.3 簡單 Agent 實作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-agent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 簡化版 Agent 實作 (示範概念)\n",
    "\n",
    "class SimpleAgent:\n",
    "    \"\"\"簡化版 ReAct Agent\"\"\"\n",
    "    \n",
    "    def __init__(self, llm, tools):\n",
    "        self.llm = llm\n",
    "        self.tools = {tool.name: tool for tool in tools}\n",
    "    \n",
    "    def run(self, question, max_steps=5):\n",
    "        \"\"\"執行 Agent\"\"\"\n",
    "        history = []\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            # 構建 prompt\n",
    "            prompt = self._build_prompt(question, history)\n",
    "            \n",
    "            # LLM 生成 thought + action\n",
    "            response = self.llm.generate(prompt)\n",
    "            \n",
    "            # 解析 response\n",
    "            if \"Final Answer:\" in response:\n",
    "                answer = response.split(\"Final Answer:\")[1].strip()\n",
    "                return answer\n",
    "            \n",
    "            # 執行工具\n",
    "            action = self._parse_action(response)\n",
    "            if action:\n",
    "                tool_name, tool_input = action\n",
    "                observation = self.tools[tool_name].run(tool_input)\n",
    "                history.append({\n",
    "                    \"thought\": response,\n",
    "                    \"action\": action,\n",
    "                    \"observation\": observation\n",
    "                })\n",
    "        \n",
    "        return \"無法在 max_steps 內找到答案\"\n",
    "    \n",
    "    def _build_prompt(self, question, history):\n",
    "        \"\"\"構建 ReAct prompt\"\"\"\n",
    "        prompt = f\"Question: {question}\\n\"\n",
    "        \n",
    "        for h in history:\n",
    "            prompt += f\"Thought: {h['thought']}\\n\"\n",
    "            prompt += f\"Action: {h['action']}\\n\"\n",
    "            prompt += f\"Observation: {h['observation']}\\n\"\n",
    "        \n",
    "        prompt += \"Thought:\"\n",
    "        return prompt\n",
    "    \n",
    "    def _parse_action(self, text):\n",
    "        \"\"\"解析 Action\"\"\"\n",
    "        # 簡化版解析\n",
    "        if \"Action:\" in text:\n",
    "            action_str = text.split(\"Action:\")[1].split(\"\\n\")[0].strip()\n",
    "            # 假設格式: ToolName[input]\n",
    "            tool_name = action_str.split(\"[\")[0].strip()\n",
    "            tool_input = action_str.split(\"[\")[1].rstrip(\"]\")\n",
    "            return (tool_name, tool_input)\n",
    "        return None\n",
    "\n",
    "# 定義工具\n",
    "class Tool:\n",
    "    def __init__(self, name, description, func):\n",
    "        self.name = name\n",
    "        self.description = description\n",
    "        self.func = func\n",
    "    \n",
    "    def run(self, input_text):\n",
    "        return self.func(input_text)\n",
    "\n",
    "# 範例工具\n",
    "def calculator(expression):\n",
    "    try:\n",
    "        result = eval(expression)\n",
    "        return str(result)\n",
    "    except:\n",
    "        return \"計算錯誤\"\n",
    "\n",
    "def search(query):\n",
    "    # 模擬搜尋\n",
    "    mock_results = {\n",
    "        \"transformer\": \"Transformer 是一種神經網路架構,由 Vaswani 等人在 2017 年提出。\",\n",
    "        \"gpt\": \"GPT 是 OpenAI 開發的生成式預訓練 Transformer 模型。\"\n",
    "    }\n",
    "    for key in mock_results:\n",
    "        if key in query.lower():\n",
    "            return mock_results[key]\n",
    "    return \"未找到相關資訊\"\n",
    "\n",
    "tools = [\n",
    "    Tool(\"Calculator\", \"執行數學計算\", calculator),\n",
    "    Tool(\"Search\", \"搜尋網路資訊\", search)\n",
    "]\n",
    "\n",
    "print(\"✅ SimpleAgent 類別定義完成\")\n",
    "print(\"\\n可用工具:\")\n",
    "for tool in tools:\n",
    "    print(f\"  • {tool.name}: {tool.description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-6",
   "metadata": {},
   "source": [
    "## 6. 生產環境部署 {#6-deployment}\n",
    "\n",
    "### 6.1 部署考量因素\n",
    "\n",
    "**1. 模型選擇**\n",
    "\n",
    "| 考量 | 小模型 (< 10B) | 大模型 (> 100B) |\n",
    "|------|---------------|----------------|\n",
    "| 延遲 | 低 (< 100ms) | 高 (> 1s) |\n",
    "| 成本 | 低 | 高 |\n",
    "| 品質 | 中 | 高 |\n",
    "| 硬體需求 | CPU/小 GPU | 多 GPU/專用硬體 |\n",
    "\n",
    "**2. 推論優化技術**\n",
    "\n",
    "**量化 (Quantization)**:\n",
    "```python\n",
    "# 8-bit 量化 (減少 75% 記憶體)\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    bnb_8bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config\n",
    ")\n",
    "```\n",
    "\n",
    "**批次處理 (Batching)**:\n",
    "- 將多個請求合併處理\n",
    "- 提高 GPU 利用率\n",
    "- 權衡延遲與吞吐量\n",
    "\n",
    "**KV Cache**:\n",
    "- 快取已計算的 Key/Value\n",
    "- 加速自回歸生成\n",
    "- 減少重複計算\n",
    "\n",
    "**3. 部署架構**\n",
    "\n",
    "**選項 A: 本地部署**\n",
    "```\n",
    "優點:\n",
    "• 數據隱私\n",
    "• 完全控制\n",
    "• 無 API 費用\n",
    "\n",
    "缺點:\n",
    "• 硬體成本高\n",
    "• 維護負擔\n",
    "• 需要 ML 工程團隊\n",
    "```\n",
    "\n",
    "**選項 B: API 服務 (OpenAI, Anthropic)**\n",
    "```\n",
    "優點:\n",
    "• 即開即用\n",
    "• 自動擴展\n",
    "• 最新模型\n",
    "\n",
    "缺點:\n",
    "• 按使用量計費\n",
    "• 數據外流\n",
    "• 依賴第三方\n",
    "```\n",
    "\n",
    "**選項 C: 混合方案**\n",
    "```\n",
    "簡單任務 → 本地小模型\n",
    "複雜任務 → API 大模型\n",
    "```\n",
    "\n",
    "### 6.2 監控與維護\n",
    "\n",
    "**關鍵指標**:\n",
    "\n",
    "1. **效能指標**\n",
    "   - 延遲 (P50, P95, P99)\n",
    "   - 吞吐量 (requests/sec)\n",
    "   - Token 速度 (tokens/sec)\n",
    "\n",
    "2. **品質指標**\n",
    "   - 幻覺率\n",
    "   - 用戶滿意度\n",
    "   - 任務成功率\n",
    "\n",
    "3. **成本指標**\n",
    "   - 每請求成本\n",
    "   - GPU 利用率\n",
    "   - 總擁有成本 (TCO)\n",
    "\n",
    "**最佳實踐**:\n",
    "\n",
    "```python\n",
    "# 1. 設置超時\n",
    "response = model.generate(..., max_time=30.0)\n",
    "\n",
    "# 2. 錯誤處理\n",
    "try:\n",
    "    result = llm_service.query(prompt)\n",
    "except TimeoutError:\n",
    "    result = fallback_response\n",
    "except Exception as e:\n",
    "    log_error(e)\n",
    "    result = error_response\n",
    "\n",
    "# 3. Rate Limiting\n",
    "from ratelimit import limits, sleep_and_retry\n",
    "\n",
    "@sleep_and_retry\n",
    "@limits(calls=10, period=60)  # 10 requests/min\n",
    "def call_llm(prompt):\n",
    "    return model.generate(prompt)\n",
    "\n",
    "# 4. 快取常見查詢\n",
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache(maxsize=1000)\n",
    "def cached_generate(prompt):\n",
    "    return model.generate(prompt)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deployment-checklist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 部署檢查清單\n",
    "\n",
    "checklist = {\n",
    "    '類別': [\n",
    "        '模型選擇',\n",
    "        '效能優化',\n",
    "        '安全性',\n",
    "        '監控',\n",
    "        '成本控制'\n",
    "    ],\n",
    "    '檢查項目': [\n",
    "        '✓ 選擇適合任務複雜度的模型規模\\n✓ 評估延遲與品質的權衡\\n✓ 考慮本地 vs API 部署',\n",
    "        '✓ 實施量化 (8-bit/4-bit)\\n✓ 使用批次處理\\n✓ 啟用 KV Cache\\n✓ 考慮模型蒸餾',\n",
    "        '✓ 輸入驗證與過濾\\n✓ 輸出內容審核\\n✓ Rate Limiting\\n✓ 數據加密',\n",
    "        '✓ 設置效能指標追蹤\\n✓ 記錄錯誤與異常\\n✓ 監控成本消耗\\n✓ A/B 測試新版本',\n",
    "        '✓ 快取常見查詢\\n✓ 使用小模型處理簡單任務\\n✓ 優化 prompt 長度\\n✓ 設置預算告警'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_checklist = pd.DataFrame(checklist)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"LLM 生產環境部署檢查清單\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "for _, row in df_checklist.iterrows():\n",
    "    print(f\"【{row['類別']}】\")\n",
    "    print(row['檢查項目'])\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-7",
   "metadata": {},
   "source": [
    "## 7. 總結 {#7-summary}\n",
    "\n",
    "### 7.1 本節核心要點\n",
    "\n",
    "✅ **Hugging Face 使用**:\n",
    "- `AutoModelForCausalLM` + `AutoTokenizer` 載入模型\n",
    "- `pipeline()` 簡化常見任務\n",
    "- 掌握生成參數 (temperature, top_k, top_p)\n",
    "\n",
    "✅ **RAG 系統**:\n",
    "- 檢索 (Retrieval) → 增強 (Augmentation) → 生成 (Generation)\n",
    "- 減少幻覺,提供來源追溯\n",
    "- 向量檢索優於關鍵字檢索\n",
    "\n",
    "✅ **LangChain 框架**:\n",
    "- 統一 LLM 介面\n",
    "- Chain 組合複雜流程\n",
    "- Memory 管理多輪對話\n",
    "\n",
    "✅ **Agent 系統**:\n",
    "- ReAct: Reasoning + Acting\n",
    "- 工具使用能力\n",
    "- 多步驟任務分解\n",
    "\n",
    "✅ **生產部署**:\n",
    "- 模型選擇: 權衡延遲/成本/品質\n",
    "- 優化技術: 量化, 批次處理, KV Cache\n",
    "- 監控: 效能/品質/成本指標\n",
    "\n",
    "### 7.2 實戰建議\n",
    "\n",
    "**從小做起**:\n",
    "1. 先用 GPT-2 / 小型開源模型熟悉流程\n",
    "2. 在 Colab/Kaggle 免費 GPU 上實驗\n",
    "3. 掌握基礎後再嘗試大模型\n",
    "\n",
    "**逐步擴展**:\n",
    "1. 基礎生成 → RAG → LangChain → Agent\n",
    "2. 本地開發 → API 測試 → 生產部署\n",
    "3. 簡單任務 → 複雜應用 → 產品化\n",
    "\n",
    "**持續學習**:\n",
    "- 關注 Hugging Face Model Hub 新模型\n",
    "- 閱讀 LangChain 官方文檔\n",
    "- 參與開源社群 (Discord, GitHub)\n",
    "\n",
    "### 7.3 CH07 課程完整總結\n",
    "\n",
    "**我們學習了什麼**:\n",
    "\n",
    "1. **CH07-01**: Transformer 架構基礎\n",
    "2. **CH07-02**: Embeddings (Token + Positional)\n",
    "3. **CH07-03**: Attention 機制深入\n",
    "4. **CH07-04**: Encoder 架構與 BERT\n",
    "5. **CH07-05**: Decoder 架構與自回歸生成\n",
    "6. **CH07-06**: 三大架構對比與選型\n",
    "7. **CH07-07**: LLM 原理 (Scaling Laws, RLHF, Prompt Engineering)\n",
    "8. **CH07-08**: 實戰應用 (RAG, LangChain, Agent, 部署)\n",
    "\n",
    "**技能樹**:\n",
    "```\n",
    "Transformer 基礎\n",
    "    ↓\n",
    "架構理解 (Encoder/Decoder/Enc-Dec)\n",
    "    ↓\n",
    "LLM 原理與使用\n",
    "    ↓\n",
    "實戰應用開發\n",
    "    ↓\n",
    "生產環境部署\n",
    "```\n",
    "\n",
    "### 7.4 延伸資源\n",
    "\n",
    "**官方文檔**:\n",
    "- [Hugging Face Transformers](https://huggingface.co/docs/transformers)\n",
    "- [LangChain Documentation](https://python.langchain.com/)\n",
    "- [OpenAI Cookbook](https://github.com/openai/openai-cookbook)\n",
    "\n",
    "**實戰項目**:\n",
    "- [LangChain Templates](https://github.com/langchain-ai/langchain/tree/master/templates)\n",
    "- [AutoGPT](https://github.com/Significant-Gravitas/AutoGPT)\n",
    "- [GPT Engineer](https://github.com/gpt-engineer-org/gpt-engineer)\n",
    "\n",
    "**社群資源**:\n",
    "- [r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/) - 本地 LLM 討論\n",
    "- [Hugging Face Discord](https://discord.gg/hugging-face)\n",
    "- [LangChain Discord](https://discord.gg/langchain)\n",
    "\n",
    "---\n",
    "\n",
    "## 🎉 恭喜完成 CH07 全部課程!\n",
    "\n",
    "你現在已經掌握:\n",
    "- ✅ Transformer 架構從零實作\n",
    "- ✅ BERT/GPT/T5 三大架構原理\n",
    "- ✅ LLM 的訓練與對齊\n",
    "- ✅ Prompt Engineering 技巧\n",
    "- ✅ RAG, LangChain, Agent 實戰\n",
    "- ✅ 生產環境部署知識\n",
    "\n",
    "**下一步建議**:\n",
    "1. 實作一個完整的 RAG 問答系統\n",
    "2. 嘗試微調開源 LLM (LoRA, QLoRA)\n",
    "3. 探索 Multimodal LLM (Vision-Language Models)\n",
    "4. 學習 LLM 安全與倫理議題\n",
    "\n",
    "---\n",
    "\n",
    "**課程完成時間**: `____年____月____日`  \n",
    "**學習心得**: ___________________________________  \n",
    "**未來計畫**: ___________________________________"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
