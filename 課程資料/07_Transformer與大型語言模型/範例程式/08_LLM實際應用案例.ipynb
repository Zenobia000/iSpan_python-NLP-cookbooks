{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# CH07-08: LLM å¯¦éš›æ‡‰ç”¨æ¡ˆä¾‹\n",
    "\n",
    "**èª²ç¨‹æ™‚é•·**: 90 åˆ†é˜  \n",
    "**é›£åº¦**: â­â­â­â­â­  \n",
    "**å‰ç½®çŸ¥è­˜**: CH07-01 è‡³ CH07-07  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š æœ¬ç¯€å­¸ç¿’ç›®æ¨™\n",
    "\n",
    "1. âœ… ä½¿ç”¨ Hugging Face Transformers è¼‰å…¥èˆ‡ä½¿ç”¨ LLM\n",
    "2. âœ… å¯¦ä½œæª¢ç´¢å¢å¼·ç”Ÿæˆ (RAG) ç³»çµ±\n",
    "3. âœ… æŒæ¡ LangChain æ¡†æ¶æ ¸å¿ƒåŠŸèƒ½\n",
    "4. âœ… è¨­è¨ˆ LLM Agent ç³»çµ±\n",
    "5. âœ… ç†è§£ç”Ÿç”¢ç’°å¢ƒéƒ¨ç½²è€ƒé‡\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“– ç›®éŒ„\n",
    "\n",
    "1. [ä½¿ç”¨ Hugging Face è¼‰å…¥ LLM](#1-huggingface)\n",
    "2. [æ–‡æœ¬ç”Ÿæˆå¯¦æˆ°](#2-text-generation)\n",
    "3. [æª¢ç´¢å¢å¼·ç”Ÿæˆ (RAG)](#3-rag)\n",
    "4. [LangChain æ¡†æ¶å…¥é–€](#4-langchain)\n",
    "5. [Agent ç³»çµ±è¨­è¨ˆ](#5-agents)\n",
    "6. [ç”Ÿç”¢ç’°å¢ƒéƒ¨ç½²](#6-deployment)\n",
    "7. [ç¸½çµ](#7-summary)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1",
   "metadata": {},
   "source": [
    "## 1. ä½¿ç”¨ Hugging Face è¼‰å…¥ LLM {#1-huggingface}\n",
    "\n",
    "### 1.1 Hugging Face Hub ç°¡ä»‹\n",
    "\n",
    "**Hugging Face** æ˜¯æœ€å¤§çš„é–‹æº AI ç¤¾ç¾¤:\n",
    "- 200,000+ é è¨“ç·´æ¨¡å‹\n",
    "- 50,000+ æ•¸æ“šé›†\n",
    "- çµ±ä¸€çš„ API ä»‹é¢\n",
    "\n",
    "**ç†±é–€é–‹æº LLM**:\n",
    "- **LLaMA 2** (Meta): 7B, 13B, 70B\n",
    "- **Mistral** (Mistral AI): 7B, 8x7B (MoE)\n",
    "- **Falcon** (TII): 7B, 40B, 180B\n",
    "- **MPT** (MosaicML): 7B, 30B\n",
    "- **Bloom** (BigScience): 176B\n",
    "\n",
    "### 1.2 å®‰è£å¿…è¦å¥—ä»¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-packages",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£å¿…è¦å¥—ä»¶ (å–æ¶ˆè¨»è§£ä»¥åŸ·è¡Œ)\n",
    "# !pip install transformers>=4.35.0\n",
    "# !pip install torch>=2.0.0\n",
    "# !pip install accelerate>=0.20.0\n",
    "# !pip install bitsandbytes>=0.40.0  # ç”¨æ–¼é‡åŒ–\n",
    "# !pip install sentence-transformers>=2.2.0  # ç”¨æ–¼ embeddings\n",
    "# !pip install faiss-cpu>=1.7.4  # ç”¨æ–¼å‘é‡æª¢ç´¢\n",
    "# !pip install langchain>=0.0.300  # LangChain æ¡†æ¶\n",
    "\n",
    "print(\"âœ… å¥—ä»¶å®‰è£æŒ‡ä»¤å·²æº–å‚™å¥½\")\n",
    "print(\"\\næ³¨æ„: å¯¦éš›åŸ·è¡Œå‰è«‹ç¢ºèªç’°å¢ƒï¼Œå»ºè­°ä½¿ç”¨è™›æ“¬ç’°å¢ƒ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¼‰å…¥åŸºç¤å¥—ä»¶\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ… åŸºç¤å¥—ä»¶è¼‰å…¥å®Œæˆ\")\n",
    "\n",
    "# å˜—è©¦è¼‰å…¥ Transformers (å¯èƒ½æœªå®‰è£)\n",
    "try:\n",
    "    from transformers import (\n",
    "        AutoModelForCausalLM,\n",
    "        AutoTokenizer,\n",
    "        pipeline,\n",
    "        BitsAndBytesConfig\n",
    "    )\n",
    "    import torch\n",
    "    \n",
    "    print(f\"âœ… Transformers {transformers.__version__} è¼‰å…¥æˆåŠŸ\")\n",
    "    print(f\"âœ… PyTorch {torch.__version__} è¼‰å…¥æˆåŠŸ\")\n",
    "    print(f\"âœ… CUDA å¯ç”¨: {torch.cuda.is_available()}\")\n",
    "    \n",
    "    HF_AVAILABLE = True\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(\"âš ï¸ Transformers æˆ– PyTorch æœªå®‰è£\")\n",
    "    print(\"   è«‹åŸ·è¡Œ: pip install transformers torch\")\n",
    "    HF_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1-2",
   "metadata": {},
   "source": [
    "### 1.3 è¼‰å…¥å°å‹ LLM (GPT-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-gpt2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if HF_AVAILABLE:\n",
    "    print(\"è¼‰å…¥ GPT-2 Small (124M åƒæ•¸)...\")\n",
    "    \n",
    "    # è¼‰å…¥ tokenizer å’Œæ¨¡å‹\n",
    "    model_name = \"gpt2\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    \n",
    "    # ç§»åˆ° GPU (å¦‚æœå¯ç”¨)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = model.to(device)\n",
    "    \n",
    "    print(f\"âœ… GPT-2 è¼‰å…¥å®Œæˆ (Device: {device})\")\n",
    "    print(f\"âœ… Tokenizer è©å½™é‡: {len(tokenizer)}\")\n",
    "    print(f\"âœ… æ¨¡å‹åƒæ•¸é‡: {model.num_parameters():,}\")\n",
    "    \n",
    "    # æ¸¬è©¦ç”Ÿæˆ\n",
    "    prompt = \"Artificial intelligence is\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=30,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.7,\n",
    "        do_sample=True\n",
    "    )\n",
    "    \n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(\"\\nç”Ÿæˆç¯„ä¾‹:\")\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Output: {generated_text}\")\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸ è«‹å…ˆå®‰è£ transformers å’Œ torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2",
   "metadata": {},
   "source": [
    "## 2. æ–‡æœ¬ç”Ÿæˆå¯¦æˆ° {#2-text-generation}\n",
    "\n",
    "### 2.1 ç”Ÿæˆåƒæ•¸è©³è§£\n",
    "\n",
    "**é—œéµåƒæ•¸**:\n",
    "\n",
    "| åƒæ•¸ | èªªæ˜ | æ¨è–¦å€¼ |\n",
    "|------|------|--------|\n",
    "| **max_new_tokens** | ç”Ÿæˆçš„æœ€å¤§ token æ•¸ | 50-500 |\n",
    "| **temperature** | éš¨æ©Ÿæ€§æ§åˆ¶ (è¶Šé«˜è¶Šéš¨æ©Ÿ) | 0.7-1.0 (å‰µæ„), 0.1-0.3 (äº‹å¯¦) |\n",
    "| **top_k** | åªå¾ top-k å€‹ token ä¸­æ¡æ¨£ | 50 |\n",
    "| **top_p** | Nucleus sampling (ç´¯ç©æ©Ÿç‡) | 0.9-0.95 |\n",
    "| **num_beams** | Beam search å¯¬åº¦ | 1 (greedy), 4-5 (å“è³ªå„ªå…ˆ) |\n",
    "| **do_sample** | æ˜¯å¦éš¨æ©Ÿæ¡æ¨£ | True (å¤šæ¨£), False (ç¢ºå®šæ€§) |\n",
    "| **repetition_penalty** | é‡è¤‡æ‡²ç½° | 1.2-1.5 |\n",
    "\n",
    "### 2.2 ä¸åŒç”Ÿæˆç­–ç•¥å°æ¯”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generation-strategies",
   "metadata": {},
   "outputs": [],
   "source": [
    "if HF_AVAILABLE:\n",
    "    prompt = \"The future of artificial intelligence will\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    strategies = [\n",
    "        {\n",
    "            'name': 'Greedy (ç¢ºå®šæ€§)',\n",
    "            'params': {\n",
    "                'max_new_tokens': 40,\n",
    "                'do_sample': False\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'name': 'Sampling (éš¨æ©Ÿ)',\n",
    "            'params': {\n",
    "                'max_new_tokens': 40,\n",
    "                'do_sample': True,\n",
    "                'temperature': 1.0\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'name': 'Top-k Sampling',\n",
    "            'params': {\n",
    "                'max_new_tokens': 40,\n",
    "                'do_sample': True,\n",
    "                'top_k': 50,\n",
    "                'temperature': 0.8\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'name': 'Nucleus (Top-p) Sampling',\n",
    "            'params': {\n",
    "                'max_new_tokens': 40,\n",
    "                'do_sample': True,\n",
    "                'top_p': 0.9,\n",
    "                'temperature': 0.8\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'name': 'Beam Search',\n",
    "            'params': {\n",
    "                'max_new_tokens': 40,\n",
    "                'num_beams': 5,\n",
    "                'early_stopping': True\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Prompt: '{prompt}'\")\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "    \n",
    "    for strategy in strategies:\n",
    "        outputs = model.generate(**inputs, **strategy['params'])\n",
    "        text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        print(f\"ã€{strategy['name']}ã€‘\")\n",
    "        print(f\"{text}\")\n",
    "        print()\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\nè§€å¯Ÿ:\")\n",
    "    print(\"â€¢ Greedy: æ¯æ¬¡éƒ½ç”Ÿæˆç›¸åŒçµæœ\")\n",
    "    print(\"â€¢ Sampling: æ¯æ¬¡çµæœä¸åŒ,æ›´æœ‰å‰µæ„\")\n",
    "    print(\"â€¢ Top-k/Top-p: å¹³è¡¡å“è³ªèˆ‡å¤šæ¨£æ€§\")\n",
    "    print(\"â€¢ Beam Search: å“è³ªæœ€é«˜ä½†ç¼ºä¹å¤šæ¨£æ€§\")\n",
    "\n",
    "else:\n",
    "    print(\"âš ï¸ éœ€è¦å®‰è£ transformers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "use-pipeline",
   "metadata": {},
   "source": [
    "### 2.3 ä½¿ç”¨ Pipeline ç°¡åŒ–æ“ä½œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pipeline-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "if HF_AVAILABLE:\n",
    "    # å»ºç«‹æ–‡æœ¬ç”Ÿæˆ pipeline\n",
    "    generator = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model_name,\n",
    "        device=0 if torch.cuda.is_available() else -1  # 0=GPU, -1=CPU\n",
    "    )\n",
    "    \n",
    "    # ç”Ÿæˆæ–‡æœ¬\n",
    "    prompt = \"Once upon a time in a magical forest,\"\n",
    "    \n",
    "    results = generator(\n",
    "        prompt,\n",
    "        max_new_tokens=60,\n",
    "        num_return_sequences=3,  # ç”Ÿæˆ 3 å€‹ç‰ˆæœ¬\n",
    "        temperature=0.8,\n",
    "        top_p=0.95,\n",
    "        do_sample=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Prompt: '{prompt}'\\n\")\n",
    "    print(\"ç”Ÿæˆçš„ 3 å€‹ç‰ˆæœ¬:\\n\")\n",
    "    \n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"ç‰ˆæœ¬ {i}:\")\n",
    "        print(result['generated_text'])\n",
    "        print()\n",
    "\n",
    "else:\n",
    "    print(\"âš ï¸ éœ€è¦å®‰è£ transformers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3",
   "metadata": {},
   "source": [
    "## 3. æª¢ç´¢å¢å¼·ç”Ÿæˆ (RAG) {#3-rag}\n",
    "\n",
    "### 3.1 RAG åŸç†\n",
    "\n",
    "**å•é¡Œ**: LLM çš„çŸ¥è­˜æ˜¯éœæ…‹çš„,ç„¡æ³•ç²å–æœ€æ–°è³‡è¨Š\n",
    "\n",
    "**è§£æ±ºæ–¹æ¡ˆ**: Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "```\n",
    "ç”¨æˆ¶å•é¡Œ\n",
    "    â†“\n",
    "â‘  æª¢ç´¢ç›¸é—œæ–‡æª” (Retrieval)\n",
    "    â†“\n",
    "    å‘é‡æ•¸æ“šåº«æœå°‹\n",
    "    â†“\n",
    "â‘¡ æ•´åˆä¸Šä¸‹æ–‡ (Augmentation)\n",
    "    â†“\n",
    "    å•é¡Œ + æª¢ç´¢åˆ°çš„æ–‡æª” â†’ Prompt\n",
    "    â†“\n",
    "â‘¢ LLM ç”Ÿæˆç­”æ¡ˆ (Generation)\n",
    "    â†“\n",
    "   æœ€çµ‚ç­”æ¡ˆ\n",
    "```\n",
    "\n",
    "**å„ªå‹¢**:\n",
    "- âœ… ç²å–æœ€æ–°è³‡è¨Š\n",
    "- âœ… æ¸›å°‘å¹»è¦º\n",
    "- âœ… å¯è¿½æº¯ä¾†æº\n",
    "- âœ… ç„¡éœ€é‡æ–°è¨“ç·´\n",
    "\n",
    "### 3.2 å¯¦ä½œç°¡å–® RAG ç³»çµ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rag-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¨¡æ“¬çŸ¥è­˜åº«\n",
    "knowledge_base = [\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"text\": \"Transformer æ˜¯ä¸€ç¨®åŸºæ–¼è‡ªæ³¨æ„åŠ›æ©Ÿåˆ¶çš„ç¥ç¶“ç¶²è·¯æ¶æ§‹,ç”± Vaswani ç­‰äººæ–¼ 2017 å¹´æå‡ºã€‚å®ƒå¾¹åº•æ”¹è®Šäº† NLP é ˜åŸŸã€‚\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 2,\n",
    "        \"text\": \"BERT æ˜¯ Google åœ¨ 2018 å¹´ç™¼å¸ƒçš„é è¨“ç·´èªè¨€æ¨¡å‹,ä½¿ç”¨ Transformer Encoder æ¶æ§‹,åœ¨å¤šå€‹ NLP ä»»å‹™ä¸Šå–å¾—çªç ´æ€§æˆæœã€‚\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 3,\n",
    "        \"text\": \"GPT-3 æ˜¯ OpenAI æ–¼ 2020 å¹´ç™¼å¸ƒçš„å¤§å‹èªè¨€æ¨¡å‹,æ“æœ‰ 175B åƒæ•¸,å±•ç¾äº†å¼·å¤§çš„ few-shot learning èƒ½åŠ›ã€‚\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 4,\n",
    "        \"text\": \"LLaMA æ˜¯ Meta åœ¨ 2023 å¹´é–‹æºçš„é«˜æ•ˆ LLM ç³»åˆ—,åƒæ•¸å¾ 7B åˆ° 65B,åœ¨ç›¸åŒåƒæ•¸é‡ä¸‹è¡¨ç¾å„ªæ–¼å…¶ä»–é–‹æºæ¨¡å‹ã€‚\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 5,\n",
    "        \"text\": \"RAG (æª¢ç´¢å¢å¼·ç”Ÿæˆ) çµåˆäº†æª¢ç´¢ç³»çµ±å’Œç”Ÿæˆæ¨¡å‹,é€šéæª¢ç´¢ç›¸é—œæ–‡æª”ä¾†å¢å¼· LLM çš„å›ç­”,æ¸›å°‘å¹»è¦ºå•é¡Œã€‚\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"âœ… çŸ¥è­˜åº«å»ºç«‹å®Œæˆ\")\n",
    "print(f\"   å…± {len(knowledge_base)} ç­†æ–‡æª”\")\n",
    "\n",
    "# ç°¡å–®çš„é—œéµå­—æª¢ç´¢ (å¯¦éš›æ‡‰ç”¨ä¸­æ‡‰ä½¿ç”¨å‘é‡æª¢ç´¢)\n",
    "def simple_retrieve(query, knowledge_base, top_k=2):\n",
    "    \"\"\"ç°¡å–®çš„é—œéµå­—åŒ¹é…æª¢ç´¢\"\"\"\n",
    "    scores = []\n",
    "    query_lower = query.lower()\n",
    "    \n",
    "    for doc in knowledge_base:\n",
    "        # è¨ˆç®—åŒ¹é…åˆ†æ•¸ (ç°¡åŒ–ç‰ˆ)\n",
    "        text_lower = doc['text'].lower()\n",
    "        score = sum(1 for word in query_lower.split() if word in text_lower)\n",
    "        scores.append((doc, score))\n",
    "    \n",
    "    # æ’åºä¸¦è¿”å› top-k\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    return [doc for doc, score in scores[:top_k]]\n",
    "\n",
    "# æ¸¬è©¦æª¢ç´¢\n",
    "test_query = \"ä»€éº¼æ˜¯ Transformer?\"\n",
    "retrieved_docs = simple_retrieve(test_query, knowledge_base)\n",
    "\n",
    "print(f\"\\næŸ¥è©¢: '{test_query}'\")\n",
    "print(\"\\næª¢ç´¢åˆ°çš„æ–‡æª”:\")\n",
    "for doc in retrieved_docs:\n",
    "    print(f\"  [{doc['id']}] {doc['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rag-qa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG å•ç­”å‡½æ•¸\n",
    "def rag_qa(query, knowledge_base, model, tokenizer, top_k=2):\n",
    "    \"\"\"\n",
    "    RAG å•ç­”ç³»çµ±\n",
    "    \n",
    "    Args:\n",
    "        query: ç”¨æˆ¶å•é¡Œ\n",
    "        knowledge_base: çŸ¥è­˜åº«\n",
    "        model: LLM æ¨¡å‹\n",
    "        tokenizer: Tokenizer\n",
    "        top_k: æª¢ç´¢æ–‡æª”æ•¸é‡\n",
    "    \n",
    "    Returns:\n",
    "        ç­”æ¡ˆæ–‡æœ¬\n",
    "    \"\"\"\n",
    "    # 1. æª¢ç´¢ç›¸é—œæ–‡æª”\n",
    "    docs = simple_retrieve(query, knowledge_base, top_k)\n",
    "    \n",
    "    # 2. æ§‹å»º prompt\n",
    "    context = \"\\n\".join([f\"- {doc['text']}\" for doc in docs])\n",
    "    \n",
    "    prompt = f\"\"\"æ ¹æ“šä»¥ä¸‹è³‡è¨Šå›ç­”å•é¡Œ:\n",
    "\n",
    "{context}\n",
    "\n",
    "å•é¡Œ: {query}\n",
    "ç­”æ¡ˆ:\"\"\"\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"æ§‹å»ºçš„ Prompt:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(prompt)\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 3. ç”Ÿæˆç­”æ¡ˆ\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        temperature=0.3,  # é™ä½éš¨æ©Ÿæ€§,æ›´äº‹å¯¦æ€§\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # æå–ç­”æ¡ˆéƒ¨åˆ†\n",
    "    answer = answer.split(\"ç­”æ¡ˆ:\")[-1].strip()\n",
    "    \n",
    "    return answer, docs\n",
    "\n",
    "# æ¸¬è©¦ RAG ç³»çµ±\n",
    "if HF_AVAILABLE:\n",
    "    queries = [\n",
    "        \"ä»€éº¼æ˜¯ Transformer?\",\n",
    "        \"GPT-3 æœ‰å¤šå°‘åƒæ•¸?\",\n",
    "        \"BERT æ˜¯ä»€éº¼æ™‚å€™ç™¼å¸ƒçš„?\"\n",
    "    ]\n",
    "    \n",
    "    for query in queries:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(f\"å•é¡Œ: {query}\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        answer, sources = rag_qa(query, knowledge_base, model, tokenizer)\n",
    "        \n",
    "        print(f\"\\nç­”æ¡ˆ: {answer}\")\n",
    "        print(f\"\\nä¾†æº:\")\n",
    "        for doc in sources:\n",
    "            print(f\"  [{doc['id']}] {doc['text'][:50]}...\")\n",
    "\n",
    "else:\n",
    "    print(\"âš ï¸ éœ€è¦è¼‰å…¥ LLM æ¨¡å‹\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vector-search",
   "metadata": {},
   "source": [
    "### 3.3 ä½¿ç”¨å‘é‡æª¢ç´¢ (é€²éš)\n",
    "\n",
    "**å¯¦éš› RAG ç³»çµ±æ‡‰ä½¿ç”¨èªç¾©ç›¸ä¼¼åº¦æª¢ç´¢**:\n",
    "\n",
    "```python\n",
    "# ä½¿ç”¨ Sentence Transformers + FAISS\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "# 1. è¼‰å…¥ embedding æ¨¡å‹\n",
    "embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# 2. å°‡æ–‡æª”è½‰æ›ç‚ºå‘é‡\n",
    "doc_embeddings = embed_model.encode([doc['text'] for doc in knowledge_base])\n",
    "\n",
    "# 3. å»ºç«‹ FAISS ç´¢å¼•\n",
    "index = faiss.IndexFlatL2(doc_embeddings.shape[1])\n",
    "index.add(doc_embeddings)\n",
    "\n",
    "# 4. æª¢ç´¢\n",
    "query_embedding = embed_model.encode([query])\n",
    "distances, indices = index.search(query_embedding, k=2)\n",
    "```\n",
    "\n",
    "**å‘é‡æª¢ç´¢ vs é—œéµå­—æª¢ç´¢**:\n",
    "\n",
    "| ç‰¹æ€§ | é—œéµå­—æª¢ç´¢ | å‘é‡æª¢ç´¢ |\n",
    "|------|-----------|----------|\n",
    "| ç²¾ç¢ºåº¦ | ä½ | é«˜ |\n",
    "| èªç¾©ç†è§£ | âŒ | âœ… |\n",
    "| åŒç¾©è© | âŒ | âœ… |\n",
    "| é€Ÿåº¦ | å¿« | éœ€ç´¢å¼•åŠ é€Ÿ |\n",
    "| å¯¦ä½œè¤‡é›œåº¦ | ä½ | ä¸­ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4",
   "metadata": {},
   "source": [
    "## 4. LangChain æ¡†æ¶å…¥é–€ {#4-langchain}\n",
    "\n",
    "### 4.1 LangChain ç°¡ä»‹\n",
    "\n",
    "**LangChain** æ˜¯æ§‹å»º LLM æ‡‰ç”¨çš„æ¡†æ¶:\n",
    "- çµ±ä¸€çš„ LLM ä»‹é¢\n",
    "- éˆå¼çµ„åˆ (Chain)\n",
    "- è¨˜æ†¶ç®¡ç† (Memory)\n",
    "- Agent ç³»çµ±\n",
    "\n",
    "**æ ¸å¿ƒæ¦‚å¿µ**:\n",
    "\n",
    "```\n",
    "LangChain\n",
    "â”œâ”€â”€ Models (LLM, Chat Model, Embeddings)\n",
    "â”œâ”€â”€ Prompts (PromptTemplate, ChatPromptTemplate)\n",
    "â”œâ”€â”€ Chains (LLMChain, SequentialChain, ...)\n",
    "â”œâ”€â”€ Memory (ConversationBufferMemory, ...)\n",
    "â”œâ”€â”€ Agents (ReAct, Plan-and-Execute, ...)\n",
    "â””â”€â”€ Tools (Search, Calculator, API calls, ...)\n",
    "```\n",
    "\n",
    "### 4.2 LangChain åŸºç¤ç¯„ä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "langchain-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain ç¯„ä¾‹ (ç¤ºç¯„ç¨‹å¼ç¢¼,éœ€å®‰è£ langchain)\n",
    "\n",
    "langchain_code = '''\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# 1. åŒ…è£ HuggingFace æ¨¡å‹ç‚º LangChain LLM\n",
    "llm = HuggingFacePipeline(pipeline=generator)\n",
    "\n",
    "# 2. å»ºç«‹ Prompt Template\n",
    "template = \"\"\"ä½ æ˜¯ä¸€å€‹å‹å–„çš„ AI åŠ©æ‰‹ã€‚\n",
    "\n",
    "å°è©±æ­·å²:\n",
    "{history}\n",
    "\n",
    "ç”¨æˆ¶: {input}\n",
    "åŠ©æ‰‹:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"history\", \"input\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "# 3. å»ºç«‹ Memory\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"history\",\n",
    "    input_key=\"input\"\n",
    ")\n",
    "\n",
    "# 4. å»ºç«‹ Chain\n",
    "conversation = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 5. å¤šè¼ªå°è©±\n",
    "print(conversation.predict(input=\"ä½ å¥½,ä½ æ˜¯èª°?\"))\n",
    "print(conversation.predict(input=\"ä½ èƒ½åšä»€éº¼?\"))\n",
    "print(conversation.predict(input=\"å‰›æ‰æˆ‘å•äº†ä»€éº¼?\"))  # æ¸¬è©¦è¨˜æ†¶\n",
    "'''\n",
    "\n",
    "print(\"LangChain å°è©±ç³»çµ±ç¯„ä¾‹ç¨‹å¼ç¢¼:\")\n",
    "print(\"=\" * 80)\n",
    "print(langchain_code)\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nèªªæ˜:\")\n",
    "print(\"â€¢ LLMChain: çµ„åˆ LLM + Prompt + Memory\")\n",
    "print(\"â€¢ ConversationBufferMemory: è¨˜ä½å®Œæ•´å°è©±æ­·å²\")\n",
    "print(\"â€¢ PromptTemplate: å‹•æ…‹å¡«å……è®Šæ•¸åˆ° prompt\")\n",
    "print(\"â€¢ è‡ªå‹•è™•ç†å¤šè¼ªå°è©±çš„ä¸Šä¸‹æ–‡å‚³é\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-5",
   "metadata": {},
   "source": [
    "## 5. Agent ç³»çµ±è¨­è¨ˆ {#5-agents}\n",
    "\n",
    "### 5.1 ä»€éº¼æ˜¯ Agent?\n",
    "\n",
    "**å®šç¾©**:\n",
    "> LLM Agent æ˜¯èƒ½å¤ è‡ªä¸»æ±ºç­–ã€ä½¿ç”¨å·¥å…·ã€åŸ·è¡Œå¤šæ­¥é©Ÿä»»å‹™çš„æ™ºèƒ½é«”ã€‚\n",
    "\n",
    "**æ ¸å¿ƒçµ„ä»¶**:\n",
    "\n",
    "```\n",
    "Agent\n",
    "â”œâ”€â”€ LLM (æ±ºç­–å¤§è…¦)\n",
    "â”œâ”€â”€ Tools (å¯ç”¨å·¥å…·)\n",
    "â”‚   â”œâ”€â”€ Search (æœå°‹)\n",
    "â”‚   â”œâ”€â”€ Calculator (è¨ˆç®—)\n",
    "â”‚   â”œâ”€â”€ Database (æ•¸æ“šåº«)\n",
    "â”‚   â””â”€â”€ API Calls (API å‘¼å«)\n",
    "â”œâ”€â”€ Memory (è¨˜æ†¶)\n",
    "â””â”€â”€ Executor (åŸ·è¡Œå™¨)\n",
    "```\n",
    "\n",
    "### 5.2 ReAct Agent æ¶æ§‹\n",
    "\n",
    "**ReAct = Reasoning + Acting**\n",
    "\n",
    "**æµç¨‹**:\n",
    "```\n",
    "1. Thought: \"æˆ‘éœ€è¦æœå°‹æœ€æ–°çš„è‚¡åƒ¹\"\n",
    "2. Action: Search(\"AAPL stock price\")\n",
    "3. Observation: \"Apple è‚¡åƒ¹ $180.25\"\n",
    "4. Thought: \"æˆ‘ç¾åœ¨çŸ¥é“è‚¡åƒ¹äº†,å¯ä»¥å›ç­”\"\n",
    "5. Final Answer: \"Apple ç•¶å‰è‚¡åƒ¹ç‚º $180.25\"\n",
    "```\n",
    "\n",
    "**ç¯„ä¾‹ Prompt**:\n",
    "```\n",
    "ä½ æœ‰ä»¥ä¸‹å·¥å…·:\n",
    "- Search: æœå°‹ç¶²è·¯è³‡è¨Š\n",
    "- Calculator: åŸ·è¡Œæ•¸å­¸è¨ˆç®—\n",
    "\n",
    "ä½¿ç”¨ä»¥ä¸‹æ ¼å¼:\n",
    "\n",
    "Question: ç”¨æˆ¶å•é¡Œ\n",
    "Thought: ä½ çš„æ€è€ƒéç¨‹\n",
    "Action: å·¥å…·åç¨±[å·¥å…·è¼¸å…¥]\n",
    "Observation: å·¥å…·è¼¸å‡º\n",
    "... (é‡è¤‡ Thought/Action/Observation)\n",
    "Thought: æˆ‘ç¾åœ¨çŸ¥é“æœ€çµ‚ç­”æ¡ˆäº†\n",
    "Final Answer: æœ€çµ‚ç­”æ¡ˆ\n",
    "\n",
    "Question: Apple è‚¡åƒ¹æ˜¯å¤šå°‘?å»å¹´åŒæœŸç›¸æ¯”æ¼²äº†å¤šå°‘?\n",
    "```\n",
    "\n",
    "### 5.3 ç°¡å–® Agent å¯¦ä½œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-agent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç°¡åŒ–ç‰ˆ Agent å¯¦ä½œ (ç¤ºç¯„æ¦‚å¿µ)\n",
    "\n",
    "class SimpleAgent:\n",
    "    \"\"\"ç°¡åŒ–ç‰ˆ ReAct Agent\"\"\"\n",
    "    \n",
    "    def __init__(self, llm, tools):\n",
    "        self.llm = llm\n",
    "        self.tools = {tool.name: tool for tool in tools}\n",
    "    \n",
    "    def run(self, question, max_steps=5):\n",
    "        \"\"\"åŸ·è¡Œ Agent\"\"\"\n",
    "        history = []\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            # æ§‹å»º prompt\n",
    "            prompt = self._build_prompt(question, history)\n",
    "            \n",
    "            # LLM ç”Ÿæˆ thought + action\n",
    "            response = self.llm.generate(prompt)\n",
    "            \n",
    "            # è§£æ response\n",
    "            if \"Final Answer:\" in response:\n",
    "                answer = response.split(\"Final Answer:\")[1].strip()\n",
    "                return answer\n",
    "            \n",
    "            # åŸ·è¡Œå·¥å…·\n",
    "            action = self._parse_action(response)\n",
    "            if action:\n",
    "                tool_name, tool_input = action\n",
    "                observation = self.tools[tool_name].run(tool_input)\n",
    "                history.append({\n",
    "                    \"thought\": response,\n",
    "                    \"action\": action,\n",
    "                    \"observation\": observation\n",
    "                })\n",
    "        \n",
    "        return \"ç„¡æ³•åœ¨ max_steps å…§æ‰¾åˆ°ç­”æ¡ˆ\"\n",
    "    \n",
    "    def _build_prompt(self, question, history):\n",
    "        \"\"\"æ§‹å»º ReAct prompt\"\"\"\n",
    "        prompt = f\"Question: {question}\\n\"\n",
    "        \n",
    "        for h in history:\n",
    "            prompt += f\"Thought: {h['thought']}\\n\"\n",
    "            prompt += f\"Action: {h['action']}\\n\"\n",
    "            prompt += f\"Observation: {h['observation']}\\n\"\n",
    "        \n",
    "        prompt += \"Thought:\"\n",
    "        return prompt\n",
    "    \n",
    "    def _parse_action(self, text):\n",
    "        \"\"\"è§£æ Action\"\"\"\n",
    "        # ç°¡åŒ–ç‰ˆè§£æ\n",
    "        if \"Action:\" in text:\n",
    "            action_str = text.split(\"Action:\")[1].split(\"\\n\")[0].strip()\n",
    "            # å‡è¨­æ ¼å¼: ToolName[input]\n",
    "            tool_name = action_str.split(\"[\")[0].strip()\n",
    "            tool_input = action_str.split(\"[\")[1].rstrip(\"]\")\n",
    "            return (tool_name, tool_input)\n",
    "        return None\n",
    "\n",
    "# å®šç¾©å·¥å…·\n",
    "class Tool:\n",
    "    def __init__(self, name, description, func):\n",
    "        self.name = name\n",
    "        self.description = description\n",
    "        self.func = func\n",
    "    \n",
    "    def run(self, input_text):\n",
    "        return self.func(input_text)\n",
    "\n",
    "# ç¯„ä¾‹å·¥å…·\n",
    "def calculator(expression):\n",
    "    try:\n",
    "        result = eval(expression)\n",
    "        return str(result)\n",
    "    except:\n",
    "        return \"è¨ˆç®—éŒ¯èª¤\"\n",
    "\n",
    "def search(query):\n",
    "    # æ¨¡æ“¬æœå°‹\n",
    "    mock_results = {\n",
    "        \"transformer\": \"Transformer æ˜¯ä¸€ç¨®ç¥ç¶“ç¶²è·¯æ¶æ§‹,ç”± Vaswani ç­‰äººåœ¨ 2017 å¹´æå‡ºã€‚\",\n",
    "        \"gpt\": \"GPT æ˜¯ OpenAI é–‹ç™¼çš„ç”Ÿæˆå¼é è¨“ç·´ Transformer æ¨¡å‹ã€‚\"\n",
    "    }\n",
    "    for key in mock_results:\n",
    "        if key in query.lower():\n",
    "            return mock_results[key]\n",
    "    return \"æœªæ‰¾åˆ°ç›¸é—œè³‡è¨Š\"\n",
    "\n",
    "tools = [\n",
    "    Tool(\"Calculator\", \"åŸ·è¡Œæ•¸å­¸è¨ˆç®—\", calculator),\n",
    "    Tool(\"Search\", \"æœå°‹ç¶²è·¯è³‡è¨Š\", search)\n",
    "]\n",
    "\n",
    "print(\"âœ… SimpleAgent é¡åˆ¥å®šç¾©å®Œæˆ\")\n",
    "print(\"\\nå¯ç”¨å·¥å…·:\")\n",
    "for tool in tools:\n",
    "    print(f\"  â€¢ {tool.name}: {tool.description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-6",
   "metadata": {},
   "source": [
    "## 6. ç”Ÿç”¢ç’°å¢ƒéƒ¨ç½² {#6-deployment}\n",
    "\n",
    "### 6.1 éƒ¨ç½²è€ƒé‡å› ç´ \n",
    "\n",
    "**1. æ¨¡å‹é¸æ“‡**\n",
    "\n",
    "| è€ƒé‡ | å°æ¨¡å‹ (< 10B) | å¤§æ¨¡å‹ (> 100B) |\n",
    "|------|---------------|----------------|\n",
    "| å»¶é² | ä½ (< 100ms) | é«˜ (> 1s) |\n",
    "| æˆæœ¬ | ä½ | é«˜ |\n",
    "| å“è³ª | ä¸­ | é«˜ |\n",
    "| ç¡¬é«”éœ€æ±‚ | CPU/å° GPU | å¤š GPU/å°ˆç”¨ç¡¬é«” |\n",
    "\n",
    "**2. æ¨è«–å„ªåŒ–æŠ€è¡“**\n",
    "\n",
    "**é‡åŒ– (Quantization)**:\n",
    "```python\n",
    "# 8-bit é‡åŒ– (æ¸›å°‘ 75% è¨˜æ†¶é«”)\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    bnb_8bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config\n",
    ")\n",
    "```\n",
    "\n",
    "**æ‰¹æ¬¡è™•ç† (Batching)**:\n",
    "- å°‡å¤šå€‹è«‹æ±‚åˆä½µè™•ç†\n",
    "- æé«˜ GPU åˆ©ç”¨ç‡\n",
    "- æ¬Šè¡¡å»¶é²èˆ‡ååé‡\n",
    "\n",
    "**KV Cache**:\n",
    "- å¿«å–å·²è¨ˆç®—çš„ Key/Value\n",
    "- åŠ é€Ÿè‡ªå›æ­¸ç”Ÿæˆ\n",
    "- æ¸›å°‘é‡è¤‡è¨ˆç®—\n",
    "\n",
    "**3. éƒ¨ç½²æ¶æ§‹**\n",
    "\n",
    "**é¸é … A: æœ¬åœ°éƒ¨ç½²**\n",
    "```\n",
    "å„ªé»:\n",
    "â€¢ æ•¸æ“šéš±ç§\n",
    "â€¢ å®Œå…¨æ§åˆ¶\n",
    "â€¢ ç„¡ API è²»ç”¨\n",
    "\n",
    "ç¼ºé»:\n",
    "â€¢ ç¡¬é«”æˆæœ¬é«˜\n",
    "â€¢ ç¶­è­·è² æ“”\n",
    "â€¢ éœ€è¦ ML å·¥ç¨‹åœ˜éšŠ\n",
    "```\n",
    "\n",
    "**é¸é … B: API æœå‹™ (OpenAI, Anthropic)**\n",
    "```\n",
    "å„ªé»:\n",
    "â€¢ å³é–‹å³ç”¨\n",
    "â€¢ è‡ªå‹•æ“´å±•\n",
    "â€¢ æœ€æ–°æ¨¡å‹\n",
    "\n",
    "ç¼ºé»:\n",
    "â€¢ æŒ‰ä½¿ç”¨é‡è¨ˆè²»\n",
    "â€¢ æ•¸æ“šå¤–æµ\n",
    "â€¢ ä¾è³´ç¬¬ä¸‰æ–¹\n",
    "```\n",
    "\n",
    "**é¸é … C: æ··åˆæ–¹æ¡ˆ**\n",
    "```\n",
    "ç°¡å–®ä»»å‹™ â†’ æœ¬åœ°å°æ¨¡å‹\n",
    "è¤‡é›œä»»å‹™ â†’ API å¤§æ¨¡å‹\n",
    "```\n",
    "\n",
    "### 6.2 ç›£æ§èˆ‡ç¶­è­·\n",
    "\n",
    "**é—œéµæŒ‡æ¨™**:\n",
    "\n",
    "1. **æ•ˆèƒ½æŒ‡æ¨™**\n",
    "   - å»¶é² (P50, P95, P99)\n",
    "   - ååé‡ (requests/sec)\n",
    "   - Token é€Ÿåº¦ (tokens/sec)\n",
    "\n",
    "2. **å“è³ªæŒ‡æ¨™**\n",
    "   - å¹»è¦ºç‡\n",
    "   - ç”¨æˆ¶æ»¿æ„åº¦\n",
    "   - ä»»å‹™æˆåŠŸç‡\n",
    "\n",
    "3. **æˆæœ¬æŒ‡æ¨™**\n",
    "   - æ¯è«‹æ±‚æˆæœ¬\n",
    "   - GPU åˆ©ç”¨ç‡\n",
    "   - ç¸½æ“æœ‰æˆæœ¬ (TCO)\n",
    "\n",
    "**æœ€ä½³å¯¦è¸**:\n",
    "\n",
    "```python\n",
    "# 1. è¨­ç½®è¶…æ™‚\n",
    "response = model.generate(..., max_time=30.0)\n",
    "\n",
    "# 2. éŒ¯èª¤è™•ç†\n",
    "try:\n",
    "    result = llm_service.query(prompt)\n",
    "except TimeoutError:\n",
    "    result = fallback_response\n",
    "except Exception as e:\n",
    "    log_error(e)\n",
    "    result = error_response\n",
    "\n",
    "# 3. Rate Limiting\n",
    "from ratelimit import limits, sleep_and_retry\n",
    "\n",
    "@sleep_and_retry\n",
    "@limits(calls=10, period=60)  # 10 requests/min\n",
    "def call_llm(prompt):\n",
    "    return model.generate(prompt)\n",
    "\n",
    "# 4. å¿«å–å¸¸è¦‹æŸ¥è©¢\n",
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache(maxsize=1000)\n",
    "def cached_generate(prompt):\n",
    "    return model.generate(prompt)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deployment-checklist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# éƒ¨ç½²æª¢æŸ¥æ¸…å–®\n",
    "\n",
    "checklist = {\n",
    "    'é¡åˆ¥': [\n",
    "        'æ¨¡å‹é¸æ“‡',\n",
    "        'æ•ˆèƒ½å„ªåŒ–',\n",
    "        'å®‰å…¨æ€§',\n",
    "        'ç›£æ§',\n",
    "        'æˆæœ¬æ§åˆ¶'\n",
    "    ],\n",
    "    'æª¢æŸ¥é …ç›®': [\n",
    "        'âœ“ é¸æ“‡é©åˆä»»å‹™è¤‡é›œåº¦çš„æ¨¡å‹è¦æ¨¡\\nâœ“ è©•ä¼°å»¶é²èˆ‡å“è³ªçš„æ¬Šè¡¡\\nâœ“ è€ƒæ…®æœ¬åœ° vs API éƒ¨ç½²',\n",
    "        'âœ“ å¯¦æ–½é‡åŒ– (8-bit/4-bit)\\nâœ“ ä½¿ç”¨æ‰¹æ¬¡è™•ç†\\nâœ“ å•Ÿç”¨ KV Cache\\nâœ“ è€ƒæ…®æ¨¡å‹è’¸é¤¾',\n",
    "        'âœ“ è¼¸å…¥é©—è­‰èˆ‡éæ¿¾\\nâœ“ è¼¸å‡ºå…§å®¹å¯©æ ¸\\nâœ“ Rate Limiting\\nâœ“ æ•¸æ“šåŠ å¯†',\n",
    "        'âœ“ è¨­ç½®æ•ˆèƒ½æŒ‡æ¨™è¿½è¹¤\\nâœ“ è¨˜éŒ„éŒ¯èª¤èˆ‡ç•°å¸¸\\nâœ“ ç›£æ§æˆæœ¬æ¶ˆè€—\\nâœ“ A/B æ¸¬è©¦æ–°ç‰ˆæœ¬',\n",
    "        'âœ“ å¿«å–å¸¸è¦‹æŸ¥è©¢\\nâœ“ ä½¿ç”¨å°æ¨¡å‹è™•ç†ç°¡å–®ä»»å‹™\\nâœ“ å„ªåŒ– prompt é•·åº¦\\nâœ“ è¨­ç½®é ç®—å‘Šè­¦'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_checklist = pd.DataFrame(checklist)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"LLM ç”Ÿç”¢ç’°å¢ƒéƒ¨ç½²æª¢æŸ¥æ¸…å–®\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "for _, row in df_checklist.iterrows():\n",
    "    print(f\"ã€{row['é¡åˆ¥']}ã€‘\")\n",
    "    print(row['æª¢æŸ¥é …ç›®'])\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-7",
   "metadata": {},
   "source": [
    "## 7. ç¸½çµ {#7-summary}\n",
    "\n",
    "### 7.1 æœ¬ç¯€æ ¸å¿ƒè¦é»\n",
    "\n",
    "âœ… **Hugging Face ä½¿ç”¨**:\n",
    "- `AutoModelForCausalLM` + `AutoTokenizer` è¼‰å…¥æ¨¡å‹\n",
    "- `pipeline()` ç°¡åŒ–å¸¸è¦‹ä»»å‹™\n",
    "- æŒæ¡ç”Ÿæˆåƒæ•¸ (temperature, top_k, top_p)\n",
    "\n",
    "âœ… **RAG ç³»çµ±**:\n",
    "- æª¢ç´¢ (Retrieval) â†’ å¢å¼· (Augmentation) â†’ ç”Ÿæˆ (Generation)\n",
    "- æ¸›å°‘å¹»è¦º,æä¾›ä¾†æºè¿½æº¯\n",
    "- å‘é‡æª¢ç´¢å„ªæ–¼é—œéµå­—æª¢ç´¢\n",
    "\n",
    "âœ… **LangChain æ¡†æ¶**:\n",
    "- çµ±ä¸€ LLM ä»‹é¢\n",
    "- Chain çµ„åˆè¤‡é›œæµç¨‹\n",
    "- Memory ç®¡ç†å¤šè¼ªå°è©±\n",
    "\n",
    "âœ… **Agent ç³»çµ±**:\n",
    "- ReAct: Reasoning + Acting\n",
    "- å·¥å…·ä½¿ç”¨èƒ½åŠ›\n",
    "- å¤šæ­¥é©Ÿä»»å‹™åˆ†è§£\n",
    "\n",
    "âœ… **ç”Ÿç”¢éƒ¨ç½²**:\n",
    "- æ¨¡å‹é¸æ“‡: æ¬Šè¡¡å»¶é²/æˆæœ¬/å“è³ª\n",
    "- å„ªåŒ–æŠ€è¡“: é‡åŒ–, æ‰¹æ¬¡è™•ç†, KV Cache\n",
    "- ç›£æ§: æ•ˆèƒ½/å“è³ª/æˆæœ¬æŒ‡æ¨™\n",
    "\n",
    "### 7.2 å¯¦æˆ°å»ºè­°\n",
    "\n",
    "**å¾å°åšèµ·**:\n",
    "1. å…ˆç”¨ GPT-2 / å°å‹é–‹æºæ¨¡å‹ç†Ÿæ‚‰æµç¨‹\n",
    "2. åœ¨ Colab/Kaggle å…è²» GPU ä¸Šå¯¦é©—\n",
    "3. æŒæ¡åŸºç¤å¾Œå†å˜—è©¦å¤§æ¨¡å‹\n",
    "\n",
    "**é€æ­¥æ“´å±•**:\n",
    "1. åŸºç¤ç”Ÿæˆ â†’ RAG â†’ LangChain â†’ Agent\n",
    "2. æœ¬åœ°é–‹ç™¼ â†’ API æ¸¬è©¦ â†’ ç”Ÿç”¢éƒ¨ç½²\n",
    "3. ç°¡å–®ä»»å‹™ â†’ è¤‡é›œæ‡‰ç”¨ â†’ ç”¢å“åŒ–\n",
    "\n",
    "**æŒçºŒå­¸ç¿’**:\n",
    "- é—œæ³¨ Hugging Face Model Hub æ–°æ¨¡å‹\n",
    "- é–±è®€ LangChain å®˜æ–¹æ–‡æª”\n",
    "- åƒèˆ‡é–‹æºç¤¾ç¾¤ (Discord, GitHub)\n",
    "\n",
    "### 7.3 CH07 èª²ç¨‹å®Œæ•´ç¸½çµ\n",
    "\n",
    "**æˆ‘å€‘å­¸ç¿’äº†ä»€éº¼**:\n",
    "\n",
    "1. **CH07-01**: Transformer æ¶æ§‹åŸºç¤\n",
    "2. **CH07-02**: Embeddings (Token + Positional)\n",
    "3. **CH07-03**: Attention æ©Ÿåˆ¶æ·±å…¥\n",
    "4. **CH07-04**: Encoder æ¶æ§‹èˆ‡ BERT\n",
    "5. **CH07-05**: Decoder æ¶æ§‹èˆ‡è‡ªå›æ­¸ç”Ÿæˆ\n",
    "6. **CH07-06**: ä¸‰å¤§æ¶æ§‹å°æ¯”èˆ‡é¸å‹\n",
    "7. **CH07-07**: LLM åŸç† (Scaling Laws, RLHF, Prompt Engineering)\n",
    "8. **CH07-08**: å¯¦æˆ°æ‡‰ç”¨ (RAG, LangChain, Agent, éƒ¨ç½²)\n",
    "\n",
    "**æŠ€èƒ½æ¨¹**:\n",
    "```\n",
    "Transformer åŸºç¤\n",
    "    â†“\n",
    "æ¶æ§‹ç†è§£ (Encoder/Decoder/Enc-Dec)\n",
    "    â†“\n",
    "LLM åŸç†èˆ‡ä½¿ç”¨\n",
    "    â†“\n",
    "å¯¦æˆ°æ‡‰ç”¨é–‹ç™¼\n",
    "    â†“\n",
    "ç”Ÿç”¢ç’°å¢ƒéƒ¨ç½²\n",
    "```\n",
    "\n",
    "### 7.4 å»¶ä¼¸è³‡æº\n",
    "\n",
    "**å®˜æ–¹æ–‡æª”**:\n",
    "- [Hugging Face Transformers](https://huggingface.co/docs/transformers)\n",
    "- [LangChain Documentation](https://python.langchain.com/)\n",
    "- [OpenAI Cookbook](https://github.com/openai/openai-cookbook)\n",
    "\n",
    "**å¯¦æˆ°é …ç›®**:\n",
    "- [LangChain Templates](https://github.com/langchain-ai/langchain/tree/master/templates)\n",
    "- [AutoGPT](https://github.com/Significant-Gravitas/AutoGPT)\n",
    "- [GPT Engineer](https://github.com/gpt-engineer-org/gpt-engineer)\n",
    "\n",
    "**ç¤¾ç¾¤è³‡æº**:\n",
    "- [r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/) - æœ¬åœ° LLM è¨è«–\n",
    "- [Hugging Face Discord](https://discord.gg/hugging-face)\n",
    "- [LangChain Discord](https://discord.gg/langchain)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ‰ æ­å–œå®Œæˆ CH07 å…¨éƒ¨èª²ç¨‹!\n",
    "\n",
    "ä½ ç¾åœ¨å·²ç¶“æŒæ¡:\n",
    "- âœ… Transformer æ¶æ§‹å¾é›¶å¯¦ä½œ\n",
    "- âœ… BERT/GPT/T5 ä¸‰å¤§æ¶æ§‹åŸç†\n",
    "- âœ… LLM çš„è¨“ç·´èˆ‡å°é½Š\n",
    "- âœ… Prompt Engineering æŠ€å·§\n",
    "- âœ… RAG, LangChain, Agent å¯¦æˆ°\n",
    "- âœ… ç”Ÿç”¢ç’°å¢ƒéƒ¨ç½²çŸ¥è­˜\n",
    "\n",
    "**ä¸‹ä¸€æ­¥å»ºè­°**:\n",
    "1. å¯¦ä½œä¸€å€‹å®Œæ•´çš„ RAG å•ç­”ç³»çµ±\n",
    "2. å˜—è©¦å¾®èª¿é–‹æº LLM (LoRA, QLoRA)\n",
    "3. æ¢ç´¢ Multimodal LLM (Vision-Language Models)\n",
    "4. å­¸ç¿’ LLM å®‰å…¨èˆ‡å€«ç†è­°é¡Œ\n",
    "\n",
    "---\n",
    "\n",
    "**èª²ç¨‹å®Œæˆæ™‚é–“**: `____å¹´____æœˆ____æ—¥`  \n",
    "**å­¸ç¿’å¿ƒå¾—**: ___________________________________  \n",
    "**æœªä¾†è¨ˆç•«**: ___________________________________"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
