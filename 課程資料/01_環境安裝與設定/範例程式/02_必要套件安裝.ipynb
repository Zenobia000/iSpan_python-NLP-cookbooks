{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH01-02: å¿…è¦å¥—ä»¶å®‰è£\n",
    "\n",
    "**èª²ç¨‹ç›®æ¨™:**\n",
    "- æŒæ¡ NLP æ ¸å¿ƒå¥—ä»¶çš„å®‰è£èˆ‡é…ç½®\n",
    "- ç†è§£ä¸åŒå¥—ä»¶çš„é©ç”¨å ´æ™¯\n",
    "- å­¸æœƒè§£æ±ºå¥—ä»¶è¡çªå•é¡Œ\n",
    "- äº†è§£ä¸­è‹±æ–‡ NLP å·¥å…·çš„å·®ç•°\n",
    "\n",
    "**å­¸ç¿’æ™‚é–“:** ç´„ 75 åˆ†é˜\n",
    "\n",
    "**å‰ç½®çŸ¥è­˜:**\n",
    "- Poetry åŸºç¤æ“ä½œ\n",
    "- Python å¥—ä»¶ç®¡ç†æ¦‚å¿µ\n",
    "- å‘½ä»¤åˆ—åŸºæœ¬æ“ä½œ\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š ç›®éŒ„\n",
    "\n",
    "1. [NLP å¥—ä»¶ç”Ÿæ…‹ç³»çµ±æ¦‚è¦½](#1)\n",
    "2. [ä¸­æ–‡ NLP å·¥å…·å®‰è£](#2)\n",
    "3. [è‹±æ–‡ NLP å·¥å…·å®‰è£](#3)\n",
    "4. [æ·±åº¦å­¸ç¿’æ¡†æ¶é¸æ“‡](#4)\n",
    "5. [Transformers ç”Ÿæ…‹å®‰è£](#5)\n",
    "6. [å¥—ä»¶ç›¸å®¹æ€§æª¢æŸ¥](#6)\n",
    "7. [æ¨¡å‹èˆ‡æ•¸æ“šä¸‹è¼‰](#7)\n",
    "8. [å¯¦æˆ°ç·´ç¿’](#8)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç’°å¢ƒè¨­å®šèˆ‡å¥—ä»¶å°å…¥\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from importlib.metadata import version, PackageNotFoundError\n",
    "from pathlib import Path\n",
    "\n",
    "# è¨­å®šä¸­æ–‡é¡¯ç¤º\n",
    "plt.rcParams['font.sans-serif'] = ['Microsoft JhengHei', 'SimHei', 'Arial Unicode MS']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# è¨­å®šé¡¯ç¤ºé¢¨æ ¼\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"âœ… ç’°å¢ƒè¨­å®šå®Œæˆ\")\n",
    "print(f\"Python ç‰ˆæœ¬: {sys.version.split()[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "## 1. NLP å¥—ä»¶ç”Ÿæ…‹ç³»çµ±æ¦‚è¦½\n",
    "\n",
    "### 1.1 NLP å·¥å…·åˆ†é¡\n",
    "\n",
    "NLP å¥—ä»¶å¯å¾**èªè¨€**ã€**æŠ€è¡“å…¸ç¯„**ã€**æ‡‰ç”¨å ´æ™¯**ä¸‰å€‹ç¶­åº¦åˆ†é¡:\n",
    "\n",
    "#### æŒ‰èªè¨€åˆ†é¡:\n",
    "- **ä¸­æ–‡å°ˆç”¨:** jieba, pkuseg, LTP\n",
    "- **è‹±æ–‡å°ˆç”¨:** NLTK, TextBlob\n",
    "- **å¤šèªè¨€:** spaCy, Stanza\n",
    "\n",
    "#### æŒ‰æŠ€è¡“å…¸ç¯„åˆ†é¡:\n",
    "- **å‚³çµ±æ–¹æ³• (è¦å‰‡/çµ±è¨ˆ):** NLTK, jieba\n",
    "- **æ·±åº¦å­¸ç¿’:** Transformers, fairseq\n",
    "\n",
    "#### æŒ‰æ‡‰ç”¨å ´æ™¯åˆ†é¡:\n",
    "- **ç ”ç©¶æ•™å­¸:** NLTK\n",
    "- **ç”Ÿç”¢ç’°å¢ƒ:** spaCy, Transformers\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP å¥—ä»¶ç”Ÿæ…‹ç³»çµ±è¦–è¦ºåŒ–\n",
    "packages_data = {\n",
    "    'å¥—ä»¶': ['jieba', 'NLTK', 'spaCy', 'Transformers', 'Stanza', 'Gensim'],\n",
    "    'é©ç”¨èªè¨€': ['ä¸­æ–‡', 'è‹±æ–‡', 'å¤šèªè¨€', 'å¤šèªè¨€', '60+ èªè¨€', 'å¤šèªè¨€'],\n",
    "    'ä¸»è¦åŠŸèƒ½': ['åˆ†è©', 'åˆ†è©/POS/è©å¹¹åŒ–', 'Pipeline è™•ç†', 'é è¨“ç·´æ¨¡å‹', 'åˆ†è©/POS/NER', 'Word2Vec/Doc2Vec'],\n",
    "    'æŠ€è¡“å…¸ç¯„': ['çµ±è¨ˆ (HMM)', 'è¦å‰‡+çµ±è¨ˆ', 'ç¥ç¶“ç¶²è·¯', 'Transformer', 'ç¥ç¶“ç¶²è·¯', 'è©å‘é‡'],\n",
    "    'å­¸ç¿’æ›²ç·š': ['ä½', 'ä¸­', 'ä¸­', 'é«˜', 'ä¸­', 'ä¸­'],\n",
    "    'æ•ˆèƒ½': ['å¿«', 'ä¸­', 'å¿«', 'æ…¢ (éœ€ GPU)', 'ä¸­', 'ä¸­']\n",
    "}\n",
    "\n",
    "df_packages = pd.DataFrame(packages_data)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "table = ax.table(cellText=df_packages.values, colLabels=df_packages.columns,\n",
    "                cellLoc='center', loc='center')\n",
    "\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1, 2.5)\n",
    "\n",
    "# è¨­å®šè¡¨é ­æ¨£å¼\n",
    "for i in range(len(df_packages.columns)):\n",
    "    table[(0, i)].set_facecolor('#4ECDC4')\n",
    "    table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "\n",
    "# è¨­å®šäº¤æ›¿è¡Œé¡è‰²\n",
    "for i in range(1, len(df_packages) + 1):\n",
    "    for j in range(len(df_packages.columns)):\n",
    "        if i % 2 == 0:\n",
    "            table[(i, j)].set_facecolor('#F7F7F7')\n",
    "\n",
    "plt.title('NLP å¥—ä»¶ç”Ÿæ…‹ç³»çµ±å°æ¯”', fontsize=18, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ“Š é¸æ“‡æŒ‡å—:\")\n",
    "print(\"  - ä¸­æ–‡åˆ†è©: jieba (å¿«é€Ÿã€è¼•é‡)\")\n",
    "print(\"  - è‹±æ–‡æ•™å­¸: NLTK (åŠŸèƒ½å®Œæ•´)\")\n",
    "print(\"  - ç”Ÿç”¢ç’°å¢ƒ: spaCy (æ•ˆèƒ½å„ªå…ˆ)\")\n",
    "print(\"  - SOTA æ¨¡å‹: Transformers (æº–ç¢ºç‡æœ€é«˜)\")\n",
    "print(\"  - å¤šèªè¨€: Stanza (æ”¯æ´ 60+ èªè¨€)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 å¥—ä»¶å®‰è£ç­–ç•¥\n",
    "\n",
    "#### åˆ†å±¤å®‰è£ç­–ç•¥ (æ¨è–¦):\n",
    "\n",
    "```bash\n",
    "# ç¬¬ 1 å±¤: åŸºç¤å·¥å…·\n",
    "poetry add numpy pandas scikit-learn matplotlib\n",
    "\n",
    "# ç¬¬ 2 å±¤: ä¸­æ–‡/è‹±æ–‡ NLP\n",
    "poetry add jieba nltk spacy\n",
    "\n",
    "# ç¬¬ 3 å±¤: æ·±åº¦å­¸ç¿’æ¡†æ¶ (äºŒé¸ä¸€)\n",
    "poetry add torch  # æˆ– tensorflow\n",
    "\n",
    "# ç¬¬ 4 å±¤: æ·±åº¦å­¸ç¿’ NLP\n",
    "poetry add transformers datasets tokenizers\n",
    "```\n",
    "\n",
    "**å„ªé»:**\n",
    "- åˆ†å±¤å®‰è£,å•é¡Œæ˜“å®šä½\n",
    "- é¿å…ä¸€æ¬¡å®‰è£éå¤šå¥—ä»¶å°è‡´è¡çª\n",
    "- ä¾¿æ–¼ç†è§£ä¾è³´é—œä¿‚\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "## 2. ä¸­æ–‡ NLP å·¥å…·å®‰è£\n",
    "\n",
    "### 2.1 jieba (çµå·´åˆ†è©)\n",
    "\n",
    "**jieba** æ˜¯æœ€æµè¡Œçš„ Python ä¸­æ–‡åˆ†è©å·¥å…·,åŸºæ–¼ HMM (éš±é¦¬å¯å¤«æ¨¡å‹) èˆ‡ Viterbi ç®—æ³•ã€‚\n",
    "\n",
    "#### å®‰è£èˆ‡é…ç½®:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£ jieba (åœ¨çµ‚ç«¯åŸ·è¡Œ)\n",
    "install_cmd = \"poetry add jieba\"\n",
    "print(f\"ğŸ“¦ å®‰è£æŒ‡ä»¤: {install_cmd}\\n\")\n",
    "\n",
    "# æª¢æŸ¥æ˜¯å¦å·²å®‰è£\n",
    "try:\n",
    "    import jieba\n",
    "    jieba_version = version('jieba')\n",
    "    print(f\"âœ… jieba å·²å®‰è£,ç‰ˆæœ¬: {jieba_version}\")\n",
    "    \n",
    "    # åŸºæœ¬æ¸¬è©¦\n",
    "    test_text = \"æˆ‘æ„›è‡ªç„¶èªè¨€è™•ç†\"\n",
    "    words = list(jieba.cut(test_text))\n",
    "    print(f\"\\nåˆ†è©æ¸¬è©¦: '{test_text}' â†’ {words}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(f\"âŒ jieba æœªå®‰è£,è«‹åŸ·è¡Œ: {install_cmd}\")\n",
    "except PackageNotFoundError:\n",
    "    print(f\"âš ï¸ jieba å·²å°å…¥ä½†ç‰ˆæœ¬è³‡è¨Šä¸å¯ç”¨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### jieba ä¸‰ç¨®åˆ†è©æ¨¡å¼:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jieba åˆ†è©æ¨¡å¼å°æ¯”\n",
    "try:\n",
    "    import jieba\n",
    "    \n",
    "    test_sentence = \"æˆ‘ä¾†åˆ°åŒ—äº¬æ¸…è¯å¤§å­¸\"\n",
    "    \n",
    "    modes = [\n",
    "        ('ç²¾ç¢ºæ¨¡å¼', list(jieba.cut(test_sentence, cut_all=False))),\n",
    "        ('å…¨æ¨¡å¼', list(jieba.cut(test_sentence, cut_all=True))),\n",
    "        ('æœå°‹å¼•æ“æ¨¡å¼', list(jieba.cut_for_search(test_sentence)))\n",
    "    ]\n",
    "    \n",
    "    print(f\"æ¸¬è©¦å¥å­: '{test_sentence}'\\n\")\n",
    "    \n",
    "    for mode_name, result in modes:\n",
    "        print(f\"{mode_name:12s}: {result}\")\n",
    "    \n",
    "    # è¦–è¦ºåŒ–å°æ¯”\n",
    "    mode_names = [m[0] for m in modes]\n",
    "    token_counts = [len(m[1]) for m in modes]\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    bars = plt.bar(mode_names, token_counts, color=['#4ECDC4', '#FF6B6B', '#FFE66D'])\n",
    "    plt.ylabel('è©æ•¸é‡', fontsize=12)\n",
    "    plt.title('jieba ä¸‰ç¨®åˆ†è©æ¨¡å¼å°æ¯”', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # åœ¨æŸ±ç‹€åœ–ä¸Šæ¨™è¨»æ•¸å€¼\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{int(height)}',\n",
    "                ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nğŸ’¡ æ¨¡å¼é¸æ“‡:\")\n",
    "    print(\"  - ç²¾ç¢ºæ¨¡å¼: é©åˆæ–‡æœ¬åˆ†æ (é è¨­)\")\n",
    "    print(\"  - å…¨æ¨¡å¼: çª®ç›¡æ‰€æœ‰å¯èƒ½,è©æ•¸æœ€å¤š\")\n",
    "    print(\"  - æœå°‹å¼•æ“æ¨¡å¼: é©åˆå»ºç«‹ç´¢å¼•\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"è«‹å…ˆå®‰è£ jieba: poetry add jieba\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 è‡ªå®šç¾©è©å…¸\n",
    "\n",
    "jieba æ”¯æ´è‡ªå®šç¾©è©å…¸,å¯æ·»åŠ é ˜åŸŸå°ˆæœ‰åè©:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è‡ªå®šç¾©è©å…¸ç¤ºä¾‹\n",
    "try:\n",
    "    import jieba\n",
    "    \n",
    "    # æ¸¬è©¦æ–‡æœ¬ (åŒ…å«å°ˆæœ‰åè©)\n",
    "    text = \"æˆ‘åœ¨å­¸ç¿’ Transformer æ¨¡å‹å’Œ BERT ç®—æ³•\"\n",
    "    \n",
    "    print(\"æ¸¬è©¦æ–‡æœ¬:\", text)\n",
    "    print(\"\\nã€æœªåŠ è¼‰è‡ªå®šç¾©è©å…¸ã€‘\")\n",
    "    result_before = list(jieba.cut(text))\n",
    "    print(f\"åˆ†è©çµæœ: {result_before}\")\n",
    "    \n",
    "    # æ·»åŠ è‡ªå®šç¾©è©\n",
    "    custom_words = [\n",
    "        ('Transformer', 10, 'n'),  # (è©, è©é », è©æ€§)\n",
    "        ('BERT', 10, 'n'),\n",
    "        ('è‡ªç„¶èªè¨€è™•ç†', 15, 'n')\n",
    "    ]\n",
    "    \n",
    "    for word, freq, pos in custom_words:\n",
    "        jieba.add_word(word, freq, pos)\n",
    "    \n",
    "    print(\"\\nã€åŠ è¼‰è‡ªå®šç¾©è©å…¸å¾Œã€‘\")\n",
    "    result_after = list(jieba.cut(text))\n",
    "    print(f\"åˆ†è©çµæœ: {result_after}\")\n",
    "    \n",
    "    # å°æ¯”è¦–è¦ºåŒ–\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # æœªåŠ è¼‰è©å…¸\n",
    "    axes[0].bar(range(len(result_before)), [1]*len(result_before), color='#FF6B6B')\n",
    "    axes[0].set_xticks(range(len(result_before)))\n",
    "    axes[0].set_xticklabels(result_before, rotation=45, ha='right')\n",
    "    axes[0].set_title('æœªåŠ è¼‰è‡ªå®šç¾©è©å…¸', fontsize=12, fontweight='bold')\n",
    "    axes[0].set_ylabel('è©ä½ç½®')\n",
    "    \n",
    "    # åŠ è¼‰è©å…¸å¾Œ\n",
    "    axes[1].bar(range(len(result_after)), [1]*len(result_after), color='#4ECDC4')\n",
    "    axes[1].set_xticks(range(len(result_after)))\n",
    "    axes[1].set_xticklabels(result_after, rotation=45, ha='right')\n",
    "    axes[1].set_title('åŠ è¼‰è‡ªå®šç¾©è©å…¸å¾Œ', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_ylabel('è©ä½ç½®')\n",
    "    \n",
    "    plt.suptitle('è‡ªå®šç¾©è©å…¸æ•ˆæœå°æ¯”', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nğŸ’¡ è‡ªå®šç¾©è©å…¸æ ¼å¼ (custom_dict.txt):\")\n",
    "    print(\"  Transformer 10 n\")\n",
    "    print(\"  BERT 10 n\")\n",
    "    print(\"  è‡ªç„¶èªè¨€è™•ç† 15 n\")\n",
    "    print(\"\\nè¼‰å…¥æ–¹å¼: jieba.load_userdict('custom_dict.txt')\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"è«‹å…ˆå®‰è£ jieba\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "## 3. è‹±æ–‡ NLP å·¥å…·å®‰è£\n",
    "\n",
    "### 3.1 NLTK (Natural Language Toolkit)\n",
    "\n",
    "**NLTK** æ˜¯ç¶“å…¸çš„è‹±æ–‡ NLP å·¥å…·åŒ…,é©åˆæ•™å­¸èˆ‡ç ”ç©¶ã€‚\n",
    "\n",
    "#### å®‰è£èˆ‡æ•¸æ“šä¸‹è¼‰:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£ NLTK\n",
    "install_cmd = \"poetry add nltk\"\n",
    "print(f\"ğŸ“¦ å®‰è£æŒ‡ä»¤: {install_cmd}\\n\")\n",
    "\n",
    "# æª¢æŸ¥ä¸¦æ¸¬è©¦ NLTK\n",
    "try:\n",
    "    import nltk\n",
    "    nltk_version = version('nltk')\n",
    "    print(f\"âœ… NLTK å·²å®‰è£,ç‰ˆæœ¬: {nltk_version}\")\n",
    "    \n",
    "    # å¿…è¦æ•¸æ“šåŒ…åˆ—è¡¨\n",
    "    required_data = [\n",
    "        ('punkt', 'åˆ†è©æ¨¡å‹'),\n",
    "        ('stopwords', 'åœç”¨è©è¡¨'),\n",
    "        ('averaged_perceptron_tagger', 'POS æ¨™è¨»å™¨'),\n",
    "        ('wordnet', 'WordNet è©å½™è³‡æ–™åº«'),\n",
    "        ('omw-1.4', 'å¤šèªè¨€ WordNet')\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nğŸ“š å¿…è¦æ•¸æ“šåŒ…:\")\n",
    "    for package, description in required_data:\n",
    "        print(f\"  - {package:30s}: {description}\")\n",
    "    \n",
    "    print(\"\\nä¸‹è¼‰æŒ‡ä»¤:\")\n",
    "    print(\"  nltk.download('punkt')\")\n",
    "    print(\"  nltk.download('stopwords')\")\n",
    "    print(\"  nltk.download('averaged_perceptron_tagger')\")\n",
    "    print(\"  nltk.download('wordnet')\")\n",
    "    print(\"  nltk.download('omw-1.4')\")\n",
    "    print(\"\\næˆ–ä¸€æ¬¡æ€§ä¸‹è¼‰: nltk.download('popular')\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(f\"âŒ NLTK æœªå®‰è£,è«‹åŸ·è¡Œ: {install_cmd}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK åŸºæœ¬åŠŸèƒ½æ¸¬è©¦\n",
    "try:\n",
    "    import nltk\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.corpus import stopwords\n",
    "    \n",
    "    # æ¸¬è©¦æ–‡æœ¬\n",
    "    text = \"Natural language processing is amazing! It enables computers to understand human language.\"\n",
    "    \n",
    "    print(f\"æ¸¬è©¦æ–‡æœ¬: {text}\\n\")\n",
    "    \n",
    "    # åˆ†è©\n",
    "    try:\n",
    "        tokens = word_tokenize(text)\n",
    "        print(f\"âœ… åˆ†è©çµæœ: {tokens}\")\n",
    "        print(f\"   è©æ•¸: {len(tokens)}\")\n",
    "    except LookupError:\n",
    "        print(\"âŒ åˆ†è©å¤±æ•—,è«‹ä¸‹è¼‰: nltk.download('punkt')\")\n",
    "        tokens = []\n",
    "    \n",
    "    # åœç”¨è©éæ¿¾\n",
    "    try:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        filtered = [w for w in tokens if w.lower() not in stop_words]\n",
    "        print(f\"\\nâœ… éæ¿¾åœç”¨è©å¾Œ: {filtered}\")\n",
    "        print(f\"   è©æ•¸: {len(filtered)}\")\n",
    "        \n",
    "        # è¦–è¦ºåŒ–å°æ¯”\n",
    "        data = {\n",
    "            'è™•ç†éšæ®µ': ['åŸå§‹åˆ†è©', 'éæ¿¾åœç”¨è©'],\n",
    "            'è©æ•¸': [len(tokens), len(filtered)]\n",
    "        }\n",
    "        \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        bars = plt.bar(data['è™•ç†éšæ®µ'], data['è©æ•¸'], color=['#FF6B6B', '#4ECDC4'])\n",
    "        plt.ylabel('è©æ•¸é‡', fontsize=12)\n",
    "        plt.title('NLTK åœç”¨è©éæ¿¾æ•ˆæœ', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{int(height)}',\n",
    "                    ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    except LookupError:\n",
    "        print(\"âŒ åœç”¨è©è™•ç†å¤±æ•—,è«‹ä¸‹è¼‰: nltk.download('stopwords')\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"è«‹å…ˆå®‰è£ NLTK: poetry add nltk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 spaCy (å·¥æ¥­ç´š NLP)\n",
    "\n",
    "**spaCy** æ˜¯é¢å‘ç”Ÿç”¢ç’°å¢ƒçš„ NLP å·¥å…·,æ•ˆèƒ½å„ªæ–¼ NLTKã€‚\n",
    "\n",
    "#### å®‰è£èˆ‡èªè¨€æ¨¡å‹ä¸‹è¼‰:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£ spaCy\n",
    "install_steps = [\n",
    "    ('1. å®‰è£ spaCy å¥—ä»¶', 'poetry add spacy'),\n",
    "    ('2. ä¸‹è¼‰è‹±æ–‡å°å‹æ¨¡å‹', 'python -m spacy download en_core_web_sm'),\n",
    "    ('3. ä¸‹è¼‰è‹±æ–‡ä¸­å‹æ¨¡å‹ (å«è©å‘é‡)', 'python -m spacy download en_core_web_md'),\n",
    "    ('4. ä¸‹è¼‰ä¸­æ–‡æ¨¡å‹', 'python -m spacy download zh_core_web_sm'),\n",
    "]\n",
    "\n",
    "print(\"ğŸ“¦ spaCy å®‰è£æ­¥é©Ÿ:\\n\")\n",
    "for step, cmd in install_steps:\n",
    "    print(f\"{step}\")\n",
    "    print(f\"   {cmd}\\n\")\n",
    "\n",
    "# æª¢æŸ¥ spaCy å®‰è£ç‹€æ…‹\n",
    "try:\n",
    "    import spacy\n",
    "    spacy_version = version('spacy')\n",
    "    print(f\"âœ… spaCy å·²å®‰è£,ç‰ˆæœ¬: {spacy_version}\")\n",
    "    \n",
    "    # æª¢æŸ¥å¯ç”¨æ¨¡å‹\n",
    "    print(\"\\nğŸ” æª¢æŸ¥å·²å®‰è£çš„èªè¨€æ¨¡å‹:\")\n",
    "    models = ['en_core_web_sm', 'en_core_web_md', 'zh_core_web_sm']\n",
    "    \n",
    "    for model in models:\n",
    "        try:\n",
    "            nlp = spacy.load(model)\n",
    "            print(f\"  âœ… {model} - å·²å®‰è£\")\n",
    "        except OSError:\n",
    "            print(f\"  âŒ {model} - æœªå®‰è£\")\n",
    "            print(f\"     ä¸‹è¼‰: python -m spacy download {model}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"âŒ spaCy æœªå®‰è£,è«‹åŸ·è¡Œ: poetry add spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spaCy åŠŸèƒ½å±•ç¤º\n",
    "try:\n",
    "    import spacy\n",
    "    \n",
    "    # è¼‰å…¥æ¨¡å‹\n",
    "    try:\n",
    "        nlp = spacy.load('en_core_web_sm')\n",
    "        \n",
    "        # è™•ç†æ–‡æœ¬\n",
    "        text = \"Apple is looking at buying U.K. startup for $1 billion\"\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        print(f\"æ¸¬è©¦æ–‡æœ¬: {text}\\n\")\n",
    "        \n",
    "        # å¯¦é«”è­˜åˆ¥ (NER)\n",
    "        print(\"ã€å‘½åå¯¦é«”è­˜åˆ¥ (NER)ã€‘\")\n",
    "        for ent in doc.ents:\n",
    "            print(f\"  {ent.text:15s} â†’ {ent.label_:10s} ({spacy.explain(ent.label_)})\")\n",
    "        \n",
    "        # è©æ€§æ¨™è¨» (POS)\n",
    "        print(\"\\nã€è©æ€§æ¨™è¨» (POS)ã€‘\")\n",
    "        for token in doc:\n",
    "            print(f\"  {token.text:10s} â†’ {token.pos_:6s} {token.dep_:10s}\")\n",
    "        \n",
    "        # è¦–è¦ºåŒ– NER çµæœ\n",
    "        entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "        if entities:\n",
    "            ent_texts, ent_labels = zip(*entities)\n",
    "            \n",
    "            plt.figure(figsize=(12, 5))\n",
    "            colors = {'ORG': '#FF6B6B', 'GPE': '#4ECDC4', 'MONEY': '#FFE66D'}\n",
    "            bar_colors = [colors.get(label, '#95E1D3') for label in ent_labels]\n",
    "            \n",
    "            plt.bar(range(len(ent_texts)), [1]*len(ent_texts), color=bar_colors)\n",
    "            plt.xticks(range(len(ent_texts)), ent_texts, rotation=45, ha='right')\n",
    "            plt.ylabel('å¯¦é«”')\n",
    "            plt.title('spaCy å‘½åå¯¦é«”è­˜åˆ¥çµæœ', fontsize=14, fontweight='bold')\n",
    "            \n",
    "            # åœ–ä¾‹\n",
    "            from matplotlib.patches import Patch\n",
    "            legend_elements = [Patch(facecolor=colors[label], label=label) for label in set(ent_labels)]\n",
    "            plt.legend(handles=legend_elements, loc='upper right')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "    except OSError:\n",
    "        print(\"âŒ è‹±æ–‡æ¨¡å‹æœªå®‰è£\")\n",
    "        print(\"è«‹åŸ·è¡Œ: python -m spacy download en_core_web_sm\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"è«‹å…ˆå®‰è£ spaCy: poetry add spacy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "## 4. æ·±åº¦å­¸ç¿’æ¡†æ¶é¸æ“‡\n",
    "\n",
    "### 4.1 PyTorch vs TensorFlow\n",
    "\n",
    "æ·±åº¦å­¸ç¿’ NLP éœ€è¦é¸æ“‡åº•å±¤æ¡†æ¶,ä¸»æµé¸é …ç‚º **PyTorch** å’Œ **TensorFlow**ã€‚\n",
    "\n",
    "#### å°æ¯”åˆ†æ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch vs TensorFlow å°æ¯”\n",
    "comparison = {\n",
    "    'ç¶­åº¦': ['å­¸ç¿’æ›²ç·š', 'ç¤¾ç¾¤æ”¯æ´', 'NLP ç”Ÿæ…‹', 'ç”Ÿç”¢éƒ¨ç½²', 'ç ”ç©¶å‹å–„åº¦', 'GPU æ”¯æ´', 'æ¨¡å‹å¯ç”¨æ€§'],\n",
    "    'PyTorch': ['ä¸­ç­‰', 'â­â­â­â­â­', 'Transformers é¦–é¸', 'éœ€é¡å¤–å·¥å…·', 'â­â­â­â­â­', 'å„ªç§€', 'Hugging Face'],\n",
    "    'TensorFlow': ['è¼ƒé™¡', 'â­â­â­â­', 'TF Hub', 'TF Serving å®Œå–„', 'â­â­â­', 'å„ªç§€', 'TF Hub']\n",
    "}\n",
    "\n",
    "df_compare = pd.DataFrame(comparison)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "table = ax.table(cellText=df_compare.values, colLabels=df_compare.columns,\n",
    "                cellLoc='center', loc='center',\n",
    "                colWidths=[0.25, 0.35, 0.35])\n",
    "\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1, 2.5)\n",
    "\n",
    "# è¨­å®šè¡¨é ­\n",
    "for i in range(len(df_compare.columns)):\n",
    "    table[(0, i)].set_facecolor('#FF6B6B')\n",
    "    table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "\n",
    "# é«˜äº® PyTorch åˆ—\n",
    "for i in range(1, len(df_compare) + 1):\n",
    "    table[(i, 1)].set_facecolor('#FFE6E6')\n",
    "\n",
    "plt.title('PyTorch vs TensorFlow å°æ¯”', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ¯ é¸æ“‡å»ºè­°:\")\n",
    "print(\"\\nPyTorch - é©åˆ:\")\n",
    "print(\"  âœ… NLP ç ”ç©¶èˆ‡å¯¦é©—\")\n",
    "print(\"  âœ… ä½¿ç”¨ Hugging Face Transformers\")\n",
    "print(\"  âœ… éœ€è¦å‹•æ…‹è¨ˆç®—åœ–\")\n",
    "print(\"  âœ… å­¸è¡“è«–æ–‡å¾©ç¾\")\n",
    "\n",
    "print(\"\\nTensorFlow - é©åˆ:\")\n",
    "print(\"  âœ… å¤§è¦æ¨¡ç”Ÿç”¢éƒ¨ç½²\")\n",
    "print(\"  âœ… ç§»å‹•ç«¯/åµŒå…¥å¼è¨­å‚™ (TF Lite)\")\n",
    "print(\"  âœ… ä¼æ¥­ç´šæ‡‰ç”¨\")\n",
    "print(\"  âœ… éœ€è¦å®Œæ•´éƒ¨ç½²æ–¹æ¡ˆ (TF Serving)\")\n",
    "\n",
    "print(\"\\nğŸ’¡ æœ¬èª²ç¨‹æ¨è–¦: PyTorch\")\n",
    "print(\"   åŸå› : Transformers ç”Ÿæ…‹ã€ç¤¾ç¾¤æ´»èºã€å­¸ç¿’è³‡æºè±å¯Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 å®‰è£æ·±åº¦å­¸ç¿’æ¡†æ¶\n",
    "\n",
    "#### PyTorch å®‰è£:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch å®‰è£æŒ‡ä»¤\n",
    "pytorch_install = {\n",
    "    'ç’°å¢ƒ': ['CPU ç‰ˆæœ¬', 'GPU ç‰ˆæœ¬ (CUDA 11.8)', 'GPU ç‰ˆæœ¬ (CUDA 12.1)', 'macOS (M1/M2)'],\n",
    "    'å®‰è£æŒ‡ä»¤': [\n",
    "        'poetry add torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu',\n",
    "        'poetry add torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118',\n",
    "        'poetry add torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121',\n",
    "        'poetry add torch torchvision torchaudio'\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"ğŸ”¥ PyTorch å®‰è£æŒ‡ä»¤:\\n\")\n",
    "for env, cmd in zip(pytorch_install['ç’°å¢ƒ'], pytorch_install['å®‰è£æŒ‡ä»¤']):\n",
    "    print(f\"{env}:\")\n",
    "    print(f\"  {cmd}\\n\")\n",
    "\n",
    "print(\"âš ï¸ æ³¨æ„äº‹é …:\")\n",
    "print(\"  1. å…ˆç¢ºèª CUDA ç‰ˆæœ¬: nvidia-smi\")\n",
    "print(\"  2. é¸æ“‡å°æ‡‰çš„ PyTorch ç‰ˆæœ¬\")\n",
    "print(\"  3. CPU ç‰ˆæœ¬é«”ç©è¼ƒå°,ä½†ç„¡æ³•ä½¿ç”¨ GPU åŠ é€Ÿ\")\n",
    "\n",
    "# æª¢æŸ¥ PyTorch å®‰è£\n",
    "try:\n",
    "    import torch\n",
    "    torch_version = version('torch')\n",
    "    print(f\"\\nâœ… PyTorch å·²å®‰è£,ç‰ˆæœ¬: {torch_version}\")\n",
    "    print(f\"   CUDA å¯ç”¨: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"   CUDA ç‰ˆæœ¬: {torch.version.cuda}\")\n",
    "        print(f\"   GPU æ•¸é‡: {torch.cuda.device_count()}\")\n",
    "        print(f\"   GPU åç¨±: {torch.cuda.get_device_name(0)}\")\n",
    "except ImportError:\n",
    "    print(\"\\nâŒ PyTorch æœªå®‰è£\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TensorFlow å®‰è£ (å¯é¸):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow å®‰è£æŒ‡ä»¤\n",
    "print(\"ğŸ”· TensorFlow å®‰è£æŒ‡ä»¤:\\n\")\n",
    "print(\"CPU ç‰ˆæœ¬:\")\n",
    "print(\"  poetry add tensorflow\\n\")\n",
    "print(\"GPU ç‰ˆæœ¬:\")\n",
    "print(\"  poetry add tensorflow[and-cuda]\\n\")\n",
    "\n",
    "# æª¢æŸ¥ TensorFlow å®‰è£\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    tf_version = version('tensorflow')\n",
    "    print(f\"âœ… TensorFlow å·²å®‰è£,ç‰ˆæœ¬: {tf_version}\")\n",
    "    \n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    print(f\"   GPU æ•¸é‡: {len(gpus)}\")\n",
    "    for gpu in gpus:\n",
    "        print(f\"   - {gpu}\")\n",
    "except ImportError:\n",
    "    print(\"âŒ TensorFlow æœªå®‰è£ (å¯é¸)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "## 5. Transformers ç”Ÿæ…‹å®‰è£\n",
    "\n",
    "### 5.1 Hugging Face Transformers\n",
    "\n",
    "**Transformers** æ˜¯ç›®å‰æœ€æµè¡Œçš„é è¨“ç·´æ¨¡å‹åº«,æ”¯æ´ BERTã€GPTã€T5 ç­‰æ•¸åƒå€‹æ¨¡å‹ã€‚\n",
    "\n",
    "#### æ ¸å¿ƒå¥—ä»¶:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformers ç”Ÿæ…‹å¥—ä»¶\n",
    "transformers_packages = {\n",
    "    'å¥—ä»¶': ['transformers', 'datasets', 'tokenizers', 'accelerate', 'evaluate'],\n",
    "    'åŠŸèƒ½': [\n",
    "        'é è¨“ç·´æ¨¡å‹åº« (BERT/GPT/T5)',\n",
    "        'æ•¸æ“šé›†è¼‰å…¥èˆ‡è™•ç†',\n",
    "        'å¿«é€Ÿ Tokenizer (Rust å¯¦ä½œ)',\n",
    "        'å¤š GPU/æ··åˆç²¾åº¦è¨“ç·´',\n",
    "        'æ¨¡å‹è©•ä¼°æŒ‡æ¨™'\n",
    "    ],\n",
    "    'å¿…è¦æ€§': ['å¿…é ˆ', 'æ¨è–¦', 'æ¨è–¦', 'é€²éš', 'é€²éš'],\n",
    "    'å®‰è£æŒ‡ä»¤': [\n",
    "        'poetry add transformers',\n",
    "        'poetry add datasets',\n",
    "        'poetry add tokenizers',\n",
    "        'poetry add accelerate',\n",
    "        'poetry add evaluate'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_transformers = pd.DataFrame(transformers_packages)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 5))\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "table = ax.table(cellText=df_transformers.values, colLabels=df_transformers.columns,\n",
    "                cellLoc='left', loc='center')\n",
    "\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1, 2.5)\n",
    "\n",
    "# è¨­å®šè¡¨é ­\n",
    "for i in range(len(df_transformers.columns)):\n",
    "    table[(0, i)].set_facecolor('#4ECDC4')\n",
    "    table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "\n",
    "# é«˜äº®å¿…é ˆå¥—ä»¶\n",
    "table[(1, 2)].set_facecolor('#FFEB3B')  # transformers\n",
    "\n",
    "plt.title('Transformers ç”Ÿæ…‹å¥—ä»¶', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ“¦ å®‰è£é †åºå»ºè­°:\")\n",
    "print(\"  1. poetry add transformers\")\n",
    "print(\"  2. poetry add datasets tokenizers\")\n",
    "print(\"  3. poetry add accelerate evaluate (é€²éšåŠŸèƒ½)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æª¢æŸ¥ Transformers å®‰è£\n",
    "transformers_check = [\n",
    "    ('transformers', 'é è¨“ç·´æ¨¡å‹'),\n",
    "    ('datasets', 'æ•¸æ“šé›†'),\n",
    "    ('tokenizers', 'Tokenizer')\n",
    "]\n",
    "\n",
    "print(\"ğŸ” æª¢æŸ¥ Transformers ç”Ÿæ…‹å¥—ä»¶å®‰è£ç‹€æ…‹:\\n\")\n",
    "\n",
    "for package, desc in transformers_check:\n",
    "    try:\n",
    "        pkg_version = version(package)\n",
    "        print(f\"  âœ… {package:15s} {pkg_version:10s} - {desc}\")\n",
    "    except PackageNotFoundError:\n",
    "        print(f\"  âŒ {package:15s} {'æœªå®‰è£':10s} - {desc}\")\n",
    "        print(f\"     å®‰è£: poetry add {package}\")\n",
    "\n",
    "# æ¸¬è©¦ Transformers\n",
    "try:\n",
    "    from transformers import AutoTokenizer, AutoModel\n",
    "    print(\"\\nâœ… Transformers æ ¸å¿ƒæ¨¡çµ„å¯æ­£å¸¸å°å…¥\")\n",
    "    print(\"\\nğŸ’¡ ä½¿ç”¨ç¯„ä¾‹:\")\n",
    "    print(\"  from transformers import AutoTokenizer, AutoModel\")\n",
    "    print(\"  tokenizer = AutoTokenizer.from_pretrained('bert-base-chinese')\")\n",
    "    print(\"  model = AutoModel.from_pretrained('bert-base-chinese')\")\n",
    "except ImportError:\n",
    "    print(\"\\nâŒ Transformers æœªå®‰è£æˆ–å°å…¥å¤±æ•—\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 æ¨¡å‹å¿«å–è¨­å®š\n",
    "\n",
    "Transformers æ¨¡å‹æª”æ¡ˆé€šå¸¸è¼ƒå¤§ (æ•¸ç™¾ MB),éœ€è¦åˆç†è¨­å®šå¿«å–ç›®éŒ„:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformers å¿«å–è¨­å®š\n",
    "import os\n",
    "\n",
    "print(\"ğŸ“ Transformers å¿«å–ç›®éŒ„è¨­å®š:\\n\")\n",
    "\n",
    "# é è¨­å¿«å–ä½ç½®\n",
    "default_cache = os.path.expanduser('~/.cache/huggingface/hub/')\n",
    "print(f\"é è¨­å¿«å–ä½ç½®: {default_cache}\")\n",
    "\n",
    "print(\"\\nè‡ªå®šç¾©å¿«å–ç›®éŒ„ (ä¸‰ç¨®æ–¹æ³•):\\n\")\n",
    "\n",
    "print(\"æ–¹æ³• 1: ç’°å¢ƒè®Šæ•¸ (å…¨åŸŸè¨­å®š)\")\n",
    "print(\"  export TRANSFORMERS_CACHE=/path/to/cache  # Linux/macOS\")\n",
    "print(\"  set TRANSFORMERS_CACHE=D:\\\\models         # Windows\\n\")\n",
    "\n",
    "print(\"æ–¹æ³• 2: Python ä»£ç¢¼è¨­å®š\")\n",
    "print(\"  import os\")\n",
    "print(\"  os.environ['TRANSFORMERS_CACHE'] = '/path/to/cache'\\n\")\n",
    "\n",
    "print(\"æ–¹æ³• 3: ä¸‹è¼‰æ™‚æŒ‡å®š\")\n",
    "print(\"  model = AutoModel.from_pretrained('bert-base-chinese',\")\n",
    "print(\"                                     cache_dir='/path/to/cache')\")\n",
    "\n",
    "# è¦–è¦ºåŒ–å¿«å–ç›®éŒ„çµæ§‹\n",
    "cache_structure = \"\"\"\n",
    "~/.cache/huggingface/\n",
    "â””â”€â”€ hub/\n",
    "    â”œâ”€â”€ models--bert-base-chinese/\n",
    "    â”‚   â”œâ”€â”€ snapshots/\n",
    "    â”‚   â”‚   â””â”€â”€ abc123.../\n",
    "    â”‚   â”‚       â”œâ”€â”€ config.json\n",
    "    â”‚   â”‚       â”œâ”€â”€ pytorch_model.bin (ç´„ 400MB)\n",
    "    â”‚   â”‚       â””â”€â”€ tokenizer.json\n",
    "    â”‚   â””â”€â”€ refs/\n",
    "    â”‚       â””â”€â”€ main â†’ abc123...\n",
    "    â””â”€â”€ models--gpt2/\n",
    "        â””â”€â”€ ...\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nğŸ“‚ å¿«å–ç›®éŒ„çµæ§‹:\")\n",
    "print(cache_structure)\n",
    "\n",
    "print(\"\\nğŸ’¡ å¿«å–ç®¡ç†å»ºè­°:\")\n",
    "print(\"  1. å®šæœŸæ¸…ç†ä¸ç”¨çš„æ¨¡å‹ (å¯æ‰‹å‹•åˆªé™¤)\")\n",
    "print(\"  2. åœ˜éšŠå…±äº«å¯è¨­å®šçµ±ä¸€å¿«å–ç›®éŒ„\")\n",
    "print(\"  3. CI/CD ç’°å¢ƒå»ºè­°ä½¿ç”¨å¿«å–åŠ é€Ÿ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a>\n",
    "## 6. å¥—ä»¶ç›¸å®¹æ€§æª¢æŸ¥\n",
    "\n",
    "### 6.1 å¸¸è¦‹ä¾è³´è¡çª\n",
    "\n",
    "æ·±åº¦å­¸ç¿’ NLP å°ˆæ¡ˆå¸¸è¦‹çš„å¥—ä»¶è¡çªå•é¡Œ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¸¸è¦‹ä¾è³´è¡çªæ¡ˆä¾‹\n",
    "conflicts = {\n",
    "    'è¡çªé¡å‹': [\n",
    "        'NumPy ç‰ˆæœ¬è¡çª',\n",
    "        'Protobuf ç‰ˆæœ¬è¡çª',\n",
    "        'PyTorch vs TensorFlow',\n",
    "        'Tokenizers ç‰ˆæœ¬ä¸åŒ¹é…'\n",
    "    ],\n",
    "    'ç—‡ç‹€': [\n",
    "        'ImportError: numpy.core.multiarray failed',\n",
    "        'TypeError: Descriptors cannot not be created directly',\n",
    "        'è¨˜æ†¶é«”è¡çª,ç„¡æ³•åŒæ™‚è¼‰å…¥',\n",
    "        'AttributeError: module has no attribute'\n",
    "    ],\n",
    "    'è§£æ±ºæ–¹æ¡ˆ': [\n",
    "        'poetry add \"numpy>=1.24,<2.0\"',\n",
    "        'poetry add \"protobuf>=3.20,<4.0\"',\n",
    "        'é¸æ“‡ä¸€å€‹æ¡†æ¶,é¿å…åŒæ™‚å®‰è£',\n",
    "        'poetry update tokenizers transformers'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_conflicts = pd.DataFrame(conflicts)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 5))\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "table = ax.table(cellText=df_conflicts.values, colLabels=df_conflicts.columns,\n",
    "                cellLoc='left', loc='center')\n",
    "\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(9)\n",
    "table.scale(1, 3)\n",
    "\n",
    "# è¨­å®šè¡¨é ­\n",
    "for i in range(len(df_conflicts.columns)):\n",
    "    table[(0, i)].set_facecolor('#FF6B6B')\n",
    "    table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "\n",
    "plt.title('å¸¸è¦‹ä¾è³´è¡çªèˆ‡è§£æ±ºæ–¹æ¡ˆ', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ”§ é é˜²è¡çªçš„æœ€ä½³å¯¦è¸:\")\n",
    "print(\"  1. ä½¿ç”¨ Poetry ç®¡ç†ä¾è³´ - è‡ªå‹•è§£æ±ºè¡çª\")\n",
    "print(\"  2. å®šæœŸæ›´æ–° poetry.lock - poetry update\")\n",
    "print(\"  3. æª¢æŸ¥ä¾è³´æ¨¹ - poetry show --tree\")\n",
    "print(\"  4. é–å®šæ ¸å¿ƒå¥—ä»¶ç‰ˆæœ¬ç¯„åœ - ä½¿ç”¨ ^ æˆ– ~ ç´„æŸ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 ç’°å¢ƒé©—è­‰è…³æœ¬\n",
    "\n",
    "å‰µå»ºè‡ªå‹•åŒ–è…³æœ¬æª¢æŸ¥æ‰€æœ‰å¥—ä»¶:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®Œæ•´ç’°å¢ƒé©—è­‰è…³æœ¬\n",
    "def check_nlp_environment():\n",
    "    \"\"\"æª¢æŸ¥ NLP é–‹ç™¼ç’°å¢ƒæ‰€æœ‰å¥—ä»¶\"\"\"\n",
    "    \n",
    "    packages = [\n",
    "        # åŸºç¤å·¥å…·\n",
    "        ('numpy', '1.24.0'),\n",
    "        ('pandas', '2.0.0'),\n",
    "        ('scikit-learn', '1.3.0'),\n",
    "        # ä¸­æ–‡ NLP\n",
    "        ('jieba', None),\n",
    "        # è‹±æ–‡ NLP\n",
    "        ('nltk', '3.8.0'),\n",
    "        ('spacy', '3.7.0'),\n",
    "        # æ·±åº¦å­¸ç¿’\n",
    "        ('torch', '2.0.0'),\n",
    "        ('transformers', '4.30.0'),\n",
    "        ('datasets', None),\n",
    "        ('tokenizers', None),\n",
    "    ]\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"  NLP ç’°å¢ƒå¥—ä»¶æª¢æŸ¥å ±å‘Š\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for package, min_version in packages:\n",
    "        try:\n",
    "            pkg_version = version(package)\n",
    "            status = \"âœ…\"\n",
    "            \n",
    "            if min_version and pkg_version < min_version:\n",
    "                status = \"âš ï¸\"\n",
    "                note = f\"å»ºè­° >= {min_version}\"\n",
    "            else:\n",
    "                note = \"OK\"\n",
    "            \n",
    "            results.append({\n",
    "                'å¥—ä»¶': package,\n",
    "                'ç‰ˆæœ¬': pkg_version,\n",
    "                'ç‹€æ…‹': status,\n",
    "                'å‚™è¨»': note\n",
    "            })\n",
    "            \n",
    "        except PackageNotFoundError:\n",
    "            results.append({\n",
    "                'å¥—ä»¶': package,\n",
    "                'ç‰ˆæœ¬': 'æœªå®‰è£',\n",
    "                'ç‹€æ…‹': 'âŒ',\n",
    "                'å‚™è¨»': f'åŸ·è¡Œ: poetry add {package}'\n",
    "            })\n",
    "    \n",
    "    # è¼¸å‡ºçµæœ\n",
    "    for r in results:\n",
    "        print(f\"  {r['ç‹€æ…‹']} {r['å¥—ä»¶']:20s} {r['ç‰ˆæœ¬']:15s} {r['å‚™è¨»']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    \n",
    "    # çµ±è¨ˆ\n",
    "    installed = sum(1 for r in results if r['ç‹€æ…‹'] == 'âœ…')\n",
    "    total = len(results)\n",
    "    \n",
    "    print(f\"  å¥—ä»¶å®‰è£ç‹€æ…‹: {installed}/{total} å·²å®‰è£\")\n",
    "    \n",
    "    if installed == total:\n",
    "        print(\"\\n  âœ… ç’°å¢ƒæª¢æŸ¥é€šé,å¯ä»¥é–‹å§‹ NLP é–‹ç™¼!\")\n",
    "    else:\n",
    "        print(\"\\n  âš ï¸ éƒ¨åˆ†å¥—ä»¶ç¼ºå¤±æˆ–ç‰ˆæœ¬éä½,è«‹æ ¹æ“šä¸Šæ–¹æç¤ºå®‰è£\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# åŸ·è¡Œæª¢æŸ¥\n",
    "results = check_nlp_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"7\"></a>\n",
    "## 7. æ¨¡å‹èˆ‡æ•¸æ“šä¸‹è¼‰\n",
    "\n",
    "### 7.1 NLTK æ•¸æ“šä¸‹è¼‰\n",
    "\n",
    "NLTK éœ€è¦é¡å¤–ä¸‹è¼‰èªæ–™åº«èˆ‡æ¨¡å‹:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK æ•¸æ“šä¸‹è¼‰è…³æœ¬\n",
    "def download_nltk_data():\n",
    "    \"\"\"æ‰¹æ¬¡ä¸‹è¼‰ NLTK å¿…è¦æ•¸æ“š\"\"\"\n",
    "    try:\n",
    "        import nltk\n",
    "        \n",
    "        data_packages = [\n",
    "            'punkt',\n",
    "            'stopwords',\n",
    "            'averaged_perceptron_tagger',\n",
    "            'wordnet',\n",
    "            'omw-1.4'\n",
    "        ]\n",
    "        \n",
    "        print(\"ğŸ“¥ é–‹å§‹ä¸‹è¼‰ NLTK æ•¸æ“šåŒ…...\\n\")\n",
    "        \n",
    "        for package in data_packages:\n",
    "            try:\n",
    "                # æª¢æŸ¥æ˜¯å¦å·²ä¸‹è¼‰\n",
    "                nltk.data.find(f'tokenizers/{package}' if package == 'punkt' else f'corpora/{package}')\n",
    "                print(f\"  âœ… {package:30s} - å·²å­˜åœ¨\")\n",
    "            except LookupError:\n",
    "                print(f\"  â¬‡ï¸  {package:30s} - ä¸‹è¼‰ä¸­...\")\n",
    "                nltk.download(package, quiet=True)\n",
    "                print(f\"  âœ… {package:30s} - ä¸‹è¼‰å®Œæˆ\")\n",
    "        \n",
    "        print(\"\\nâœ… NLTK æ•¸æ“šåŒ…ä¸‹è¼‰å®Œæˆ!\")\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"âŒ NLTK æœªå®‰è£,è«‹å…ˆåŸ·è¡Œ: poetry add nltk\")\n",
    "\n",
    "# åŸ·è¡Œä¸‹è¼‰ (å–æ¶ˆè¨»è§£ä»¥åŸ·è¡Œ)\n",
    "# download_nltk_data()\n",
    "\n",
    "print(\"ğŸ’¡ ä½¿ç”¨èªªæ˜:\")\n",
    "print(\"  - å–æ¶ˆä¸Šæ–¹è¨»è§£åŸ·è¡Œè‡ªå‹•ä¸‹è¼‰\")\n",
    "print(\"  - æˆ–æ‰‹å‹•åŸ·è¡Œ: nltk.download('popular')\")\n",
    "print(\"  - æˆ–ä½¿ç”¨ GUI: nltk.download()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 spaCy èªè¨€æ¨¡å‹\n",
    "\n",
    "spaCy æ¨¡å‹å¤§å°èˆ‡åŠŸèƒ½å°æ¯”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spaCy æ¨¡å‹å°æ¯”\n",
    "spacy_models = {\n",
    "    'æ¨¡å‹': [\n",
    "        'en_core_web_sm',\n",
    "        'en_core_web_md',\n",
    "        'en_core_web_lg',\n",
    "        'zh_core_web_sm',\n",
    "        'zh_core_web_md'\n",
    "    ],\n",
    "    'èªè¨€': ['è‹±æ–‡', 'è‹±æ–‡', 'è‹±æ–‡', 'ä¸­æ–‡', 'ä¸­æ–‡'],\n",
    "    'å¤§å°': ['12 MB', '40 MB', '560 MB', '40 MB', '80 MB'],\n",
    "    'è©å‘é‡': ['âŒ', 'âœ… (300 ç¶­)', 'âœ… (300 ç¶­)', 'âŒ', 'âœ…'],\n",
    "    'åŠŸèƒ½': [\n",
    "        'POS, NER, Parser',\n",
    "        'POS, NER, Parser, Vectors',\n",
    "        'POS, NER, Parser, Vectors',\n",
    "        'POS, NER, Parser',\n",
    "        'POS, NER, Parser, Vectors'\n",
    "    ],\n",
    "    'æ¨è–¦å ´æ™¯': [\n",
    "        'å¿«é€Ÿé–‹ç™¼/æ¸¬è©¦',\n",
    "        'ä¸€èˆ¬æ‡‰ç”¨',\n",
    "        'é«˜æº–ç¢ºç‡éœ€æ±‚',\n",
    "        'ä¸­æ–‡åŸºæœ¬è™•ç†',\n",
    "        'ä¸­æ–‡èªç¾©åˆ†æ'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_spacy = pd.DataFrame(spacy_models)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "table = ax.table(cellText=df_spacy.values, colLabels=df_spacy.columns,\n",
    "                cellLoc='center', loc='center')\n",
    "\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1, 2.5)\n",
    "\n",
    "# è¨­å®šè¡¨é ­\n",
    "for i in range(len(df_spacy.columns)):\n",
    "    table[(0, i)].set_facecolor('#4ECDC4')\n",
    "    table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "\n",
    "# é«˜äº®æ¨è–¦æ¨¡å‹\n",
    "table[(2, 0)].set_facecolor('#FFEB3B')  # en_core_web_md\n",
    "table[(4, 0)].set_facecolor('#FFEB3B')  # zh_core_web_sm\n",
    "\n",
    "plt.title('spaCy èªè¨€æ¨¡å‹å°æ¯”', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ“¥ ä¸‹è¼‰æŒ‡ä»¤:\")\n",
    "print(\"  python -m spacy download en_core_web_md  # æ¨è–¦ (è‹±æ–‡)\")\n",
    "print(\"  python -m spacy download zh_core_web_sm  # æ¨è–¦ (ä¸­æ–‡)\")\n",
    "\n",
    "print(\"\\nğŸ’¡ é¸æ“‡å»ºè­°:\")\n",
    "print(\"  - é–‹ç™¼æ¸¬è©¦: ä½¿ç”¨ sm (å°å‹) æ¨¡å‹\")\n",
    "print(\"  - ç”Ÿç”¢ç’°å¢ƒ: ä½¿ç”¨ md (ä¸­å‹) æ¨¡å‹ (å«è©å‘é‡)\")\n",
    "print(\"  - é«˜æº–ç¢ºç‡: ä½¿ç”¨ lg (å¤§å‹) æ¨¡å‹ (éœ€æ›´å¤šè¨˜æ†¶é«”)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"8\"></a>\n",
    "## 8. å¯¦æˆ°ç·´ç¿’\n",
    "\n",
    "### ç·´ç¿’ 1: å®Œæ•´ç’°å¢ƒæ­å»º\n",
    "\n",
    "æ ¹æ“šä»¥ä¸‹æ­¥é©Ÿ,å¾é›¶æ­å»ºå®Œæ•´ NLP ç’°å¢ƒ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç·´ç¿’ 1: å®Œæ•´ç’°å¢ƒæ­å»ºæ­¥é©Ÿ\n",
    "exercise1 = \"\"\"\n",
    "ğŸ“ ç·´ç¿’ 1: å¾é›¶æ­å»º NLP ç’°å¢ƒ\n",
    "\n",
    "æ­¥é©Ÿ 1: å‰µå»ºå°ˆæ¡ˆ\n",
    "  poetry new nlp-complete-env\n",
    "  cd nlp-complete-env\n",
    "  poetry config virtualenvs.in-project true\n",
    "\n",
    "æ­¥é©Ÿ 2: å®‰è£åŸºç¤å·¥å…·\n",
    "  poetry add numpy pandas scikit-learn matplotlib seaborn\n",
    "\n",
    "æ­¥é©Ÿ 3: å®‰è£ä¸­æ–‡ NLP\n",
    "  poetry add jieba\n",
    "\n",
    "æ­¥é©Ÿ 4: å®‰è£è‹±æ–‡ NLP\n",
    "  poetry add nltk spacy\n",
    "  python -m spacy download en_core_web_md\n",
    "\n",
    "æ­¥é©Ÿ 5: å®‰è£æ·±åº¦å­¸ç¿’æ¡†æ¶\n",
    "  poetry add torch torchvision torchaudio\n",
    "\n",
    "æ­¥é©Ÿ 6: å®‰è£ Transformers ç”Ÿæ…‹\n",
    "  poetry add transformers datasets tokenizers\n",
    "\n",
    "æ­¥é©Ÿ 7: ä¸‹è¼‰ NLTK æ•¸æ“š\n",
    "  python -c \"import nltk; nltk.download('popular')\"\n",
    "\n",
    "æ­¥é©Ÿ 8: ç’°å¢ƒé©—è­‰\n",
    "  poetry run python -c \"import jieba, nltk, spacy, torch, transformers; print('âœ… All OK')\"\n",
    "\n",
    "æ­¥é©Ÿ 9: æª¢æŸ¥ä¾è³´æ¨¹\n",
    "  poetry show --tree\n",
    "\n",
    "æ­¥é©Ÿ 10: æäº¤é…ç½®\n",
    "  git init\n",
    "  git add pyproject.toml poetry.lock\n",
    "  git commit -m \"feat: initial NLP environment setup\"\n",
    "\"\"\"\n",
    "\n",
    "print(exercise1)\n",
    "\n",
    "print(\"\\nğŸ¯ é æœŸçµæœ:\")\n",
    "print(\"  - æ‰€æœ‰å¥—ä»¶æ­£ç¢ºå®‰è£\")\n",
    "print(\"  - ä¾è³´ç„¡è¡çª\")\n",
    "print(\"  - èªè¨€æ¨¡å‹å·²ä¸‹è¼‰\")\n",
    "print(\"  - å¯æ­£å¸¸å°å…¥æ‰€æœ‰å¥—ä»¶\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ç·´ç¿’ 2: å¥—ä»¶åŠŸèƒ½æ¸¬è©¦\n",
    "\n",
    "æ¸¬è©¦æ‰€æœ‰å·²å®‰è£å¥—ä»¶çš„æ ¸å¿ƒåŠŸèƒ½:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç·´ç¿’ 2: ç¶œåˆåŠŸèƒ½æ¸¬è©¦\n",
    "def test_all_packages():\n",
    "    \"\"\"æ¸¬è©¦æ‰€æœ‰ NLP å¥—ä»¶æ ¸å¿ƒåŠŸèƒ½\"\"\"\n",
    "    \n",
    "    print(\"ğŸ§ª NLP å¥—ä»¶åŠŸèƒ½æ¸¬è©¦\\n\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # æ¸¬è©¦ 1: jieba\n",
    "    try:\n",
    "        import jieba\n",
    "        result = list(jieba.cut(\"æˆ‘æ„›è‡ªç„¶èªè¨€è™•ç†\"))\n",
    "        print(f\"âœ… jieba åˆ†è©: {result}\")\n",
    "    except:\n",
    "        print(\"âŒ jieba æ¸¬è©¦å¤±æ•—\")\n",
    "    \n",
    "    # æ¸¬è©¦ 2: NLTK\n",
    "    try:\n",
    "        from nltk.tokenize import word_tokenize\n",
    "        result = word_tokenize(\"NLP is awesome\")\n",
    "        print(f\"âœ… NLTK åˆ†è©: {result}\")\n",
    "    except:\n",
    "        print(\"âŒ NLTK æ¸¬è©¦å¤±æ•— (å¯èƒ½ç¼ºå°‘æ•¸æ“šåŒ…)\")\n",
    "    \n",
    "    # æ¸¬è©¦ 3: spaCy\n",
    "    try:\n",
    "        import spacy\n",
    "        nlp = spacy.load('en_core_web_sm')\n",
    "        doc = nlp(\"Apple is a company\")\n",
    "        entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "        print(f\"âœ… spaCy NER: {entities}\")\n",
    "    except:\n",
    "        print(\"âŒ spaCy æ¸¬è©¦å¤±æ•— (å¯èƒ½ç¼ºå°‘æ¨¡å‹)\")\n",
    "    \n",
    "    # æ¸¬è©¦ 4: PyTorch\n",
    "    try:\n",
    "        import torch\n",
    "        x = torch.tensor([1, 2, 3])\n",
    "        print(f\"âœ… PyTorch: {x}, CUDA={torch.cuda.is_available()}\")\n",
    "    except:\n",
    "        print(\"âŒ PyTorch æ¸¬è©¦å¤±æ•—\")\n",
    "    \n",
    "    # æ¸¬è©¦ 5: Transformers\n",
    "    try:\n",
    "        from transformers import AutoTokenizer\n",
    "        print(f\"âœ… Transformers: æ¨¡çµ„è¼‰å…¥æˆåŠŸ\")\n",
    "    except:\n",
    "        print(\"âŒ Transformers æ¸¬è©¦å¤±æ•—\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"\\næ¸¬è©¦å®Œæˆ!\")\n",
    "\n",
    "# åŸ·è¡Œæ¸¬è©¦ (å–æ¶ˆè¨»è§£)\n",
    "# test_all_packages()\n",
    "\n",
    "print(\"ğŸ’¡ å–æ¶ˆä¸Šæ–¹è¨»è§£åŸ·è¡Œå®Œæ•´æ¸¬è©¦\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“š æœ¬èª²ç¸½çµ\n",
    "\n",
    "### æ ¸å¿ƒè¦é»å›é¡§:\n",
    "\n",
    "1. **NLP å¥—ä»¶åˆ†é¡:**\n",
    "   - ä¸­æ–‡å°ˆç”¨: jieba (è¼•é‡ã€å¿«é€Ÿ)\n",
    "   - è‹±æ–‡å·¥å…·: NLTK (æ•™å­¸)ã€spaCy (ç”Ÿç”¢)\n",
    "   - æ·±åº¦å­¸ç¿’: Transformers (SOTA æ¨¡å‹)\n",
    "\n",
    "2. **å®‰è£ç­–ç•¥:**\n",
    "   - åˆ†å±¤å®‰è£ (åŸºç¤ â†’ NLP â†’ æ·±åº¦å­¸ç¿’)\n",
    "   - ä½¿ç”¨ Poetry ç®¡ç†ä¾è³´\n",
    "   - å®šæœŸæª¢æŸ¥ç›¸å®¹æ€§\n",
    "\n",
    "3. **æ·±åº¦å­¸ç¿’æ¡†æ¶:**\n",
    "   - PyTorch: NLP ç ”ç©¶é¦–é¸,Transformers ç”Ÿæ…‹\n",
    "   - TensorFlow: ç”Ÿç”¢éƒ¨ç½²å®Œå–„\n",
    "\n",
    "4. **æ¨¡å‹ç®¡ç†:**\n",
    "   - NLTK: éœ€ä¸‹è¼‰æ•¸æ“šåŒ… (punkt, stopwords ç­‰)\n",
    "   - spaCy: éœ€ä¸‹è¼‰èªè¨€æ¨¡å‹ (en_core_web_md ç­‰)\n",
    "   - Transformers: è‡ªå‹•å¿«å–æ¨¡å‹ (~/.cache/huggingface)\n",
    "\n",
    "5. **ç›¸å®¹æ€§æª¢æŸ¥:**\n",
    "   - ä½¿ç”¨ç’°å¢ƒé©—è­‰è…³æœ¬\n",
    "   - å®šæœŸåŸ·è¡Œ poetry update\n",
    "   - æª¢æŸ¥ä¾è³´æ¨¹ (poetry show --tree)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ ä¸‹ç¯€é å‘Š\n",
    "\n",
    "**CH01-03: é–‹ç™¼ç’°å¢ƒæ¸¬è©¦**\n",
    "\n",
    "æˆ‘å€‘å°‡æ·±å…¥æ¢è¨:\n",
    "- å®Œæ•´ç’°å¢ƒæª¢æ¸¬è…³æœ¬\n",
    "- GPU ç’°å¢ƒæ¸¬è©¦ (CUDA, cuDNN)\n",
    "- æ¨¡å‹è¼‰å…¥é©—è­‰\n",
    "- æ•ˆèƒ½åŸºæº–æ¸¬è©¦\n",
    "- å¸¸è¦‹å•é¡Œè¨ºæ–·èˆ‡æ’é™¤\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“– å»¶ä¼¸é–±è®€\n",
    "\n",
    "1. **å®˜æ–¹æ–‡æª”:**\n",
    "   - [jieba GitHub](https://github.com/fxsjy/jieba)\n",
    "   - [NLTK å®˜æ–¹æ–‡æª”](https://www.nltk.org/)\n",
    "   - [spaCy å®˜æ–¹æ–‡æª”](https://spacy.io/)\n",
    "   - [Transformers å®˜æ–¹æ–‡æª”](https://huggingface.co/docs/transformers/)\n",
    "\n",
    "2. **æ¨¡å‹è³‡æº:**\n",
    "   - [Hugging Face Hub](https://huggingface.co/models)\n",
    "   - [spaCy æ¨¡å‹åˆ—è¡¨](https://spacy.io/models)\n",
    "   - [NLTK æ•¸æ“šä¸‹è¼‰](https://www.nltk.org/data.html)\n",
    "\n",
    "3. **é€²éšä¸»é¡Œ:**\n",
    "   - [ä¸­æ–‡åˆ†è©ç®—æ³•è©³è§£](https://github.com/hankcs/HanLP)\n",
    "   - [spaCy é€²éšæ•™ç¨‹](https://course.spacy.io/)\n",
    "   - [Transformers èª²ç¨‹](https://huggingface.co/course/)\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ™‹ å•é¡Œè¨è«–\n",
    "\n",
    "æœ‰ä»»ä½•å•é¡Œå—?æ­¡è¿åœ¨è¨è«–å€æå•!\n",
    "\n",
    "---\n",
    "\n",
    "**èª²ç¨‹è³‡è¨Š:**\n",
    "- **ä½œè€…:** iSpan NLP Team\n",
    "- **ç‰ˆæœ¬:** v1.0\n",
    "- **æœ€å¾Œæ›´æ–°:** 2025-10-17\n",
    "- **æˆæ¬Š:** MIT License (åƒ…ä¾›æ•™å­¸ä½¿ç”¨)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
