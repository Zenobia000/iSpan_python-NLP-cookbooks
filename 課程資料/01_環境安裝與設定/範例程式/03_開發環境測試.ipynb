{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH01-03: é–‹ç™¼ç’°å¢ƒæ¸¬è©¦\n",
    "\n",
    "**èª²ç¨‹ç›®æ¨™:**\n",
    "- å­¸æœƒæª¢æ¸¬é–‹ç™¼ç’°å¢ƒæ˜¯å¦æ­£ç¢ºè¨­å®š\n",
    "- æŒæ¡ GPU ç’°å¢ƒçš„æ¸¬è©¦æ–¹æ³•\n",
    "- äº†è§£å¸¸è¦‹å•é¡Œçš„è¨ºæ–·èˆ‡æ’é™¤\n",
    "- å»ºç«‹ç’°å¢ƒæ¸¬è©¦è‡ªå‹•åŒ–è…³æœ¬\n",
    "\n",
    "**å­¸ç¿’æ™‚é–“:** ç´„ 60 åˆ†é˜\n",
    "\n",
    "**å‰ç½®çŸ¥è­˜:**\n",
    "- Poetry åŸºç¤æ“ä½œ\n",
    "- NLP å¥—ä»¶å®‰è£\n",
    "- Python åŸºç¤èªæ³•\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š ç›®éŒ„\n",
    "\n",
    "1. [ç’°å¢ƒæª¢æ¸¬æ¸…å–®](#1)\n",
    "2. [ç³»çµ±è³‡è¨Šé©—è­‰](#2)\n",
    "3. [å¥—ä»¶ç‰ˆæœ¬æª¢æŸ¥](#3)\n",
    "4. [GPU ç’°å¢ƒæ¸¬è©¦](#4)\n",
    "5. [NLP å·¥å…·åŠŸèƒ½æ¸¬è©¦](#5)\n",
    "6. [æ•ˆèƒ½åŸºæº–æ¸¬è©¦](#6)\n",
    "7. [å•é¡Œè¨ºæ–·èˆ‡æ’é™¤](#7)\n",
    "8. [è‡ªå‹•åŒ–æ¸¬è©¦è…³æœ¬](#8)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç’°å¢ƒè¨­å®šèˆ‡å¥—ä»¶å°å…¥\n",
    "import sys\n",
    "import os\n",
    "import platform\n",
    "import subprocess\n",
    "from importlib.metadata import version, PackageNotFoundError\n",
    "from pathlib import Path\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# è¨­å®šä¸­æ–‡é¡¯ç¤º\n",
    "plt.rcParams['font.sans-serif'] = ['Microsoft JhengHei', 'SimHei', 'Arial Unicode MS']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# è¨­å®šé¡¯ç¤ºé¢¨æ ¼\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"âœ… ç’°å¢ƒè¨­å®šå®Œæˆ\")\n",
    "print(f\"Python ç‰ˆæœ¬: {sys.version.split()[0]}\")\n",
    "print(f\"ç³»çµ±å¹³å°: {platform.system()} {platform.release()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "## 1. ç’°å¢ƒæª¢æ¸¬æ¸…å–®\n",
    "\n",
    "### 1.1 ç’°å¢ƒæ¸¬è©¦çš„å±¤æ¬¡æ¶æ§‹\n",
    "\n",
    "NLP é–‹ç™¼ç’°å¢ƒæ¸¬è©¦å¯åˆ†ç‚ºå››å€‹å±¤æ¬¡:\n",
    "\n",
    "1. **ç³»çµ±å±¤ (System Level)** - ä½œæ¥­ç³»çµ±ã€Python ç‰ˆæœ¬ã€è¨˜æ†¶é«”\n",
    "2. **å¥—ä»¶å±¤ (Package Level)** - æ‰€æœ‰ä¾è³´å¥—ä»¶ç‰ˆæœ¬æª¢æŸ¥\n",
    "3. **ç¡¬é«”å±¤ (Hardware Level)** - GPUã€CUDAã€cuDNN é©—è­‰\n",
    "4. **åŠŸèƒ½å±¤ (Functional Level)** - NLP å·¥å…·åŠŸèƒ½æ¸¬è©¦\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç’°å¢ƒæª¢æ¸¬æ¸…å–®è¦–è¦ºåŒ–\n",
    "checklist_data = {\n",
    "    'æª¢æ¸¬å±¤æ¬¡': [\n",
    "        'ç³»çµ±å±¤',\n",
    "        'ç³»çµ±å±¤',\n",
    "        'ç³»çµ±å±¤',\n",
    "        'å¥—ä»¶å±¤',\n",
    "        'å¥—ä»¶å±¤',\n",
    "        'ç¡¬é«”å±¤',\n",
    "        'ç¡¬é«”å±¤',\n",
    "        'åŠŸèƒ½å±¤',\n",
    "        'åŠŸèƒ½å±¤'\n",
    "    ],\n",
    "    'æª¢æ¸¬é …ç›®': [\n",
    "        'OS ç‰ˆæœ¬',\n",
    "        'Python ç‰ˆæœ¬',\n",
    "        'å¯ç”¨è¨˜æ†¶é«”',\n",
    "        'æ ¸å¿ƒå¥—ä»¶ç‰ˆæœ¬',\n",
    "        'ä¾è³´å®Œæ•´æ€§',\n",
    "        'CUDA å¯ç”¨æ€§',\n",
    "        'GPU è¨˜æ†¶é«”',\n",
    "        'åˆ†è©åŠŸèƒ½',\n",
    "        'æ¨¡å‹è¼‰å…¥'\n",
    "    ],\n",
    "    'æª¢æ¸¬æ–¹æ³•': [\n",
    "        'platform.system()',\n",
    "        'sys.version',\n",
    "        'psutil.virtual_memory()',\n",
    "        'importlib.metadata.version()',\n",
    "        'poetry show',\n",
    "        'torch.cuda.is_available()',\n",
    "        'torch.cuda.get_device_properties()',\n",
    "        'jieba.cut() / word_tokenize()',\n",
    "        'AutoModel.from_pretrained()'\n",
    "    ],\n",
    "    'é‡è¦æ€§': ['â­â­â­', 'â­â­â­â­â­', 'â­â­â­', 'â­â­â­â­â­', 'â­â­â­â­', \n",
    "              'â­â­â­â­', 'â­â­â­', 'â­â­â­â­â­', 'â­â­â­â­']\n",
    "}\n",
    "\n",
    "df_checklist = pd.DataFrame(checklist_data)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "table = ax.table(cellText=df_checklist.values, colLabels=df_checklist.columns,\n",
    "                cellLoc='left', loc='center')\n",
    "\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1, 2.2)\n",
    "\n",
    "# è¨­å®šè¡¨é ­æ¨£å¼\n",
    "for i in range(len(df_checklist.columns)):\n",
    "    table[(0, i)].set_facecolor('#4ECDC4')\n",
    "    table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "\n",
    "# è¨­å®šå±¤æ¬¡é¡è‰²\n",
    "colors = {\n",
    "    'ç³»çµ±å±¤': '#E8F4F8',\n",
    "    'å¥—ä»¶å±¤': '#FFF9E6',\n",
    "    'ç¡¬é«”å±¤': '#FFE6E6',\n",
    "    'åŠŸèƒ½å±¤': '#E6F7E6'\n",
    "}\n",
    "\n",
    "for i in range(1, len(df_checklist) + 1):\n",
    "    layer = df_checklist.iloc[i-1]['æª¢æ¸¬å±¤æ¬¡']\n",
    "    for j in range(len(df_checklist.columns)):\n",
    "        table[(i, j)].set_facecolor(colors[layer])\n",
    "\n",
    "plt.title('ç’°å¢ƒæª¢æ¸¬æ¸…å–®', fontsize=18, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ“‹ ç’°å¢ƒæª¢æ¸¬æµç¨‹:\")\n",
    "print(\"  1ï¸âƒ£ ç³»çµ±å±¤ â†’ 2ï¸âƒ£ å¥—ä»¶å±¤ â†’ 3ï¸âƒ£ ç¡¬é«”å±¤ â†’ 4ï¸âƒ£ åŠŸèƒ½å±¤\")\n",
    "print(\"  ç”±åº•å±¤åˆ°æ‡‰ç”¨å±¤,é€æ­¥é©—è­‰ç’°å¢ƒå®Œæ•´æ€§\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "## 2. ç³»çµ±è³‡è¨Šé©—è­‰\n",
    "\n",
    "### 2.1 ä½œæ¥­ç³»çµ±èˆ‡ Python ç’°å¢ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç³»çµ±è³‡è¨Šæª¢æ¸¬\n",
    "def check_system_info():\n",
    "    \"\"\"æª¢æŸ¥ç³»çµ±åŸºæœ¬è³‡è¨Š\"\"\"\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"  ç³»çµ±è³‡è¨Šæª¢æ¸¬\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    system_info = {\n",
    "        'ä½œæ¥­ç³»çµ±': f\"{platform.system()} {platform.release()}\",\n",
    "        'CPU æ¶æ§‹': platform.machine(),\n",
    "        'Python ç‰ˆæœ¬': sys.version.split()[0],\n",
    "        'Python åŸ·è¡Œæª”': sys.executable,\n",
    "        'å·¥ä½œç›®éŒ„': os.getcwd(),\n",
    "    }\n",
    "    \n",
    "    for key, value in system_info.items():\n",
    "        print(f\"  {key:15s}: {value}\")\n",
    "    \n",
    "    # æª¢æŸ¥è™›æ“¬ç’°å¢ƒ\n",
    "    in_venv = hasattr(sys, 'real_prefix') or (hasattr(sys, 'base_prefix') and sys.base_prefix != sys.prefix)\n",
    "    \n",
    "    if in_venv:\n",
    "        print(f\"\\n  âœ… æ­£åœ¨è™›æ“¬ç’°å¢ƒä¸­\")\n",
    "        print(f\"  ç’°å¢ƒè·¯å¾‘: {sys.prefix}\")\n",
    "    else:\n",
    "        print(f\"\\n  âš ï¸ æœªåœ¨è™›æ“¬ç’°å¢ƒä¸­ (ä½¿ç”¨ç³»çµ± Python)\")\n",
    "        print(f\"  å»ºè­°: poetry shell\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# åŸ·è¡Œæª¢æŸ¥\n",
    "check_system_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 è¨˜æ†¶é«”è³‡è¨Šæª¢æŸ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨˜æ†¶é«”è³‡è¨Šæª¢æŸ¥\n",
    "def check_memory():\n",
    "    \"\"\"æª¢æŸ¥ç³»çµ±è¨˜æ†¶é«”è³‡è¨Š\"\"\"\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"  è¨˜æ†¶é«”è³‡è¨Š\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    try:\n",
    "        import psutil\n",
    "        \n",
    "        memory = psutil.virtual_memory()\n",
    "        \n",
    "        total_gb = memory.total / (1024**3)\n",
    "        available_gb = memory.available / (1024**3)\n",
    "        used_gb = memory.used / (1024**3)\n",
    "        \n",
    "        print(f\"  ç¸½è¨˜æ†¶é«”:   {total_gb:.2f} GB\")\n",
    "        print(f\"  å·²ä½¿ç”¨:     {used_gb:.2f} GB\")\n",
    "        print(f\"  å¯ç”¨:       {available_gb:.2f} GB\")\n",
    "        print(f\"  ä½¿ç”¨ç‡:     {memory.percent}%\")\n",
    "        \n",
    "        # è¦–è¦ºåŒ–è¨˜æ†¶é«”ä½¿ç”¨\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        \n",
    "        sizes = [used_gb, available_gb]\n",
    "        labels = [f'å·²ä½¿ç”¨\\n{used_gb:.1f} GB', f'å¯ç”¨\\n{available_gb:.1f} GB']\n",
    "        colors = ['#FF6B6B', '#4ECDC4']\n",
    "        explode = (0.05, 0)\n",
    "        \n",
    "        ax.pie(sizes, explode=explode, labels=labels, colors=colors,\n",
    "              autopct='%1.1f%%', shadow=True, startangle=90,\n",
    "              textprops={'fontsize': 12, 'weight': 'bold'})\n",
    "        \n",
    "        ax.set_title(f'ç³»çµ±è¨˜æ†¶é«”ä½¿ç”¨ç‹€æ³ (ç¸½è¨ˆ: {total_gb:.1f} GB)', \n",
    "                    fontsize=16, fontweight='bold', pad=20)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # å»ºè­°\n",
    "        if memory.percent > 80:\n",
    "            print(\"\\n  âš ï¸ è¨˜æ†¶é«”ä½¿ç”¨ç‡éé«˜ (>80%)\")\n",
    "            print(\"  å»ºè­°: é—œé–‰ä¸å¿…è¦çš„æ‡‰ç”¨ç¨‹å¼\")\n",
    "        elif available_gb < 2:\n",
    "            print(\"\\n  âš ï¸ å¯ç”¨è¨˜æ†¶é«”ä¸è¶³ (<2GB)\")\n",
    "            print(\"  å»ºè­°: è¨“ç·´å¤§å‹æ¨¡å‹å¯èƒ½é‡åˆ°å•é¡Œ\")\n",
    "        else:\n",
    "            print(\"\\n  âœ… è¨˜æ†¶é«”å……è¶³,å¯æ­£å¸¸é‹è¡Œ NLP ä»»å‹™\")\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"  âš ï¸ psutil æœªå®‰è£,ç„¡æ³•æª¢æ¸¬è¨˜æ†¶é«”\")\n",
    "        print(\"  å®‰è£: poetry add psutil\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# åŸ·è¡Œæª¢æŸ¥\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "## 3. å¥—ä»¶ç‰ˆæœ¬æª¢æŸ¥\n",
    "\n",
    "### 3.1 æ ¸å¿ƒå¥—ä»¶ç‰ˆæœ¬é©—è­‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ ¸å¿ƒå¥—ä»¶ç‰ˆæœ¬æª¢æŸ¥\n",
    "def check_package_versions():\n",
    "    \"\"\"æª¢æŸ¥æ‰€æœ‰æ ¸å¿ƒå¥—ä»¶ç‰ˆæœ¬\"\"\"\n",
    "    \n",
    "    packages = [\n",
    "        # åŸºç¤å·¥å…·\n",
    "        ('numpy', '1.24.0'),\n",
    "        ('pandas', '2.0.0'),\n",
    "        ('scikit-learn', '1.3.0'),\n",
    "        ('matplotlib', '3.7.0'),\n",
    "        # ä¸­æ–‡ NLP\n",
    "        ('jieba', None),\n",
    "        # è‹±æ–‡ NLP\n",
    "        ('nltk', '3.8.0'),\n",
    "        ('spacy', '3.7.0'),\n",
    "        # æ·±åº¦å­¸ç¿’\n",
    "        ('torch', '2.0.0'),\n",
    "        ('tensorflow', '2.10.0'),\n",
    "        # Transformers ç”Ÿæ…‹\n",
    "        ('transformers', '4.30.0'),\n",
    "        ('datasets', None),\n",
    "        ('tokenizers', None),\n",
    "    ]\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"  æ ¸å¿ƒå¥—ä»¶ç‰ˆæœ¬æª¢æŸ¥\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for package, min_version in packages:\n",
    "        try:\n",
    "            pkg_version = version(package)\n",
    "            status = \"âœ…\"\n",
    "            note = \"OK\"\n",
    "            \n",
    "            if min_version:\n",
    "                # ç°¡å–®ç‰ˆæœ¬æ¯”è¼ƒ (å¯¦éš›æ‡‰ä½¿ç”¨ packaging.version)\n",
    "                if pkg_version < min_version:\n",
    "                    status = \"âš ï¸\"\n",
    "                    note = f\"å»ºè­° >= {min_version}\"\n",
    "            \n",
    "            print(f\"  {status} {package:20s} {pkg_version:15s} {note}\")\n",
    "            results.append({'package': package, 'installed': True, 'version': pkg_version})\n",
    "            \n",
    "        except PackageNotFoundError:\n",
    "            print(f\"  âŒ {package:20s} {'æœªå®‰è£':15s} poetry add {package}\")\n",
    "            results.append({'package': package, 'installed': False, 'version': None})\n",
    "    \n",
    "    # çµ±è¨ˆ\n",
    "    installed = sum(1 for r in results if r['installed'])\n",
    "    total = len(results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"  å¥—ä»¶å®‰è£ç‹€æ…‹: {installed}/{total} å·²å®‰è£ ({installed/total*100:.1f}%)\")\n",
    "    \n",
    "    if installed == total:\n",
    "        print(\"\\n  âœ… æ‰€æœ‰æ ¸å¿ƒå¥—ä»¶å·²æ­£ç¢ºå®‰è£\")\n",
    "    else:\n",
    "        print(\"\\n  âš ï¸ éƒ¨åˆ†å¥—ä»¶ç¼ºå¤±,è«‹æ ¹æ“šä¸Šæ–¹æç¤ºå®‰è£\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# åŸ·è¡Œæª¢æŸ¥\n",
    "package_results = check_package_versions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¦–è¦ºåŒ–å¥—ä»¶å®‰è£ç‹€æ…‹\n",
    "if package_results:\n",
    "    installed_count = sum(1 for r in package_results if r['installed'])\n",
    "    missing_count = len(package_results) - installed_count\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # åœ“é¤…åœ–\n",
    "    sizes = [installed_count, missing_count]\n",
    "    labels = [f'å·²å®‰è£\\n{installed_count}', f'æœªå®‰è£\\n{missing_count}']\n",
    "    colors = ['#4ECDC4', '#FF6B6B']\n",
    "    explode = (0.05, 0.05)\n",
    "    \n",
    "    axes[0].pie(sizes, explode=explode, labels=labels, colors=colors,\n",
    "               autopct='%1.1f%%', shadow=True, startangle=90,\n",
    "               textprops={'fontsize': 12, 'weight': 'bold'})\n",
    "    axes[0].set_title('å¥—ä»¶å®‰è£ç‹€æ…‹', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # é•·æ¢åœ– (åˆ†é¡çµ±è¨ˆ)\n",
    "    categories = ['åŸºç¤å·¥å…·', 'ä¸­æ–‡ NLP', 'è‹±æ–‡ NLP', 'æ·±åº¦å­¸ç¿’', 'Transformers']\n",
    "    category_ranges = [\n",
    "        (0, 4),   # åŸºç¤å·¥å…·\n",
    "        (4, 5),   # ä¸­æ–‡ NLP\n",
    "        (5, 7),   # è‹±æ–‡ NLP\n",
    "        (7, 9),   # æ·±åº¦å­¸ç¿’\n",
    "        (9, 12)   # Transformers\n",
    "    ]\n",
    "    \n",
    "    category_installed = []\n",
    "    for start, end in category_ranges:\n",
    "        count = sum(1 for r in package_results[start:end] if r['installed'])\n",
    "        category_installed.append(count)\n",
    "    \n",
    "    bars = axes[1].bar(categories, category_installed, color='#4ECDC4')\n",
    "    axes[1].set_ylabel('å·²å®‰è£å¥—ä»¶æ•¸', fontsize=12)\n",
    "    axes[1].set_title('å„é¡åˆ¥å¥—ä»¶å®‰è£ç‹€æ…‹', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_ylim(0, max([r[1]-r[0] for r in category_ranges]) + 1)\n",
    "    \n",
    "    # æ¨™è¨»æ•¸å€¼\n",
    "    for bar, total_range in zip(bars, category_ranges):\n",
    "        height = bar.get_height()\n",
    "        total = total_range[1] - total_range[0]\n",
    "        axes[1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{int(height)}/{total}',\n",
    "                    ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "## 4. GPU ç’°å¢ƒæ¸¬è©¦\n",
    "\n",
    "### 4.1 PyTorch GPU æª¢æ¸¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch GPU ç’°å¢ƒæ¸¬è©¦\n",
    "def test_pytorch_gpu():\n",
    "    \"\"\"æ¸¬è©¦ PyTorch GPU å¯ç”¨æ€§\"\"\"\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"  PyTorch GPU ç’°å¢ƒæ¸¬è©¦\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    try:\n",
    "        import torch\n",
    "        \n",
    "        print(f\"  PyTorch ç‰ˆæœ¬: {torch.__version__}\")\n",
    "        print(f\"  CUDA ç·¨è­¯ç‰ˆæœ¬: {torch.version.cuda}\")\n",
    "        print(f\"  cuDNN ç‰ˆæœ¬: {torch.backends.cudnn.version()}\")\n",
    "        print(f\"  CUDA å¯ç”¨: {torch.cuda.is_available()}\")\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"\\n  âœ… GPU ç’°å¢ƒå¯ç”¨\")\n",
    "            print(f\"  GPU æ•¸é‡: {torch.cuda.device_count()}\")\n",
    "            \n",
    "            for i in range(torch.cuda.device_count()):\n",
    "                props = torch.cuda.get_device_properties(i)\n",
    "                print(f\"\\n  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "                print(f\"    è¨˜æ†¶é«”: {props.total_memory / 1e9:.2f} GB\")\n",
    "                print(f\"    è¨ˆç®—èƒ½åŠ›: {props.major}.{props.minor}\")\n",
    "            \n",
    "            # GPU è¨ˆç®—æ¸¬è©¦\n",
    "            print(\"\\n  ğŸ§ª GPU è¨ˆç®—æ¸¬è©¦:\")\n",
    "            x = torch.rand(1000, 1000).cuda()\n",
    "            y = torch.rand(1000, 1000).cuda()\n",
    "            \n",
    "            start = time.time()\n",
    "            z = torch.matmul(x, y)\n",
    "            torch.cuda.synchronize()\n",
    "            gpu_time = time.time() - start\n",
    "            \n",
    "            print(f\"    çŸ©é™£ä¹˜æ³• (1000x1000): {gpu_time*1000:.2f} ms\")\n",
    "            print(f\"    çµæœå½¢ç‹€: {z.shape}\")\n",
    "            \n",
    "            # GPU è¨˜æ†¶é«”ä½¿ç”¨\n",
    "            allocated = torch.cuda.memory_allocated() / 1e6\n",
    "            cached = torch.cuda.memory_reserved() / 1e6\n",
    "            print(f\"\\n  GPU è¨˜æ†¶é«”ä½¿ç”¨:\")\n",
    "            print(f\"    å·²åˆ†é…: {allocated:.2f} MB\")\n",
    "            print(f\"    å·²å¿«å–: {cached:.2f} MB\")\n",
    "            \n",
    "        else:\n",
    "            print(\"\\n  âš ï¸ CUDA ä¸å¯ç”¨,è«‹æª¢æŸ¥:\")\n",
    "            print(\"    1. NVIDIA é©…å‹•æ˜¯å¦å®‰è£: nvidia-smi\")\n",
    "            print(\"    2. PyTorch æ˜¯å¦ç‚º GPU ç‰ˆæœ¬\")\n",
    "            print(\"    3. CUDA ç‰ˆæœ¬æ˜¯å¦åŒ¹é…\")\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"  âŒ PyTorch æœªå®‰è£\")\n",
    "        print(\"  å®‰è£: poetry add torch\")\n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ GPU æ¸¬è©¦å¤±æ•—: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# åŸ·è¡Œæ¸¬è©¦\n",
    "test_pytorch_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 TensorFlow GPU æª¢æ¸¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow GPU ç’°å¢ƒæ¸¬è©¦\n",
    "def test_tensorflow_gpu():\n",
    "    \"\"\"æ¸¬è©¦ TensorFlow GPU å¯ç”¨æ€§\"\"\"\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"  TensorFlow GPU ç’°å¢ƒæ¸¬è©¦\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    try:\n",
    "        import tensorflow as tf\n",
    "        \n",
    "        print(f\"  TensorFlow ç‰ˆæœ¬: {tf.__version__}\")\n",
    "        \n",
    "        # GPU è£ç½®åˆ—è¡¨\n",
    "        gpus = tf.config.list_physical_devices('GPU')\n",
    "        print(f\"  GPU è£ç½®æ•¸é‡: {len(gpus)}\")\n",
    "        \n",
    "        if len(gpus) > 0:\n",
    "            print(\"\\n  âœ… GPU ç’°å¢ƒå¯ç”¨\")\n",
    "            for gpu in gpus:\n",
    "                print(f\"    - {gpu}\")\n",
    "            \n",
    "            # GPU è¨ˆç®—æ¸¬è©¦\n",
    "            print(\"\\n  ğŸ§ª GPU è¨ˆç®—æ¸¬è©¦:\")\n",
    "            with tf.device('/GPU:0'):\n",
    "                x = tf.random.normal([1000, 1000])\n",
    "                y = tf.random.normal([1000, 1000])\n",
    "                \n",
    "                start = time.time()\n",
    "                z = tf.matmul(x, y)\n",
    "                tf_time = (time.time() - start) * 1000\n",
    "                \n",
    "                print(f\"    çŸ©é™£ä¹˜æ³• (1000x1000): {tf_time:.2f} ms\")\n",
    "                print(f\"    çµæœå½¢ç‹€: {z.shape}\")\n",
    "        else:\n",
    "            print(\"\\n  âš ï¸ TensorFlow åµæ¸¬ä¸åˆ° GPU\")\n",
    "            print(\"    æª¢æŸ¥é …ç›®:\")\n",
    "            print(\"    1. TensorFlow ç‰ˆæœ¬æ˜¯å¦æ”¯æ´ GPU\")\n",
    "            print(\"    2. CUDA/cuDNN ç‰ˆæœ¬æ˜¯å¦ç›¸å®¹\")\n",
    "            print(\"    3. å®‰è£ GPU ç‰ˆæœ¬: pip install tensorflow[and-cuda]\")\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"  âš ï¸ TensorFlow æœªå®‰è£ (å¯é¸)\")\n",
    "        print(\"  å®‰è£: poetry add tensorflow\")\n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ GPU æ¸¬è©¦å¤±æ•—: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# åŸ·è¡Œæ¸¬è©¦\n",
    "test_tensorflow_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 NVIDIA é©…å‹•æª¢æŸ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NVIDIA é©…å‹•æª¢æŸ¥\n",
    "def check_nvidia_driver():\n",
    "    \"\"\"æª¢æŸ¥ NVIDIA é©…å‹•ç‹€æ…‹\"\"\"\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"  NVIDIA é©…å‹•æª¢æ¸¬\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(['nvidia-smi'], \n",
    "                              capture_output=True, text=True, check=True)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(\"\\n  âœ… NVIDIA é©…å‹•å·²å®‰è£\\n\")\n",
    "            # é¡¯ç¤º nvidia-smi è¼¸å‡º\n",
    "            print(result.stdout)\n",
    "        else:\n",
    "            print(\"  âŒ nvidia-smi åŸ·è¡Œå¤±æ•—\")\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(\"\\n  âŒ nvidia-smi æœªæ‰¾åˆ°\")\n",
    "        print(\"\\n  å¯èƒ½åŸå› :\")\n",
    "        print(\"    1. æœªå®‰è£ NVIDIA é©…å‹•\")\n",
    "        print(\"    2. é NVIDIA GPU ç³»çµ±\")\n",
    "        print(\"    3. é©…å‹•æœªæ­£ç¢ºåŠ å…¥ PATH\")\n",
    "        print(\"\\n  è§£æ±ºæ–¹æ¡ˆ:\")\n",
    "        print(\"    - ä¸‹è¼‰é©…å‹•: https://www.nvidia.com/drivers\")\n",
    "        print(\"    - æˆ–ä½¿ç”¨ CPU ç‰ˆæœ¬çš„æ·±åº¦å­¸ç¿’æ¡†æ¶\")\n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ æª¢æ¸¬å¤±æ•—: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# åŸ·è¡Œæª¢æŸ¥\n",
    "check_nvidia_driver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "## 5. NLP å·¥å…·åŠŸèƒ½æ¸¬è©¦\n",
    "\n",
    "### 5.1 ä¸­æ–‡åˆ†è©æ¸¬è©¦ (jieba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jieba åŠŸèƒ½æ¸¬è©¦\n",
    "def test_jieba():\n",
    "    \"\"\"æ¸¬è©¦ jieba åˆ†è©åŠŸèƒ½\"\"\"\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"  jieba åˆ†è©æ¸¬è©¦\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    try:\n",
    "        import jieba\n",
    "        \n",
    "        test_text = \"æˆ‘æ„›è‡ªç„¶èªè¨€è™•ç†,æ·±åº¦å­¸ç¿’å¾ˆæœ‰è¶£\"\n",
    "        \n",
    "        print(f\"\\n  æ¸¬è©¦æ–‡æœ¬: {test_text}\")\n",
    "        \n",
    "        # ç²¾ç¢ºæ¨¡å¼\n",
    "        result_precise = list(jieba.cut(test_text))\n",
    "        print(f\"  ç²¾ç¢ºæ¨¡å¼: {result_precise}\")\n",
    "        \n",
    "        # å…¨æ¨¡å¼\n",
    "        result_full = list(jieba.cut(test_text, cut_all=True))\n",
    "        print(f\"  å…¨æ¨¡å¼:   {result_full}\")\n",
    "        \n",
    "        # æœå°‹å¼•æ“æ¨¡å¼\n",
    "        result_search = list(jieba.cut_for_search(test_text))\n",
    "        print(f\"  æœå°‹æ¨¡å¼: {result_search}\")\n",
    "        \n",
    "        print(\"\\n  âœ… jieba åˆ†è©åŠŸèƒ½æ­£å¸¸\")\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"\\n  âŒ jieba æœªå®‰è£\")\n",
    "        print(\"  å®‰è£: poetry add jieba\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n  âŒ æ¸¬è©¦å¤±æ•—: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# åŸ·è¡Œæ¸¬è©¦\n",
    "test_jieba()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 è‹±æ–‡ NLP å·¥å…·æ¸¬è©¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK èˆ‡ spaCy åŠŸèƒ½æ¸¬è©¦\n",
    "def test_english_nlp():\n",
    "    \"\"\"æ¸¬è©¦è‹±æ–‡ NLP å·¥å…·\"\"\"\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"  è‹±æ–‡ NLP å·¥å…·æ¸¬è©¦\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # NLTK æ¸¬è©¦\n",
    "    print(\"\\n  ã€NLTKã€‘\")\n",
    "    try:\n",
    "        from nltk.tokenize import word_tokenize\n",
    "        \n",
    "        text = \"Natural language processing is amazing!\"\n",
    "        tokens = word_tokenize(text)\n",
    "        print(f\"    æ¸¬è©¦æ–‡æœ¬: {text}\")\n",
    "        print(f\"    åˆ†è©çµæœ: {tokens}\")\n",
    "        print(f\"    âœ… NLTK åˆ†è©æ­£å¸¸\")\n",
    "        \n",
    "    except LookupError:\n",
    "        print(\"    âŒ NLTK æ•¸æ“šåŒ…æœªä¸‹è¼‰\")\n",
    "        print(\"    åŸ·è¡Œ: nltk.download('punkt')\")\n",
    "    except ImportError:\n",
    "        print(\"    âŒ NLTK æœªå®‰è£\")\n",
    "    except Exception as e:\n",
    "        print(f\"    âŒ æ¸¬è©¦å¤±æ•—: {e}\")\n",
    "    \n",
    "    # spaCy æ¸¬è©¦\n",
    "    print(\"\\n  ã€spaCyã€‘\")\n",
    "    try:\n",
    "        import spacy\n",
    "        \n",
    "        nlp = spacy.load('en_core_web_sm')\n",
    "        text = \"Apple is looking at buying U.K. startup\"\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        print(f\"    æ¸¬è©¦æ–‡æœ¬: {text}\")\n",
    "        \n",
    "        # å¯¦é«”è­˜åˆ¥\n",
    "        entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "        print(f\"    å¯¦é«”è­˜åˆ¥: {entities}\")\n",
    "        \n",
    "        # è©æ€§æ¨™è¨»\n",
    "        pos_tags = [(token.text, token.pos_) for token in doc[:3]]\n",
    "        print(f\"    è©æ€§æ¨™è¨»: {pos_tags}\")\n",
    "        print(f\"    âœ… spaCy åŠŸèƒ½æ­£å¸¸\")\n",
    "        \n",
    "    except OSError:\n",
    "        print(\"    âŒ spaCy æ¨¡å‹æœªä¸‹è¼‰\")\n",
    "        print(\"    åŸ·è¡Œ: python -m spacy download en_core_web_sm\")\n",
    "    except ImportError:\n",
    "        print(\"    âŒ spaCy æœªå®‰è£\")\n",
    "    except Exception as e:\n",
    "        print(f\"    âŒ æ¸¬è©¦å¤±æ•—: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# åŸ·è¡Œæ¸¬è©¦\n",
    "test_english_nlp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Transformers æ¨¡å‹è¼‰å…¥æ¸¬è©¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformers æ¨¡å‹æ¸¬è©¦\n",
    "def test_transformers():\n",
    "    \"\"\"æ¸¬è©¦ Transformers æ¨¡å‹è¼‰å…¥\"\"\"\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"  Transformers æ¨¡å‹æ¸¬è©¦\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    try:\n",
    "        from transformers import AutoTokenizer, AutoModel\n",
    "        \n",
    "        print(\"\\n  âœ… Transformers æ¨¡çµ„è¼‰å…¥æˆåŠŸ\")\n",
    "        \n",
    "        # æ¸¬è©¦å°å‹æ¨¡å‹ (é¿å…é•·æ™‚é–“ä¸‹è¼‰)\n",
    "        print(\"\\n  ğŸ§ª æ¸¬è©¦æ¨¡å‹è¼‰å…¥ (distilbert-base-uncased):\")\n",
    "        \n",
    "        try:\n",
    "            tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "            print(\"    âœ… Tokenizer è¼‰å…¥æˆåŠŸ\")\n",
    "            \n",
    "            # ç°¡å–®æ¸¬è©¦\n",
    "            text = \"Hello, this is a test.\"\n",
    "            tokens = tokenizer.tokenize(text)\n",
    "            print(f\"    æ¸¬è©¦æ–‡æœ¬: {text}\")\n",
    "            print(f\"    Token åŒ–: {tokens}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    âš ï¸ æ¨¡å‹ä¸‹è¼‰å¯èƒ½éœ€è¦æ™‚é–“æˆ–ç¶²è·¯é€£ç·š\")\n",
    "            print(f\"    éŒ¯èª¤: {e}\")\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"\\n  âŒ Transformers æœªå®‰è£\")\n",
    "        print(\"  å®‰è£: poetry add transformers\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n  âŒ æ¸¬è©¦å¤±æ•—: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# åŸ·è¡Œæ¸¬è©¦ (å–æ¶ˆè¨»è§£,é¦–æ¬¡å¯èƒ½éœ€è¦ä¸‹è¼‰æ¨¡å‹)\n",
    "# test_transformers()\n",
    "\n",
    "print(\"ğŸ’¡ å–æ¶ˆè¨»è§£åŸ·è¡Œ Transformers æ¸¬è©¦ (é¦–æ¬¡æœƒä¸‹è¼‰æ¨¡å‹)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a>\n",
    "## 6. æ•ˆèƒ½åŸºæº–æ¸¬è©¦\n",
    "\n",
    "### 6.1 CPU vs GPU æ•ˆèƒ½å°æ¯”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU vs GPU æ•ˆèƒ½åŸºæº–æ¸¬è©¦\n",
    "def benchmark_cpu_gpu():\n",
    "    \"\"\"CPU èˆ‡ GPU æ•ˆèƒ½å°æ¯”æ¸¬è©¦\"\"\"\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"  CPU vs GPU æ•ˆèƒ½åŸºæº–æ¸¬è©¦\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    try:\n",
    "        import torch\n",
    "        import numpy as np\n",
    "        \n",
    "        size = 5000\n",
    "        \n",
    "        # NumPy (CPU) æ¸¬è©¦\n",
    "        print(\"\\n  ã€NumPy CPU æ¸¬è©¦ã€‘\")\n",
    "        a = np.random.rand(size, size)\n",
    "        b = np.random.rand(size, size)\n",
    "        \n",
    "        start = time.time()\n",
    "        c = np.dot(a, b)\n",
    "        cpu_time = time.time() - start\n",
    "        \n",
    "        print(f\"    çŸ©é™£å¤§å°: {size} x {size}\")\n",
    "        print(f\"    è¨ˆç®—æ™‚é–“: {cpu_time:.4f} ç§’\")\n",
    "        \n",
    "        # PyTorch GPU æ¸¬è©¦\n",
    "        if torch.cuda.is_available():\n",
    "            print(\"\\n  ã€PyTorch GPU æ¸¬è©¦ã€‘\")\n",
    "            \n",
    "            device = torch.device('cuda')\n",
    "            x = torch.rand(size, size, device=device)\n",
    "            y = torch.rand(size, size, device=device)\n",
    "            \n",
    "            torch.cuda.synchronize()\n",
    "            start = time.time()\n",
    "            z = torch.matmul(x, y)\n",
    "            torch.cuda.synchronize()\n",
    "            gpu_time = time.time() - start\n",
    "            \n",
    "            print(f\"    çŸ©é™£å¤§å°: {size} x {size}\")\n",
    "            print(f\"    è¨ˆç®—æ™‚é–“: {gpu_time:.4f} ç§’\")\n",
    "            print(f\"\\n  ğŸ“Š åŠ é€Ÿæ¯”: {cpu_time / gpu_time:.2f}x\")\n",
    "            \n",
    "            # è¦–è¦ºåŒ–å°æ¯”\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            \n",
    "            devices = ['CPU\\n(NumPy)', 'GPU\\n(PyTorch)']\n",
    "            times = [cpu_time, gpu_time]\n",
    "            colors = ['#FF6B6B', '#4ECDC4']\n",
    "            \n",
    "            bars = plt.bar(devices, times, color=colors)\n",
    "            plt.ylabel('è¨ˆç®—æ™‚é–“ (ç§’)', fontsize=12)\n",
    "            plt.title(f'çŸ©é™£ä¹˜æ³•æ•ˆèƒ½å°æ¯” ({size}x{size})', fontsize=14, fontweight='bold')\n",
    "            \n",
    "            # æ¨™è¨»æ™‚é–“\n",
    "            for bar, t in zip(bars, times):\n",
    "                height = bar.get_height()\n",
    "                plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                        f'{t:.4f}s',\n",
    "                        ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "            \n",
    "            # åŠ é€Ÿæ¯”æ–‡å­—\n",
    "            plt.text(0.5, max(times) * 0.8, \n",
    "                    f'GPU åŠ é€Ÿ: {cpu_time/gpu_time:.1f}x',\n",
    "                    ha='center', fontsize=16, fontweight='bold',\n",
    "                    bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.5))\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "        else:\n",
    "            print(\"\\n  âš ï¸ GPU ä¸å¯ç”¨,ç„¡æ³•é€²è¡Œå°æ¯”æ¸¬è©¦\")\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"\\n  âŒ PyTorch æœªå®‰è£\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n  âŒ æ¸¬è©¦å¤±æ•—: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# åŸ·è¡Œæ¸¬è©¦ (å–æ¶ˆè¨»è§£,éœ€è¦ GPU)\n",
    "# benchmark_cpu_gpu()\n",
    "\n",
    "print(\"ğŸ’¡ å–æ¶ˆè¨»è§£åŸ·è¡Œæ•ˆèƒ½æ¸¬è©¦ (éœ€è¦ GPU)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"7\"></a>\n",
    "## 7. å•é¡Œè¨ºæ–·èˆ‡æ’é™¤\n",
    "\n",
    "### 7.1 å¸¸è¦‹å•é¡Œè¨ºæ–·æ±ºç­–æ¨¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¸¸è¦‹å•é¡Œèˆ‡è§£æ±ºæ–¹æ¡ˆ\n",
    "common_issues = {\n",
    "    'å•é¡Œ': [\n",
    "        'ModuleNotFoundError',\n",
    "        'CUDA out of memory',\n",
    "        'ImportError: libcudnn.so.8',\n",
    "        'LookupError: punkt not found',\n",
    "        'OSError: en_core_web_sm not found',\n",
    "        'Version conflict'\n",
    "    ],\n",
    "    'åŸå› ': [\n",
    "        'å¥—ä»¶æœªå®‰è£æˆ–è™›æ“¬ç’°å¢ƒæœªå•Ÿå‹•',\n",
    "        'GPU è¨˜æ†¶é«”ä¸è¶³',\n",
    "        'cuDNN æœªå®‰è£æˆ–ç‰ˆæœ¬ä¸åŒ¹é…',\n",
    "        'NLTK æ•¸æ“šåŒ…æœªä¸‹è¼‰',\n",
    "        'spaCy æ¨¡å‹æœªä¸‹è¼‰',\n",
    "        'å¥—ä»¶ç‰ˆæœ¬è¡çª'\n",
    "    ],\n",
    "    'è§£æ±ºæ–¹æ¡ˆ': [\n",
    "        '1. poetry shell\\n2. poetry install',\n",
    "        '1. æ¸›å°‘ batch_size\\n2. torch.cuda.empty_cache()',\n",
    "        '1. é‡è£ PyTorch (å« cuDNN)\\n2. æª¢æŸ¥ CUDA ç‰ˆæœ¬',\n",
    "        \"nltk.download('punkt')\",\n",
    "        'python -m spacy download en_core_web_sm',\n",
    "        '1. poetry update\\n2. poetry lock --no-update'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_issues = pd.DataFrame(common_issues)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 7))\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "table = ax.table(cellText=df_issues.values, colLabels=df_issues.columns,\n",
    "                cellLoc='left', loc='center')\n",
    "\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(9)\n",
    "table.scale(1, 3)\n",
    "\n",
    "# è¨­å®šè¡¨é ­\n",
    "for i in range(len(df_issues.columns)):\n",
    "    table[(0, i)].set_facecolor('#FF6B6B')\n",
    "    table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "\n",
    "# äº¤æ›¿è¡Œé¡è‰²\n",
    "for i in range(1, len(df_issues) + 1):\n",
    "    for j in range(len(df_issues.columns)):\n",
    "        if i % 2 == 0:\n",
    "            table[(i, j)].set_facecolor('#F0F0F0')\n",
    "\n",
    "plt.title('å¸¸è¦‹å•é¡Œè¨ºæ–·èˆ‡è§£æ±ºæ–¹æ¡ˆ', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ” è¨ºæ–·æµç¨‹:\")\n",
    "print(\"  1. ç¢ºèªéŒ¯èª¤è¨Šæ¯é—œéµå­—\")\n",
    "print(\"  2. æ ¹æ“šä¸Šè¡¨æ‰¾åˆ°å°æ‡‰å•é¡Œ\")\n",
    "print(\"  3. åŸ·è¡Œå»ºè­°çš„è§£æ±ºæ–¹æ¡ˆ\")\n",
    "print(\"  4. é‡æ–°åŸ·è¡Œæ¸¬è©¦é©—è­‰\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"8\"></a>\n",
    "## 8. è‡ªå‹•åŒ–æ¸¬è©¦è…³æœ¬\n",
    "\n",
    "### 8.1 å®Œæ•´ç’°å¢ƒæª¢æ¸¬è…³æœ¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®Œæ•´ç’°å¢ƒæª¢æ¸¬è…³æœ¬\n",
    "def run_full_environment_test():\n",
    "    \"\"\"åŸ·è¡Œå®Œæ•´ç’°å¢ƒæª¢æ¸¬\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"  NLP é–‹ç™¼ç’°å¢ƒå®Œæ•´æª¢æ¸¬å ±å‘Š\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"  æª¢æ¸¬æ™‚é–“: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    test_results = []\n",
    "    \n",
    "    # æ¸¬è©¦ 1: ç³»çµ±è³‡è¨Š\n",
    "    print(\"\\n[1/6] ç³»çµ±è³‡è¨Šæª¢æ¸¬...\")\n",
    "    try:\n",
    "        check_system_info()\n",
    "        test_results.append(('ç³»çµ±è³‡è¨Š', 'âœ…'))\n",
    "    except:\n",
    "        test_results.append(('ç³»çµ±è³‡è¨Š', 'âŒ'))\n",
    "    \n",
    "    # æ¸¬è©¦ 2: å¥—ä»¶ç‰ˆæœ¬\n",
    "    print(\"\\n[2/6] å¥—ä»¶ç‰ˆæœ¬æª¢æŸ¥...\")\n",
    "    try:\n",
    "        results = check_package_versions()\n",
    "        installed = sum(1 for r in results if r['installed'])\n",
    "        if installed == len(results):\n",
    "            test_results.append(('å¥—ä»¶ç‰ˆæœ¬', 'âœ…'))\n",
    "        else:\n",
    "            test_results.append(('å¥—ä»¶ç‰ˆæœ¬', 'âš ï¸'))\n",
    "    except:\n",
    "        test_results.append(('å¥—ä»¶ç‰ˆæœ¬', 'âŒ'))\n",
    "    \n",
    "    # æ¸¬è©¦ 3: GPU ç’°å¢ƒ\n",
    "    print(\"\\n[3/6] GPU ç’°å¢ƒæ¸¬è©¦...\")\n",
    "    try:\n",
    "        test_pytorch_gpu()\n",
    "        test_results.append(('GPU ç’°å¢ƒ', 'âœ…'))\n",
    "    except:\n",
    "        test_results.append(('GPU ç’°å¢ƒ', 'âš ï¸'))\n",
    "    \n",
    "    # æ¸¬è©¦ 4: jieba\n",
    "    print(\"\\n[4/6] jieba æ¸¬è©¦...\")\n",
    "    try:\n",
    "        test_jieba()\n",
    "        test_results.append(('jieba', 'âœ…'))\n",
    "    except:\n",
    "        test_results.append(('jieba', 'âŒ'))\n",
    "    \n",
    "    # æ¸¬è©¦ 5: è‹±æ–‡ NLP\n",
    "    print(\"\\n[5/6] è‹±æ–‡ NLP å·¥å…·æ¸¬è©¦...\")\n",
    "    try:\n",
    "        test_english_nlp()\n",
    "        test_results.append(('è‹±æ–‡ NLP', 'âœ…'))\n",
    "    except:\n",
    "        test_results.append(('è‹±æ–‡ NLP', 'âš ï¸'))\n",
    "    \n",
    "    # æ¸¬è©¦ 6: è¨˜æ†¶é«”\n",
    "    print(\"\\n[6/6] è¨˜æ†¶é«”æª¢æŸ¥...\")\n",
    "    try:\n",
    "        check_memory()\n",
    "        test_results.append(('è¨˜æ†¶é«”', 'âœ…'))\n",
    "    except:\n",
    "        test_results.append(('è¨˜æ†¶é«”', 'âš ï¸'))\n",
    "    \n",
    "    # ç¸½çµå ±å‘Š\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"  æª¢æ¸¬çµæœç¸½è¦½\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for test_name, result in test_results:\n",
    "        print(f\"  {result} {test_name}\")\n",
    "    \n",
    "    # çµ±è¨ˆ\n",
    "    passed = sum(1 for _, r in test_results if r == 'âœ…')\n",
    "    warning = sum(1 for _, r in test_results if r == 'âš ï¸')\n",
    "    failed = sum(1 for _, r in test_results if r == 'âŒ')\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"  é€šé: {passed} | è­¦å‘Š: {warning} | å¤±æ•—: {failed}\")\n",
    "    \n",
    "    if failed == 0 and warning == 0:\n",
    "        print(\"\\n  ğŸ‰ ç’°å¢ƒæª¢æ¸¬å…¨éƒ¨é€šé,å¯ä»¥é–‹å§‹ NLP é–‹ç™¼!\")\n",
    "    elif failed == 0:\n",
    "        print(\"\\n  âš ï¸ éƒ¨åˆ†é …ç›®æœ‰è­¦å‘Š,å»ºè­°æª¢æŸ¥ä¸¦ä¿®å¾©\")\n",
    "    else:\n",
    "        print(\"\\n  âŒ éƒ¨åˆ†æ¸¬è©¦å¤±æ•—,è«‹æ ¹æ“šä¸Šæ–¹æç¤ºä¿®å¾©å•é¡Œ\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return test_results\n",
    "\n",
    "# åŸ·è¡Œå®Œæ•´æ¸¬è©¦ (å–æ¶ˆè¨»è§£)\n",
    "# results = run_full_environment_test()\n",
    "\n",
    "print(\"ğŸ’¡ å–æ¶ˆè¨»è§£åŸ·è¡Œå®Œæ•´ç’°å¢ƒæª¢æ¸¬\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 ç”Ÿæˆ HTML æ¸¬è©¦å ±å‘Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”Ÿæˆ HTML ç’°å¢ƒå ±å‘Š\n",
    "def generate_html_report(test_results):\n",
    "    \"\"\"ç”Ÿæˆ HTML æ ¼å¼çš„ç’°å¢ƒæ¸¬è©¦å ±å‘Š\"\"\"\n",
    "    \n",
    "    html_template = \"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html>\n",
    "    <head>\n",
    "        <meta charset=\"UTF-8\">\n",
    "        <title>NLP ç’°å¢ƒæª¢æ¸¬å ±å‘Š</title>\n",
    "        <style>\n",
    "            body {{ font-family: 'Microsoft JhengHei', sans-serif; margin: 40px; }}\n",
    "            h1 {{ color: #4ECDC4; }}\n",
    "            table {{ border-collapse: collapse; width: 100%; margin: 20px 0; }}\n",
    "            th, td {{ border: 1px solid #ddd; padding: 12px; text-align: left; }}\n",
    "            th {{ background-color: #4ECDC4; color: white; }}\n",
    "            tr:nth-child(even) {{ background-color: #f2f2f2; }}\n",
    "            .pass {{ color: green; font-weight: bold; }}\n",
    "            .warning {{ color: orange; font-weight: bold; }}\n",
    "            .fail {{ color: red; font-weight: bold; }}\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1>ğŸ” NLP é–‹ç™¼ç’°å¢ƒæª¢æ¸¬å ±å‘Š</h1>\n",
    "        <p><strong>æª¢æ¸¬æ™‚é–“:</strong> {timestamp}</p>\n",
    "        <p><strong>ç³»çµ±è³‡è¨Š:</strong> {system_info}</p>\n",
    "        \n",
    "        <h2>æ¸¬è©¦çµæœ</h2>\n",
    "        <table>\n",
    "            <tr>\n",
    "                <th>æ¸¬è©¦é …ç›®</th>\n",
    "                <th>çµæœ</th>\n",
    "                <th>ç‹€æ…‹</th>\n",
    "            </tr>\n",
    "            {test_rows}\n",
    "        </table>\n",
    "        \n",
    "        <h2>ç¸½çµ</h2>\n",
    "        <p>{summary}</p>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    \n",
    "    # ç”Ÿæˆæ¸¬è©¦è¡Œ\n",
    "    test_rows = \"\"\n",
    "    for test_name, result in test_results:\n",
    "        status_class = 'pass' if result == 'âœ…' else ('warning' if result == 'âš ï¸' else 'fail')\n",
    "        status_text = 'é€šé' if result == 'âœ…' else ('è­¦å‘Š' if result == 'âš ï¸' else 'å¤±æ•—')\n",
    "        test_rows += f\"<tr><td>{test_name}</td><td class='{status_class}'>{result}</td><td>{status_text}</td></tr>\\n\"\n",
    "    \n",
    "    # çµ±è¨ˆ\n",
    "    passed = sum(1 for _, r in test_results if r == 'âœ…')\n",
    "    total = len(test_results)\n",
    "    \n",
    "    summary = f\"é€šé {passed}/{total} é …æ¸¬è©¦\"\n",
    "    if passed == total:\n",
    "        summary += \" - ç’°å¢ƒé…ç½®å®Œç¾! ğŸ‰\"\n",
    "    \n",
    "    # å¡«å……æ¨¡æ¿\n",
    "    html = html_template.format(\n",
    "        timestamp=time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        system_info=f\"{platform.system()} {platform.release()}, Python {sys.version.split()[0]}\",\n",
    "        test_rows=test_rows,\n",
    "        summary=summary\n",
    "    )\n",
    "    \n",
    "    # å„²å­˜å ±å‘Š\n",
    "    report_path = Path('environment_report.html')\n",
    "    report_path.write_text(html, encoding='utf-8')\n",
    "    \n",
    "    print(f\"\\nâœ… HTML å ±å‘Šå·²ç”Ÿæˆ: {report_path.absolute()}\")\n",
    "    print(f\"   è«‹ä½¿ç”¨ç€è¦½å™¨é–‹å•ŸæŸ¥çœ‹\")\n",
    "\n",
    "# ä½¿ç”¨ç¯„ä¾‹ (éœ€è¦å…ˆåŸ·è¡Œ run_full_environment_test)\n",
    "# generate_html_report(results)\n",
    "\n",
    "print(\"ğŸ’¡ åŸ·è¡Œå®Œæ•´æ¸¬è©¦å¾Œ,å¯ç”Ÿæˆ HTML å ±å‘Š\")\n",
    "print(\"   1. results = run_full_environment_test()\")\n",
    "print(\"   2. generate_html_report(results)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“š æœ¬èª²ç¸½çµ\n",
    "\n",
    "### æ ¸å¿ƒè¦é»å›é¡§:\n",
    "\n",
    "1. **ç’°å¢ƒæª¢æ¸¬å››å±¤æ¶æ§‹:**\n",
    "   - ç³»çµ±å±¤: OSã€Pythonã€è¨˜æ†¶é«”\n",
    "   - å¥—ä»¶å±¤: ç‰ˆæœ¬ã€ä¾è³´å®Œæ•´æ€§\n",
    "   - ç¡¬é«”å±¤: GPUã€CUDAã€cuDNN\n",
    "   - åŠŸèƒ½å±¤: NLP å·¥å…·åŠŸèƒ½æ¸¬è©¦\n",
    "\n",
    "2. **GPU ç’°å¢ƒé©—è­‰:**\n",
    "   - PyTorch: torch.cuda.is_available()\n",
    "   - TensorFlow: tf.config.list_physical_devices('GPU')\n",
    "   - NVIDIA é©…å‹•: nvidia-smi\n",
    "\n",
    "3. **NLP å·¥å…·æ¸¬è©¦:**\n",
    "   - jieba: åˆ†è©åŠŸèƒ½\n",
    "   - NLTK: éœ€ä¸‹è¼‰æ•¸æ“šåŒ…\n",
    "   - spaCy: éœ€ä¸‹è¼‰èªè¨€æ¨¡å‹\n",
    "   - Transformers: æ¨¡å‹è¼‰å…¥èˆ‡æ¨ç†\n",
    "\n",
    "4. **å¸¸è¦‹å•é¡Œè¨ºæ–·:**\n",
    "   - ModuleNotFoundError â†’ è™›æ“¬ç’°å¢ƒæœªå•Ÿå‹•\n",
    "   - CUDA out of memory â†’ æ¸›å°‘ batch_size\n",
    "   - æ•¸æ“šåŒ…æœªæ‰¾åˆ° â†’ ä¸‹è¼‰å°æ‡‰è³‡æº\n",
    "\n",
    "5. **è‡ªå‹•åŒ–æ¸¬è©¦:**\n",
    "   - å®Œæ•´ç’°å¢ƒæª¢æ¸¬è…³æœ¬\n",
    "   - HTML å ±å‘Šç”Ÿæˆ\n",
    "   - CI/CD æ•´åˆ\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ ä¸‹ç¯€é å‘Š\n",
    "\n",
    "**CH02-01: ä»€éº¼æ˜¯è‡ªç„¶èªè¨€è™•ç†**\n",
    "\n",
    "æˆ‘å€‘å°‡é–‹å§‹æ¢è¨:\n",
    "- NLP çš„å®šç¾©èˆ‡æ‡‰ç”¨å ´æ™¯\n",
    "- NLP æŠ€è¡“ç™¼å±•å²\n",
    "- NLP æ ¸å¿ƒä»»å‹™åˆ†é¡\n",
    "- ä¸­æ–‡ NLP çš„ç‰¹æ®ŠæŒ‘æˆ°\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“– å»¶ä¼¸é–±è®€\n",
    "\n",
    "1. **ç’°å¢ƒé…ç½®æŒ‡å—:**\n",
    "   - [PyTorch CUDA æ–‡æª”](https://pytorch.org/get-started/locally/)\n",
    "   - [TensorFlow GPU æ”¯æ´](https://www.tensorflow.org/install/gpu)\n",
    "   - [NVIDIA CUDA å·¥å…·åŒ…](https://developer.nvidia.com/cuda-toolkit)\n",
    "\n",
    "2. **è¨ºæ–·å·¥å…·:**\n",
    "   - [nvidia-smi ä½¿ç”¨æŒ‡å—](https://developer.nvidia.com/nvidia-system-management-interface)\n",
    "   - [PyTorch æ•ˆèƒ½åˆ†æ](https://pytorch.org/tutorials/recipes/recipes/benchmark.html)\n",
    "\n",
    "3. **æœ€ä½³å¯¦è¸:**\n",
    "   - [ç’°å¢ƒéš”é›¢ç­–ç•¥](https://python-poetry.org/docs/managing-environments/)\n",
    "   - [CI/CD ç’°å¢ƒæ¸¬è©¦](https://docs.github.com/en/actions)\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ™‹ å•é¡Œè¨è«–\n",
    "\n",
    "æœ‰ä»»ä½•å•é¡Œå—?æ­¡è¿åœ¨è¨è«–å€æå•!\n",
    "\n",
    "---\n",
    "\n",
    "**èª²ç¨‹è³‡è¨Š:**\n",
    "- **ä½œè€…:** iSpan NLP Team\n",
    "- **ç‰ˆæœ¬:** v1.0\n",
    "- **æœ€å¾Œæ›´æ–°:** 2025-10-17\n",
    "- **æˆæ¬Š:** MIT License (åƒ…ä¾›æ•™å­¸ä½¿ç”¨)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
