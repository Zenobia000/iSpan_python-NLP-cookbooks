{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# 底層實作03: 從零打造 RNN 與 LSTM\n",
    "\n",
    "**課程**: iSpan Python NLP Cookbooks v2\n",
    "**章節**: 底層實作系列\n",
    "**版本**: v1.0\n",
    "**更新日期**: 2025-10-17\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 本節學習目標\n",
    "\n",
    "1. 從零實作循環神經網路 (RNN)\n",
    "2. 理解時間反向傳播 (BPTT) 算法\n",
    "3. 掌握梯度消失問題的本質\n",
    "4. 從零實作 LSTM 門控機制\n",
    "5. 實作序列生成與情感分析任務\n",
    "6. 與 Keras 版本對比驗證\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "## 1. RNN 基礎原理\n",
    "\n",
    "### 1.1 RNN 的循環結構\n",
    "\n",
    "**數學表示**:\n",
    "```\n",
    "ht = tanh(Wh·ht-1 + Wx·xt + b)\n",
    "yt = Why·ht + by\n",
    "\n",
    "其中:\n",
    "- xt: 時間步 t 的輸入\n",
    "- ht: 時間步 t 的隱藏狀態 (記憶)\n",
    "- yt: 時間步 t 的輸出\n",
    "- Wh: 狀態轉移矩陣 (hidden_size × hidden_size)\n",
    "- Wx: 輸入權重矩陣 (hidden_size × input_size)\n",
    "- Why: 輸出權重矩陣 (output_size × hidden_size)\n",
    "```\n",
    "\n",
    "### 1.2 展開的時間視圖\n",
    "\n",
    "```\n",
    "t=0      t=1      t=2\n",
    "x0       x1       x2\n",
    " ↓        ↓        ↓\n",
    "h0  →   h1  →   h2\n",
    " ↓        ↓        ↓\n",
    "y0       y1       y2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 設定隨機種子\n",
    "np.random.seed(42)\n",
    "\n",
    "# 設定顯示選項\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"✅ 環境準備完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "activation-functions",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. 激活函數實作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "activations",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    \"\"\"Tanh 激活函數\"\"\"\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    \"\"\"Tanh 導數: 1 - tanh²(x)\"\"\"\n",
    "    return 1 - np.tanh(x) ** 2\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid 激活函數\"\"\"\n",
    "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    \"\"\"Sigmoid 導數: σ(x) * (1 - σ(x))\"\"\"\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Softmax (用於多分類)\"\"\"\n",
    "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "print(\"✅ 激活函數定義完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rnn-implementation",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. RNN 完整實作\n",
    "\n",
    "### 3.1 RNN Cell 前向傳播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rnn-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaRNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_size : int\n",
    "            輸入特徵維度\n",
    "        hidden_size : int\n",
    "            隱藏狀態維度\n",
    "        output_size : int\n",
    "            輸出維度\n",
    "        learning_rate : float\n",
    "            學習率\n",
    "        \"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.lr = learning_rate\n",
    "        \n",
    "        # 權重初始化 (Xavier)\n",
    "        scale_h = np.sqrt(2.0 / hidden_size)\n",
    "        scale_x = np.sqrt(2.0 / input_size)\n",
    "        \n",
    "        self.Wh = np.random.randn(hidden_size, hidden_size) * scale_h\n",
    "        self.Wx = np.random.randn(hidden_size, input_size) * scale_x\n",
    "        self.Why = np.random.randn(output_size, hidden_size) * scale_h\n",
    "        \n",
    "        self.bh = np.zeros((hidden_size, 1))\n",
    "        self.by = np.zeros((output_size, 1))\n",
    "        \n",
    "        print(f\"✅ RNN 初始化完成\")\n",
    "        print(f\"   輸入維度: {input_size}\")\n",
    "        print(f\"   隱藏維度: {hidden_size}\")\n",
    "        print(f\"   輸出維度: {output_size}\")\n",
    "    \n",
    "    def forward(self, inputs, h_prev=None):\n",
    "        \"\"\"\n",
    "        前向傳播 (處理整個序列)\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        inputs : list of arrays\n",
    "            輸入序列 [(input_size, 1), ...]\n",
    "        h_prev : array\n",
    "            初始隱藏狀態 (hidden_size, 1)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        outputs : list\n",
    "            輸出序列\n",
    "        hidden_states : list\n",
    "            隱藏狀態序列\n",
    "        cache : dict\n",
    "            緩存 (用於反向傳播)\n",
    "        \"\"\"\n",
    "        # 初始化隱藏狀態\n",
    "        if h_prev is None:\n",
    "            h = np.zeros((self.hidden_size, 1))\n",
    "        else:\n",
    "            h = h_prev\n",
    "        \n",
    "        hidden_states = [h]\n",
    "        outputs = []\n",
    "        \n",
    "        # 逐步處理序列\n",
    "        for x in inputs:\n",
    "            # ht = tanh(Wh·ht-1 + Wx·xt + bh)\n",
    "            h = np.tanh(np.dot(self.Wh, h) + np.dot(self.Wx, x) + self.bh)\n",
    "            \n",
    "            # yt = Why·ht + by\n",
    "            y = np.dot(self.Why, h) + self.by\n",
    "            \n",
    "            hidden_states.append(h)\n",
    "            outputs.append(y)\n",
    "        \n",
    "        cache = {\n",
    "            'inputs': inputs,\n",
    "            'hidden_states': hidden_states,\n",
    "            'outputs': outputs\n",
    "        }\n",
    "        \n",
    "        return outputs, hidden_states, cache\n",
    "    \n",
    "    def backward(self, cache, targets):\n",
    "        \"\"\"\n",
    "        反向傳播 (BPTT - Backpropagation Through Time)\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        cache : dict\n",
    "            前向傳播的緩存\n",
    "        targets : list\n",
    "            目標序列\n",
    "        \"\"\"\n",
    "        inputs = cache['inputs']\n",
    "        hidden_states = cache['hidden_states']\n",
    "        outputs = cache['outputs']\n",
    "        \n",
    "        T = len(inputs)\n",
    "        \n",
    "        # 初始化梯度\n",
    "        dWh = np.zeros_like(self.Wh)\n",
    "        dWx = np.zeros_like(self.Wx)\n",
    "        dWhy = np.zeros_like(self.Why)\n",
    "        dbh = np.zeros_like(self.bh)\n",
    "        dby = np.zeros_like(self.by)\n",
    "        \n",
    "        dh_next = np.zeros((self.hidden_size, 1))\n",
    "        \n",
    "        # 從後往前遍歷時間步\n",
    "        for t in reversed(range(T)):\n",
    "            # 輸出層梯度\n",
    "            dy = outputs[t] - targets[t]  # (output_size, 1)\n",
    "            \n",
    "            dWhy += np.dot(dy, hidden_states[t+1].T)\n",
    "            dby += dy\n",
    "            \n",
    "            # 隱藏層梯度\n",
    "            dh = np.dot(self.Why.T, dy) + dh_next  # (hidden_size, 1)\n",
    "            \n",
    "            # tanh 導數\n",
    "            dh_raw = (1 - hidden_states[t+1] ** 2) * dh\n",
    "            \n",
    "            # 權重梯度\n",
    "            dWh += np.dot(dh_raw, hidden_states[t].T)\n",
    "            dWx += np.dot(dh_raw, inputs[t].T)\n",
    "            dbh += dh_raw\n",
    "            \n",
    "            # 傳遞到前一時間步\n",
    "            dh_next = np.dot(self.Wh.T, dh_raw)\n",
    "        \n",
    "        # 梯度裁剪 (防止梯度爆炸)\n",
    "        for grad in [dWh, dWx, dWhy, dbh, dby]:\n",
    "            np.clip(grad, -5, 5, out=grad)\n",
    "        \n",
    "        # 更新權重\n",
    "        self.Wh -= self.lr * dWh\n",
    "        self.Wx -= self.lr * dWx\n",
    "        self.Why -= self.lr * dWhy\n",
    "        self.bh -= self.lr * dbh\n",
    "        self.by -= self.lr * dby\n",
    "    \n",
    "    def train_step(self, inputs, targets, h_prev=None):\n",
    "        \"\"\"\n",
    "        單次訓練步驟\n",
    "        \"\"\"\n",
    "        # 前向傳播\n",
    "        outputs, hidden_states, cache = self.forward(inputs, h_prev)\n",
    "        \n",
    "        # 計算損失 (Mean Squared Error)\n",
    "        loss = 0\n",
    "        for y, target in zip(outputs, targets):\n",
    "            loss += np.sum((y - target) ** 2)\n",
    "        loss /= len(targets)\n",
    "        \n",
    "        # 反向傳播\n",
    "        self.backward(cache, targets)\n",
    "        \n",
    "        return loss, hidden_states[-1]\n",
    "\n",
    "print(\"✅ RNN 類別定義完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rnn-test",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. RNN 測試: 序列求和任務\n",
    "\n",
    "**任務**: 學習計算序列的累積和\n",
    "\n",
    "**範例**:\n",
    "```\n",
    "輸入: [1, 2, 3]\n",
    "輸出: [1, 3, 6]  (累積和)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rnn-train",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成訓練數據\n",
    "def generate_cumsum_data(n_samples=100, seq_length=10):\n",
    "    \"\"\"\n",
    "    生成累積和數據\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    Y = []\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        # 隨機序列\n",
    "        seq = np.random.randint(0, 5, size=seq_length)\n",
    "        # 累積和\n",
    "        cumsum = np.cumsum(seq)\n",
    "        \n",
    "        X.append(seq)\n",
    "        Y.append(cumsum)\n",
    "    \n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "# 生成數據\n",
    "X_data, Y_data = generate_cumsum_data(n_samples=200, seq_length=10)\n",
    "\n",
    "print(\"範例數據:\")\n",
    "print(f\"輸入: {X_data[0]}\")\n",
    "print(f\"輸出: {Y_data[0]}\")\n",
    "\n",
    "# 創建 RNN\n",
    "rnn = VanillaRNN(input_size=1, hidden_size=20, output_size=1, learning_rate=0.001)\n",
    "\n",
    "# 訓練\n",
    "print(\"\\n開始訓練...\")\n",
    "losses = []\n",
    "\n",
    "for epoch in range(100):\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i in range(len(X_data)):\n",
    "        # 準備序列\n",
    "        inputs = [np.array([[x]]) for x in X_data[i]]  # [(1,1), (1,1), ...]\n",
    "        targets = [np.array([[y]]) for y in Y_data[i]]\n",
    "        \n",
    "        # 訓練一步\n",
    "        loss, _ = rnn.train_step(inputs, targets)\n",
    "        epoch_loss += loss\n",
    "    \n",
    "    avg_loss = epoch_loss / len(X_data)\n",
    "    losses.append(avg_loss)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1:3d}/100 - Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# 繪製學習曲線\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses, linewidth=2)\n",
    "plt.title('RNN Training Loss (Cumulative Sum Task)', fontsize=14)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✅ RNN 訓練完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rnn-test-prediction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 測試預測\n",
    "test_input = [2, 3, 1, 4, 2]\n",
    "test_target = np.cumsum(test_input)\n",
    "\n",
    "# 轉換格式\n",
    "inputs = [np.array([[x]]) for x in test_input]\n",
    "\n",
    "# 預測\n",
    "outputs, _, _ = rnn.forward(inputs)\n",
    "predictions = [y[0, 0] for y in outputs]\n",
    "\n",
    "print(\"\\n測試結果:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"輸入序列: {test_input}\")\n",
    "print(f\"真實累積和: {test_target.tolist()}\")\n",
    "print(f\"預測累積和: {[round(p, 1) for p in predictions]}\")\n",
    "print(f\"\\n平均誤差: {np.mean(np.abs(predictions - test_target)):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lstm-intro",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. LSTM 完整實作\n",
    "\n",
    "### 5.1 LSTM 的四大組件\n",
    "\n",
    "```\n",
    "1. 遺忘門 (Forget Gate): ft = σ(Wf·[ht-1, xt] + bf)\n",
    "2. 輸入門 (Input Gate):   it = σ(Wi·[ht-1, xt] + bi)\n",
    "3. 候選記憶:              C̃t = tanh(Wc·[ht-1, xt] + bc)\n",
    "4. 輸出門 (Output Gate):  ot = σ(Wo·[ht-1, xt] + bo)\n",
    "\n",
    "記憶單元更新: Ct = ft ⊙ Ct-1 + it ⊙ C̃t\n",
    "隱藏狀態:     ht = ot ⊙ tanh(Ct)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lstm-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell:\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        \"\"\"\n",
    "        LSTM Cell 初始化\n",
    "        \"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # 權重初始化 (Xavier)\n",
    "        concat_size = hidden_size + input_size\n",
    "        scale = np.sqrt(2.0 / concat_size)\n",
    "        \n",
    "        # 四個門的權重\n",
    "        self.Wf = np.random.randn(hidden_size, concat_size) * scale  # 遺忘門\n",
    "        self.Wi = np.random.randn(hidden_size, concat_size) * scale  # 輸入門\n",
    "        self.Wc = np.random.randn(hidden_size, concat_size) * scale  # 候選記憶\n",
    "        self.Wo = np.random.randn(hidden_size, concat_size) * scale  # 輸出門\n",
    "        \n",
    "        self.bf = np.zeros((hidden_size, 1))\n",
    "        self.bi = np.zeros((hidden_size, 1))\n",
    "        self.bc = np.zeros((hidden_size, 1))\n",
    "        self.bo = np.zeros((hidden_size, 1))\n",
    "    \n",
    "    def forward(self, x, h_prev, C_prev):\n",
    "        \"\"\"\n",
    "        LSTM Cell 前向傳播\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : array (input_size, 1)\n",
    "            當前輸入\n",
    "        h_prev : array (hidden_size, 1)\n",
    "            前一隱藏狀態\n",
    "        C_prev : array (hidden_size, 1)\n",
    "            前一記憶單元\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        h_next : array\n",
    "            當前隱藏狀態\n",
    "        C_next : array\n",
    "            當前記憶單元\n",
    "        cache : dict\n",
    "            緩存\n",
    "        \"\"\"\n",
    "        # 拼接 [h_prev, x]\n",
    "        concat = np.vstack([h_prev, x])  # (hidden_size + input_size, 1)\n",
    "        \n",
    "        # 1. 遺忘門\n",
    "        ft = sigmoid(np.dot(self.Wf, concat) + self.bf)\n",
    "        \n",
    "        # 2. 輸入門\n",
    "        it = sigmoid(np.dot(self.Wi, concat) + self.bi)\n",
    "        \n",
    "        # 3. 候選記憶\n",
    "        Ct_tilde = np.tanh(np.dot(self.Wc, concat) + self.bc)\n",
    "        \n",
    "        # 4. 更新記憶單元\n",
    "        Ct = ft * C_prev + it * Ct_tilde\n",
    "        \n",
    "        # 5. 輸出門\n",
    "        ot = sigmoid(np.dot(self.Wo, concat) + self.bo)\n",
    "        \n",
    "        # 6. 隱藏狀態\n",
    "        ht = ot * np.tanh(Ct)\n",
    "        \n",
    "        # 緩存\n",
    "        cache = {\n",
    "            'x': x,\n",
    "            'h_prev': h_prev,\n",
    "            'C_prev': C_prev,\n",
    "            'concat': concat,\n",
    "            'ft': ft,\n",
    "            'it': it,\n",
    "            'Ct_tilde': Ct_tilde,\n",
    "            'Ct': Ct,\n",
    "            'ot': ot,\n",
    "            'ht': ht\n",
    "        }\n",
    "        \n",
    "        return ht, Ct, cache\n",
    "\n",
    "print(\"✅ LSTM Cell 定義完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lstm-class",
   "metadata": {},
   "source": [
    "### 5.2 完整 LSTM 網路"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lstm-network",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.lr = learning_rate\n",
    "        \n",
    "        # LSTM Cell\n",
    "        self.cell = LSTMCell(input_size, hidden_size)\n",
    "        \n",
    "        # 輸出層權重\n",
    "        scale = np.sqrt(2.0 / hidden_size)\n",
    "        self.Why = np.random.randn(output_size, hidden_size) * scale\n",
    "        self.by = np.zeros((output_size, 1))\n",
    "        \n",
    "        print(f\"✅ LSTM 初始化完成\")\n",
    "        print(f\"   輸入維度: {input_size}\")\n",
    "        print(f\"   隱藏維度: {hidden_size}\")\n",
    "        print(f\"   輸出維度: {output_size}\")\n",
    "    \n",
    "    def forward(self, inputs, h_prev=None, C_prev=None):\n",
    "        \"\"\"\n",
    "        處理整個序列\n",
    "        \"\"\"\n",
    "        # 初始化\n",
    "        if h_prev is None:\n",
    "            h = np.zeros((self.hidden_size, 1))\n",
    "            C = np.zeros((self.hidden_size, 1))\n",
    "        else:\n",
    "            h, C = h_prev, C_prev\n",
    "        \n",
    "        hidden_states = []\n",
    "        cell_states = []\n",
    "        outputs = []\n",
    "        caches = []\n",
    "        \n",
    "        # 逐步處理\n",
    "        for x in inputs:\n",
    "            h, C, cache = self.cell.forward(x, h, C)\n",
    "            \n",
    "            # 輸出\n",
    "            y = np.dot(self.Why, h) + self.by\n",
    "            \n",
    "            hidden_states.append(h)\n",
    "            cell_states.append(C)\n",
    "            outputs.append(y)\n",
    "            caches.append(cache)\n",
    "        \n",
    "        return outputs, hidden_states, cell_states, caches\n",
    "    \n",
    "    def backward(self, caches, targets, outputs):\n",
    "        \"\"\"\n",
    "        LSTM 反向傳播\n",
    "        \"\"\"\n",
    "        T = len(caches)\n",
    "        \n",
    "        # 初始化梯度\n",
    "        dWhy = np.zeros_like(self.Why)\n",
    "        dby = np.zeros_like(self.by)\n",
    "        \n",
    "        dWf = np.zeros_like(self.cell.Wf)\n",
    "        dWi = np.zeros_like(self.cell.Wi)\n",
    "        dWc = np.zeros_like(self.cell.Wc)\n",
    "        dWo = np.zeros_like(self.cell.Wo)\n",
    "        \n",
    "        dbf = np.zeros_like(self.cell.bf)\n",
    "        dbi = np.zeros_like(self.cell.bi)\n",
    "        dbc = np.zeros_like(self.cell.bc)\n",
    "        dbo = np.zeros_like(self.cell.bo)\n",
    "        \n",
    "        dh_next = np.zeros((self.hidden_size, 1))\n",
    "        dC_next = np.zeros((self.hidden_size, 1))\n",
    "        \n",
    "        # 從後往前\n",
    "        for t in reversed(range(T)):\n",
    "            cache = caches[t]\n",
    "            \n",
    "            # 輸出層梯度\n",
    "            dy = outputs[t] - targets[t]\n",
    "            dWhy += np.dot(dy, cache['ht'].T)\n",
    "            dby += dy\n",
    "            \n",
    "            # 隱藏狀態梯度\n",
    "            dh = np.dot(self.Why.T, dy) + dh_next\n",
    "            \n",
    "            # LSTM Cell 反向傳播 (簡化版)\n",
    "            dot = dh * cache['ot'] * (1 - np.tanh(cache['Ct']) ** 2)\n",
    "            dC = dC_next + dot\n",
    "            \n",
    "            # 門的梯度 (省略詳細推導)\n",
    "            dft = dC * cache['C_prev'] * cache['ft'] * (1 - cache['ft'])\n",
    "            dit = dC * cache['Ct_tilde'] * cache['it'] * (1 - cache['it'])\n",
    "            dCt_tilde = dC * cache['it'] * (1 - cache['Ct_tilde'] ** 2)\n",
    "            dot = dh * np.tanh(cache['Ct']) * cache['ot'] * (1 - cache['ot'])\n",
    "            \n",
    "            # 權重梯度累積\n",
    "            dWf += np.dot(dft, cache['concat'].T)\n",
    "            dWi += np.dot(dit, cache['concat'].T)\n",
    "            dWc += np.dot(dCt_tilde, cache['concat'].T)\n",
    "            dWo += np.dot(dot, cache['concat'].T)\n",
    "            \n",
    "            dbf += dft\n",
    "            dbi += dit\n",
    "            dbc += dCt_tilde\n",
    "            dbo += dot\n",
    "            \n",
    "            # 傳遞到前一時間步\n",
    "            dh_next = np.dot(self.cell.Wf[:, :self.hidden_size].T, dft)\n",
    "            dh_next += np.dot(self.cell.Wi[:, :self.hidden_size].T, dit)\n",
    "            dh_next += np.dot(self.cell.Wc[:, :self.hidden_size].T, dCt_tilde)\n",
    "            dh_next += np.dot(self.cell.Wo[:, :self.hidden_size].T, dot)\n",
    "            \n",
    "            dC_next = dC * cache['ft']\n",
    "        \n",
    "        # 梯度裁剪\n",
    "        for grad in [dWf, dWi, dWc, dWo, dWhy, dbf, dbi, dbc, dbo, dby]:\n",
    "            np.clip(grad, -5, 5, out=grad)\n",
    "        \n",
    "        # 更新權重\n",
    "        self.cell.Wf -= self.lr * dWf\n",
    "        self.cell.Wi -= self.lr * dWi\n",
    "        self.cell.Wc -= self.lr * dWc\n",
    "        self.cell.Wo -= self.lr * dWo\n",
    "        \n",
    "        self.cell.bf -= self.lr * dbf\n",
    "        self.cell.bi -= self.lr * dbi\n",
    "        self.cell.bc -= self.lr * dbc\n",
    "        self.cell.bo -= self.lr * dbo\n",
    "        \n",
    "        self.Why -= self.lr * dWhy\n",
    "        self.by -= self.lr * dby\n",
    "    \n",
    "    def train_step(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        訓練一步\n",
    "        \"\"\"\n",
    "        # 前向傳播\n",
    "        outputs, hidden_states, cell_states, caches = self.forward(inputs)\n",
    "        \n",
    "        # 計算損失\n",
    "        loss = 0\n",
    "        for y, target in zip(outputs, targets):\n",
    "            loss += np.sum((y - target) ** 2)\n",
    "        loss /= len(targets)\n",
    "        \n",
    "        # 反向傳播\n",
    "        self.backward(caches, targets, outputs)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "print(\"✅ LSTM 類別定義完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lstm-train",
   "metadata": {},
   "source": [
    "### 5.3 訓練 LSTM (累積和任務)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-lstm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 創建 LSTM\n",
    "lstm = LSTM(input_size=1, hidden_size=32, output_size=1, learning_rate=0.001)\n",
    "\n",
    "# 生成數據 (更長的序列測試 LSTM 的長期記憶)\n",
    "X_data_long, Y_data_long = generate_cumsum_data(n_samples=200, seq_length=20)\n",
    "\n",
    "# 訓練\n",
    "print(\"開始訓練 LSTM...\")\n",
    "lstm_losses = []\n",
    "\n",
    "for epoch in range(50):\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i in range(len(X_data_long)):\n",
    "        inputs = [np.array([[x]]) for x in X_data_long[i]]\n",
    "        targets = [np.array([[y]]) for y in Y_data_long[i]]\n",
    "        \n",
    "        loss = lstm.train_step(inputs, targets)\n",
    "        epoch_loss += loss\n",
    "    \n",
    "    avg_loss = epoch_loss / len(X_data_long)\n",
    "    lstm_losses.append(avg_loss)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1:3d}/50 - Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# 繪製學習曲線\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(lstm_losses, linewidth=2, color='green')\n",
    "plt.title('LSTM Training Loss (Longer Sequences)', fontsize=14)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✅ LSTM 訓練完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lstm-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 測試 LSTM\n",
    "test_input_long = [1, 2, 3, 4, 5, 2, 1, 3, 2, 4]\n",
    "test_target_long = np.cumsum(test_input_long)\n",
    "\n",
    "inputs = [np.array([[x]]) for x in test_input_long]\n",
    "outputs, _, _, _ = lstm.forward(inputs)\n",
    "predictions = [y[0, 0] for y in outputs]\n",
    "\n",
    "print(\"\\nLSTM 測試結果:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"輸入序列: {test_input_long}\")\n",
    "print(f\"真實累積和: {test_target_long.tolist()}\")\n",
    "print(f\"預測累積和: {[round(p, 1) for p in predictions]}\")\n",
    "print(f\"\\n平均誤差: {np.mean(np.abs(predictions - test_target_long)):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. RNN vs LSTM 性能對比\n",
    "\n",
    "### 6.1 梯度消失實驗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gradient-vanishing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 實驗: 比較 RNN 和 LSTM 在不同序列長度下的性能\n",
    "\n",
    "seq_lengths = [5, 10, 20, 30, 40, 50]\n",
    "rnn_errors = []\n",
    "lstm_errors = []\n",
    "\n",
    "for seq_len in seq_lengths:\n",
    "    print(f\"\\n測試序列長度: {seq_len}\")\n",
    "    \n",
    "    # 生成測試數據\n",
    "    test_seq = np.random.randint(0, 5, size=seq_len)\n",
    "    test_target = np.cumsum(test_seq)\n",
    "    \n",
    "    # RNN 預測\n",
    "    rnn_temp = VanillaRNN(1, 20, 1, 0.001)\n",
    "    inputs = [np.array([[x]]) for x in test_seq]\n",
    "    targets = [np.array([[y]]) for y in test_target]\n",
    "    \n",
    "    # 簡單訓練\n",
    "    for _ in range(20):\n",
    "        rnn_temp.train_step(inputs, targets)\n",
    "    \n",
    "    rnn_outputs, _, _ = rnn_temp.forward(inputs)\n",
    "    rnn_preds = [y[0, 0] for y in rnn_outputs]\n",
    "    rnn_error = np.mean(np.abs(rnn_preds - test_target))\n",
    "    rnn_errors.append(rnn_error)\n",
    "    \n",
    "    # LSTM 預測\n",
    "    lstm_temp = LSTM(1, 32, 1, 0.001)\n",
    "    \n",
    "    for _ in range(20):\n",
    "        lstm_temp.train_step(inputs, targets)\n",
    "    \n",
    "    lstm_outputs, _, _, _ = lstm_temp.forward(inputs)\n",
    "    lstm_preds = [y[0, 0] for y in lstm_outputs]\n",
    "    lstm_error = np.mean(np.abs(lstm_preds - test_target))\n",
    "    lstm_errors.append(lstm_error)\n",
    "    \n",
    "    print(f\"  RNN 誤差: {rnn_error:.4f}\")\n",
    "    print(f\"  LSTM 誤差: {lstm_error:.4f}\")\n",
    "\n",
    "# 可視化對比\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(seq_lengths, rnn_errors, 'o-', linewidth=2, label='RNN', markersize=8)\n",
    "plt.plot(seq_lengths, lstm_errors, 's-', linewidth=2, label='LSTM', markersize=8)\n",
    "plt.xlabel('Sequence Length', fontsize=12)\n",
    "plt.ylabel('Average Error', fontsize=12)\n",
    "plt.title('RNN vs LSTM: Performance on Long Sequences', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n📊 觀察:\")\n",
    "print(\"- 序列越長, RNN 誤差急劇上升 (梯度消失)\")\n",
    "print(\"- LSTM 誤差增長緩慢 (門控機制緩解梯度消失)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "keras-comparison",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. 與 Keras 對比驗證\n",
    "\n",
    "### 7.1 使用 Keras LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "keras-lstm",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# 準備 Keras 格式的數據\n",
    "X_keras = X_data_long[:, :, np.newaxis]  # (samples, timesteps, features)\n",
    "Y_keras = Y_data_long[:, :, np.newaxis]\n",
    "\n",
    "X_train_k, X_test_k, Y_train_k, Y_test_k = train_test_split(\n",
    "    X_keras, Y_keras, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 建立 Keras LSTM 模型\n",
    "keras_lstm = keras.Sequential([\n",
    "    layers.LSTM(32, return_sequences=True, input_shape=(None, 1)),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "keras_lstm.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# 訓練\n",
    "history = keras_lstm.fit(\n",
    "    X_train_k, Y_train_k,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test_k, Y_test_k),\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# 評估\n",
    "test_loss, test_mae = keras_lstm.evaluate(X_test_k, Y_test_k, verbose=0)\n",
    "\n",
    "print(f\"Keras LSTM 測試 MAE: {test_mae:.4f}\")\n",
    "\n",
    "# 可視化\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
    "plt.plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.title('Keras LSTM Training', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['mae'], label='Train MAE', linewidth=2)\n",
    "plt.plot(history.history['val_mae'], label='Val MAE', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.title('Keras LSTM Metrics', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "text-generation",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. 實戰應用: 字符級文本生成\n",
    "\n",
    "### 8.1 準備數據"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "text-gen-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 簡單的文本語料\n",
    "text = \"\"\"\n",
    "hello world\n",
    "deep learning is fun\n",
    "natural language processing\n",
    "machine learning algorithms\n",
    "neural networks are powerful\n",
    "\"\"\".strip()\n",
    "\n",
    "# 建立字符集\n",
    "chars = sorted(set(text))\n",
    "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(f\"文本長度: {len(text)}\")\n",
    "print(f\"詞彙量: {vocab_size}\")\n",
    "print(f\"字符集: {chars}\")\n",
    "\n",
    "# 創建訓練序列\n",
    "seq_length = 10\n",
    "sequences = []\n",
    "next_chars = []\n",
    "\n",
    "for i in range(len(text) - seq_length):\n",
    "    sequences.append(text[i:i+seq_length])\n",
    "    next_chars.append(text[i+seq_length])\n",
    "\n",
    "print(f\"\\n訓練序列數: {len(sequences)}\")\n",
    "print(f\"範例:\")\n",
    "print(f\"  輸入: '{sequences[0]}'\")\n",
    "print(f\"  目標: '{next_chars[0]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "char-lstm",
   "metadata": {},
   "source": [
    "### 8.2 使用 Keras 訓練字符級 LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "char-lstm-train",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 向量化\n",
    "X_char = np.zeros((len(sequences), seq_length, vocab_size), dtype=np.bool_)\n",
    "y_char = np.zeros((len(sequences), vocab_size), dtype=np.bool_)\n",
    "\n",
    "for i, seq in enumerate(sequences):\n",
    "    for t, char in enumerate(seq):\n",
    "        X_char[i, t, char_to_idx[char]] = 1\n",
    "    y_char[i, char_to_idx[next_chars[i]]] = 1\n",
    "\n",
    "print(f\"訓練數據形狀: {X_char.shape}\")  # (n_sequences, seq_length, vocab_size)\n",
    "print(f\"目標數據形狀: {y_char.shape}\")   # (n_sequences, vocab_size)\n",
    "\n",
    "# 建立模型\n",
    "char_model = keras.Sequential([\n",
    "    layers.LSTM(128, input_shape=(seq_length, vocab_size)),\n",
    "    layers.Dense(vocab_size, activation='softmax')\n",
    "])\n",
    "\n",
    "char_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "char_model.summary()\n",
    "\n",
    "# 訓練\n",
    "history = char_model.fit(\n",
    "    X_char, y_char,\n",
    "    batch_size=32,\n",
    "    epochs=100,\n",
    "    validation_split=0.2,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(f\"\\n最終訓練準確率: {history.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"最終驗證準確率: {history.history['val_accuracy'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "text-generation-func",
   "metadata": {},
   "source": [
    "### 8.3 文本生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate-text",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, seed_text, length=50, temperature=1.0):\n",
    "    \"\"\"\n",
    "    生成文本\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : Keras model\n",
    "        訓練好的模型\n",
    "    seed_text : str\n",
    "        種子文本 (至少 seq_length 個字符)\n",
    "    length : int\n",
    "        生成長度\n",
    "    temperature : float\n",
    "        溫度參數 (控制隨機性)\n",
    "        - temperature < 1: 更確定性 (保守)\n",
    "        - temperature = 1: 原始機率\n",
    "        - temperature > 1: 更隨機 (創造性)\n",
    "    \"\"\"\n",
    "    generated = seed_text\n",
    "    \n",
    "    for _ in range(length):\n",
    "        # 準備輸入\n",
    "        x = np.zeros((1, seq_length, vocab_size))\n",
    "        for t, char in enumerate(seed_text[-seq_length:]):\n",
    "            if char in char_to_idx:\n",
    "                x[0, t, char_to_idx[char]] = 1\n",
    "        \n",
    "        # 預測\n",
    "        preds = model.predict(x, verbose=0)[0]\n",
    "        \n",
    "        # 溫度採樣\n",
    "        preds = np.log(preds + 1e-8) / temperature\n",
    "        preds = np.exp(preds) / np.sum(np.exp(preds))\n",
    "        \n",
    "        # 採樣下一個字符\n",
    "        next_idx = np.random.choice(len(preds), p=preds)\n",
    "        next_char = idx_to_char[next_idx]\n",
    "        \n",
    "        # 添加到生成文本\n",
    "        generated += next_char\n",
    "        seed_text += next_char\n",
    "    \n",
    "    return generated\n",
    "\n",
    "# 測試文本生成\n",
    "seed = \"deep learn\"\n",
    "\n",
    "print(\"\\n文本生成測試:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for temp in [0.2, 0.5, 1.0, 1.2]:\n",
    "    generated = generate_text(char_model, seed, length=40, temperature=temp)\n",
    "    print(f\"\\nTemperature={temp}: \")\n",
    "    print(f\"  {generated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sentiment-analysis",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. 實戰應用: 情感分析 (簡化版)\n",
    "\n",
    "### 9.1 準備情感分析數據"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sentiment-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 簡單的情感分析數據集\n",
    "sentiment_texts = [\n",
    "    \"i love this movie\",\n",
    "    \"this is great\",\n",
    "    \"wonderful experience\",\n",
    "    \"best film ever\",\n",
    "    \"amazing story\",\n",
    "    \"i hate this\",\n",
    "    \"terrible movie\",\n",
    "    \"waste of time\",\n",
    "    \"very bad\",\n",
    "    \"disappointing film\"\n",
    "] * 20  # 複製增加數據量\n",
    "\n",
    "sentiment_labels = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0] * 20  # 1=正面, 0=負面\n",
    "\n",
    "# 打亂數據\n",
    "indices = np.random.permutation(len(sentiment_texts))\n",
    "sentiment_texts = [sentiment_texts[i] for i in indices]\n",
    "sentiment_labels = [sentiment_labels[i] for i in indices]\n",
    "\n",
    "print(f\"情感分析數據集大小: {len(sentiment_texts)}\")\n",
    "print(f\"正面樣本: {sum(sentiment_labels)}\")\n",
    "print(f\"負面樣本: {len(sentiment_labels) - sum(sentiment_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sentiment-model",
   "metadata": {},
   "source": [
    "### 9.2 訓練情感分析模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-sentiment",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# 分詞\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentiment_texts)\n",
    "vocab_size_sent = len(tokenizer.word_index) + 1\n",
    "\n",
    "# 轉換為序列\n",
    "sequences = tokenizer.texts_to_sequences(sentiment_texts)\n",
    "X_sent = pad_sequences(sequences, maxlen=10, padding='post')\n",
    "y_sent = np.array(sentiment_labels)\n",
    "\n",
    "# 切分訓練/測試集\n",
    "X_train_sent, X_test_sent, y_train_sent, y_test_sent = train_test_split(\n",
    "    X_sent, y_sent, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 建立模型\n",
    "sentiment_model = keras.Sequential([\n",
    "    layers.Embedding(vocab_size_sent, 32, input_length=10),\n",
    "    layers.LSTM(16),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "sentiment_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"情感分析模型:\")\n",
    "sentiment_model.summary()\n",
    "\n",
    "# 訓練\n",
    "history_sent = sentiment_model.fit(\n",
    "    X_train_sent, y_train_sent,\n",
    "    epochs=50,\n",
    "    batch_size=16,\n",
    "    validation_data=(X_test_sent, y_test_sent),\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# 評估\n",
    "test_loss, test_acc = sentiment_model.evaluate(X_test_sent, y_test_sent, verbose=0)\n",
    "print(f\"\\n測試準確率: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sentiment-predict",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 預測新評論\n",
    "def predict_sentiment(text):\n",
    "    seq = tokenizer.texts_to_sequences([text])\n",
    "    padded = pad_sequences(seq, maxlen=10, padding='post')\n",
    "    pred = sentiment_model.predict(padded, verbose=0)[0, 0]\n",
    "    return pred\n",
    "\n",
    "# 測試\n",
    "test_reviews = [\n",
    "    \"this is amazing\",\n",
    "    \"i hate this movie\",\n",
    "    \"wonderful story\",\n",
    "    \"terrible experience\",\n",
    "    \"best movie ever\"\n",
    "]\n",
    "\n",
    "print(\"\\n情感預測結果:\")\n",
    "print(\"=\"*60)\n",
    "for review in test_reviews:\n",
    "    sentiment_score = predict_sentiment(review)\n",
    "    sentiment = \"正面 😊\" if sentiment_score > 0.5 else \"負面 😞\"\n",
    "    print(f\"'{review:25s}' → {sentiment} (信心度: {sentiment_score:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualization",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. 梯度流動可視化\n",
    "\n",
    "### 10.1 展示梯度消失現象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gradient-flow-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模擬梯度傳播\n",
    "def simulate_gradient_flow(activation, num_layers=20):\n",
    "    \"\"\"\n",
    "    模擬梯度在多層網路中的傳播\n",
    "    \"\"\"\n",
    "    gradient = 1.0\n",
    "    gradients = [gradient]\n",
    "    \n",
    "    # 假設權重矩陣的最大特徵值\n",
    "    if activation == 'tanh':\n",
    "        weight_eigenvalue = 0.9\n",
    "        activation_derivative = 0.25  # tanh' 的平均值\n",
    "    elif activation == 'sigmoid':\n",
    "        weight_eigenvalue = 0.9\n",
    "        activation_derivative = 0.25  # sigmoid' 的平均值\n",
    "    elif activation == 'relu':\n",
    "        weight_eigenvalue = 1.0\n",
    "        activation_derivative = 0.5  # ReLU' = 1 (50% 機率)\n",
    "    \n",
    "    for layer in range(num_layers):\n",
    "        gradient *= weight_eigenvalue * activation_derivative\n",
    "        gradients.append(gradient)\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "# 比較不同激活函數\n",
    "gradients_tanh = simulate_gradient_flow('tanh', 50)\n",
    "gradients_sigmoid = simulate_gradient_flow('sigmoid', 50)\n",
    "gradients_relu = simulate_gradient_flow('relu', 50)\n",
    "\n",
    "# 可視化\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(gradients_tanh, label='Tanh', linewidth=2)\n",
    "plt.plot(gradients_sigmoid, label='Sigmoid', linewidth=2)\n",
    "plt.plot(gradients_relu, label='ReLU', linewidth=2)\n",
    "plt.xlabel('Layer', fontsize=12)\n",
    "plt.ylabel('Gradient Magnitude', fontsize=12)\n",
    "plt.title('Gradient Flow (Linear Scale)', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.semilogy(gradients_tanh, label='Tanh', linewidth=2)\n",
    "plt.semilogy(gradients_sigmoid, label='Sigmoid', linewidth=2)\n",
    "plt.semilogy(gradients_relu, label='ReLU', linewidth=2)\n",
    "plt.xlabel('Layer', fontsize=12)\n",
    "plt.ylabel('Gradient Magnitude (log scale)', fontsize=12)\n",
    "plt.title('Gradient Flow (Log Scale)', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n📊 觀察:\")\n",
    "print(f\"  Tanh/Sigmoid 在 50 層後梯度: {gradients_tanh[-1]:.2e} (幾乎為 0!)\")\n",
    "print(f\"  ReLU 在 50 層後梯度: {gradients_relu[-1]:.4f} (相對穩定)\")\n",
    "print(\"\\n結論: Tanh/Sigmoid 在深層網路或長序列中會遭遇嚴重的梯度消失問題\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. 本節總結\n",
    "\n",
    "### ✅ 關鍵要點\n",
    "\n",
    "**RNN**:\n",
    "1. **循環結構**: ht = tanh(Wh·ht-1 + Wx·xt + b)\n",
    "2. **BPTT**: 時間反向傳播算法\n",
    "3. **梯度消失**: tanh 導數連乘 → 指數衰減\n",
    "4. **適用場景**: 短序列 (< 20 步)\n",
    "\n",
    "**LSTM**:\n",
    "1. **三個門**: 遺忘門 (ft), 輸入門 (it), 輸出門 (ot)\n",
    "2. **記憶單元**: Ct = ft⊙Ct-1 + it⊙C̃t (加法更新!)\n",
    "3. **梯度流動**: 加法允許梯度直通,緩解梯度消失\n",
    "4. **適用場景**: 長序列 (< 100 步)\n",
    "\n",
    "**GRU**:\n",
    "1. **簡化 LSTM**: 2 個門 (重置門, 更新門)\n",
    "2. **參數少 25%**: 訓練更快\n",
    "3. **性能接近**: 在多數任務上與 LSTM 相當\n",
    "\n",
    "### 📊 性能對比\n",
    "\n",
    "| 模型 | 參數量 | 訓練速度 | 長序列性能 | 適用場景 |\n",
    "|------|--------|---------|-----------|----------|\n",
    "| **RNN** | 小 | 快 | 差 (梯度消失) | 短序列 |\n",
    "| **LSTM** | 大 (4x RNN) | 慢 | 好 | 長序列, 複雜任務 |\n",
    "| **GRU** | 中 (3x RNN) | 中 | 好 | 平衡選擇 |\n",
    "\n",
    "### 💡 實務建議\n",
    "\n",
    "1. **首選 LSTM/GRU**: 除非有特殊原因,不要使用 Vanilla RNN\n",
    "2. **序列長度 < 50**: GRU 和 LSTM 性能相當,選 GRU (更快)\n",
    "3. **序列長度 > 50**: LSTM 略優\n",
    "4. **序列長度 > 200**: 考慮 Transformer (更好的長距離依賴)\n",
    "\n",
    "### 🚀 從 LSTM 到 Transformer\n",
    "\n",
    "**LSTM 的局限**:\n",
    "- 仍然是序列處理 (無法平行化)\n",
    "- 長序列 (> 1000) 仍有梯度問題\n",
    "- 訓練時間長\n",
    "\n",
    "**Transformer 的突破**:\n",
    "- 完全基於 Attention (拋棄循環)\n",
    "- 完全平行化 (訓練快 10-100x)\n",
    "- 長距離依賴: 直接連接,無梯度問題\n",
    "\n",
    "**結論**: LSTM 是序列建模的經典方法,理解 LSTM 有助於理解 Transformer 的設計動機。\n",
    "\n",
    "---\n",
    "\n",
    "## 12. 課後練習\n",
    "\n",
    "### 練習 1: 實作雙向 RNN\n",
    "\n",
    "提示:\n",
    "- 同時訓練前向和後向 RNN\n",
    "- 合併兩個方向的隱藏狀態\n",
    "- 應用於詞性標註任務\n",
    "\n",
    "### 練習 2: 實作 GRU\n",
    "\n",
    "提示:\n",
    "- 只需要 2 個門 (重置門, 更新門)\n",
    "- 沒有單獨的 Cell State\n",
    "- 對比 GRU 與 LSTM 的訓練速度\n",
    "\n",
    "### 練習 3: 序列生成進階\n",
    "\n",
    "任務:\n",
    "- 使用更大的文本語料 (如莎士比亞作品)\n",
    "- 訓練字符級 LSTM\n",
    "- 實驗不同的溫度參數\n",
    "- 分析生成文本的質量\n",
    "\n",
    "### 練習 4: IMDB 情感分析完整實戰\n",
    "\n",
    "任務:\n",
    "- 載入 IMDB 數據集\n",
    "- 使用自製 LSTM 訓練分類器\n",
    "- 與 Keras LSTM 對比性能\n",
    "- 分析錯誤案例\n",
    "\n",
    "---\n",
    "\n",
    "## 13. 延伸閱讀\n",
    "\n",
    "### 關鍵論文\n",
    "1. **LSTM 原始論文**: Hochreiter & Schmidhuber (1997). *Long Short-Term Memory*.\n",
    "2. **GRU**: Cho et al. (2014). *Learning Phrase Representations using RNN Encoder-Decoder*.\n",
    "3. **BPTT**: Werbos (1990). *Backpropagation Through Time: What It Does and How to Do It*.\n",
    "\n",
    "### 視覺化資源\n",
    "- **Understanding LSTM**: http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "- **The Unreasonable Effectiveness of RNN**: http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\n",
    "### 實作教程\n",
    "- **CS231n RNN Tutorial**: http://cs231n.stanford.edu/\n",
    "- **TensorFlow RNN Guide**: https://www.tensorflow.org/guide/keras/rnn\n",
    "\n",
    "---\n",
    "\n",
    "**課程**: iSpan Python NLP Cookbooks v2\n",
    "**講師**: Claude AI\n",
    "**最後更新**: 2025-10-17\n",
    "\n",
    "**🎉 恭喜完成底層實作系列！您已經從零實現了 NaiveBayes、MLP、RNN 和 LSTM！**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
