{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# åº•å±¤å¯¦ä½œ03: å¾é›¶æ‰“é€  RNN èˆ‡ LSTM\n",
    "\n",
    "**èª²ç¨‹**: iSpan Python NLP Cookbooks v2\n",
    "**ç« ç¯€**: åº•å±¤å¯¦ä½œç³»åˆ—\n",
    "**ç‰ˆæœ¬**: v1.0\n",
    "**æ›´æ–°æ—¥æœŸ**: 2025-10-17\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š æœ¬ç¯€å­¸ç¿’ç›®æ¨™\n",
    "\n",
    "1. å¾é›¶å¯¦ä½œå¾ªç’°ç¥ç¶“ç¶²è·¯ (RNN)\n",
    "2. ç†è§£æ™‚é–“åå‘å‚³æ’­ (BPTT) ç®—æ³•\n",
    "3. æŒæ¡æ¢¯åº¦æ¶ˆå¤±å•é¡Œçš„æœ¬è³ª\n",
    "4. å¾é›¶å¯¦ä½œ LSTM é–€æ§æ©Ÿåˆ¶\n",
    "5. å¯¦ä½œåºåˆ—ç”Ÿæˆèˆ‡æƒ…æ„Ÿåˆ†æä»»å‹™\n",
    "6. èˆ‡ Keras ç‰ˆæœ¬å°æ¯”é©—è­‰\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "## 1. RNN åŸºç¤åŸç†\n",
    "\n",
    "### 1.1 RNN çš„å¾ªç’°çµæ§‹\n",
    "\n",
    "**æ•¸å­¸è¡¨ç¤º**:\n",
    "```\n",
    "ht = tanh(WhÂ·ht-1 + WxÂ·xt + b)\n",
    "yt = WhyÂ·ht + by\n",
    "\n",
    "å…¶ä¸­:\n",
    "- xt: æ™‚é–“æ­¥ t çš„è¼¸å…¥\n",
    "- ht: æ™‚é–“æ­¥ t çš„éš±è—ç‹€æ…‹ (è¨˜æ†¶)\n",
    "- yt: æ™‚é–“æ­¥ t çš„è¼¸å‡º\n",
    "- Wh: ç‹€æ…‹è½‰ç§»çŸ©é™£ (hidden_size Ã— hidden_size)\n",
    "- Wx: è¼¸å…¥æ¬Šé‡çŸ©é™£ (hidden_size Ã— input_size)\n",
    "- Why: è¼¸å‡ºæ¬Šé‡çŸ©é™£ (output_size Ã— hidden_size)\n",
    "```\n",
    "\n",
    "### 1.2 å±•é–‹çš„æ™‚é–“è¦–åœ–\n",
    "\n",
    "```\n",
    "t=0      t=1      t=2\n",
    "x0       x1       x2\n",
    " â†“        â†“        â†“\n",
    "h0  â†’   h1  â†’   h2\n",
    " â†“        â†“        â†“\n",
    "y0       y1       y2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# è¨­å®šéš¨æ©Ÿç¨®å­\n",
    "np.random.seed(42)\n",
    "\n",
    "# è¨­å®šé¡¯ç¤ºé¸é …\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"âœ… ç’°å¢ƒæº–å‚™å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "activation-functions",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. æ¿€æ´»å‡½æ•¸å¯¦ä½œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "activations",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    \"\"\"Tanh æ¿€æ´»å‡½æ•¸\"\"\"\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    \"\"\"Tanh å°æ•¸: 1 - tanhÂ²(x)\"\"\"\n",
    "    return 1 - np.tanh(x) ** 2\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid æ¿€æ´»å‡½æ•¸\"\"\"\n",
    "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    \"\"\"Sigmoid å°æ•¸: Ïƒ(x) * (1 - Ïƒ(x))\"\"\"\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Softmax (ç”¨æ–¼å¤šåˆ†é¡)\"\"\"\n",
    "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "print(\"âœ… æ¿€æ´»å‡½æ•¸å®šç¾©å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rnn-implementation",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. RNN å®Œæ•´å¯¦ä½œ\n",
    "\n",
    "### 3.1 RNN Cell å‰å‘å‚³æ’­"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rnn-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaRNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_size : int\n",
    "            è¼¸å…¥ç‰¹å¾µç¶­åº¦\n",
    "        hidden_size : int\n",
    "            éš±è—ç‹€æ…‹ç¶­åº¦\n",
    "        output_size : int\n",
    "            è¼¸å‡ºç¶­åº¦\n",
    "        learning_rate : float\n",
    "            å­¸ç¿’ç‡\n",
    "        \"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.lr = learning_rate\n",
    "        \n",
    "        # æ¬Šé‡åˆå§‹åŒ– (Xavier)\n",
    "        scale_h = np.sqrt(2.0 / hidden_size)\n",
    "        scale_x = np.sqrt(2.0 / input_size)\n",
    "        \n",
    "        self.Wh = np.random.randn(hidden_size, hidden_size) * scale_h\n",
    "        self.Wx = np.random.randn(hidden_size, input_size) * scale_x\n",
    "        self.Why = np.random.randn(output_size, hidden_size) * scale_h\n",
    "        \n",
    "        self.bh = np.zeros((hidden_size, 1))\n",
    "        self.by = np.zeros((output_size, 1))\n",
    "        \n",
    "        print(f\"âœ… RNN åˆå§‹åŒ–å®Œæˆ\")\n",
    "        print(f\"   è¼¸å…¥ç¶­åº¦: {input_size}\")\n",
    "        print(f\"   éš±è—ç¶­åº¦: {hidden_size}\")\n",
    "        print(f\"   è¼¸å‡ºç¶­åº¦: {output_size}\")\n",
    "    \n",
    "    def forward(self, inputs, h_prev=None):\n",
    "        \"\"\"\n",
    "        å‰å‘å‚³æ’­ (è™•ç†æ•´å€‹åºåˆ—)\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        inputs : list of arrays\n",
    "            è¼¸å…¥åºåˆ— [(input_size, 1), ...]\n",
    "        h_prev : array\n",
    "            åˆå§‹éš±è—ç‹€æ…‹ (hidden_size, 1)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        outputs : list\n",
    "            è¼¸å‡ºåºåˆ—\n",
    "        hidden_states : list\n",
    "            éš±è—ç‹€æ…‹åºåˆ—\n",
    "        cache : dict\n",
    "            ç·©å­˜ (ç”¨æ–¼åå‘å‚³æ’­)\n",
    "        \"\"\"\n",
    "        # åˆå§‹åŒ–éš±è—ç‹€æ…‹\n",
    "        if h_prev is None:\n",
    "            h = np.zeros((self.hidden_size, 1))\n",
    "        else:\n",
    "            h = h_prev\n",
    "        \n",
    "        hidden_states = [h]\n",
    "        outputs = []\n",
    "        \n",
    "        # é€æ­¥è™•ç†åºåˆ—\n",
    "        for x in inputs:\n",
    "            # ht = tanh(WhÂ·ht-1 + WxÂ·xt + bh)\n",
    "            h = np.tanh(np.dot(self.Wh, h) + np.dot(self.Wx, x) + self.bh)\n",
    "            \n",
    "            # yt = WhyÂ·ht + by\n",
    "            y = np.dot(self.Why, h) + self.by\n",
    "            \n",
    "            hidden_states.append(h)\n",
    "            outputs.append(y)\n",
    "        \n",
    "        cache = {\n",
    "            'inputs': inputs,\n",
    "            'hidden_states': hidden_states,\n",
    "            'outputs': outputs\n",
    "        }\n",
    "        \n",
    "        return outputs, hidden_states, cache\n",
    "    \n",
    "    def backward(self, cache, targets):\n",
    "        \"\"\"\n",
    "        åå‘å‚³æ’­ (BPTT - Backpropagation Through Time)\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        cache : dict\n",
    "            å‰å‘å‚³æ’­çš„ç·©å­˜\n",
    "        targets : list\n",
    "            ç›®æ¨™åºåˆ—\n",
    "        \"\"\"\n",
    "        inputs = cache['inputs']\n",
    "        hidden_states = cache['hidden_states']\n",
    "        outputs = cache['outputs']\n",
    "        \n",
    "        T = len(inputs)\n",
    "        \n",
    "        # åˆå§‹åŒ–æ¢¯åº¦\n",
    "        dWh = np.zeros_like(self.Wh)\n",
    "        dWx = np.zeros_like(self.Wx)\n",
    "        dWhy = np.zeros_like(self.Why)\n",
    "        dbh = np.zeros_like(self.bh)\n",
    "        dby = np.zeros_like(self.by)\n",
    "        \n",
    "        dh_next = np.zeros((self.hidden_size, 1))\n",
    "        \n",
    "        # å¾å¾Œå¾€å‰éæ­·æ™‚é–“æ­¥\n",
    "        for t in reversed(range(T)):\n",
    "            # è¼¸å‡ºå±¤æ¢¯åº¦\n",
    "            dy = outputs[t] - targets[t]  # (output_size, 1)\n",
    "            \n",
    "            dWhy += np.dot(dy, hidden_states[t+1].T)\n",
    "            dby += dy\n",
    "            \n",
    "            # éš±è—å±¤æ¢¯åº¦\n",
    "            dh = np.dot(self.Why.T, dy) + dh_next  # (hidden_size, 1)\n",
    "            \n",
    "            # tanh å°æ•¸\n",
    "            dh_raw = (1 - hidden_states[t+1] ** 2) * dh\n",
    "            \n",
    "            # æ¬Šé‡æ¢¯åº¦\n",
    "            dWh += np.dot(dh_raw, hidden_states[t].T)\n",
    "            dWx += np.dot(dh_raw, inputs[t].T)\n",
    "            dbh += dh_raw\n",
    "            \n",
    "            # å‚³éåˆ°å‰ä¸€æ™‚é–“æ­¥\n",
    "            dh_next = np.dot(self.Wh.T, dh_raw)\n",
    "        \n",
    "        # æ¢¯åº¦è£å‰ª (é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸)\n",
    "        for grad in [dWh, dWx, dWhy, dbh, dby]:\n",
    "            np.clip(grad, -5, 5, out=grad)\n",
    "        \n",
    "        # æ›´æ–°æ¬Šé‡\n",
    "        self.Wh -= self.lr * dWh\n",
    "        self.Wx -= self.lr * dWx\n",
    "        self.Why -= self.lr * dWhy\n",
    "        self.bh -= self.lr * dbh\n",
    "        self.by -= self.lr * dby\n",
    "    \n",
    "    def train_step(self, inputs, targets, h_prev=None):\n",
    "        \"\"\"\n",
    "        å–®æ¬¡è¨“ç·´æ­¥é©Ÿ\n",
    "        \"\"\"\n",
    "        # å‰å‘å‚³æ’­\n",
    "        outputs, hidden_states, cache = self.forward(inputs, h_prev)\n",
    "        \n",
    "        # è¨ˆç®—æå¤± (Mean Squared Error)\n",
    "        loss = 0\n",
    "        for y, target in zip(outputs, targets):\n",
    "            loss += np.sum((y - target) ** 2)\n",
    "        loss /= len(targets)\n",
    "        \n",
    "        # åå‘å‚³æ’­\n",
    "        self.backward(cache, targets)\n",
    "        \n",
    "        return loss, hidden_states[-1]\n",
    "\n",
    "print(\"âœ… RNN é¡åˆ¥å®šç¾©å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rnn-test",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. RNN æ¸¬è©¦: åºåˆ—æ±‚å’Œä»»å‹™\n",
    "\n",
    "**ä»»å‹™**: å­¸ç¿’è¨ˆç®—åºåˆ—çš„ç´¯ç©å’Œ\n",
    "\n",
    "**ç¯„ä¾‹**:\n",
    "```\n",
    "è¼¸å…¥: [1, 2, 3]\n",
    "è¼¸å‡º: [1, 3, 6]  (ç´¯ç©å’Œ)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rnn-train",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”Ÿæˆè¨“ç·´æ•¸æ“š\n",
    "def generate_cumsum_data(n_samples=100, seq_length=10):\n",
    "    \"\"\"\n",
    "    ç”Ÿæˆç´¯ç©å’Œæ•¸æ“š\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    Y = []\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        # éš¨æ©Ÿåºåˆ—\n",
    "        seq = np.random.randint(0, 5, size=seq_length)\n",
    "        # ç´¯ç©å’Œ\n",
    "        cumsum = np.cumsum(seq)\n",
    "        \n",
    "        X.append(seq)\n",
    "        Y.append(cumsum)\n",
    "    \n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "# ç”Ÿæˆæ•¸æ“š\n",
    "X_data, Y_data = generate_cumsum_data(n_samples=200, seq_length=10)\n",
    "\n",
    "print(\"ç¯„ä¾‹æ•¸æ“š:\")\n",
    "print(f\"è¼¸å…¥: {X_data[0]}\")\n",
    "print(f\"è¼¸å‡º: {Y_data[0]}\")\n",
    "\n",
    "# å‰µå»º RNN\n",
    "rnn = VanillaRNN(input_size=1, hidden_size=20, output_size=1, learning_rate=0.001)\n",
    "\n",
    "# è¨“ç·´\n",
    "print(\"\\né–‹å§‹è¨“ç·´...\")\n",
    "losses = []\n",
    "\n",
    "for epoch in range(100):\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i in range(len(X_data)):\n",
    "        # æº–å‚™åºåˆ—\n",
    "        inputs = [np.array([[x]]) for x in X_data[i]]  # [(1,1), (1,1), ...]\n",
    "        targets = [np.array([[y]]) for y in Y_data[i]]\n",
    "        \n",
    "        # è¨“ç·´ä¸€æ­¥\n",
    "        loss, _ = rnn.train_step(inputs, targets)\n",
    "        epoch_loss += loss\n",
    "    \n",
    "    avg_loss = epoch_loss / len(X_data)\n",
    "    losses.append(avg_loss)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1:3d}/100 - Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# ç¹ªè£½å­¸ç¿’æ›²ç·š\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses, linewidth=2)\n",
    "plt.title('RNN Training Loss (Cumulative Sum Task)', fontsize=14)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… RNN è¨“ç·´å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rnn-test-prediction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¸¬è©¦é æ¸¬\n",
    "test_input = [2, 3, 1, 4, 2]\n",
    "test_target = np.cumsum(test_input)\n",
    "\n",
    "# è½‰æ›æ ¼å¼\n",
    "inputs = [np.array([[x]]) for x in test_input]\n",
    "\n",
    "# é æ¸¬\n",
    "outputs, _, _ = rnn.forward(inputs)\n",
    "predictions = [y[0, 0] for y in outputs]\n",
    "\n",
    "print(\"\\næ¸¬è©¦çµæœ:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"è¼¸å…¥åºåˆ—: {test_input}\")\n",
    "print(f\"çœŸå¯¦ç´¯ç©å’Œ: {test_target.tolist()}\")\n",
    "print(f\"é æ¸¬ç´¯ç©å’Œ: {[round(p, 1) for p in predictions]}\")\n",
    "print(f\"\\nå¹³å‡èª¤å·®: {np.mean(np.abs(predictions - test_target)):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lstm-intro",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. LSTM å®Œæ•´å¯¦ä½œ\n",
    "\n",
    "### 5.1 LSTM çš„å››å¤§çµ„ä»¶\n",
    "\n",
    "```\n",
    "1. éºå¿˜é–€ (Forget Gate): ft = Ïƒ(WfÂ·[ht-1, xt] + bf)\n",
    "2. è¼¸å…¥é–€ (Input Gate):   it = Ïƒ(WiÂ·[ht-1, xt] + bi)\n",
    "3. å€™é¸è¨˜æ†¶:              CÌƒt = tanh(WcÂ·[ht-1, xt] + bc)\n",
    "4. è¼¸å‡ºé–€ (Output Gate):  ot = Ïƒ(WoÂ·[ht-1, xt] + bo)\n",
    "\n",
    "è¨˜æ†¶å–®å…ƒæ›´æ–°: Ct = ft âŠ™ Ct-1 + it âŠ™ CÌƒt\n",
    "éš±è—ç‹€æ…‹:     ht = ot âŠ™ tanh(Ct)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lstm-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell:\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        \"\"\"\n",
    "        LSTM Cell åˆå§‹åŒ–\n",
    "        \"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # æ¬Šé‡åˆå§‹åŒ– (Xavier)\n",
    "        concat_size = hidden_size + input_size\n",
    "        scale = np.sqrt(2.0 / concat_size)\n",
    "        \n",
    "        # å››å€‹é–€çš„æ¬Šé‡\n",
    "        self.Wf = np.random.randn(hidden_size, concat_size) * scale  # éºå¿˜é–€\n",
    "        self.Wi = np.random.randn(hidden_size, concat_size) * scale  # è¼¸å…¥é–€\n",
    "        self.Wc = np.random.randn(hidden_size, concat_size) * scale  # å€™é¸è¨˜æ†¶\n",
    "        self.Wo = np.random.randn(hidden_size, concat_size) * scale  # è¼¸å‡ºé–€\n",
    "        \n",
    "        self.bf = np.zeros((hidden_size, 1))\n",
    "        self.bi = np.zeros((hidden_size, 1))\n",
    "        self.bc = np.zeros((hidden_size, 1))\n",
    "        self.bo = np.zeros((hidden_size, 1))\n",
    "    \n",
    "    def forward(self, x, h_prev, C_prev):\n",
    "        \"\"\"\n",
    "        LSTM Cell å‰å‘å‚³æ’­\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : array (input_size, 1)\n",
    "            ç•¶å‰è¼¸å…¥\n",
    "        h_prev : array (hidden_size, 1)\n",
    "            å‰ä¸€éš±è—ç‹€æ…‹\n",
    "        C_prev : array (hidden_size, 1)\n",
    "            å‰ä¸€è¨˜æ†¶å–®å…ƒ\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        h_next : array\n",
    "            ç•¶å‰éš±è—ç‹€æ…‹\n",
    "        C_next : array\n",
    "            ç•¶å‰è¨˜æ†¶å–®å…ƒ\n",
    "        cache : dict\n",
    "            ç·©å­˜\n",
    "        \"\"\"\n",
    "        # æ‹¼æ¥ [h_prev, x]\n",
    "        concat = np.vstack([h_prev, x])  # (hidden_size + input_size, 1)\n",
    "        \n",
    "        # 1. éºå¿˜é–€\n",
    "        ft = sigmoid(np.dot(self.Wf, concat) + self.bf)\n",
    "        \n",
    "        # 2. è¼¸å…¥é–€\n",
    "        it = sigmoid(np.dot(self.Wi, concat) + self.bi)\n",
    "        \n",
    "        # 3. å€™é¸è¨˜æ†¶\n",
    "        Ct_tilde = np.tanh(np.dot(self.Wc, concat) + self.bc)\n",
    "        \n",
    "        # 4. æ›´æ–°è¨˜æ†¶å–®å…ƒ\n",
    "        Ct = ft * C_prev + it * Ct_tilde\n",
    "        \n",
    "        # 5. è¼¸å‡ºé–€\n",
    "        ot = sigmoid(np.dot(self.Wo, concat) + self.bo)\n",
    "        \n",
    "        # 6. éš±è—ç‹€æ…‹\n",
    "        ht = ot * np.tanh(Ct)\n",
    "        \n",
    "        # ç·©å­˜\n",
    "        cache = {\n",
    "            'x': x,\n",
    "            'h_prev': h_prev,\n",
    "            'C_prev': C_prev,\n",
    "            'concat': concat,\n",
    "            'ft': ft,\n",
    "            'it': it,\n",
    "            'Ct_tilde': Ct_tilde,\n",
    "            'Ct': Ct,\n",
    "            'ot': ot,\n",
    "            'ht': ht\n",
    "        }\n",
    "        \n",
    "        return ht, Ct, cache\n",
    "\n",
    "print(\"âœ… LSTM Cell å®šç¾©å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lstm-class",
   "metadata": {},
   "source": [
    "### 5.2 å®Œæ•´ LSTM ç¶²è·¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lstm-network",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.lr = learning_rate\n",
    "        \n",
    "        # LSTM Cell\n",
    "        self.cell = LSTMCell(input_size, hidden_size)\n",
    "        \n",
    "        # è¼¸å‡ºå±¤æ¬Šé‡\n",
    "        scale = np.sqrt(2.0 / hidden_size)\n",
    "        self.Why = np.random.randn(output_size, hidden_size) * scale\n",
    "        self.by = np.zeros((output_size, 1))\n",
    "        \n",
    "        print(f\"âœ… LSTM åˆå§‹åŒ–å®Œæˆ\")\n",
    "        print(f\"   è¼¸å…¥ç¶­åº¦: {input_size}\")\n",
    "        print(f\"   éš±è—ç¶­åº¦: {hidden_size}\")\n",
    "        print(f\"   è¼¸å‡ºç¶­åº¦: {output_size}\")\n",
    "    \n",
    "    def forward(self, inputs, h_prev=None, C_prev=None):\n",
    "        \"\"\"\n",
    "        è™•ç†æ•´å€‹åºåˆ—\n",
    "        \"\"\"\n",
    "        # åˆå§‹åŒ–\n",
    "        if h_prev is None:\n",
    "            h = np.zeros((self.hidden_size, 1))\n",
    "            C = np.zeros((self.hidden_size, 1))\n",
    "        else:\n",
    "            h, C = h_prev, C_prev\n",
    "        \n",
    "        hidden_states = []\n",
    "        cell_states = []\n",
    "        outputs = []\n",
    "        caches = []\n",
    "        \n",
    "        # é€æ­¥è™•ç†\n",
    "        for x in inputs:\n",
    "            h, C, cache = self.cell.forward(x, h, C)\n",
    "            \n",
    "            # è¼¸å‡º\n",
    "            y = np.dot(self.Why, h) + self.by\n",
    "            \n",
    "            hidden_states.append(h)\n",
    "            cell_states.append(C)\n",
    "            outputs.append(y)\n",
    "            caches.append(cache)\n",
    "        \n",
    "        return outputs, hidden_states, cell_states, caches\n",
    "    \n",
    "    def backward(self, caches, targets, outputs):\n",
    "        \"\"\"\n",
    "        LSTM åå‘å‚³æ’­\n",
    "        \"\"\"\n",
    "        T = len(caches)\n",
    "        \n",
    "        # åˆå§‹åŒ–æ¢¯åº¦\n",
    "        dWhy = np.zeros_like(self.Why)\n",
    "        dby = np.zeros_like(self.by)\n",
    "        \n",
    "        dWf = np.zeros_like(self.cell.Wf)\n",
    "        dWi = np.zeros_like(self.cell.Wi)\n",
    "        dWc = np.zeros_like(self.cell.Wc)\n",
    "        dWo = np.zeros_like(self.cell.Wo)\n",
    "        \n",
    "        dbf = np.zeros_like(self.cell.bf)\n",
    "        dbi = np.zeros_like(self.cell.bi)\n",
    "        dbc = np.zeros_like(self.cell.bc)\n",
    "        dbo = np.zeros_like(self.cell.bo)\n",
    "        \n",
    "        dh_next = np.zeros((self.hidden_size, 1))\n",
    "        dC_next = np.zeros((self.hidden_size, 1))\n",
    "        \n",
    "        # å¾å¾Œå¾€å‰\n",
    "        for t in reversed(range(T)):\n",
    "            cache = caches[t]\n",
    "            \n",
    "            # è¼¸å‡ºå±¤æ¢¯åº¦\n",
    "            dy = outputs[t] - targets[t]\n",
    "            dWhy += np.dot(dy, cache['ht'].T)\n",
    "            dby += dy\n",
    "            \n",
    "            # éš±è—ç‹€æ…‹æ¢¯åº¦\n",
    "            dh = np.dot(self.Why.T, dy) + dh_next\n",
    "            \n",
    "            # LSTM Cell åå‘å‚³æ’­ (ç°¡åŒ–ç‰ˆ)\n",
    "            dot = dh * cache['ot'] * (1 - np.tanh(cache['Ct']) ** 2)\n",
    "            dC = dC_next + dot\n",
    "            \n",
    "            # é–€çš„æ¢¯åº¦ (çœç•¥è©³ç´°æ¨å°)\n",
    "            dft = dC * cache['C_prev'] * cache['ft'] * (1 - cache['ft'])\n",
    "            dit = dC * cache['Ct_tilde'] * cache['it'] * (1 - cache['it'])\n",
    "            dCt_tilde = dC * cache['it'] * (1 - cache['Ct_tilde'] ** 2)\n",
    "            dot = dh * np.tanh(cache['Ct']) * cache['ot'] * (1 - cache['ot'])\n",
    "            \n",
    "            # æ¬Šé‡æ¢¯åº¦ç´¯ç©\n",
    "            dWf += np.dot(dft, cache['concat'].T)\n",
    "            dWi += np.dot(dit, cache['concat'].T)\n",
    "            dWc += np.dot(dCt_tilde, cache['concat'].T)\n",
    "            dWo += np.dot(dot, cache['concat'].T)\n",
    "            \n",
    "            dbf += dft\n",
    "            dbi += dit\n",
    "            dbc += dCt_tilde\n",
    "            dbo += dot\n",
    "            \n",
    "            # å‚³éåˆ°å‰ä¸€æ™‚é–“æ­¥\n",
    "            dh_next = np.dot(self.cell.Wf[:, :self.hidden_size].T, dft)\n",
    "            dh_next += np.dot(self.cell.Wi[:, :self.hidden_size].T, dit)\n",
    "            dh_next += np.dot(self.cell.Wc[:, :self.hidden_size].T, dCt_tilde)\n",
    "            dh_next += np.dot(self.cell.Wo[:, :self.hidden_size].T, dot)\n",
    "            \n",
    "            dC_next = dC * cache['ft']\n",
    "        \n",
    "        # æ¢¯åº¦è£å‰ª\n",
    "        for grad in [dWf, dWi, dWc, dWo, dWhy, dbf, dbi, dbc, dbo, dby]:\n",
    "            np.clip(grad, -5, 5, out=grad)\n",
    "        \n",
    "        # æ›´æ–°æ¬Šé‡\n",
    "        self.cell.Wf -= self.lr * dWf\n",
    "        self.cell.Wi -= self.lr * dWi\n",
    "        self.cell.Wc -= self.lr * dWc\n",
    "        self.cell.Wo -= self.lr * dWo\n",
    "        \n",
    "        self.cell.bf -= self.lr * dbf\n",
    "        self.cell.bi -= self.lr * dbi\n",
    "        self.cell.bc -= self.lr * dbc\n",
    "        self.cell.bo -= self.lr * dbo\n",
    "        \n",
    "        self.Why -= self.lr * dWhy\n",
    "        self.by -= self.lr * dby\n",
    "    \n",
    "    def train_step(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        è¨“ç·´ä¸€æ­¥\n",
    "        \"\"\"\n",
    "        # å‰å‘å‚³æ’­\n",
    "        outputs, hidden_states, cell_states, caches = self.forward(inputs)\n",
    "        \n",
    "        # è¨ˆç®—æå¤±\n",
    "        loss = 0\n",
    "        for y, target in zip(outputs, targets):\n",
    "            loss += np.sum((y - target) ** 2)\n",
    "        loss /= len(targets)\n",
    "        \n",
    "        # åå‘å‚³æ’­\n",
    "        self.backward(caches, targets, outputs)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "print(\"âœ… LSTM é¡åˆ¥å®šç¾©å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lstm-train",
   "metadata": {},
   "source": [
    "### 5.3 è¨“ç·´ LSTM (ç´¯ç©å’Œä»»å‹™)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-lstm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‰µå»º LSTM\n",
    "lstm = LSTM(input_size=1, hidden_size=32, output_size=1, learning_rate=0.001)\n",
    "\n",
    "# ç”Ÿæˆæ•¸æ“š (æ›´é•·çš„åºåˆ—æ¸¬è©¦ LSTM çš„é•·æœŸè¨˜æ†¶)\n",
    "X_data_long, Y_data_long = generate_cumsum_data(n_samples=200, seq_length=20)\n",
    "\n",
    "# è¨“ç·´\n",
    "print(\"é–‹å§‹è¨“ç·´ LSTM...\")\n",
    "lstm_losses = []\n",
    "\n",
    "for epoch in range(50):\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i in range(len(X_data_long)):\n",
    "        inputs = [np.array([[x]]) for x in X_data_long[i]]\n",
    "        targets = [np.array([[y]]) for y in Y_data_long[i]]\n",
    "        \n",
    "        loss = lstm.train_step(inputs, targets)\n",
    "        epoch_loss += loss\n",
    "    \n",
    "    avg_loss = epoch_loss / len(X_data_long)\n",
    "    lstm_losses.append(avg_loss)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1:3d}/50 - Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# ç¹ªè£½å­¸ç¿’æ›²ç·š\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(lstm_losses, linewidth=2, color='green')\n",
    "plt.title('LSTM Training Loss (Longer Sequences)', fontsize=14)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… LSTM è¨“ç·´å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lstm-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¸¬è©¦ LSTM\n",
    "test_input_long = [1, 2, 3, 4, 5, 2, 1, 3, 2, 4]\n",
    "test_target_long = np.cumsum(test_input_long)\n",
    "\n",
    "inputs = [np.array([[x]]) for x in test_input_long]\n",
    "outputs, _, _, _ = lstm.forward(inputs)\n",
    "predictions = [y[0, 0] for y in outputs]\n",
    "\n",
    "print(\"\\nLSTM æ¸¬è©¦çµæœ:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"è¼¸å…¥åºåˆ—: {test_input_long}\")\n",
    "print(f\"çœŸå¯¦ç´¯ç©å’Œ: {test_target_long.tolist()}\")\n",
    "print(f\"é æ¸¬ç´¯ç©å’Œ: {[round(p, 1) for p in predictions]}\")\n",
    "print(f\"\\nå¹³å‡èª¤å·®: {np.mean(np.abs(predictions - test_target_long)):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. RNN vs LSTM æ€§èƒ½å°æ¯”\n",
    "\n",
    "### 6.1 æ¢¯åº¦æ¶ˆå¤±å¯¦é©—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gradient-vanishing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¦é©—: æ¯”è¼ƒ RNN å’Œ LSTM åœ¨ä¸åŒåºåˆ—é•·åº¦ä¸‹çš„æ€§èƒ½\n",
    "\n",
    "seq_lengths = [5, 10, 20, 30, 40, 50]\n",
    "rnn_errors = []\n",
    "lstm_errors = []\n",
    "\n",
    "for seq_len in seq_lengths:\n",
    "    print(f\"\\næ¸¬è©¦åºåˆ—é•·åº¦: {seq_len}\")\n",
    "    \n",
    "    # ç”Ÿæˆæ¸¬è©¦æ•¸æ“š\n",
    "    test_seq = np.random.randint(0, 5, size=seq_len)\n",
    "    test_target = np.cumsum(test_seq)\n",
    "    \n",
    "    # RNN é æ¸¬\n",
    "    rnn_temp = VanillaRNN(1, 20, 1, 0.001)\n",
    "    inputs = [np.array([[x]]) for x in test_seq]\n",
    "    targets = [np.array([[y]]) for y in test_target]\n",
    "    \n",
    "    # ç°¡å–®è¨“ç·´\n",
    "    for _ in range(20):\n",
    "        rnn_temp.train_step(inputs, targets)\n",
    "    \n",
    "    rnn_outputs, _, _ = rnn_temp.forward(inputs)\n",
    "    rnn_preds = [y[0, 0] for y in rnn_outputs]\n",
    "    rnn_error = np.mean(np.abs(rnn_preds - test_target))\n",
    "    rnn_errors.append(rnn_error)\n",
    "    \n",
    "    # LSTM é æ¸¬\n",
    "    lstm_temp = LSTM(1, 32, 1, 0.001)\n",
    "    \n",
    "    for _ in range(20):\n",
    "        lstm_temp.train_step(inputs, targets)\n",
    "    \n",
    "    lstm_outputs, _, _, _ = lstm_temp.forward(inputs)\n",
    "    lstm_preds = [y[0, 0] for y in lstm_outputs]\n",
    "    lstm_error = np.mean(np.abs(lstm_preds - test_target))\n",
    "    lstm_errors.append(lstm_error)\n",
    "    \n",
    "    print(f\"  RNN èª¤å·®: {rnn_error:.4f}\")\n",
    "    print(f\"  LSTM èª¤å·®: {lstm_error:.4f}\")\n",
    "\n",
    "# å¯è¦–åŒ–å°æ¯”\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(seq_lengths, rnn_errors, 'o-', linewidth=2, label='RNN', markersize=8)\n",
    "plt.plot(seq_lengths, lstm_errors, 's-', linewidth=2, label='LSTM', markersize=8)\n",
    "plt.xlabel('Sequence Length', fontsize=12)\n",
    "plt.ylabel('Average Error', fontsize=12)\n",
    "plt.title('RNN vs LSTM: Performance on Long Sequences', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ“Š è§€å¯Ÿ:\")\n",
    "print(\"- åºåˆ—è¶Šé•·, RNN èª¤å·®æ€¥åŠ‡ä¸Šå‡ (æ¢¯åº¦æ¶ˆå¤±)\")\n",
    "print(\"- LSTM èª¤å·®å¢é•·ç·©æ…¢ (é–€æ§æ©Ÿåˆ¶ç·©è§£æ¢¯åº¦æ¶ˆå¤±)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "keras-comparison",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. èˆ‡ Keras å°æ¯”é©—è­‰\n",
    "\n",
    "### 7.1 ä½¿ç”¨ Keras LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "keras-lstm",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# æº–å‚™ Keras æ ¼å¼çš„æ•¸æ“š\n",
    "X_keras = X_data_long[:, :, np.newaxis]  # (samples, timesteps, features)\n",
    "Y_keras = Y_data_long[:, :, np.newaxis]\n",
    "\n",
    "X_train_k, X_test_k, Y_train_k, Y_test_k = train_test_split(\n",
    "    X_keras, Y_keras, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# å»ºç«‹ Keras LSTM æ¨¡å‹\n",
    "keras_lstm = keras.Sequential([\n",
    "    layers.LSTM(32, return_sequences=True, input_shape=(None, 1)),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "keras_lstm.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# è¨“ç·´\n",
    "history = keras_lstm.fit(\n",
    "    X_train_k, Y_train_k,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test_k, Y_test_k),\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# è©•ä¼°\n",
    "test_loss, test_mae = keras_lstm.evaluate(X_test_k, Y_test_k, verbose=0)\n",
    "\n",
    "print(f\"Keras LSTM æ¸¬è©¦ MAE: {test_mae:.4f}\")\n",
    "\n",
    "# å¯è¦–åŒ–\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
    "plt.plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.title('Keras LSTM Training', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['mae'], label='Train MAE', linewidth=2)\n",
    "plt.plot(history.history['val_mae'], label='Val MAE', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.title('Keras LSTM Metrics', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "text-generation",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. å¯¦æˆ°æ‡‰ç”¨: å­—ç¬¦ç´šæ–‡æœ¬ç”Ÿæˆ\n",
    "\n",
    "### 8.1 æº–å‚™æ•¸æ“š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "text-gen-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç°¡å–®çš„æ–‡æœ¬èªæ–™\n",
    "text = \"\"\"\n",
    "hello world\n",
    "deep learning is fun\n",
    "natural language processing\n",
    "machine learning algorithms\n",
    "neural networks are powerful\n",
    "\"\"\".strip()\n",
    "\n",
    "# å»ºç«‹å­—ç¬¦é›†\n",
    "chars = sorted(set(text))\n",
    "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(f\"æ–‡æœ¬é•·åº¦: {len(text)}\")\n",
    "print(f\"è©å½™é‡: {vocab_size}\")\n",
    "print(f\"å­—ç¬¦é›†: {chars}\")\n",
    "\n",
    "# å‰µå»ºè¨“ç·´åºåˆ—\n",
    "seq_length = 10\n",
    "sequences = []\n",
    "next_chars = []\n",
    "\n",
    "for i in range(len(text) - seq_length):\n",
    "    sequences.append(text[i:i+seq_length])\n",
    "    next_chars.append(text[i+seq_length])\n",
    "\n",
    "print(f\"\\nè¨“ç·´åºåˆ—æ•¸: {len(sequences)}\")\n",
    "print(f\"ç¯„ä¾‹:\")\n",
    "print(f\"  è¼¸å…¥: '{sequences[0]}'\")\n",
    "print(f\"  ç›®æ¨™: '{next_chars[0]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "char-lstm",
   "metadata": {},
   "source": [
    "### 8.2 ä½¿ç”¨ Keras è¨“ç·´å­—ç¬¦ç´š LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "char-lstm-train",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‘é‡åŒ–\n",
    "X_char = np.zeros((len(sequences), seq_length, vocab_size), dtype=np.bool_)\n",
    "y_char = np.zeros((len(sequences), vocab_size), dtype=np.bool_)\n",
    "\n",
    "for i, seq in enumerate(sequences):\n",
    "    for t, char in enumerate(seq):\n",
    "        X_char[i, t, char_to_idx[char]] = 1\n",
    "    y_char[i, char_to_idx[next_chars[i]]] = 1\n",
    "\n",
    "print(f\"è¨“ç·´æ•¸æ“šå½¢ç‹€: {X_char.shape}\")  # (n_sequences, seq_length, vocab_size)\n",
    "print(f\"ç›®æ¨™æ•¸æ“šå½¢ç‹€: {y_char.shape}\")   # (n_sequences, vocab_size)\n",
    "\n",
    "# å»ºç«‹æ¨¡å‹\n",
    "char_model = keras.Sequential([\n",
    "    layers.LSTM(128, input_shape=(seq_length, vocab_size)),\n",
    "    layers.Dense(vocab_size, activation='softmax')\n",
    "])\n",
    "\n",
    "char_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "char_model.summary()\n",
    "\n",
    "# è¨“ç·´\n",
    "history = char_model.fit(\n",
    "    X_char, y_char,\n",
    "    batch_size=32,\n",
    "    epochs=100,\n",
    "    validation_split=0.2,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(f\"\\næœ€çµ‚è¨“ç·´æº–ç¢ºç‡: {history.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"æœ€çµ‚é©—è­‰æº–ç¢ºç‡: {history.history['val_accuracy'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "text-generation-func",
   "metadata": {},
   "source": [
    "### 8.3 æ–‡æœ¬ç”Ÿæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate-text",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, seed_text, length=50, temperature=1.0):\n",
    "    \"\"\"\n",
    "    ç”Ÿæˆæ–‡æœ¬\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : Keras model\n",
    "        è¨“ç·´å¥½çš„æ¨¡å‹\n",
    "    seed_text : str\n",
    "        ç¨®å­æ–‡æœ¬ (è‡³å°‘ seq_length å€‹å­—ç¬¦)\n",
    "    length : int\n",
    "        ç”Ÿæˆé•·åº¦\n",
    "    temperature : float\n",
    "        æº«åº¦åƒæ•¸ (æ§åˆ¶éš¨æ©Ÿæ€§)\n",
    "        - temperature < 1: æ›´ç¢ºå®šæ€§ (ä¿å®ˆ)\n",
    "        - temperature = 1: åŸå§‹æ©Ÿç‡\n",
    "        - temperature > 1: æ›´éš¨æ©Ÿ (å‰µé€ æ€§)\n",
    "    \"\"\"\n",
    "    generated = seed_text\n",
    "    \n",
    "    for _ in range(length):\n",
    "        # æº–å‚™è¼¸å…¥\n",
    "        x = np.zeros((1, seq_length, vocab_size))\n",
    "        for t, char in enumerate(seed_text[-seq_length:]):\n",
    "            if char in char_to_idx:\n",
    "                x[0, t, char_to_idx[char]] = 1\n",
    "        \n",
    "        # é æ¸¬\n",
    "        preds = model.predict(x, verbose=0)[0]\n",
    "        \n",
    "        # æº«åº¦æ¡æ¨£\n",
    "        preds = np.log(preds + 1e-8) / temperature\n",
    "        preds = np.exp(preds) / np.sum(np.exp(preds))\n",
    "        \n",
    "        # æ¡æ¨£ä¸‹ä¸€å€‹å­—ç¬¦\n",
    "        next_idx = np.random.choice(len(preds), p=preds)\n",
    "        next_char = idx_to_char[next_idx]\n",
    "        \n",
    "        # æ·»åŠ åˆ°ç”Ÿæˆæ–‡æœ¬\n",
    "        generated += next_char\n",
    "        seed_text += next_char\n",
    "    \n",
    "    return generated\n",
    "\n",
    "# æ¸¬è©¦æ–‡æœ¬ç”Ÿæˆ\n",
    "seed = \"deep learn\"\n",
    "\n",
    "print(\"\\næ–‡æœ¬ç”Ÿæˆæ¸¬è©¦:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for temp in [0.2, 0.5, 1.0, 1.2]:\n",
    "    generated = generate_text(char_model, seed, length=40, temperature=temp)\n",
    "    print(f\"\\nTemperature={temp}: \")\n",
    "    print(f\"  {generated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sentiment-analysis",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. å¯¦æˆ°æ‡‰ç”¨: æƒ…æ„Ÿåˆ†æ (ç°¡åŒ–ç‰ˆ)\n",
    "\n",
    "### 9.1 æº–å‚™æƒ…æ„Ÿåˆ†ææ•¸æ“š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sentiment-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç°¡å–®çš„æƒ…æ„Ÿåˆ†ææ•¸æ“šé›†\n",
    "sentiment_texts = [\n",
    "    \"i love this movie\",\n",
    "    \"this is great\",\n",
    "    \"wonderful experience\",\n",
    "    \"best film ever\",\n",
    "    \"amazing story\",\n",
    "    \"i hate this\",\n",
    "    \"terrible movie\",\n",
    "    \"waste of time\",\n",
    "    \"very bad\",\n",
    "    \"disappointing film\"\n",
    "] * 20  # è¤‡è£½å¢åŠ æ•¸æ“šé‡\n",
    "\n",
    "sentiment_labels = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0] * 20  # 1=æ­£é¢, 0=è² é¢\n",
    "\n",
    "# æ‰“äº‚æ•¸æ“š\n",
    "indices = np.random.permutation(len(sentiment_texts))\n",
    "sentiment_texts = [sentiment_texts[i] for i in indices]\n",
    "sentiment_labels = [sentiment_labels[i] for i in indices]\n",
    "\n",
    "print(f\"æƒ…æ„Ÿåˆ†ææ•¸æ“šé›†å¤§å°: {len(sentiment_texts)}\")\n",
    "print(f\"æ­£é¢æ¨£æœ¬: {sum(sentiment_labels)}\")\n",
    "print(f\"è² é¢æ¨£æœ¬: {len(sentiment_labels) - sum(sentiment_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sentiment-model",
   "metadata": {},
   "source": [
    "### 9.2 è¨“ç·´æƒ…æ„Ÿåˆ†ææ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-sentiment",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# åˆ†è©\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentiment_texts)\n",
    "vocab_size_sent = len(tokenizer.word_index) + 1\n",
    "\n",
    "# è½‰æ›ç‚ºåºåˆ—\n",
    "sequences = tokenizer.texts_to_sequences(sentiment_texts)\n",
    "X_sent = pad_sequences(sequences, maxlen=10, padding='post')\n",
    "y_sent = np.array(sentiment_labels)\n",
    "\n",
    "# åˆ‡åˆ†è¨“ç·´/æ¸¬è©¦é›†\n",
    "X_train_sent, X_test_sent, y_train_sent, y_test_sent = train_test_split(\n",
    "    X_sent, y_sent, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# å»ºç«‹æ¨¡å‹\n",
    "sentiment_model = keras.Sequential([\n",
    "    layers.Embedding(vocab_size_sent, 32, input_length=10),\n",
    "    layers.LSTM(16),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "sentiment_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"æƒ…æ„Ÿåˆ†ææ¨¡å‹:\")\n",
    "sentiment_model.summary()\n",
    "\n",
    "# è¨“ç·´\n",
    "history_sent = sentiment_model.fit(\n",
    "    X_train_sent, y_train_sent,\n",
    "    epochs=50,\n",
    "    batch_size=16,\n",
    "    validation_data=(X_test_sent, y_test_sent),\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# è©•ä¼°\n",
    "test_loss, test_acc = sentiment_model.evaluate(X_test_sent, y_test_sent, verbose=0)\n",
    "print(f\"\\næ¸¬è©¦æº–ç¢ºç‡: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sentiment-predict",
   "metadata": {},
   "outputs": [],
   "source": [
    "# é æ¸¬æ–°è©•è«–\n",
    "def predict_sentiment(text):\n",
    "    seq = tokenizer.texts_to_sequences([text])\n",
    "    padded = pad_sequences(seq, maxlen=10, padding='post')\n",
    "    pred = sentiment_model.predict(padded, verbose=0)[0, 0]\n",
    "    return pred\n",
    "\n",
    "# æ¸¬è©¦\n",
    "test_reviews = [\n",
    "    \"this is amazing\",\n",
    "    \"i hate this movie\",\n",
    "    \"wonderful story\",\n",
    "    \"terrible experience\",\n",
    "    \"best movie ever\"\n",
    "]\n",
    "\n",
    "print(\"\\næƒ…æ„Ÿé æ¸¬çµæœ:\")\n",
    "print(\"=\"*60)\n",
    "for review in test_reviews:\n",
    "    sentiment_score = predict_sentiment(review)\n",
    "    sentiment = \"æ­£é¢ ğŸ˜Š\" if sentiment_score > 0.5 else \"è² é¢ ğŸ˜\"\n",
    "    print(f\"'{review:25s}' â†’ {sentiment} (ä¿¡å¿ƒåº¦: {sentiment_score:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualization",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. æ¢¯åº¦æµå‹•å¯è¦–åŒ–\n",
    "\n",
    "### 10.1 å±•ç¤ºæ¢¯åº¦æ¶ˆå¤±ç¾è±¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gradient-flow-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¨¡æ“¬æ¢¯åº¦å‚³æ’­\n",
    "def simulate_gradient_flow(activation, num_layers=20):\n",
    "    \"\"\"\n",
    "    æ¨¡æ“¬æ¢¯åº¦åœ¨å¤šå±¤ç¶²è·¯ä¸­çš„å‚³æ’­\n",
    "    \"\"\"\n",
    "    gradient = 1.0\n",
    "    gradients = [gradient]\n",
    "    \n",
    "    # å‡è¨­æ¬Šé‡çŸ©é™£çš„æœ€å¤§ç‰¹å¾µå€¼\n",
    "    if activation == 'tanh':\n",
    "        weight_eigenvalue = 0.9\n",
    "        activation_derivative = 0.25  # tanh' çš„å¹³å‡å€¼\n",
    "    elif activation == 'sigmoid':\n",
    "        weight_eigenvalue = 0.9\n",
    "        activation_derivative = 0.25  # sigmoid' çš„å¹³å‡å€¼\n",
    "    elif activation == 'relu':\n",
    "        weight_eigenvalue = 1.0\n",
    "        activation_derivative = 0.5  # ReLU' = 1 (50% æ©Ÿç‡)\n",
    "    \n",
    "    for layer in range(num_layers):\n",
    "        gradient *= weight_eigenvalue * activation_derivative\n",
    "        gradients.append(gradient)\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "# æ¯”è¼ƒä¸åŒæ¿€æ´»å‡½æ•¸\n",
    "gradients_tanh = simulate_gradient_flow('tanh', 50)\n",
    "gradients_sigmoid = simulate_gradient_flow('sigmoid', 50)\n",
    "gradients_relu = simulate_gradient_flow('relu', 50)\n",
    "\n",
    "# å¯è¦–åŒ–\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(gradients_tanh, label='Tanh', linewidth=2)\n",
    "plt.plot(gradients_sigmoid, label='Sigmoid', linewidth=2)\n",
    "plt.plot(gradients_relu, label='ReLU', linewidth=2)\n",
    "plt.xlabel('Layer', fontsize=12)\n",
    "plt.ylabel('Gradient Magnitude', fontsize=12)\n",
    "plt.title('Gradient Flow (Linear Scale)', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.semilogy(gradients_tanh, label='Tanh', linewidth=2)\n",
    "plt.semilogy(gradients_sigmoid, label='Sigmoid', linewidth=2)\n",
    "plt.semilogy(gradients_relu, label='ReLU', linewidth=2)\n",
    "plt.xlabel('Layer', fontsize=12)\n",
    "plt.ylabel('Gradient Magnitude (log scale)', fontsize=12)\n",
    "plt.title('Gradient Flow (Log Scale)', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ“Š è§€å¯Ÿ:\")\n",
    "print(f\"  Tanh/Sigmoid åœ¨ 50 å±¤å¾Œæ¢¯åº¦: {gradients_tanh[-1]:.2e} (å¹¾ä¹ç‚º 0!)\")\n",
    "print(f\"  ReLU åœ¨ 50 å±¤å¾Œæ¢¯åº¦: {gradients_relu[-1]:.4f} (ç›¸å°ç©©å®š)\")\n",
    "print(\"\\nçµè«–: Tanh/Sigmoid åœ¨æ·±å±¤ç¶²è·¯æˆ–é•·åºåˆ—ä¸­æœƒé­é‡åš´é‡çš„æ¢¯åº¦æ¶ˆå¤±å•é¡Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. æœ¬ç¯€ç¸½çµ\n",
    "\n",
    "### âœ… é—œéµè¦é»\n",
    "\n",
    "**RNN**:\n",
    "1. **å¾ªç’°çµæ§‹**: ht = tanh(WhÂ·ht-1 + WxÂ·xt + b)\n",
    "2. **BPTT**: æ™‚é–“åå‘å‚³æ’­ç®—æ³•\n",
    "3. **æ¢¯åº¦æ¶ˆå¤±**: tanh å°æ•¸é€£ä¹˜ â†’ æŒ‡æ•¸è¡°æ¸›\n",
    "4. **é©ç”¨å ´æ™¯**: çŸ­åºåˆ— (< 20 æ­¥)\n",
    "\n",
    "**LSTM**:\n",
    "1. **ä¸‰å€‹é–€**: éºå¿˜é–€ (ft), è¼¸å…¥é–€ (it), è¼¸å‡ºé–€ (ot)\n",
    "2. **è¨˜æ†¶å–®å…ƒ**: Ct = ftâŠ™Ct-1 + itâŠ™CÌƒt (åŠ æ³•æ›´æ–°!)\n",
    "3. **æ¢¯åº¦æµå‹•**: åŠ æ³•å…è¨±æ¢¯åº¦ç›´é€š,ç·©è§£æ¢¯åº¦æ¶ˆå¤±\n",
    "4. **é©ç”¨å ´æ™¯**: é•·åºåˆ— (< 100 æ­¥)\n",
    "\n",
    "**GRU**:\n",
    "1. **ç°¡åŒ– LSTM**: 2 å€‹é–€ (é‡ç½®é–€, æ›´æ–°é–€)\n",
    "2. **åƒæ•¸å°‘ 25%**: è¨“ç·´æ›´å¿«\n",
    "3. **æ€§èƒ½æ¥è¿‘**: åœ¨å¤šæ•¸ä»»å‹™ä¸Šèˆ‡ LSTM ç›¸ç•¶\n",
    "\n",
    "### ğŸ“Š æ€§èƒ½å°æ¯”\n",
    "\n",
    "| æ¨¡å‹ | åƒæ•¸é‡ | è¨“ç·´é€Ÿåº¦ | é•·åºåˆ—æ€§èƒ½ | é©ç”¨å ´æ™¯ |\n",
    "|------|--------|---------|-----------|----------|\n",
    "| **RNN** | å° | å¿« | å·® (æ¢¯åº¦æ¶ˆå¤±) | çŸ­åºåˆ— |\n",
    "| **LSTM** | å¤§ (4x RNN) | æ…¢ | å¥½ | é•·åºåˆ—, è¤‡é›œä»»å‹™ |\n",
    "| **GRU** | ä¸­ (3x RNN) | ä¸­ | å¥½ | å¹³è¡¡é¸æ“‡ |\n",
    "\n",
    "### ğŸ’¡ å¯¦å‹™å»ºè­°\n",
    "\n",
    "1. **é¦–é¸ LSTM/GRU**: é™¤éæœ‰ç‰¹æ®ŠåŸå› ,ä¸è¦ä½¿ç”¨ Vanilla RNN\n",
    "2. **åºåˆ—é•·åº¦ < 50**: GRU å’Œ LSTM æ€§èƒ½ç›¸ç•¶,é¸ GRU (æ›´å¿«)\n",
    "3. **åºåˆ—é•·åº¦ > 50**: LSTM ç•¥å„ª\n",
    "4. **åºåˆ—é•·åº¦ > 200**: è€ƒæ…® Transformer (æ›´å¥½çš„é•·è·é›¢ä¾è³´)\n",
    "\n",
    "### ğŸš€ å¾ LSTM åˆ° Transformer\n",
    "\n",
    "**LSTM çš„å±€é™**:\n",
    "- ä»ç„¶æ˜¯åºåˆ—è™•ç† (ç„¡æ³•å¹³è¡ŒåŒ–)\n",
    "- é•·åºåˆ— (> 1000) ä»æœ‰æ¢¯åº¦å•é¡Œ\n",
    "- è¨“ç·´æ™‚é–“é•·\n",
    "\n",
    "**Transformer çš„çªç ´**:\n",
    "- å®Œå…¨åŸºæ–¼ Attention (æ‹‹æ£„å¾ªç’°)\n",
    "- å®Œå…¨å¹³è¡ŒåŒ– (è¨“ç·´å¿« 10-100x)\n",
    "- é•·è·é›¢ä¾è³´: ç›´æ¥é€£æ¥,ç„¡æ¢¯åº¦å•é¡Œ\n",
    "\n",
    "**çµè«–**: LSTM æ˜¯åºåˆ—å»ºæ¨¡çš„ç¶“å…¸æ–¹æ³•,ç†è§£ LSTM æœ‰åŠ©æ–¼ç†è§£ Transformer çš„è¨­è¨ˆå‹•æ©Ÿã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## 12. èª²å¾Œç·´ç¿’\n",
    "\n",
    "### ç·´ç¿’ 1: å¯¦ä½œé›™å‘ RNN\n",
    "\n",
    "æç¤º:\n",
    "- åŒæ™‚è¨“ç·´å‰å‘å’Œå¾Œå‘ RNN\n",
    "- åˆä½µå…©å€‹æ–¹å‘çš„éš±è—ç‹€æ…‹\n",
    "- æ‡‰ç”¨æ–¼è©æ€§æ¨™è¨»ä»»å‹™\n",
    "\n",
    "### ç·´ç¿’ 2: å¯¦ä½œ GRU\n",
    "\n",
    "æç¤º:\n",
    "- åªéœ€è¦ 2 å€‹é–€ (é‡ç½®é–€, æ›´æ–°é–€)\n",
    "- æ²’æœ‰å–®ç¨çš„ Cell State\n",
    "- å°æ¯” GRU èˆ‡ LSTM çš„è¨“ç·´é€Ÿåº¦\n",
    "\n",
    "### ç·´ç¿’ 3: åºåˆ—ç”Ÿæˆé€²éš\n",
    "\n",
    "ä»»å‹™:\n",
    "- ä½¿ç”¨æ›´å¤§çš„æ–‡æœ¬èªæ–™ (å¦‚èå£«æ¯”äºä½œå“)\n",
    "- è¨“ç·´å­—ç¬¦ç´š LSTM\n",
    "- å¯¦é©—ä¸åŒçš„æº«åº¦åƒæ•¸\n",
    "- åˆ†æç”Ÿæˆæ–‡æœ¬çš„è³ªé‡\n",
    "\n",
    "### ç·´ç¿’ 4: IMDB æƒ…æ„Ÿåˆ†æå®Œæ•´å¯¦æˆ°\n",
    "\n",
    "ä»»å‹™:\n",
    "- è¼‰å…¥ IMDB æ•¸æ“šé›†\n",
    "- ä½¿ç”¨è‡ªè£½ LSTM è¨“ç·´åˆ†é¡å™¨\n",
    "- èˆ‡ Keras LSTM å°æ¯”æ€§èƒ½\n",
    "- åˆ†æéŒ¯èª¤æ¡ˆä¾‹\n",
    "\n",
    "---\n",
    "\n",
    "## 13. å»¶ä¼¸é–±è®€\n",
    "\n",
    "### é—œéµè«–æ–‡\n",
    "1. **LSTM åŸå§‹è«–æ–‡**: Hochreiter & Schmidhuber (1997). *Long Short-Term Memory*.\n",
    "2. **GRU**: Cho et al. (2014). *Learning Phrase Representations using RNN Encoder-Decoder*.\n",
    "3. **BPTT**: Werbos (1990). *Backpropagation Through Time: What It Does and How to Do It*.\n",
    "\n",
    "### è¦–è¦ºåŒ–è³‡æº\n",
    "- **Understanding LSTM**: http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "- **The Unreasonable Effectiveness of RNN**: http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\n",
    "### å¯¦ä½œæ•™ç¨‹\n",
    "- **CS231n RNN Tutorial**: http://cs231n.stanford.edu/\n",
    "- **TensorFlow RNN Guide**: https://www.tensorflow.org/guide/keras/rnn\n",
    "\n",
    "---\n",
    "\n",
    "**èª²ç¨‹**: iSpan Python NLP Cookbooks v2\n",
    "**è¬›å¸«**: Claude AI\n",
    "**æœ€å¾Œæ›´æ–°**: 2025-10-17\n",
    "\n",
    "**ğŸ‰ æ­å–œå®Œæˆåº•å±¤å¯¦ä½œç³»åˆ—ï¼æ‚¨å·²ç¶“å¾é›¶å¯¦ç¾äº† NaiveBayesã€MLPã€RNN å’Œ LSTMï¼**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
