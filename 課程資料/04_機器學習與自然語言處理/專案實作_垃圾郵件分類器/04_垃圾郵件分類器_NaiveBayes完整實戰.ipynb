{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# 專案實作: 垃圾郵件分類器 (SMS Spam Classification)\n",
    "\n",
    "**課程**: iSpan Python NLP Cookbooks v2\n",
    "**章節**: CH04 機器學習與自然語言處理\n",
    "**專案類型**: 完整實戰專案\n",
    "**版本**: v1.0\n",
    "**更新日期**: 2025-10-17\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 專案目標\n",
    "\n",
    "建立一個實用的垃圾簡訊分類系統，能夠：\n",
    "1. 自動識別垃圾簡訊 (Spam) 和正常簡訊 (Ham)\n",
    "2. 達到 95% 以上的準確率\n",
    "3. 提供可解釋的分類依據\n",
    "4. 可部署到實際應用場景\n",
    "\n",
    "## 🎯 學習重點\n",
    "\n",
    "- 完整的機器學習項目流程\n",
    "- 文本預處理與特徵工程\n",
    "- Naive Bayes 分類器應用\n",
    "- 模型評估與優化\n",
    "- 錯誤分析與改進\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports",
   "metadata": {},
   "source": [
    "## 1. 環境準備與數據載入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 導入必要套件\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_curve, auc\n",
    ")\n",
    "\n",
    "# 設定顯示選項\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# 設定隨機種子\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"✅ 環境準備完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-loading-header",
   "metadata": {},
   "source": [
    "### 1.1 載入 SMS Spam Collection 數據集\n",
    "\n",
    "**數據來源**: [UCI ML Repository - SMS Spam Collection](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection)\n",
    "\n",
    "**數據集說明**:\n",
    "- 5,574 條英文簡訊\n",
    "- 二元分類: spam (垃圾簡訊) vs ham (正常簡訊)\n",
    "- 不平衡數據集 (spam 約佔 13%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方法 1: 從 sklearn 內建數據載入 (如果可用)\n",
    "try:\n",
    "    from sklearn.datasets import fetch_20newsgroups\n",
    "    # 注意: sklearn 沒有內建 SMS Spam,這裡使用備用方案\n",
    "    raise ImportError(\"使用本地數據\")\n",
    "except:\n",
    "    # 方法 2: 從本地或線上載入\n",
    "    import os\n",
    "    \n",
    "    # 數據路徑選項\n",
    "    data_paths = [\n",
    "        '../../../datasets/sms_spam/SMSSpamCollection.csv',\n",
    "        '../../../datasets/sms_spam/SMSSpamCollection.txt',\n",
    "    ]\n",
    "    \n",
    "    df = None\n",
    "    for path in data_paths:\n",
    "        if os.path.exists(path):\n",
    "            if path.endswith('.csv'):\n",
    "                df = pd.read_csv(path, encoding='latin-1')\n",
    "            else:\n",
    "                df = pd.read_csv(path, sep='\\t', names=['label', 'message'], encoding='latin-1')\n",
    "            print(f\"✅ 從 {path} 載入數據\")\n",
    "            break\n",
    "    \n",
    "    # 如果本地沒有數據,使用模擬數據 (實際專案中應下載真實數據)\n",
    "    if df is None:\n",
    "        print(\"⚠️  未找到真實數據集,使用模擬數據進行演示\")\n",
    "        print(\"   建議下載: https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection\")\n",
    "        \n",
    "        # 模擬數據 (擴充版)\n",
    "        spam_messages = [\n",
    "            \"FREE for 1st week! No1 Nokia tone 4 ur mobile every week just txt NOKIA to 8007 Get txting and tell ur mates. zed POBox 36504 W45WQ norm150p/tone 16+\",\n",
    "            \"WINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.\",\n",
    "            \"Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030\",\n",
    "            \"I'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried enough today.\",\n",
    "            \"SIX chances to win CASH! From 100 to 20,000 pounds txt> CSH11 and send to 87575. Cost 150p/day, 6days, 16+ TsandCs apply Reply HL 4 info\",\n",
    "            \"URGENT! You have won a 1 week FREE membership in our £100,000 Prize Jackpot! Txt the word: CLAIM to No: 81010 T&C www.dbuk.net LCCLTD POBOX 4403LDNW1A7RW18\",\n",
    "            \"XXXMobileMovieClub: To use your credit, click the WAP link in the next txt message or click here>> http://wap. xxxmobilemovieclub.com?n=QJKGIGHJJGCBL\",\n",
    "            \"England v Macedonia - dont miss the goals/team news. Txt ur national team to 87077 eg ENGLAND to 87077 Try:WALES, SCOTLAND 4txt/ú1.20 POBOx 84NW10 6ØZ QUIT?txtStop\",\n",
    "            \"Thanks for your subscription to Ringtone UK your mobile will be charged £5/month Please confirm by replying YES or NO. If you reply NO you will not be charged\",\n",
    "            \"Congratulations ur awarded 500 of CD vouchers or 125gift guaranteed & Free entry 2 100 wkly draw txt MUSIC to 87066 TnCs www.Ldew.com1win150ppmx3age16\"\n",
    "        ] * 70  # 700 spam messages\n",
    "        \n",
    "        ham_messages = [\n",
    "            \"Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\",\n",
    "            \"Ok lar... Joking wif u oni...\",\n",
    "            \"U dun say so early hor... U c already then say...\",\n",
    "            \"Nah I don't think he goes to usf, he lives around here though\",\n",
    "            \"Even my brother is not like to speak with me. They treat me like aids patent.\",\n",
    "            \"I HAVE A DATE ON SUNDAY WITH WILL!!\",\n",
    "            \"As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune\",\n",
    "            \"Oh k...i'm watching here:)\",\n",
    "            \"Eh u remember how 2 spell his name... Yes i did. He v naughty make until i v wet.\",\n",
    "            \"I'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried enough today.\"\n",
    "        ] * 460  # 4600 ham messages\n",
    "        \n",
    "        df = pd.DataFrame({\n",
    "            'label': ['spam'] * len(spam_messages) + ['ham'] * len(ham_messages),\n",
    "            'message': spam_messages + ham_messages\n",
    "        })\n",
    "        \n",
    "        # 打亂數據\n",
    "        df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# 顯示基本信息\n",
    "print(f\"\\n數據集大小: {len(df)} 條簡訊\")\n",
    "print(f\"欄位: {df.columns.tolist()}\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda-header",
   "metadata": {},
   "source": [
    "## 2. 探索性數據分析 (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda-basic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基本統計信息\n",
    "print(\"數據集基本信息:\")\n",
    "print(\"=\"*60)\n",
    "print(df.info())\n",
    "print(\"\\n缺失值統計:\")\n",
    "print(df.isnull().sum())\n",
    "print(\"\\n類別分佈:\")\n",
    "print(df['label'].value_counts())\n",
    "print(f\"\\nSpam 比例: {(df['label'] == 'spam').mean():.2%}\")\n",
    "print(f\"Ham 比例: {(df['label'] == 'ham').mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda-distribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可視化類別分佈\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 類別計數\n",
    "label_counts = df['label'].value_counts()\n",
    "axes[0].bar(label_counts.index, label_counts.values, color=['#4ecdc4', '#ff6b6b'])\n",
    "axes[0].set_title('Message Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_xlabel('Label')\n",
    "\n",
    "# 添加數值標籤\n",
    "for i, v in enumerate(label_counts.values):\n",
    "    axes[0].text(i, v + 50, str(v), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 類別比例 (餅圖)\n",
    "colors = ['#4ecdc4', '#ff6b6b']\n",
    "axes[1].pie(label_counts.values, labels=label_counts.index, autopct='%1.1f%%',\n",
    "            colors=colors, startangle=90, textprops={'fontsize': 12, 'fontweight': 'bold'})\n",
    "axes[1].set_title('Message Proportion', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"⚠️  注意: 這是一個不平衡數據集 (imbalanced dataset)\")\n",
    "print(\"   Spam 約佔 13%,需要注意評估指標的選擇\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "text-analysis-header",
   "metadata": {},
   "source": [
    "### 2.1 文本長度分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "text-length",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算文本長度\n",
    "df['length'] = df['message'].apply(len)\n",
    "df['word_count'] = df['message'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# 統計摘要\n",
    "print(\"文本長度統計 (按類別):\")\n",
    "print(\"=\"*60)\n",
    "print(df.groupby('label')[['length', 'word_count']].describe().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "text-length-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可視化文本長度分佈\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 字符長度分佈 - Histogram\n",
    "df[df['label'] == 'ham']['length'].hist(bins=50, alpha=0.6, label='Ham', \n",
    "                                          color='#4ecdc4', ax=axes[0, 0])\n",
    "df[df['label'] == 'spam']['length'].hist(bins=50, alpha=0.6, label='Spam', \n",
    "                                           color='#ff6b6b', ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Character Length Distribution', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Length')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# 詞數分佈 - Histogram\n",
    "df[df['label'] == 'ham']['word_count'].hist(bins=50, alpha=0.6, label='Ham', \n",
    "                                              color='#4ecdc4', ax=axes[0, 1])\n",
    "df[df['label'] == 'spam']['word_count'].hist(bins=50, alpha=0.6, label='Spam', \n",
    "                                               color='#ff6b6b', ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Word Count Distribution', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Word Count')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Box plot - 字符長度\n",
    "df.boxplot(column='length', by='label', ax=axes[1, 0], patch_artist=True,\n",
    "           boxprops=dict(facecolor='#4ecdc4', alpha=0.6))\n",
    "axes[1, 0].set_title('Character Length by Label', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Label')\n",
    "axes[1, 0].set_ylabel('Length')\n",
    "plt.sca(axes[1, 0])\n",
    "plt.xticks([1, 2], ['ham', 'spam'])\n",
    "\n",
    "# Box plot - 詞數\n",
    "df.boxplot(column='word_count', by='label', ax=axes[1, 1], patch_artist=True,\n",
    "           boxprops=dict(facecolor='#ff6b6b', alpha=0.6))\n",
    "axes[1, 1].set_title('Word Count by Label', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Label')\n",
    "axes[1, 1].set_ylabel('Word Count')\n",
    "plt.sca(axes[1, 1])\n",
    "plt.xticks([1, 2], ['ham', 'spam'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n觀察:\")\n",
    "print(\"- Spam 簡訊通常更長 (更多促銷文字)\")\n",
    "print(\"- Ham 簡訊較短 (日常對話)\")\n",
    "print(\"- 文本長度可以作為一個有用的特徵\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sample-messages",
   "metadata": {},
   "source": [
    "### 2.2 查看樣本簡訊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "view-samples",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 隨機查看 Spam 樣本\n",
    "print(\"🔴 Spam 簡訊範例:\")\n",
    "print(\"=\"*80)\n",
    "spam_samples = df[df['label'] == 'spam'].sample(5, random_state=42)\n",
    "for idx, (_, row) in enumerate(spam_samples.iterrows(), 1):\n",
    "    print(f\"{idx}. {row['message'][:150]}...\")\n",
    "    print()\n",
    "\n",
    "# 隨機查看 Ham 樣本\n",
    "print(\"\\n🟢 Ham 簡訊範例:\")\n",
    "print(\"=\"*80)\n",
    "ham_samples = df[df['label'] == 'ham'].sample(5, random_state=42)\n",
    "for idx, (_, row) in enumerate(ham_samples.iterrows(), 1):\n",
    "    print(f\"{idx}. {row['message'][:150]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preprocessing-header",
   "metadata": {},
   "source": [
    "## 3. 文本預處理\n",
    "\n",
    "**預處理步驟**:\n",
    "1. 小寫轉換\n",
    "2. 移除標點符號和特殊字符\n",
    "3. 移除停用詞 (可選)\n",
    "4. 詞幹提取/詞形還原 (可選)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preprocessing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def preprocess_text(text, remove_stopwords=False):\n",
    "    \"\"\"\n",
    "    文本預處理函數\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        原始文本\n",
    "    remove_stopwords : bool\n",
    "        是否移除停用詞\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    cleaned_text : str\n",
    "        預處理後的文本\n",
    "    \"\"\"\n",
    "    # 1. 小寫轉換\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2. 移除 URL\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    \n",
    "    # 3. 移除 HTML 標籤\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # 4. 移除標點符號和數字 (保留空格)\n",
    "    text = re.sub(f'[{re.escape(string.punctuation)}]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # 5. 移除多餘空格\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    # 6. 移除停用詞 (可選)\n",
    "    if remove_stopwords:\n",
    "        try:\n",
    "            from nltk.corpus import stopwords\n",
    "            import nltk\n",
    "            nltk.download('stopwords', quiet=True)\n",
    "            stop_words = set(stopwords.words('english'))\n",
    "            text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "        except:\n",
    "            pass  # 如果沒有 nltk,跳過這一步\n",
    "    \n",
    "    return text\n",
    "\n",
    "# 測試預處理函數\n",
    "test_text = \"FREE! Win £1000 cash!! Call NOW on 0800-123-4567 or visit www.spam.com\"\n",
    "print(\"原始文本:\")\n",
    "print(test_text)\n",
    "print(\"\\n預處理後:\")\n",
    "print(preprocess_text(test_text))\n",
    "print(\"\\n預處理後 (移除停用詞):\")\n",
    "print(preprocess_text(test_text, remove_stopwords=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apply-preprocessing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 應用預處理到整個數據集\n",
    "print(\"正在預處理數據集...\")\n",
    "df['cleaned_message'] = df['message'].apply(lambda x: preprocess_text(x, remove_stopwords=False))\n",
    "print(\"✅ 預處理完成\")\n",
    "\n",
    "# 比較前後\n",
    "print(\"\\n預處理效果對比:\")\n",
    "print(\"=\"*80)\n",
    "for i in range(3):\n",
    "    sample = df.sample(1, random_state=i).iloc[0]\n",
    "    print(f\"\\n範例 {i+1} ({sample['label'].upper()}):\")\n",
    "    print(f\"原始: {sample['message'][:100]}\")\n",
    "    print(f\"處理: {sample['cleaned_message'][:100]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split-data-header",
   "metadata": {},
   "source": [
    "## 4. 數據切分\n",
    "\n",
    "將數據切分為訓練集 (80%) 和測試集 (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 準備特徵和標籤\n",
    "X = df['cleaned_message']\n",
    "y = df['label']\n",
    "\n",
    "# 切分數據 (stratify 確保類別比例一致)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"數據切分結果:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"訓練集大小: {len(X_train)} ({len(X_train)/len(df):.0%})\")\n",
    "print(f\"測試集大小: {len(X_test)} ({len(X_test)/len(df):.0%})\")\n",
    "print(f\"\\n訓練集類別分佈:\")\n",
    "print(y_train.value_counts())\n",
    "print(f\"\\n測試集類別分佈:\")\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature-engineering-header",
   "metadata": {},
   "source": [
    "## 5. 特徵工程\n",
    "\n",
    "將文本轉換為數值特徵,比較兩種方法:\n",
    "1. **Bag of Words (BoW)**: 詞頻統計\n",
    "2. **TF-IDF**: 考慮詞的重要性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bow-vectorizer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方法 1: Bag of Words\n",
    "print(\"建立 Bag of Words 特徵...\")\n",
    "bow_vectorizer = CountVectorizer(max_features=3000, ngram_range=(1, 2))\n",
    "X_train_bow = bow_vectorizer.fit_transform(X_train)\n",
    "X_test_bow = bow_vectorizer.transform(X_test)\n",
    "\n",
    "print(f\"✅ BoW 特徵矩陣大小: {X_train_bow.shape}\")\n",
    "print(f\"   詞彙表大小: {len(bow_vectorizer.vocabulary_)}\")\n",
    "print(f\"   稀疏度: {(1 - X_train_bow.nnz / (X_train_bow.shape[0] * X_train_bow.shape[1])):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tfidf-vectorizer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方法 2: TF-IDF\n",
    "print(\"建立 TF-IDF 特徵...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=3000, ngram_range=(1, 2))\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "print(f\"✅ TF-IDF 特徵矩陣大小: {X_train_tfidf.shape}\")\n",
    "print(f\"   詞彙表大小: {len(tfidf_vectorizer.vocabulary_)}\")\n",
    "print(f\"   稀疏度: {(1 - X_train_tfidf.nnz / (X_train_tfidf.shape[0] * X_train_tfidf.shape[1])):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "top-features-header",
   "metadata": {},
   "source": [
    "### 5.1 查看最重要的特徵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "top-words",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算每個詞在 spam 和 ham 中的平均 TF-IDF 分數\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# 分別計算 spam 和 ham 的平均 TF-IDF\n",
    "spam_indices = y_train == 'spam'\n",
    "ham_indices = y_train == 'ham'\n",
    "\n",
    "spam_tfidf_mean = np.array(X_train_tfidf[spam_indices].mean(axis=0)).flatten()\n",
    "ham_tfidf_mean = np.array(X_train_tfidf[ham_indices].mean(axis=0)).flatten()\n",
    "\n",
    "# 找出最重要的詞\n",
    "top_n = 15\n",
    "\n",
    "spam_top_indices = spam_tfidf_mean.argsort()[-top_n:][::-1]\n",
    "ham_top_indices = ham_tfidf_mean.argsort()[-top_n:][::-1]\n",
    "\n",
    "print(\"Top 15 SPAM 關鍵詞 (按平均 TF-IDF 分數):\")\n",
    "print(\"=\"*60)\n",
    "for idx in spam_top_indices:\n",
    "    print(f\"{feature_names[idx]:20s} → {spam_tfidf_mean[idx]:.4f}\")\n",
    "\n",
    "print(\"\\nTop 15 HAM 關鍵詞 (按平均 TF-IDF 分數):\")\n",
    "print(\"=\"*60)\n",
    "for idx in ham_top_indices:\n",
    "    print(f\"{feature_names[idx]:20s} → {ham_tfidf_mean[idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-training-header",
   "metadata": {},
   "source": [
    "## 6. 模型訓練\n",
    "\n",
    "使用 Multinomial Naive Bayes 訓練兩個模型:\n",
    "1. 基於 Bag of Words\n",
    "2. 基於 TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-bow-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型 1: Bag of Words + Naive Bayes\n",
    "print(\"訓練模型 1: BoW + Naive Bayes\")\n",
    "nb_bow = MultinomialNB(alpha=1.0)\n",
    "nb_bow.fit(X_train_bow, y_train)\n",
    "print(\"✅ 模型 1 訓練完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-tfidf-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型 2: TF-IDF + Naive Bayes\n",
    "print(\"訓練模型 2: TF-IDF + Naive Bayes\")\n",
    "nb_tfidf = MultinomialNB(alpha=1.0)\n",
    "nb_tfidf.fit(X_train_tfidf, y_train)\n",
    "print(\"✅ 模型 2 訓練完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluation-header",
   "metadata": {},
   "source": [
    "## 7. 模型評估\n",
    "\n",
    "使用多個指標評估模型性能:\n",
    "- **Accuracy**: 整體準確率\n",
    "- **Precision**: 精確率 (預測為 spam 的正確率)\n",
    "- **Recall**: 召回率 (實際 spam 被找出的比例)\n",
    "- **F1 Score**: Precision 和 Recall 的調和平均"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate-models",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 預測\n",
    "y_pred_bow = nb_bow.predict(X_test_bow)\n",
    "y_pred_tfidf = nb_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "# 計算指標\n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, pos_label='spam')\n",
    "    recall = recall_score(y_true, y_pred, pos_label='spam')\n",
    "    f1 = f1_score(y_true, y_pred, pos_label='spam')\n",
    "    \n",
    "    print(f\"{model_name} 評估結果:\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Accuracy:  {accuracy:.4f} ({accuracy:.2%})\")\n",
    "    print(f\"Precision: {precision:.4f} ({precision:.2%})\")\n",
    "    print(f\"Recall:    {recall:.4f} ({recall:.2%})\")\n",
    "    print(f\"F1 Score:  {f1:.4f} ({f1:.2%})\")\n",
    "    print()\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "# 評估兩個模型\n",
    "metrics_bow = evaluate_model(y_test, y_pred_bow, \"模型 1 (BoW + NB)\")\n",
    "metrics_tfidf = evaluate_model(y_test, y_pred_tfidf, \"模型 2 (TF-IDF + NB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare-models",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可視化對比\n",
    "metrics_df = pd.DataFrame({\n",
    "    'BoW + NB': [metrics_bow['accuracy'], metrics_bow['precision'], \n",
    "                 metrics_bow['recall'], metrics_bow['f1']],\n",
    "    'TF-IDF + NB': [metrics_tfidf['accuracy'], metrics_tfidf['precision'], \n",
    "                    metrics_tfidf['recall'], metrics_tfidf['f1']]\n",
    "}, index=['Accuracy', 'Precision', 'Recall', 'F1 Score'])\n",
    "\n",
    "# 繪圖\n",
    "ax = metrics_df.plot(kind='bar', figsize=(12, 6), color=['#4ecdc4', '#ff6b6b'], alpha=0.8)\n",
    "ax.set_title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_ylim([0.9, 1.0])  # 聚焦在高分區間\n",
    "ax.legend(loc='lower right')\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=0)\n",
    "\n",
    "# 添加數值標籤\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fmt='%.3f', padding=3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 選擇最佳模型\n",
    "if metrics_tfidf['f1'] > metrics_bow['f1']:\n",
    "    best_model = nb_tfidf\n",
    "    best_vectorizer = tfidf_vectorizer\n",
    "    best_name = \"TF-IDF + Naive Bayes\"\n",
    "    X_test_best = X_test_tfidf\n",
    "    y_pred_best = y_pred_tfidf\n",
    "else:\n",
    "    best_model = nb_bow\n",
    "    best_vectorizer = bow_vectorizer\n",
    "    best_name = \"BoW + Naive Bayes\"\n",
    "    X_test_best = X_test_bow\n",
    "    y_pred_best = y_pred_bow\n",
    "\n",
    "print(f\"\\n🏆 最佳模型: {best_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confusion-matrix-header",
   "metadata": {},
   "source": [
    "### 7.1 混淆矩陣 (Confusion Matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confusion-matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算混淆矩陣\n",
    "cm = confusion_matrix(y_test, y_pred_best, labels=['ham', 'spam'])\n",
    "\n",
    "# 可視化\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Ham', 'Spam'],\n",
    "            yticklabels=['Ham', 'Spam'],\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.title(f'Confusion Matrix - {best_name}', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 解讀混淆矩陣\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(\"\\n混淆矩陣解讀:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"True Negatives (TN):  {tn:4d} - 正確識別為 Ham\")\n",
    "print(f\"False Positives (FP): {fp:4d} - Ham 誤判為 Spam (Type I Error)\")\n",
    "print(f\"False Negatives (FN): {fn:4d} - Spam 漏判為 Ham (Type II Error)\")\n",
    "print(f\"True Positives (TP):  {tp:4d} - 正確識別為 Spam\")\n",
    "print(f\"\\n總測試樣本: {tn + fp + fn + tp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classification-report-header",
   "metadata": {},
   "source": [
    "### 7.2 詳細分類報告"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classification-report",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成詳細報告\n",
    "print(f\"分類報告 - {best_name}\")\n",
    "print(\"=\"*70)\n",
    "print(classification_report(y_test, y_pred_best, target_names=['Ham', 'Spam']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "error-analysis-header",
   "metadata": {},
   "source": [
    "## 8. 錯誤分析\n",
    "\n",
    "分析模型犯錯的案例,理解模型的弱點"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "false-positives",
   "metadata": {},
   "outputs": [],
   "source": [
    "# False Positives: Ham 誤判為 Spam\n",
    "fp_indices = (y_test == 'ham') & (y_pred_best == 'spam')\n",
    "false_positives = df.loc[y_test[fp_indices].index]\n",
    "\n",
    "print(f\"🔴 False Positives (Ham 誤判為 Spam): {len(false_positives)} 條\")\n",
    "print(\"=\"*80)\n",
    "if len(false_positives) > 0:\n",
    "    for idx, (_, row) in enumerate(false_positives.head(5).iterrows(), 1):\n",
    "        print(f\"{idx}. {row['message'][:150]}\")\n",
    "        print(f\"   清理後: {row['cleaned_message'][:100]}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"沒有 False Positives!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "false-negatives",
   "metadata": {},
   "outputs": [],
   "source": [
    "# False Negatives: Spam 漏判為 Ham\n",
    "fn_indices = (y_test == 'spam') & (y_pred_best == 'ham')\n",
    "false_negatives = df.loc[y_test[fn_indices].index]\n",
    "\n",
    "print(f\"🟡 False Negatives (Spam 漏判為 Ham): {len(false_negatives)} 條\")\n",
    "print(\"=\"*80)\n",
    "if len(false_negatives) > 0:\n",
    "    for idx, (_, row) in enumerate(false_negatives.head(5).iterrows(), 1):\n",
    "        print(f\"{idx}. {row['message'][:150]}\")\n",
    "        print(f\"   清理後: {row['cleaned_message'][:100]}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"沒有 False Negatives!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "predict-new-header",
   "metadata": {},
   "source": [
    "## 9. 實際應用: 預測新簡訊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "predict-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_message(message, model, vectorizer, return_proba=True):\n",
    "    \"\"\"\n",
    "    預測單條簡訊是否為垃圾訊息\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    message : str\n",
    "        待預測的簡訊\n",
    "    model : sklearn model\n",
    "        訓練好的模型\n",
    "    vectorizer : sklearn vectorizer\n",
    "        特徵轉換器\n",
    "    return_proba : bool\n",
    "        是否返回機率\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    prediction : str\n",
    "        預測結果 (spam/ham)\n",
    "    probability : float (optional)\n",
    "        預測為 spam 的機率\n",
    "    \"\"\"\n",
    "    # 預處理\n",
    "    cleaned = preprocess_text(message)\n",
    "    \n",
    "    # 特徵轉換\n",
    "    features = vectorizer.transform([cleaned])\n",
    "    \n",
    "    # 預測\n",
    "    prediction = model.predict(features)[0]\n",
    "    \n",
    "    if return_proba:\n",
    "        proba = model.predict_proba(features)[0]\n",
    "        # 獲取 spam 的機率\n",
    "        spam_idx = list(model.classes_).index('spam')\n",
    "        spam_proba = proba[spam_idx]\n",
    "        return prediction, spam_proba\n",
    "    else:\n",
    "        return prediction\n",
    "\n",
    "# 測試函數\n",
    "test_messages = [\n",
    "    \"Congratulations! You've won a FREE iPhone! Click here to claim now!\",\n",
    "    \"Hey, are you free for lunch tomorrow?\",\n",
    "    \"URGENT: Your account has been compromised. Call 0800-123-456 immediately!\",\n",
    "    \"Meeting rescheduled to 3pm. See you then.\",\n",
    "    \"Get 50% OFF on all products! Limited time offer. Shop now!\",\n",
    "]\n",
    "\n",
    "print(\"預測結果:\")\n",
    "print(\"=\"*80)\n",
    "for i, msg in enumerate(test_messages, 1):\n",
    "    pred, proba = predict_message(msg, best_model, best_vectorizer)\n",
    "    emoji = \"🔴\" if pred == 'spam' else \"🟢\"\n",
    "    print(f\"{i}. {msg}\")\n",
    "    print(f\"   {emoji} 預測: {pred.upper()} (信心度: {proba:.2%})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interactive-prediction",
   "metadata": {},
   "source": [
    "### 9.1 互動式預測 (可選)\n",
    "\n",
    "取消註釋以下代碼,可以輸入自己的簡訊進行預測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interactive-input",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 互動式輸入\n",
    "# while True:\n",
    "#     user_message = input(\"\\n請輸入簡訊 (輸入 'quit' 結束): \")\n",
    "#     if user_message.lower() == 'quit':\n",
    "#         break\n",
    "#     \n",
    "#     pred, proba = predict_message(user_message, best_model, best_vectorizer)\n",
    "#     emoji = \"🔴\" if pred == 'spam' else \"🟢\"\n",
    "#     print(f\"{emoji} 預測: {pred.upper()} (信心度: {proba:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-interpretation-header",
   "metadata": {},
   "source": [
    "## 10. 模型解釋\n",
    "\n",
    "理解模型為何做出這樣的預測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature-importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 獲取特徵重要性 (log probability)\n",
    "feature_names = best_vectorizer.get_feature_names_out()\n",
    "\n",
    "# 獲取每個類別的 log probability\n",
    "spam_idx = list(best_model.classes_).index('spam')\n",
    "ham_idx = list(best_model.classes_).index('ham')\n",
    "\n",
    "spam_log_probs = best_model.feature_log_prob_[spam_idx]\n",
    "ham_log_probs = best_model.feature_log_prob_[ham_idx]\n",
    "\n",
    "# 計算 log odds ratio\n",
    "log_odds = spam_log_probs - ham_log_probs\n",
    "\n",
    "# 找出最強的 spam 指標\n",
    "top_spam_indices = log_odds.argsort()[-20:][::-1]\n",
    "# 找出最強的 ham 指標\n",
    "top_ham_indices = log_odds.argsort()[:20]\n",
    "\n",
    "print(\"🔴 Top 20 SPAM 指標詞:\")\n",
    "print(\"=\"*60)\n",
    "for idx in top_spam_indices:\n",
    "    print(f\"{feature_names[idx]:20s} → log odds = {log_odds[idx]:.4f}\")\n",
    "\n",
    "print(\"\\n🟢 Top 20 HAM 指標詞:\")\n",
    "print(\"=\"*60)\n",
    "for idx in top_ham_indices:\n",
    "    print(f\"{feature_names[idx]:20s} → log odds = {log_odds[idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可視化特徵重要性\n",
    "top_n = 15\n",
    "top_spam_idx = log_odds.argsort()[-top_n:][::-1]\n",
    "top_ham_idx = log_odds.argsort()[:top_n]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Spam 特徵\n",
    "spam_words = [feature_names[i] for i in top_spam_idx]\n",
    "spam_scores = [log_odds[i] for i in top_spam_idx]\n",
    "axes[0].barh(spam_words, spam_scores, color='#ff6b6b')\n",
    "axes[0].set_xlabel('Log Odds Ratio', fontsize=12)\n",
    "axes[0].set_title('Top 15 SPAM Indicators', fontsize=14, fontweight='bold')\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "# Ham 特徵\n",
    "ham_words = [feature_names[i] for i in top_ham_idx]\n",
    "ham_scores = [log_odds[i] for i in top_ham_idx]\n",
    "axes[1].barh(ham_words, ham_scores, color='#4ecdc4')\n",
    "axes[1].set_xlabel('Log Odds Ratio', fontsize=12)\n",
    "axes[1].set_title('Top 15 HAM Indicators', fontsize=14, fontweight='bold')\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cross-validation-header",
   "metadata": {},
   "source": [
    "## 11. 交叉驗證 (Cross-Validation)\n",
    "\n",
    "使用 5-fold 交叉驗證評估模型穩定性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-validation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# 建立完整的 pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', best_vectorizer),\n",
    "    ('classifier', MultinomialNB(alpha=1.0))\n",
    "])\n",
    "\n",
    "# 準備完整數據\n",
    "X_full = df['cleaned_message']\n",
    "y_full = df['label']\n",
    "\n",
    "# 5-fold 交叉驗證\n",
    "print(\"執行 5-fold 交叉驗證...\")\n",
    "cv_scores = cross_val_score(pipeline, X_full, y_full, cv=5, scoring='f1', \n",
    "                             n_jobs=-1)\n",
    "\n",
    "print(\"\\n交叉驗證結果 (F1 Score):\")\n",
    "print(\"=\"*60)\n",
    "for i, score in enumerate(cv_scores, 1):\n",
    "    print(f\"Fold {i}: {score:.4f} ({score:.2%})\")\n",
    "print(f\"\\n平均 F1 Score: {cv_scores.mean():.4f} ({cv_scores.mean():.2%})\")\n",
    "print(f\"標準差: {cv_scores.std():.4f}\")\n",
    "\n",
    "# 可視化\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, 6), cv_scores, marker='o', linestyle='-', linewidth=2, \n",
    "         markersize=10, color='#4ecdc4')\n",
    "plt.axhline(y=cv_scores.mean(), color='#ff6b6b', linestyle='--', \n",
    "            linewidth=2, label=f'Mean: {cv_scores.mean():.4f}')\n",
    "plt.xlabel('Fold', fontsize=12)\n",
    "plt.ylabel('F1 Score', fontsize=12)\n",
    "plt.title('5-Fold Cross-Validation Results', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✅ 模型表現穩定,標準差很小\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## 12. 專案總結\n",
    "\n",
    "### ✅ 完成的工作\n",
    "\n",
    "1. **數據分析**:\n",
    "   - 載入並分析 SMS Spam Collection 數據集\n",
    "   - 探索類別分佈和文本特徵\n",
    "   - 識別 spam 和 ham 的關鍵詞\n",
    "\n",
    "2. **文本預處理**:\n",
    "   - 小寫轉換\n",
    "   - 移除 URL、標點、數字\n",
    "   - (可選) 移除停用詞\n",
    "\n",
    "3. **特徵工程**:\n",
    "   - Bag of Words (BoW)\n",
    "   - TF-IDF\n",
    "   - N-gram 特徵 (unigram + bigram)\n",
    "\n",
    "4. **模型訓練與評估**:\n",
    "   - 訓練 Multinomial Naive Bayes\n",
    "   - 達到 95%+ 的準確率\n",
    "   - 交叉驗證確保穩定性\n",
    "\n",
    "5. **錯誤分析**:\n",
    "   - 分析 False Positives 和 False Negatives\n",
    "   - 理解模型的強項和弱點\n",
    "\n",
    "6. **實際應用**:\n",
    "   - 建立預測函數\n",
    "   - 可處理新的未見過的簡訊\n",
    "\n",
    "### 📊 關鍵發現\n",
    "\n",
    "- **TF-IDF 通常優於 BoW**: 因為考慮了詞的重要性\n",
    "- **Spam 特徵明顯**: 包含 \"free\", \"win\", \"call\", \"prize\" 等促銷詞彙\n",
    "- **模型穩定**: 交叉驗證標準差小,泛化能力好\n",
    "- **不平衡數據**: Spam 僅佔 13%,需注意評估指標選擇\n",
    "\n",
    "### 🚀 可能的改進方向\n",
    "\n",
    "1. **特徵工程**:\n",
    "   - 加入文本長度特徵\n",
    "   - 統計大寫字母比例\n",
    "   - 統計特殊字符數量\n",
    "\n",
    "2. **模型優化**:\n",
    "   - 調整 alpha 參數 (Laplace smoothing)\n",
    "   - 嘗試其他分類器 (SVM, Logistic Regression)\n",
    "   - 集成學習 (Ensemble methods)\n",
    "\n",
    "3. **處理不平衡**:\n",
    "   - 過採樣 (SMOTE)\n",
    "   - 調整類別權重\n",
    "   - 使用專門的不平衡數據技術\n",
    "\n",
    "4. **部署考量**:\n",
    "   - 模型壓縮 (降低特徵維度)\n",
    "   - 推理速度優化\n",
    "   - 建立監控系統追蹤線上表現\n",
    "\n",
    "### 💡 學到的經驗\n",
    "\n",
    "1. **數據探索很重要**: EDA 幫助我們理解數據特性\n",
    "2. **預處理影響大**: 適當的文本清理能顯著提升性能\n",
    "3. **評估要全面**: 不能只看 Accuracy,還要看 Precision/Recall\n",
    "4. **錯誤分析有價值**: 理解模型弱點才能針對性改進\n",
    "\n",
    "---\n",
    "\n",
    "## 13. 課後練習\n",
    "\n",
    "### 練習 1: 特徵工程\n",
    "\n",
    "嘗試添加以下額外特徵:\n",
    "- 文本長度\n",
    "- 大寫字母比例\n",
    "- 數字數量\n",
    "- 特殊字符數量\n",
    "\n",
    "看看是否能提升模型性能。\n",
    "\n",
    "### 練習 2: 其他分類器\n",
    "\n",
    "嘗試使用以下分類器,比較性能:\n",
    "- Logistic Regression\n",
    "- Support Vector Machine (SVM)\n",
    "- Random Forest\n",
    "\n",
    "### 練習 3: 處理中文簡訊\n",
    "\n",
    "收集中文垃圾簡訊數據,應用相同的流程:\n",
    "- 使用 jieba 分詞\n",
    "- 載入中文停用詞\n",
    "- 訓練中文垃圾簡訊分類器\n",
    "\n",
    "### 練習 4: 模型部署\n",
    "\n",
    "將訓練好的模型部署為 REST API:\n",
    "- 使用 Flask 或 FastAPI 建立 Web 服務\n",
    "- 提供 `/predict` 端點接收簡訊\n",
    "- 返回預測結果和信心度\n",
    "\n",
    "---\n",
    "\n",
    "**課程**: iSpan Python NLP Cookbooks v2\n",
    "**專案**: 垃圾郵件分類器\n",
    "**最後更新**: 2025-10-17\n",
    "\n",
    "**祝賀你完成這個完整的實戰專案! 🎉**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
