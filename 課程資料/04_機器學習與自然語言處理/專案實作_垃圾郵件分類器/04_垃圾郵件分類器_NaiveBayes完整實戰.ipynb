{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# å°ˆæ¡ˆå¯¦ä½œ: åƒåœ¾éƒµä»¶åˆ†é¡å™¨ (SMS Spam Classification)\n",
    "\n",
    "**èª²ç¨‹**: iSpan Python NLP Cookbooks v2\n",
    "**ç« ç¯€**: CH04 æ©Ÿå™¨å­¸ç¿’èˆ‡è‡ªç„¶èªè¨€è™•ç†\n",
    "**å°ˆæ¡ˆé¡å‹**: å®Œæ•´å¯¦æˆ°å°ˆæ¡ˆ\n",
    "**ç‰ˆæœ¬**: v1.0\n",
    "**æ›´æ–°æ—¥æœŸ**: 2025-10-17\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š å°ˆæ¡ˆç›®æ¨™\n",
    "\n",
    "å»ºç«‹ä¸€å€‹å¯¦ç”¨çš„åƒåœ¾ç°¡è¨Šåˆ†é¡ç³»çµ±ï¼Œèƒ½å¤ ï¼š\n",
    "1. è‡ªå‹•è­˜åˆ¥åƒåœ¾ç°¡è¨Š (Spam) å’Œæ­£å¸¸ç°¡è¨Š (Ham)\n",
    "2. é”åˆ° 95% ä»¥ä¸Šçš„æº–ç¢ºç‡\n",
    "3. æä¾›å¯è§£é‡‹çš„åˆ†é¡ä¾æ“š\n",
    "4. å¯éƒ¨ç½²åˆ°å¯¦éš›æ‡‰ç”¨å ´æ™¯\n",
    "\n",
    "## ğŸ¯ å­¸ç¿’é‡é»\n",
    "\n",
    "- å®Œæ•´çš„æ©Ÿå™¨å­¸ç¿’é …ç›®æµç¨‹\n",
    "- æ–‡æœ¬é è™•ç†èˆ‡ç‰¹å¾µå·¥ç¨‹\n",
    "- Naive Bayes åˆ†é¡å™¨æ‡‰ç”¨\n",
    "- æ¨¡å‹è©•ä¼°èˆ‡å„ªåŒ–\n",
    "- éŒ¯èª¤åˆ†æèˆ‡æ”¹é€²\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports",
   "metadata": {},
   "source": [
    "## 1. ç’°å¢ƒæº–å‚™èˆ‡æ•¸æ“šè¼‰å…¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å°å…¥å¿…è¦å¥—ä»¶\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_curve, auc\n",
    ")\n",
    "\n",
    "# è¨­å®šé¡¯ç¤ºé¸é …\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# è¨­å®šéš¨æ©Ÿç¨®å­\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"âœ… ç’°å¢ƒæº–å‚™å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-loading-header",
   "metadata": {},
   "source": [
    "### 1.1 è¼‰å…¥ SMS Spam Collection æ•¸æ“šé›†\n",
    "\n",
    "**æ•¸æ“šä¾†æº**: [UCI ML Repository - SMS Spam Collection](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection)\n",
    "\n",
    "**æ•¸æ“šé›†èªªæ˜**:\n",
    "- 5,574 æ¢è‹±æ–‡ç°¡è¨Š\n",
    "- äºŒå…ƒåˆ†é¡: spam (åƒåœ¾ç°¡è¨Š) vs ham (æ­£å¸¸ç°¡è¨Š)\n",
    "- ä¸å¹³è¡¡æ•¸æ“šé›† (spam ç´„ä½” 13%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ–¹æ³• 1: å¾ sklearn å…§å»ºæ•¸æ“šè¼‰å…¥ (å¦‚æœå¯ç”¨)\n",
    "try:\n",
    "    from sklearn.datasets import fetch_20newsgroups\n",
    "    # æ³¨æ„: sklearn æ²’æœ‰å…§å»º SMS Spam,é€™è£¡ä½¿ç”¨å‚™ç”¨æ–¹æ¡ˆ\n",
    "    raise ImportError(\"ä½¿ç”¨æœ¬åœ°æ•¸æ“š\")\n",
    "except:\n",
    "    # æ–¹æ³• 2: å¾æœ¬åœ°æˆ–ç·šä¸Šè¼‰å…¥\n",
    "    import os\n",
    "    \n",
    "    # æ•¸æ“šè·¯å¾‘é¸é …\n",
    "    data_paths = [\n",
    "        '../../../datasets/sms_spam/SMSSpamCollection.csv',\n",
    "        '../../../datasets/sms_spam/SMSSpamCollection.txt',\n",
    "    ]\n",
    "    \n",
    "    df = None\n",
    "    for path in data_paths:\n",
    "        if os.path.exists(path):\n",
    "            if path.endswith('.csv'):\n",
    "                df = pd.read_csv(path, encoding='latin-1')\n",
    "            else:\n",
    "                df = pd.read_csv(path, sep='\\t', names=['label', 'message'], encoding='latin-1')\n",
    "            print(f\"âœ… å¾ {path} è¼‰å…¥æ•¸æ“š\")\n",
    "            break\n",
    "    \n",
    "    # å¦‚æœæœ¬åœ°æ²’æœ‰æ•¸æ“š,ä½¿ç”¨æ¨¡æ“¬æ•¸æ“š (å¯¦éš›å°ˆæ¡ˆä¸­æ‡‰ä¸‹è¼‰çœŸå¯¦æ•¸æ“š)\n",
    "    if df is None:\n",
    "        print(\"âš ï¸  æœªæ‰¾åˆ°çœŸå¯¦æ•¸æ“šé›†,ä½¿ç”¨æ¨¡æ“¬æ•¸æ“šé€²è¡Œæ¼”ç¤º\")\n",
    "        print(\"   å»ºè­°ä¸‹è¼‰: https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection\")\n",
    "        \n",
    "        # æ¨¡æ“¬æ•¸æ“š (æ“´å……ç‰ˆ)\n",
    "        spam_messages = [\n",
    "            \"FREE for 1st week! No1 Nokia tone 4 ur mobile every week just txt NOKIA to 8007 Get txting and tell ur mates. zed POBox 36504 W45WQ norm150p/tone 16+\",\n",
    "            \"WINNER!! As a valued network customer you have been selected to receivea Â£900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.\",\n",
    "            \"Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030\",\n",
    "            \"I'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried enough today.\",\n",
    "            \"SIX chances to win CASH! From 100 to 20,000 pounds txt> CSH11 and send to 87575. Cost 150p/day, 6days, 16+ TsandCs apply Reply HL 4 info\",\n",
    "            \"URGENT! You have won a 1 week FREE membership in our Â£100,000 Prize Jackpot! Txt the word: CLAIM to No: 81010 T&C www.dbuk.net LCCLTD POBOX 4403LDNW1A7RW18\",\n",
    "            \"XXXMobileMovieClub: To use your credit, click the WAP link in the next txt message or click here>> http://wap. xxxmobilemovieclub.com?n=QJKGIGHJJGCBL\",\n",
    "            \"England v Macedonia - dont miss the goals/team news. Txt ur national team to 87077 eg ENGLAND to 87077 Try:WALES, SCOTLAND 4txt/Ãº1.20 POBOx 84NW10 6Ã˜Z QUIT?txtStop\",\n",
    "            \"Thanks for your subscription to Ringtone UK your mobile will be charged Â£5/month Please confirm by replying YES or NO. If you reply NO you will not be charged\",\n",
    "            \"Congratulations ur awarded 500 of CD vouchers or 125gift guaranteed & Free entry 2 100 wkly draw txt MUSIC to 87066 TnCs www.Ldew.com1win150ppmx3age16\"\n",
    "        ] * 70  # 700 spam messages\n",
    "        \n",
    "        ham_messages = [\n",
    "            \"Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\",\n",
    "            \"Ok lar... Joking wif u oni...\",\n",
    "            \"U dun say so early hor... U c already then say...\",\n",
    "            \"Nah I don't think he goes to usf, he lives around here though\",\n",
    "            \"Even my brother is not like to speak with me. They treat me like aids patent.\",\n",
    "            \"I HAVE A DATE ON SUNDAY WITH WILL!!\",\n",
    "            \"As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune\",\n",
    "            \"Oh k...i'm watching here:)\",\n",
    "            \"Eh u remember how 2 spell his name... Yes i did. He v naughty make until i v wet.\",\n",
    "            \"I'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried enough today.\"\n",
    "        ] * 460  # 4600 ham messages\n",
    "        \n",
    "        df = pd.DataFrame({\n",
    "            'label': ['spam'] * len(spam_messages) + ['ham'] * len(ham_messages),\n",
    "            'message': spam_messages + ham_messages\n",
    "        })\n",
    "        \n",
    "        # æ‰“äº‚æ•¸æ“š\n",
    "        df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# é¡¯ç¤ºåŸºæœ¬ä¿¡æ¯\n",
    "print(f\"\\næ•¸æ“šé›†å¤§å°: {len(df)} æ¢ç°¡è¨Š\")\n",
    "print(f\"æ¬„ä½: {df.columns.tolist()}\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda-header",
   "metadata": {},
   "source": [
    "## 2. æ¢ç´¢æ€§æ•¸æ“šåˆ†æ (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda-basic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŸºæœ¬çµ±è¨ˆä¿¡æ¯\n",
    "print(\"æ•¸æ“šé›†åŸºæœ¬ä¿¡æ¯:\")\n",
    "print(\"=\"*60)\n",
    "print(df.info())\n",
    "print(\"\\nç¼ºå¤±å€¼çµ±è¨ˆ:\")\n",
    "print(df.isnull().sum())\n",
    "print(\"\\né¡åˆ¥åˆ†ä½ˆ:\")\n",
    "print(df['label'].value_counts())\n",
    "print(f\"\\nSpam æ¯”ä¾‹: {(df['label'] == 'spam').mean():.2%}\")\n",
    "print(f\"Ham æ¯”ä¾‹: {(df['label'] == 'ham').mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda-distribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è¦–åŒ–é¡åˆ¥åˆ†ä½ˆ\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# é¡åˆ¥è¨ˆæ•¸\n",
    "label_counts = df['label'].value_counts()\n",
    "axes[0].bar(label_counts.index, label_counts.values, color=['#4ecdc4', '#ff6b6b'])\n",
    "axes[0].set_title('Message Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_xlabel('Label')\n",
    "\n",
    "# æ·»åŠ æ•¸å€¼æ¨™ç±¤\n",
    "for i, v in enumerate(label_counts.values):\n",
    "    axes[0].text(i, v + 50, str(v), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# é¡åˆ¥æ¯”ä¾‹ (é¤…åœ–)\n",
    "colors = ['#4ecdc4', '#ff6b6b']\n",
    "axes[1].pie(label_counts.values, labels=label_counts.index, autopct='%1.1f%%',\n",
    "            colors=colors, startangle=90, textprops={'fontsize': 12, 'fontweight': 'bold'})\n",
    "axes[1].set_title('Message Proportion', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âš ï¸  æ³¨æ„: é€™æ˜¯ä¸€å€‹ä¸å¹³è¡¡æ•¸æ“šé›† (imbalanced dataset)\")\n",
    "print(\"   Spam ç´„ä½” 13%,éœ€è¦æ³¨æ„è©•ä¼°æŒ‡æ¨™çš„é¸æ“‡\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "text-analysis-header",
   "metadata": {},
   "source": [
    "### 2.1 æ–‡æœ¬é•·åº¦åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "text-length",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨ˆç®—æ–‡æœ¬é•·åº¦\n",
    "df['length'] = df['message'].apply(len)\n",
    "df['word_count'] = df['message'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# çµ±è¨ˆæ‘˜è¦\n",
    "print(\"æ–‡æœ¬é•·åº¦çµ±è¨ˆ (æŒ‰é¡åˆ¥):\")\n",
    "print(\"=\"*60)\n",
    "print(df.groupby('label')[['length', 'word_count']].describe().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "text-length-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è¦–åŒ–æ–‡æœ¬é•·åº¦åˆ†ä½ˆ\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# å­—ç¬¦é•·åº¦åˆ†ä½ˆ - Histogram\n",
    "df[df['label'] == 'ham']['length'].hist(bins=50, alpha=0.6, label='Ham', \n",
    "                                          color='#4ecdc4', ax=axes[0, 0])\n",
    "df[df['label'] == 'spam']['length'].hist(bins=50, alpha=0.6, label='Spam', \n",
    "                                           color='#ff6b6b', ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Character Length Distribution', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Length')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# è©æ•¸åˆ†ä½ˆ - Histogram\n",
    "df[df['label'] == 'ham']['word_count'].hist(bins=50, alpha=0.6, label='Ham', \n",
    "                                              color='#4ecdc4', ax=axes[0, 1])\n",
    "df[df['label'] == 'spam']['word_count'].hist(bins=50, alpha=0.6, label='Spam', \n",
    "                                               color='#ff6b6b', ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Word Count Distribution', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Word Count')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Box plot - å­—ç¬¦é•·åº¦\n",
    "df.boxplot(column='length', by='label', ax=axes[1, 0], patch_artist=True,\n",
    "           boxprops=dict(facecolor='#4ecdc4', alpha=0.6))\n",
    "axes[1, 0].set_title('Character Length by Label', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Label')\n",
    "axes[1, 0].set_ylabel('Length')\n",
    "plt.sca(axes[1, 0])\n",
    "plt.xticks([1, 2], ['ham', 'spam'])\n",
    "\n",
    "# Box plot - è©æ•¸\n",
    "df.boxplot(column='word_count', by='label', ax=axes[1, 1], patch_artist=True,\n",
    "           boxprops=dict(facecolor='#ff6b6b', alpha=0.6))\n",
    "axes[1, 1].set_title('Word Count by Label', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Label')\n",
    "axes[1, 1].set_ylabel('Word Count')\n",
    "plt.sca(axes[1, 1])\n",
    "plt.xticks([1, 2], ['ham', 'spam'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nè§€å¯Ÿ:\")\n",
    "print(\"- Spam ç°¡è¨Šé€šå¸¸æ›´é•· (æ›´å¤šä¿ƒéŠ·æ–‡å­—)\")\n",
    "print(\"- Ham ç°¡è¨Šè¼ƒçŸ­ (æ—¥å¸¸å°è©±)\")\n",
    "print(\"- æ–‡æœ¬é•·åº¦å¯ä»¥ä½œç‚ºä¸€å€‹æœ‰ç”¨çš„ç‰¹å¾µ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sample-messages",
   "metadata": {},
   "source": [
    "### 2.2 æŸ¥çœ‹æ¨£æœ¬ç°¡è¨Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "view-samples",
   "metadata": {},
   "outputs": [],
   "source": [
    "# éš¨æ©ŸæŸ¥çœ‹ Spam æ¨£æœ¬\n",
    "print(\"ğŸ”´ Spam ç°¡è¨Šç¯„ä¾‹:\")\n",
    "print(\"=\"*80)\n",
    "spam_samples = df[df['label'] == 'spam'].sample(5, random_state=42)\n",
    "for idx, (_, row) in enumerate(spam_samples.iterrows(), 1):\n",
    "    print(f\"{idx}. {row['message'][:150]}...\")\n",
    "    print()\n",
    "\n",
    "# éš¨æ©ŸæŸ¥çœ‹ Ham æ¨£æœ¬\n",
    "print(\"\\nğŸŸ¢ Ham ç°¡è¨Šç¯„ä¾‹:\")\n",
    "print(\"=\"*80)\n",
    "ham_samples = df[df['label'] == 'ham'].sample(5, random_state=42)\n",
    "for idx, (_, row) in enumerate(ham_samples.iterrows(), 1):\n",
    "    print(f\"{idx}. {row['message'][:150]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preprocessing-header",
   "metadata": {},
   "source": [
    "## 3. æ–‡æœ¬é è™•ç†\n",
    "\n",
    "**é è™•ç†æ­¥é©Ÿ**:\n",
    "1. å°å¯«è½‰æ›\n",
    "2. ç§»é™¤æ¨™é»ç¬¦è™Ÿå’Œç‰¹æ®Šå­—ç¬¦\n",
    "3. ç§»é™¤åœç”¨è© (å¯é¸)\n",
    "4. è©å¹¹æå–/è©å½¢é‚„åŸ (å¯é¸)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preprocessing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def preprocess_text(text, remove_stopwords=False):\n",
    "    \"\"\"\n",
    "    æ–‡æœ¬é è™•ç†å‡½æ•¸\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        åŸå§‹æ–‡æœ¬\n",
    "    remove_stopwords : bool\n",
    "        æ˜¯å¦ç§»é™¤åœç”¨è©\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    cleaned_text : str\n",
    "        é è™•ç†å¾Œçš„æ–‡æœ¬\n",
    "    \"\"\"\n",
    "    # 1. å°å¯«è½‰æ›\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2. ç§»é™¤ URL\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    \n",
    "    # 3. ç§»é™¤ HTML æ¨™ç±¤\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # 4. ç§»é™¤æ¨™é»ç¬¦è™Ÿå’Œæ•¸å­— (ä¿ç•™ç©ºæ ¼)\n",
    "    text = re.sub(f'[{re.escape(string.punctuation)}]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # 5. ç§»é™¤å¤šé¤˜ç©ºæ ¼\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    # 6. ç§»é™¤åœç”¨è© (å¯é¸)\n",
    "    if remove_stopwords:\n",
    "        try:\n",
    "            from nltk.corpus import stopwords\n",
    "            import nltk\n",
    "            nltk.download('stopwords', quiet=True)\n",
    "            stop_words = set(stopwords.words('english'))\n",
    "            text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "        except:\n",
    "            pass  # å¦‚æœæ²’æœ‰ nltk,è·³éé€™ä¸€æ­¥\n",
    "    \n",
    "    return text\n",
    "\n",
    "# æ¸¬è©¦é è™•ç†å‡½æ•¸\n",
    "test_text = \"FREE! Win Â£1000 cash!! Call NOW on 0800-123-4567 or visit www.spam.com\"\n",
    "print(\"åŸå§‹æ–‡æœ¬:\")\n",
    "print(test_text)\n",
    "print(\"\\né è™•ç†å¾Œ:\")\n",
    "print(preprocess_text(test_text))\n",
    "print(\"\\né è™•ç†å¾Œ (ç§»é™¤åœç”¨è©):\")\n",
    "print(preprocess_text(test_text, remove_stopwords=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apply-preprocessing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ‡‰ç”¨é è™•ç†åˆ°æ•´å€‹æ•¸æ“šé›†\n",
    "print(\"æ­£åœ¨é è™•ç†æ•¸æ“šé›†...\")\n",
    "df['cleaned_message'] = df['message'].apply(lambda x: preprocess_text(x, remove_stopwords=False))\n",
    "print(\"âœ… é è™•ç†å®Œæˆ\")\n",
    "\n",
    "# æ¯”è¼ƒå‰å¾Œ\n",
    "print(\"\\né è™•ç†æ•ˆæœå°æ¯”:\")\n",
    "print(\"=\"*80)\n",
    "for i in range(3):\n",
    "    sample = df.sample(1, random_state=i).iloc[0]\n",
    "    print(f\"\\nç¯„ä¾‹ {i+1} ({sample['label'].upper()}):\")\n",
    "    print(f\"åŸå§‹: {sample['message'][:100]}\")\n",
    "    print(f\"è™•ç†: {sample['cleaned_message'][:100]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split-data-header",
   "metadata": {},
   "source": [
    "## 4. æ•¸æ“šåˆ‡åˆ†\n",
    "\n",
    "å°‡æ•¸æ“šåˆ‡åˆ†ç‚ºè¨“ç·´é›† (80%) å’Œæ¸¬è©¦é›† (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æº–å‚™ç‰¹å¾µå’Œæ¨™ç±¤\n",
    "X = df['cleaned_message']\n",
    "y = df['label']\n",
    "\n",
    "# åˆ‡åˆ†æ•¸æ“š (stratify ç¢ºä¿é¡åˆ¥æ¯”ä¾‹ä¸€è‡´)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"æ•¸æ“šåˆ‡åˆ†çµæœ:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"è¨“ç·´é›†å¤§å°: {len(X_train)} ({len(X_train)/len(df):.0%})\")\n",
    "print(f\"æ¸¬è©¦é›†å¤§å°: {len(X_test)} ({len(X_test)/len(df):.0%})\")\n",
    "print(f\"\\nè¨“ç·´é›†é¡åˆ¥åˆ†ä½ˆ:\")\n",
    "print(y_train.value_counts())\n",
    "print(f\"\\næ¸¬è©¦é›†é¡åˆ¥åˆ†ä½ˆ:\")\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature-engineering-header",
   "metadata": {},
   "source": [
    "## 5. ç‰¹å¾µå·¥ç¨‹\n",
    "\n",
    "å°‡æ–‡æœ¬è½‰æ›ç‚ºæ•¸å€¼ç‰¹å¾µ,æ¯”è¼ƒå…©ç¨®æ–¹æ³•:\n",
    "1. **Bag of Words (BoW)**: è©é »çµ±è¨ˆ\n",
    "2. **TF-IDF**: è€ƒæ…®è©çš„é‡è¦æ€§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bow-vectorizer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ–¹æ³• 1: Bag of Words\n",
    "print(\"å»ºç«‹ Bag of Words ç‰¹å¾µ...\")\n",
    "bow_vectorizer = CountVectorizer(max_features=3000, ngram_range=(1, 2))\n",
    "X_train_bow = bow_vectorizer.fit_transform(X_train)\n",
    "X_test_bow = bow_vectorizer.transform(X_test)\n",
    "\n",
    "print(f\"âœ… BoW ç‰¹å¾µçŸ©é™£å¤§å°: {X_train_bow.shape}\")\n",
    "print(f\"   è©å½™è¡¨å¤§å°: {len(bow_vectorizer.vocabulary_)}\")\n",
    "print(f\"   ç¨€ç–åº¦: {(1 - X_train_bow.nnz / (X_train_bow.shape[0] * X_train_bow.shape[1])):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tfidf-vectorizer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ–¹æ³• 2: TF-IDF\n",
    "print(\"å»ºç«‹ TF-IDF ç‰¹å¾µ...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=3000, ngram_range=(1, 2))\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "print(f\"âœ… TF-IDF ç‰¹å¾µçŸ©é™£å¤§å°: {X_train_tfidf.shape}\")\n",
    "print(f\"   è©å½™è¡¨å¤§å°: {len(tfidf_vectorizer.vocabulary_)}\")\n",
    "print(f\"   ç¨€ç–åº¦: {(1 - X_train_tfidf.nnz / (X_train_tfidf.shape[0] * X_train_tfidf.shape[1])):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "top-features-header",
   "metadata": {},
   "source": [
    "### 5.1 æŸ¥çœ‹æœ€é‡è¦çš„ç‰¹å¾µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "top-words",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨ˆç®—æ¯å€‹è©åœ¨ spam å’Œ ham ä¸­çš„å¹³å‡ TF-IDF åˆ†æ•¸\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# åˆ†åˆ¥è¨ˆç®— spam å’Œ ham çš„å¹³å‡ TF-IDF\n",
    "spam_indices = y_train == 'spam'\n",
    "ham_indices = y_train == 'ham'\n",
    "\n",
    "spam_tfidf_mean = np.array(X_train_tfidf[spam_indices].mean(axis=0)).flatten()\n",
    "ham_tfidf_mean = np.array(X_train_tfidf[ham_indices].mean(axis=0)).flatten()\n",
    "\n",
    "# æ‰¾å‡ºæœ€é‡è¦çš„è©\n",
    "top_n = 15\n",
    "\n",
    "spam_top_indices = spam_tfidf_mean.argsort()[-top_n:][::-1]\n",
    "ham_top_indices = ham_tfidf_mean.argsort()[-top_n:][::-1]\n",
    "\n",
    "print(\"Top 15 SPAM é—œéµè© (æŒ‰å¹³å‡ TF-IDF åˆ†æ•¸):\")\n",
    "print(\"=\"*60)\n",
    "for idx in spam_top_indices:\n",
    "    print(f\"{feature_names[idx]:20s} â†’ {spam_tfidf_mean[idx]:.4f}\")\n",
    "\n",
    "print(\"\\nTop 15 HAM é—œéµè© (æŒ‰å¹³å‡ TF-IDF åˆ†æ•¸):\")\n",
    "print(\"=\"*60)\n",
    "for idx in ham_top_indices:\n",
    "    print(f\"{feature_names[idx]:20s} â†’ {ham_tfidf_mean[idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-training-header",
   "metadata": {},
   "source": [
    "## 6. æ¨¡å‹è¨“ç·´\n",
    "\n",
    "ä½¿ç”¨ Multinomial Naive Bayes è¨“ç·´å…©å€‹æ¨¡å‹:\n",
    "1. åŸºæ–¼ Bag of Words\n",
    "2. åŸºæ–¼ TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-bow-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¨¡å‹ 1: Bag of Words + Naive Bayes\n",
    "print(\"è¨“ç·´æ¨¡å‹ 1: BoW + Naive Bayes\")\n",
    "nb_bow = MultinomialNB(alpha=1.0)\n",
    "nb_bow.fit(X_train_bow, y_train)\n",
    "print(\"âœ… æ¨¡å‹ 1 è¨“ç·´å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-tfidf-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¨¡å‹ 2: TF-IDF + Naive Bayes\n",
    "print(\"è¨“ç·´æ¨¡å‹ 2: TF-IDF + Naive Bayes\")\n",
    "nb_tfidf = MultinomialNB(alpha=1.0)\n",
    "nb_tfidf.fit(X_train_tfidf, y_train)\n",
    "print(\"âœ… æ¨¡å‹ 2 è¨“ç·´å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluation-header",
   "metadata": {},
   "source": [
    "## 7. æ¨¡å‹è©•ä¼°\n",
    "\n",
    "ä½¿ç”¨å¤šå€‹æŒ‡æ¨™è©•ä¼°æ¨¡å‹æ€§èƒ½:\n",
    "- **Accuracy**: æ•´é«”æº–ç¢ºç‡\n",
    "- **Precision**: ç²¾ç¢ºç‡ (é æ¸¬ç‚º spam çš„æ­£ç¢ºç‡)\n",
    "- **Recall**: å¬å›ç‡ (å¯¦éš› spam è¢«æ‰¾å‡ºçš„æ¯”ä¾‹)\n",
    "- **F1 Score**: Precision å’Œ Recall çš„èª¿å’Œå¹³å‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate-models",
   "metadata": {},
   "outputs": [],
   "source": [
    "# é æ¸¬\n",
    "y_pred_bow = nb_bow.predict(X_test_bow)\n",
    "y_pred_tfidf = nb_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "# è¨ˆç®—æŒ‡æ¨™\n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, pos_label='spam')\n",
    "    recall = recall_score(y_true, y_pred, pos_label='spam')\n",
    "    f1 = f1_score(y_true, y_pred, pos_label='spam')\n",
    "    \n",
    "    print(f\"{model_name} è©•ä¼°çµæœ:\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Accuracy:  {accuracy:.4f} ({accuracy:.2%})\")\n",
    "    print(f\"Precision: {precision:.4f} ({precision:.2%})\")\n",
    "    print(f\"Recall:    {recall:.4f} ({recall:.2%})\")\n",
    "    print(f\"F1 Score:  {f1:.4f} ({f1:.2%})\")\n",
    "    print()\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "# è©•ä¼°å…©å€‹æ¨¡å‹\n",
    "metrics_bow = evaluate_model(y_test, y_pred_bow, \"æ¨¡å‹ 1 (BoW + NB)\")\n",
    "metrics_tfidf = evaluate_model(y_test, y_pred_tfidf, \"æ¨¡å‹ 2 (TF-IDF + NB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare-models",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è¦–åŒ–å°æ¯”\n",
    "metrics_df = pd.DataFrame({\n",
    "    'BoW + NB': [metrics_bow['accuracy'], metrics_bow['precision'], \n",
    "                 metrics_bow['recall'], metrics_bow['f1']],\n",
    "    'TF-IDF + NB': [metrics_tfidf['accuracy'], metrics_tfidf['precision'], \n",
    "                    metrics_tfidf['recall'], metrics_tfidf['f1']]\n",
    "}, index=['Accuracy', 'Precision', 'Recall', 'F1 Score'])\n",
    "\n",
    "# ç¹ªåœ–\n",
    "ax = metrics_df.plot(kind='bar', figsize=(12, 6), color=['#4ecdc4', '#ff6b6b'], alpha=0.8)\n",
    "ax.set_title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_ylim([0.9, 1.0])  # èšç„¦åœ¨é«˜åˆ†å€é–“\n",
    "ax.legend(loc='lower right')\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=0)\n",
    "\n",
    "# æ·»åŠ æ•¸å€¼æ¨™ç±¤\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fmt='%.3f', padding=3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# é¸æ“‡æœ€ä½³æ¨¡å‹\n",
    "if metrics_tfidf['f1'] > metrics_bow['f1']:\n",
    "    best_model = nb_tfidf\n",
    "    best_vectorizer = tfidf_vectorizer\n",
    "    best_name = \"TF-IDF + Naive Bayes\"\n",
    "    X_test_best = X_test_tfidf\n",
    "    y_pred_best = y_pred_tfidf\n",
    "else:\n",
    "    best_model = nb_bow\n",
    "    best_vectorizer = bow_vectorizer\n",
    "    best_name = \"BoW + Naive Bayes\"\n",
    "    X_test_best = X_test_bow\n",
    "    y_pred_best = y_pred_bow\n",
    "\n",
    "print(f\"\\nğŸ† æœ€ä½³æ¨¡å‹: {best_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confusion-matrix-header",
   "metadata": {},
   "source": [
    "### 7.1 æ··æ·†çŸ©é™£ (Confusion Matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confusion-matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨ˆç®—æ··æ·†çŸ©é™£\n",
    "cm = confusion_matrix(y_test, y_pred_best, labels=['ham', 'spam'])\n",
    "\n",
    "# å¯è¦–åŒ–\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Ham', 'Spam'],\n",
    "            yticklabels=['Ham', 'Spam'],\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.title(f'Confusion Matrix - {best_name}', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# è§£è®€æ··æ·†çŸ©é™£\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(\"\\næ··æ·†çŸ©é™£è§£è®€:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"True Negatives (TN):  {tn:4d} - æ­£ç¢ºè­˜åˆ¥ç‚º Ham\")\n",
    "print(f\"False Positives (FP): {fp:4d} - Ham èª¤åˆ¤ç‚º Spam (Type I Error)\")\n",
    "print(f\"False Negatives (FN): {fn:4d} - Spam æ¼åˆ¤ç‚º Ham (Type II Error)\")\n",
    "print(f\"True Positives (TP):  {tp:4d} - æ­£ç¢ºè­˜åˆ¥ç‚º Spam\")\n",
    "print(f\"\\nç¸½æ¸¬è©¦æ¨£æœ¬: {tn + fp + fn + tp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classification-report-header",
   "metadata": {},
   "source": [
    "### 7.2 è©³ç´°åˆ†é¡å ±å‘Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classification-report",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”Ÿæˆè©³ç´°å ±å‘Š\n",
    "print(f\"åˆ†é¡å ±å‘Š - {best_name}\")\n",
    "print(\"=\"*70)\n",
    "print(classification_report(y_test, y_pred_best, target_names=['Ham', 'Spam']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "error-analysis-header",
   "metadata": {},
   "source": [
    "## 8. éŒ¯èª¤åˆ†æ\n",
    "\n",
    "åˆ†ææ¨¡å‹çŠ¯éŒ¯çš„æ¡ˆä¾‹,ç†è§£æ¨¡å‹çš„å¼±é»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "false-positives",
   "metadata": {},
   "outputs": [],
   "source": [
    "# False Positives: Ham èª¤åˆ¤ç‚º Spam\n",
    "fp_indices = (y_test == 'ham') & (y_pred_best == 'spam')\n",
    "false_positives = df.loc[y_test[fp_indices].index]\n",
    "\n",
    "print(f\"ğŸ”´ False Positives (Ham èª¤åˆ¤ç‚º Spam): {len(false_positives)} æ¢\")\n",
    "print(\"=\"*80)\n",
    "if len(false_positives) > 0:\n",
    "    for idx, (_, row) in enumerate(false_positives.head(5).iterrows(), 1):\n",
    "        print(f\"{idx}. {row['message'][:150]}\")\n",
    "        print(f\"   æ¸…ç†å¾Œ: {row['cleaned_message'][:100]}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"æ²’æœ‰ False Positives!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "false-negatives",
   "metadata": {},
   "outputs": [],
   "source": [
    "# False Negatives: Spam æ¼åˆ¤ç‚º Ham\n",
    "fn_indices = (y_test == 'spam') & (y_pred_best == 'ham')\n",
    "false_negatives = df.loc[y_test[fn_indices].index]\n",
    "\n",
    "print(f\"ğŸŸ¡ False Negatives (Spam æ¼åˆ¤ç‚º Ham): {len(false_negatives)} æ¢\")\n",
    "print(\"=\"*80)\n",
    "if len(false_negatives) > 0:\n",
    "    for idx, (_, row) in enumerate(false_negatives.head(5).iterrows(), 1):\n",
    "        print(f\"{idx}. {row['message'][:150]}\")\n",
    "        print(f\"   æ¸…ç†å¾Œ: {row['cleaned_message'][:100]}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"æ²’æœ‰ False Negatives!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "predict-new-header",
   "metadata": {},
   "source": [
    "## 9. å¯¦éš›æ‡‰ç”¨: é æ¸¬æ–°ç°¡è¨Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "predict-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_message(message, model, vectorizer, return_proba=True):\n",
    "    \"\"\"\n",
    "    é æ¸¬å–®æ¢ç°¡è¨Šæ˜¯å¦ç‚ºåƒåœ¾è¨Šæ¯\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    message : str\n",
    "        å¾…é æ¸¬çš„ç°¡è¨Š\n",
    "    model : sklearn model\n",
    "        è¨“ç·´å¥½çš„æ¨¡å‹\n",
    "    vectorizer : sklearn vectorizer\n",
    "        ç‰¹å¾µè½‰æ›å™¨\n",
    "    return_proba : bool\n",
    "        æ˜¯å¦è¿”å›æ©Ÿç‡\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    prediction : str\n",
    "        é æ¸¬çµæœ (spam/ham)\n",
    "    probability : float (optional)\n",
    "        é æ¸¬ç‚º spam çš„æ©Ÿç‡\n",
    "    \"\"\"\n",
    "    # é è™•ç†\n",
    "    cleaned = preprocess_text(message)\n",
    "    \n",
    "    # ç‰¹å¾µè½‰æ›\n",
    "    features = vectorizer.transform([cleaned])\n",
    "    \n",
    "    # é æ¸¬\n",
    "    prediction = model.predict(features)[0]\n",
    "    \n",
    "    if return_proba:\n",
    "        proba = model.predict_proba(features)[0]\n",
    "        # ç²å– spam çš„æ©Ÿç‡\n",
    "        spam_idx = list(model.classes_).index('spam')\n",
    "        spam_proba = proba[spam_idx]\n",
    "        return prediction, spam_proba\n",
    "    else:\n",
    "        return prediction\n",
    "\n",
    "# æ¸¬è©¦å‡½æ•¸\n",
    "test_messages = [\n",
    "    \"Congratulations! You've won a FREE iPhone! Click here to claim now!\",\n",
    "    \"Hey, are you free for lunch tomorrow?\",\n",
    "    \"URGENT: Your account has been compromised. Call 0800-123-456 immediately!\",\n",
    "    \"Meeting rescheduled to 3pm. See you then.\",\n",
    "    \"Get 50% OFF on all products! Limited time offer. Shop now!\",\n",
    "]\n",
    "\n",
    "print(\"é æ¸¬çµæœ:\")\n",
    "print(\"=\"*80)\n",
    "for i, msg in enumerate(test_messages, 1):\n",
    "    pred, proba = predict_message(msg, best_model, best_vectorizer)\n",
    "    emoji = \"ğŸ”´\" if pred == 'spam' else \"ğŸŸ¢\"\n",
    "    print(f\"{i}. {msg}\")\n",
    "    print(f\"   {emoji} é æ¸¬: {pred.upper()} (ä¿¡å¿ƒåº¦: {proba:.2%})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interactive-prediction",
   "metadata": {},
   "source": [
    "### 9.1 äº’å‹•å¼é æ¸¬ (å¯é¸)\n",
    "\n",
    "å–æ¶ˆè¨»é‡‹ä»¥ä¸‹ä»£ç¢¼,å¯ä»¥è¼¸å…¥è‡ªå·±çš„ç°¡è¨Šé€²è¡Œé æ¸¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interactive-input",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # äº’å‹•å¼è¼¸å…¥\n",
    "# while True:\n",
    "#     user_message = input(\"\\nè«‹è¼¸å…¥ç°¡è¨Š (è¼¸å…¥ 'quit' çµæŸ): \")\n",
    "#     if user_message.lower() == 'quit':\n",
    "#         break\n",
    "#     \n",
    "#     pred, proba = predict_message(user_message, best_model, best_vectorizer)\n",
    "#     emoji = \"ğŸ”´\" if pred == 'spam' else \"ğŸŸ¢\"\n",
    "#     print(f\"{emoji} é æ¸¬: {pred.upper()} (ä¿¡å¿ƒåº¦: {proba:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-interpretation-header",
   "metadata": {},
   "source": [
    "## 10. æ¨¡å‹è§£é‡‹\n",
    "\n",
    "ç†è§£æ¨¡å‹ç‚ºä½•åšå‡ºé€™æ¨£çš„é æ¸¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature-importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç²å–ç‰¹å¾µé‡è¦æ€§ (log probability)\n",
    "feature_names = best_vectorizer.get_feature_names_out()\n",
    "\n",
    "# ç²å–æ¯å€‹é¡åˆ¥çš„ log probability\n",
    "spam_idx = list(best_model.classes_).index('spam')\n",
    "ham_idx = list(best_model.classes_).index('ham')\n",
    "\n",
    "spam_log_probs = best_model.feature_log_prob_[spam_idx]\n",
    "ham_log_probs = best_model.feature_log_prob_[ham_idx]\n",
    "\n",
    "# è¨ˆç®— log odds ratio\n",
    "log_odds = spam_log_probs - ham_log_probs\n",
    "\n",
    "# æ‰¾å‡ºæœ€å¼·çš„ spam æŒ‡æ¨™\n",
    "top_spam_indices = log_odds.argsort()[-20:][::-1]\n",
    "# æ‰¾å‡ºæœ€å¼·çš„ ham æŒ‡æ¨™\n",
    "top_ham_indices = log_odds.argsort()[:20]\n",
    "\n",
    "print(\"ğŸ”´ Top 20 SPAM æŒ‡æ¨™è©:\")\n",
    "print(\"=\"*60)\n",
    "for idx in top_spam_indices:\n",
    "    print(f\"{feature_names[idx]:20s} â†’ log odds = {log_odds[idx]:.4f}\")\n",
    "\n",
    "print(\"\\nğŸŸ¢ Top 20 HAM æŒ‡æ¨™è©:\")\n",
    "print(\"=\"*60)\n",
    "for idx in top_ham_indices:\n",
    "    print(f\"{feature_names[idx]:20s} â†’ log odds = {log_odds[idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è¦–åŒ–ç‰¹å¾µé‡è¦æ€§\n",
    "top_n = 15\n",
    "top_spam_idx = log_odds.argsort()[-top_n:][::-1]\n",
    "top_ham_idx = log_odds.argsort()[:top_n]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Spam ç‰¹å¾µ\n",
    "spam_words = [feature_names[i] for i in top_spam_idx]\n",
    "spam_scores = [log_odds[i] for i in top_spam_idx]\n",
    "axes[0].barh(spam_words, spam_scores, color='#ff6b6b')\n",
    "axes[0].set_xlabel('Log Odds Ratio', fontsize=12)\n",
    "axes[0].set_title('Top 15 SPAM Indicators', fontsize=14, fontweight='bold')\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "# Ham ç‰¹å¾µ\n",
    "ham_words = [feature_names[i] for i in top_ham_idx]\n",
    "ham_scores = [log_odds[i] for i in top_ham_idx]\n",
    "axes[1].barh(ham_words, ham_scores, color='#4ecdc4')\n",
    "axes[1].set_xlabel('Log Odds Ratio', fontsize=12)\n",
    "axes[1].set_title('Top 15 HAM Indicators', fontsize=14, fontweight='bold')\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cross-validation-header",
   "metadata": {},
   "source": [
    "## 11. äº¤å‰é©—è­‰ (Cross-Validation)\n",
    "\n",
    "ä½¿ç”¨ 5-fold äº¤å‰é©—è­‰è©•ä¼°æ¨¡å‹ç©©å®šæ€§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-validation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# å»ºç«‹å®Œæ•´çš„ pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', best_vectorizer),\n",
    "    ('classifier', MultinomialNB(alpha=1.0))\n",
    "])\n",
    "\n",
    "# æº–å‚™å®Œæ•´æ•¸æ“š\n",
    "X_full = df['cleaned_message']\n",
    "y_full = df['label']\n",
    "\n",
    "# 5-fold äº¤å‰é©—è­‰\n",
    "print(\"åŸ·è¡Œ 5-fold äº¤å‰é©—è­‰...\")\n",
    "cv_scores = cross_val_score(pipeline, X_full, y_full, cv=5, scoring='f1', \n",
    "                             n_jobs=-1)\n",
    "\n",
    "print(\"\\näº¤å‰é©—è­‰çµæœ (F1 Score):\")\n",
    "print(\"=\"*60)\n",
    "for i, score in enumerate(cv_scores, 1):\n",
    "    print(f\"Fold {i}: {score:.4f} ({score:.2%})\")\n",
    "print(f\"\\nå¹³å‡ F1 Score: {cv_scores.mean():.4f} ({cv_scores.mean():.2%})\")\n",
    "print(f\"æ¨™æº–å·®: {cv_scores.std():.4f}\")\n",
    "\n",
    "# å¯è¦–åŒ–\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, 6), cv_scores, marker='o', linestyle='-', linewidth=2, \n",
    "         markersize=10, color='#4ecdc4')\n",
    "plt.axhline(y=cv_scores.mean(), color='#ff6b6b', linestyle='--', \n",
    "            linewidth=2, label=f'Mean: {cv_scores.mean():.4f}')\n",
    "plt.xlabel('Fold', fontsize=12)\n",
    "plt.ylabel('F1 Score', fontsize=12)\n",
    "plt.title('5-Fold Cross-Validation Results', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… æ¨¡å‹è¡¨ç¾ç©©å®š,æ¨™æº–å·®å¾ˆå°\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## 12. å°ˆæ¡ˆç¸½çµ\n",
    "\n",
    "### âœ… å®Œæˆçš„å·¥ä½œ\n",
    "\n",
    "1. **æ•¸æ“šåˆ†æ**:\n",
    "   - è¼‰å…¥ä¸¦åˆ†æ SMS Spam Collection æ•¸æ“šé›†\n",
    "   - æ¢ç´¢é¡åˆ¥åˆ†ä½ˆå’Œæ–‡æœ¬ç‰¹å¾µ\n",
    "   - è­˜åˆ¥ spam å’Œ ham çš„é—œéµè©\n",
    "\n",
    "2. **æ–‡æœ¬é è™•ç†**:\n",
    "   - å°å¯«è½‰æ›\n",
    "   - ç§»é™¤ URLã€æ¨™é»ã€æ•¸å­—\n",
    "   - (å¯é¸) ç§»é™¤åœç”¨è©\n",
    "\n",
    "3. **ç‰¹å¾µå·¥ç¨‹**:\n",
    "   - Bag of Words (BoW)\n",
    "   - TF-IDF\n",
    "   - N-gram ç‰¹å¾µ (unigram + bigram)\n",
    "\n",
    "4. **æ¨¡å‹è¨“ç·´èˆ‡è©•ä¼°**:\n",
    "   - è¨“ç·´ Multinomial Naive Bayes\n",
    "   - é”åˆ° 95%+ çš„æº–ç¢ºç‡\n",
    "   - äº¤å‰é©—è­‰ç¢ºä¿ç©©å®šæ€§\n",
    "\n",
    "5. **éŒ¯èª¤åˆ†æ**:\n",
    "   - åˆ†æ False Positives å’Œ False Negatives\n",
    "   - ç†è§£æ¨¡å‹çš„å¼·é …å’Œå¼±é»\n",
    "\n",
    "6. **å¯¦éš›æ‡‰ç”¨**:\n",
    "   - å»ºç«‹é æ¸¬å‡½æ•¸\n",
    "   - å¯è™•ç†æ–°çš„æœªè¦‹éçš„ç°¡è¨Š\n",
    "\n",
    "### ğŸ“Š é—œéµç™¼ç¾\n",
    "\n",
    "- **TF-IDF é€šå¸¸å„ªæ–¼ BoW**: å› ç‚ºè€ƒæ…®äº†è©çš„é‡è¦æ€§\n",
    "- **Spam ç‰¹å¾µæ˜é¡¯**: åŒ…å« \"free\", \"win\", \"call\", \"prize\" ç­‰ä¿ƒéŠ·è©å½™\n",
    "- **æ¨¡å‹ç©©å®š**: äº¤å‰é©—è­‰æ¨™æº–å·®å°,æ³›åŒ–èƒ½åŠ›å¥½\n",
    "- **ä¸å¹³è¡¡æ•¸æ“š**: Spam åƒ…ä½” 13%,éœ€æ³¨æ„è©•ä¼°æŒ‡æ¨™é¸æ“‡\n",
    "\n",
    "### ğŸš€ å¯èƒ½çš„æ”¹é€²æ–¹å‘\n",
    "\n",
    "1. **ç‰¹å¾µå·¥ç¨‹**:\n",
    "   - åŠ å…¥æ–‡æœ¬é•·åº¦ç‰¹å¾µ\n",
    "   - çµ±è¨ˆå¤§å¯«å­—æ¯æ¯”ä¾‹\n",
    "   - çµ±è¨ˆç‰¹æ®Šå­—ç¬¦æ•¸é‡\n",
    "\n",
    "2. **æ¨¡å‹å„ªåŒ–**:\n",
    "   - èª¿æ•´ alpha åƒæ•¸ (Laplace smoothing)\n",
    "   - å˜—è©¦å…¶ä»–åˆ†é¡å™¨ (SVM, Logistic Regression)\n",
    "   - é›†æˆå­¸ç¿’ (Ensemble methods)\n",
    "\n",
    "3. **è™•ç†ä¸å¹³è¡¡**:\n",
    "   - éæ¡æ¨£ (SMOTE)\n",
    "   - èª¿æ•´é¡åˆ¥æ¬Šé‡\n",
    "   - ä½¿ç”¨å°ˆé–€çš„ä¸å¹³è¡¡æ•¸æ“šæŠ€è¡“\n",
    "\n",
    "4. **éƒ¨ç½²è€ƒé‡**:\n",
    "   - æ¨¡å‹å£“ç¸® (é™ä½ç‰¹å¾µç¶­åº¦)\n",
    "   - æ¨ç†é€Ÿåº¦å„ªåŒ–\n",
    "   - å»ºç«‹ç›£æ§ç³»çµ±è¿½è¹¤ç·šä¸Šè¡¨ç¾\n",
    "\n",
    "### ğŸ’¡ å­¸åˆ°çš„ç¶“é©—\n",
    "\n",
    "1. **æ•¸æ“šæ¢ç´¢å¾ˆé‡è¦**: EDA å¹«åŠ©æˆ‘å€‘ç†è§£æ•¸æ“šç‰¹æ€§\n",
    "2. **é è™•ç†å½±éŸ¿å¤§**: é©ç•¶çš„æ–‡æœ¬æ¸…ç†èƒ½é¡¯è‘—æå‡æ€§èƒ½\n",
    "3. **è©•ä¼°è¦å…¨é¢**: ä¸èƒ½åªçœ‹ Accuracy,é‚„è¦çœ‹ Precision/Recall\n",
    "4. **éŒ¯èª¤åˆ†ææœ‰åƒ¹å€¼**: ç†è§£æ¨¡å‹å¼±é»æ‰èƒ½é‡å°æ€§æ”¹é€²\n",
    "\n",
    "---\n",
    "\n",
    "## 13. èª²å¾Œç·´ç¿’\n",
    "\n",
    "### ç·´ç¿’ 1: ç‰¹å¾µå·¥ç¨‹\n",
    "\n",
    "å˜—è©¦æ·»åŠ ä»¥ä¸‹é¡å¤–ç‰¹å¾µ:\n",
    "- æ–‡æœ¬é•·åº¦\n",
    "- å¤§å¯«å­—æ¯æ¯”ä¾‹\n",
    "- æ•¸å­—æ•¸é‡\n",
    "- ç‰¹æ®Šå­—ç¬¦æ•¸é‡\n",
    "\n",
    "çœ‹çœ‹æ˜¯å¦èƒ½æå‡æ¨¡å‹æ€§èƒ½ã€‚\n",
    "\n",
    "### ç·´ç¿’ 2: å…¶ä»–åˆ†é¡å™¨\n",
    "\n",
    "å˜—è©¦ä½¿ç”¨ä»¥ä¸‹åˆ†é¡å™¨,æ¯”è¼ƒæ€§èƒ½:\n",
    "- Logistic Regression\n",
    "- Support Vector Machine (SVM)\n",
    "- Random Forest\n",
    "\n",
    "### ç·´ç¿’ 3: è™•ç†ä¸­æ–‡ç°¡è¨Š\n",
    "\n",
    "æ”¶é›†ä¸­æ–‡åƒåœ¾ç°¡è¨Šæ•¸æ“š,æ‡‰ç”¨ç›¸åŒçš„æµç¨‹:\n",
    "- ä½¿ç”¨ jieba åˆ†è©\n",
    "- è¼‰å…¥ä¸­æ–‡åœç”¨è©\n",
    "- è¨“ç·´ä¸­æ–‡åƒåœ¾ç°¡è¨Šåˆ†é¡å™¨\n",
    "\n",
    "### ç·´ç¿’ 4: æ¨¡å‹éƒ¨ç½²\n",
    "\n",
    "å°‡è¨“ç·´å¥½çš„æ¨¡å‹éƒ¨ç½²ç‚º REST API:\n",
    "- ä½¿ç”¨ Flask æˆ– FastAPI å»ºç«‹ Web æœå‹™\n",
    "- æä¾› `/predict` ç«¯é»æ¥æ”¶ç°¡è¨Š\n",
    "- è¿”å›é æ¸¬çµæœå’Œä¿¡å¿ƒåº¦\n",
    "\n",
    "---\n",
    "\n",
    "**èª²ç¨‹**: iSpan Python NLP Cookbooks v2\n",
    "**å°ˆæ¡ˆ**: åƒåœ¾éƒµä»¶åˆ†é¡å™¨\n",
    "**æœ€å¾Œæ›´æ–°**: 2025-10-17\n",
    "\n",
    "**ç¥è³€ä½ å®Œæˆé€™å€‹å®Œæ•´çš„å¯¦æˆ°å°ˆæ¡ˆ! ğŸ‰**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
