{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# åº•å±¤å¯¦ä½œ01: å¾é›¶æ‰“é€  Naive Bayes åˆ†é¡å™¨\n",
    "\n",
    "**èª²ç¨‹**: iSpan Python NLP Cookbooks v2\n",
    "**ç« ç¯€**: åº•å±¤å¯¦ä½œç³»åˆ—\n",
    "**ç‰ˆæœ¬**: v1.0\n",
    "**æ›´æ–°æ—¥æœŸ**: 2025-10-17\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š æœ¬ç¯€å­¸ç¿’ç›®æ¨™\n",
    "\n",
    "1. å¾æ•¸å­¸åŸç†æ¨å° Naive Bayes ç®—æ³•\n",
    "2. ç†è§£è²è‘‰æ–¯å®šç†èˆ‡æ¢ä»¶ç¨ç«‹å‡è¨­\n",
    "3. å¾é›¶å¯¦ä½œ Multinomial Naive Bayes\n",
    "4. æŒæ¡ Laplace Smoothing æŠ€å·§\n",
    "5. èˆ‡ scikit-learn ç‰ˆæœ¬å°æ¯”é©—è­‰\n",
    "\n",
    "---\n",
    "\n",
    "## 1. è²è‘‰æ–¯å®šç†æ•¸å­¸æ¨å°\n",
    "\n",
    "### 1.1 è²è‘‰æ–¯å®šç† (Bayes' Theorem)\n",
    "\n",
    "**å®šç¾©**: æè¿°åœ¨å·²çŸ¥æŸäº›æ¢ä»¶ä¸‹,äº‹ä»¶ç™¼ç”Ÿçš„æ©Ÿç‡\n",
    "\n",
    "```\n",
    "P(A|B) = P(B|A) * P(A) / P(B)\n",
    "\n",
    "å…¶ä¸­:\n",
    "- P(A|B): å¾Œé©—æ©Ÿç‡ (Posterior) - åœ¨ B ç™¼ç”Ÿæ¢ä»¶ä¸‹ A çš„æ©Ÿç‡\n",
    "- P(B|A): ä¼¼ç„¶ (Likelihood) - åœ¨ A ç™¼ç”Ÿæ¢ä»¶ä¸‹ B çš„æ©Ÿç‡\n",
    "- P(A):   å…ˆé©—æ©Ÿç‡ (Prior) - A ç™¼ç”Ÿçš„æ©Ÿç‡\n",
    "- P(B):   è­‰æ“š (Evidence) - B ç™¼ç”Ÿçš„æ©Ÿç‡\n",
    "```\n",
    "\n",
    "### 1.2 æ‡‰ç”¨åˆ°æ–‡æœ¬åˆ†é¡\n",
    "\n",
    "**ç›®æ¨™**: çµ¦å®šæ–‡æœ¬ D,é æ¸¬é¡åˆ¥ C\n",
    "\n",
    "```\n",
    "P(C|D) = P(D|C) * P(C) / P(D)\n",
    "\n",
    "ç°¡åŒ– (å› ç‚º P(D) å°æ‰€æœ‰é¡åˆ¥ç›¸åŒ):\n",
    "C_pred = argmax_C [ P(D|C) * P(C) ]\n",
    "\n",
    "å±•é–‹æ–‡æœ¬ D = {w1, w2, ..., wn} (è©åºåˆ—):\n",
    "C_pred = argmax_C [ P(w1, w2, ..., wn | C) * P(C) ]\n",
    "```\n",
    "\n",
    "### 1.3 æ¢ä»¶ç¨ç«‹å‡è¨­ (Naive Assumption)\n",
    "\n",
    "**å‡è¨­**: æ¯å€‹è©å½™åœ¨é¡åˆ¥ C ä¸‹**ç›¸äº’ç¨ç«‹**\n",
    "\n",
    "```\n",
    "P(w1, w2, ..., wn | C) = P(w1|C) * P(w2|C) * ... * P(wn|C)\n",
    "\n",
    "ç°¡åŒ–ç‚º:\n",
    "P(w1, w2, ..., wn | C) = âˆ P(wi|C)  (i=1 to n)\n",
    "\n",
    "æœ€çµ‚åˆ†é¡å…¬å¼:\n",
    "C_pred = argmax_C [ P(C) * âˆ P(wi|C) ]\n",
    "\n",
    "å–å°æ•¸é¿å…ä¸‹æº¢ (underflow):\n",
    "C_pred = argmax_C [ log P(C) + Î£ log P(wi|C) ]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# è¦–è¦ºåŒ–è²è‘‰æ–¯å®šç†\n",
    "def visualize_bayes_theorem():\n",
    "    # æ¨¡æ“¬æ•¸æ“š: åƒåœ¾éƒµä»¶ vs æ­£å¸¸éƒµä»¶\n",
    "    categories = ['Spam', 'Ham']\n",
    "    priors = [0.3, 0.7]  # P(Spam)=0.3, P(Ham)=0.7\n",
    "    \n",
    "    # ä¼¼ç„¶: P(åŒ…å«\"free\"|é¡åˆ¥)\n",
    "    likelihoods = [0.8, 0.2]  # Spamä¸­80%æœ‰\"free\", Hamä¸­20%æœ‰\"free\"\n",
    "    \n",
    "    # è¨ˆç®—å¾Œé©—æ©Ÿç‡ P(é¡åˆ¥|\"free\")\n",
    "    evidence = sum(p * l for p, l in zip(priors, likelihoods))\n",
    "    posteriors = [(l * p) / evidence for p, l in zip(priors, likelihoods)]\n",
    "    \n",
    "    # ç¹ªåœ–\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    axes[0].bar(categories, priors, color=['#ff6b6b', '#4ecdc4'])\n",
    "    axes[0].set_title('Prior P(C)', fontsize=14)\n",
    "    axes[0].set_ylabel('Probability')\n",
    "    \n",
    "    axes[1].bar(categories, likelihoods, color=['#ff6b6b', '#4ecdc4'])\n",
    "    axes[1].set_title('Likelihood P(\"free\"|C)', fontsize=14)\n",
    "    axes[1].set_ylabel('Probability')\n",
    "    \n",
    "    axes[2].bar(categories, posteriors, color=['#ff6b6b', '#4ecdc4'])\n",
    "    axes[2].set_title('Posterior P(C|\"free\")', fontsize=14)\n",
    "    axes[2].set_ylabel('Probability')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"è²è‘‰æ–¯æ›´æ–°éç¨‹:\")\n",
    "    print(f\"Prior:     Spam={priors[0]:.2f}, Ham={priors[1]:.2f}\")\n",
    "    print(f\"Posterior: Spam={posteriors[0]:.2f}, Ham={posteriors[1]:.2f}\")\n",
    "    print(f\"\\nçµè«–: çœ‹åˆ°'free'å¾Œ,Spamæ©Ÿç‡å¾ {priors[0]:.0%} ä¸Šå‡åˆ° {posteriors[0]:.0%}\")\n",
    "\n",
    "visualize_bayes_theorem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Multinomial Naive Bayes å¯¦ä½œ\n",
    "\n",
    "### 2.1 ç®—æ³•æ­¥é©Ÿ\n",
    "\n",
    "**è¨“ç·´éšæ®µ**:\n",
    "1. è¨ˆç®—æ¯å€‹é¡åˆ¥çš„å…ˆé©—æ©Ÿç‡ P(C)\n",
    "2. è¨ˆç®—æ¯å€‹è©åœ¨æ¯å€‹é¡åˆ¥ä¸­çš„æ¢ä»¶æ©Ÿç‡ P(w|C)\n",
    "3. æ‡‰ç”¨ Laplace Smoothing é¿å…é›¶æ©Ÿç‡\n",
    "\n",
    "**é æ¸¬éšæ®µ**:\n",
    "1. å°æ–°æ–‡æœ¬è¨ˆç®— log P(C) + Î£ log P(wi|C)\n",
    "2. é¸æ“‡åˆ†æ•¸æœ€é«˜çš„é¡åˆ¥\n",
    "\n",
    "### 2.2 å¾é›¶å¯¦ä½œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier:\n",
    "    def __init__(self, alpha=1.0):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        alpha : float\n",
    "            Laplace smoothing åƒæ•¸ (åŠ æ³•å¹³æ»‘)\n",
    "        \"\"\"\n",
    "        self.alpha = alpha\n",
    "        self.class_log_prior = {}     # log P(C)\n",
    "        self.feature_log_prob = {}    # log P(w|C)\n",
    "        self.classes = None\n",
    "        self.vocabulary = set()\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        è¨“ç·´æ¨¡å‹\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : list of str\n",
    "            è¨“ç·´æ–‡æœ¬åˆ—è¡¨\n",
    "        y : list of str/int\n",
    "            è¨“ç·´æ¨™ç±¤åˆ—è¡¨\n",
    "        \"\"\"\n",
    "        n_samples = len(X)\n",
    "        self.classes = np.unique(y)\n",
    "        \n",
    "        # å»ºç«‹è©å½™è¡¨\n",
    "        for text in X:\n",
    "            self.vocabulary.update(text.lower().split())\n",
    "        \n",
    "        vocab_size = len(self.vocabulary)\n",
    "        \n",
    "        # å°æ¯å€‹é¡åˆ¥è¨ˆç®—çµ±è¨ˆé‡\n",
    "        for c in self.classes:\n",
    "            # ç²å–è©²é¡åˆ¥çš„æ‰€æœ‰æ–‡æœ¬\n",
    "            c_texts = [X[i] for i in range(n_samples) if y[i] == c]\n",
    "            n_c = len(c_texts)\n",
    "            \n",
    "            # è¨ˆç®—å…ˆé©—æ©Ÿç‡ P(C)\n",
    "            self.class_log_prior[c] = np.log(n_c / n_samples)\n",
    "            \n",
    "            # çµ±è¨ˆè©é »\n",
    "            word_counts = defaultdict(int)\n",
    "            total_words = 0\n",
    "            \n",
    "            for text in c_texts:\n",
    "                words = text.lower().split()\n",
    "                for word in words:\n",
    "                    word_counts[word] += 1\n",
    "                    total_words += 1\n",
    "            \n",
    "            # è¨ˆç®—æ¢ä»¶æ©Ÿç‡ P(w|C) with Laplace Smoothing\n",
    "            self.feature_log_prob[c] = {}\n",
    "            for word in self.vocabulary:\n",
    "                count = word_counts[word]\n",
    "                # Laplace Smoothing: (count + alpha) / (total + alpha * vocab_size)\n",
    "                prob = (count + self.alpha) / (total_words + self.alpha * vocab_size)\n",
    "                self.feature_log_prob[c][word] = np.log(prob)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        é æ¸¬æ–°æ–‡æœ¬\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : list of str\n",
    "            å¾…é æ¸¬æ–‡æœ¬åˆ—è¡¨\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        predictions : list\n",
    "            é æ¸¬çš„é¡åˆ¥åˆ—è¡¨\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        \n",
    "        for text in X:\n",
    "            words = text.lower().split()\n",
    "            class_scores = {}\n",
    "            \n",
    "            for c in self.classes:\n",
    "                # åˆå§‹åŒ–ç‚º log P(C)\n",
    "                score = self.class_log_prior[c]\n",
    "                \n",
    "                # ç´¯åŠ  Î£ log P(wi|C)\n",
    "                for word in words:\n",
    "                    if word in self.vocabulary:\n",
    "                        score += self.feature_log_prob[c][word]\n",
    "                \n",
    "                class_scores[c] = score\n",
    "            \n",
    "            # é¸æ“‡åˆ†æ•¸æœ€é«˜çš„é¡åˆ¥\n",
    "            predicted_class = max(class_scores, key=class_scores.get)\n",
    "            predictions.append(predicted_class)\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        é æ¸¬æ©Ÿç‡\n",
    "        \"\"\"\n",
    "        probabilities = []\n",
    "        \n",
    "        for text in X:\n",
    "            words = text.lower().split()\n",
    "            class_scores = {}\n",
    "            \n",
    "            for c in self.classes:\n",
    "                score = self.class_log_prior[c]\n",
    "                for word in words:\n",
    "                    if word in self.vocabulary:\n",
    "                        score += self.feature_log_prob[c][word]\n",
    "                class_scores[c] = score\n",
    "            \n",
    "            # è½‰æ›ç‚ºæ©Ÿç‡ (softmax)\n",
    "            scores = np.array(list(class_scores.values()))\n",
    "            exp_scores = np.exp(scores - np.max(scores))  # æ•¸å€¼ç©©å®š\n",
    "            probs = exp_scores / exp_scores.sum()\n",
    "            \n",
    "            probabilities.append(dict(zip(class_scores.keys(), probs)))\n",
    "        \n",
    "        return probabilities\n",
    "\n",
    "# æ¸¬è©¦å¯¦ä½œ\n",
    "print(\"âœ… Naive Bayes åˆ†é¡å™¨å¯¦ä½œå®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Laplace Smoothing æ·±å…¥ç†è§£\n",
    "\n",
    "### 3.1 é›¶æ©Ÿç‡å•é¡Œ\n",
    "\n",
    "**å•é¡Œ**: å¦‚æœè¨“ç·´é›†ä¸­æŸå€‹è©å¾æœªåœ¨æŸé¡åˆ¥å‡ºç¾,å‰‡ P(w|C) = 0\n",
    "\n",
    "```\n",
    "è¨“ç·´é›†:\n",
    "Spam: \"buy free now\"\n",
    "Ham:  \"hello friend\"\n",
    "\n",
    "æ¸¬è©¦: \"free hello\"\n",
    "P(Spam|\"free hello\") = P(Spam) * P(free|Spam) * P(hello|Spam)\n",
    "                     = 0.5 * 0.33 * 0  â† hello å¾æœªåœ¨ Spam å‡ºç¾\n",
    "                     = 0  â† æ•´å€‹æ©Ÿç‡è®Š 0!\n",
    "```\n",
    "\n",
    "### 3.2 Laplace Smoothing è§£æ±ºæ–¹æ¡ˆ\n",
    "\n",
    "**å…¬å¼**:\n",
    "```\n",
    "P(wi|C) = (count(wi, C) + Î±) / (count(C) + Î± * |V|)\n",
    "\n",
    "å…¶ä¸­:\n",
    "- Î±: smoothing åƒæ•¸ (é€šå¸¸ç‚º 1)\n",
    "- |V|: è©å½™è¡¨å¤§å°\n",
    "- count(wi, C): è© wi åœ¨é¡åˆ¥ C ä¸­å‡ºç¾æ¬¡æ•¸\n",
    "- count(C): é¡åˆ¥ C çš„ç¸½è©æ•¸\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¼”ç¤º Laplace Smoothing æ•ˆæœ\n",
    "def compare_smoothing():\n",
    "    # æ¨¡æ“¬æ•¸æ“š\n",
    "    word_counts = {'buy': 2, 'free': 3, 'now': 1}  # Spam è©é »\n",
    "    total_words = sum(word_counts.values())  # 6\n",
    "    vocab_size = 10  # å‡è¨­è©å½™è¡¨æœ‰ 10 å€‹è©\n",
    "    \n",
    "    test_word = 'hello'  # è¨“ç·´é›†ä¸­æœªå‡ºç¾çš„è©\n",
    "    \n",
    "    # ä¸ä½¿ç”¨ smoothing\n",
    "    prob_no_smooth = word_counts.get(test_word, 0) / total_words\n",
    "    \n",
    "    # ä½¿ç”¨ Laplace smoothing (Î±=1)\n",
    "    prob_smooth = (word_counts.get(test_word, 0) + 1) / (total_words + vocab_size)\n",
    "    \n",
    "    print(\"Laplace Smoothing æ•ˆæœå°æ¯”:\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"è©å½™: '{test_word}' (è¨“ç·´é›†ä¸­æœªå‡ºç¾)\")\n",
    "    print(f\"\\nä¸ä½¿ç”¨ Smoothing:\")\n",
    "    print(f\"  P('{test_word}'|Spam) = {word_counts.get(test_word, 0)}/{total_words} = {prob_no_smooth}\")\n",
    "    print(f\"\\nä½¿ç”¨ Laplace Smoothing (Î±=1):\")\n",
    "    print(f\"  P('{test_word}'|Spam) = ({word_counts.get(test_word, 0)}+1)/({total_words}+{vocab_size}) = {prob_smooth:.4f}\")\n",
    "    print(f\"\\nçµæœ: Smoothing é¿å…äº†é›¶æ©Ÿç‡å•é¡Œ!\")\n",
    "\n",
    "compare_smoothing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. å¯¦æˆ°æ¡ˆä¾‹: åƒåœ¾éƒµä»¶åˆ†é¡\n",
    "\n",
    "### 4.1 æº–å‚™æ•¸æ“šé›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åƒåœ¾éƒµä»¶åˆ†é¡æ•¸æ“šé›†\n",
    "spam_emails = [\n",
    "    \"free money now\",\n",
    "    \"win prizes click here\",\n",
    "    \"buy cheap products\",\n",
    "    \"get rich quick\",\n",
    "    \"earn money fast\",\n",
    "    \"free gift click now\",\n",
    "    \"win big prizes today\",\n",
    "    \"make money online\"\n",
    "]\n",
    "\n",
    "ham_emails = [\n",
    "    \"hello friend how are you\",\n",
    "    \"meeting tomorrow at noon\",\n",
    "    \"please review the document\",\n",
    "    \"thanks for your help\",\n",
    "    \"see you next week\",\n",
    "    \"happy birthday dear friend\",\n",
    "    \"project deadline is friday\",\n",
    "    \"let me know your thoughts\"\n",
    "]\n",
    "\n",
    "# åˆä½µæ•¸æ“š\n",
    "X_train = spam_emails + ham_emails\n",
    "y_train = ['spam'] * len(spam_emails) + ['ham'] * len(ham_emails)\n",
    "\n",
    "print(f\"è¨“ç·´é›†å¤§å°: {len(X_train)}\")\n",
    "print(f\"Spam æ¨£æœ¬: {y_train.count('spam')}\")\n",
    "print(f\"Ham æ¨£æœ¬:  {y_train.count('ham')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 è¨“ç·´èˆ‡é æ¸¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨“ç·´æ¨¡å‹\n",
    "nb_classifier = NaiveBayesClassifier(alpha=1.0)\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# æ¸¬è©¦æ•¸æ“š\n",
    "X_test = [\n",
    "    \"free money click here\",      # æ‡‰è©²æ˜¯ spam\n",
    "    \"meeting next week\",           # æ‡‰è©²æ˜¯ ham\n",
    "    \"win big prizes now\",          # æ‡‰è©²æ˜¯ spam\n",
    "    \"thanks for the document\",     # æ‡‰è©²æ˜¯ ham\n",
    "    \"buy free products\",           # æ‡‰è©²æ˜¯ spam\n",
    "]\n",
    "\n",
    "# é æ¸¬\n",
    "predictions = nb_classifier.predict(X_test)\n",
    "probabilities = nb_classifier.predict_proba(X_test)\n",
    "\n",
    "# é¡¯ç¤ºçµæœ\n",
    "print(\"\\né æ¸¬çµæœ:\")\n",
    "print(\"=\"*70)\n",
    "for i, (text, pred, prob) in enumerate(zip(X_test, predictions, probabilities), 1):\n",
    "    print(f\"{i}. æ–‡æœ¬: '{text}'\")\n",
    "    print(f\"   é æ¸¬: {pred.upper()}\")\n",
    "    print(f\"   æ©Ÿç‡: Spam={prob['spam']:.2%}, Ham={prob['ham']:.2%}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 æŸ¥çœ‹å­¸ç¿’åˆ°çš„ç‰¹å¾µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ‰¾å‡ºæ¯å€‹é¡åˆ¥æœ€é‡è¦çš„è©\n",
    "def get_top_features(classifier, top_n=5):\n",
    "    for class_name in classifier.classes:\n",
    "        # ç²å–è©²é¡åˆ¥çš„è©æ©Ÿç‡\n",
    "        word_probs = classifier.feature_log_prob[class_name]\n",
    "        \n",
    "        # æ’åº (log æ©Ÿç‡è¶Šå¤§è¶Šé‡è¦)\n",
    "        sorted_words = sorted(word_probs.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        print(f\"\\n{class_name.upper()} æœ€é‡è¦çš„ {top_n} å€‹è©:\")\n",
    "        for word, log_prob in sorted_words[:top_n]:\n",
    "            prob = np.exp(log_prob)\n",
    "            print(f\"  {word:15s} â†’ P(w|{class_name}) = {prob:.4f}\")\n",
    "\n",
    "get_top_features(nb_classifier, top_n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. èˆ‡ scikit-learn å°æ¯”é©—è­‰\n",
    "\n",
    "### 5.1 ä½¿ç”¨ç›¸åŒæ•¸æ“šè¨“ç·´ sklearn ç‰ˆæœ¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# å»ºç«‹ sklearn Pipeline\n",
    "sklearn_pipeline = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('classifier', MultinomialNB(alpha=1.0))\n",
    "])\n",
    "\n",
    "# è¨“ç·´\n",
    "sklearn_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# é æ¸¬\n",
    "sklearn_predictions = sklearn_pipeline.predict(X_test)\n",
    "sklearn_proba = sklearn_pipeline.predict_proba(X_test)\n",
    "\n",
    "print(\"sklearn é æ¸¬çµæœ:\")\n",
    "print(\"=\"*70)\n",
    "classes = sklearn_pipeline.classes_\n",
    "for i, (text, pred, proba) in enumerate(zip(X_test, sklearn_predictions, sklearn_proba), 1):\n",
    "    print(f\"{i}. æ–‡æœ¬: '{text}'\")\n",
    "    print(f\"   é æ¸¬: {pred.upper()}\")\n",
    "    prob_dict = dict(zip(classes, proba))\n",
    "    print(f\"   æ©Ÿç‡: Spam={prob_dict.get('spam', 0):.2%}, Ham={prob_dict.get('ham', 0):.2%}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 çµæœå°æ¯”åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# å°æ¯”é æ¸¬çµæœ\n",
    "comparison_df = pd.DataFrame({\n",
    "    'æ–‡æœ¬': X_test,\n",
    "    'è‡ªè£½é æ¸¬': predictions,\n",
    "    'sklearné æ¸¬': sklearn_predictions,\n",
    "    'çµæœä¸€è‡´': [p1 == p2 for p1, p2 in zip(predictions, sklearn_predictions)]\n",
    "})\n",
    "\n",
    "print(\"\\né æ¸¬çµæœå°æ¯”:\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# è¨ˆç®—ä¸€è‡´ç‡\n",
    "accuracy = comparison_df['çµæœä¸€è‡´'].mean()\n",
    "print(f\"\\nä¸€è‡´ç‡: {accuracy:.0%}\")\n",
    "\n",
    "if accuracy == 1.0:\n",
    "    print(\"âœ… å®Œç¾! è‡ªè£½ç‰ˆæœ¬èˆ‡ sklearn é æ¸¬å®Œå…¨ä¸€è‡´\")\n",
    "else:\n",
    "    print(\"âš ï¸  å­˜åœ¨å·®ç•°,éœ€æª¢æŸ¥å¯¦ä½œç´°ç¯€\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. æ•ˆèƒ½åˆ†æèˆ‡å¯è¦–åŒ–\n",
    "\n",
    "### 6.1 æ··æ·†çŸ©é™£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "# åœ¨è¨“ç·´é›†ä¸Šè©•ä¼°\n",
    "train_predictions = nb_classifier.predict(X_train)\n",
    "\n",
    "# è¨ˆç®—æ··æ·†çŸ©é™£\n",
    "cm = confusion_matrix(y_train, train_predictions, labels=['spam', 'ham'])\n",
    "\n",
    "# ç¹ªè£½æ··æ·†çŸ©é™£\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Spam', 'Ham'],\n",
    "            yticklabels=['Spam', 'Ham'])\n",
    "plt.title('Confusion Matrix (Training Set)', fontsize=14)\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "# åˆ†é¡å ±å‘Š\n",
    "print(\"\\nåˆ†é¡å ±å‘Š (è¨“ç·´é›†):\")\n",
    "print(classification_report(y_train, train_predictions, target_names=['spam', 'ham']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 ç‰¹å¾µé‡è¦æ€§å¯è¦–åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¯”è¼ƒ Spam vs Ham çš„è©æ©Ÿç‡\n",
    "def visualize_feature_importance(classifier, top_n=10):\n",
    "    # ç²å–å…±åŒè©å½™\n",
    "    spam_probs = classifier.feature_log_prob['spam']\n",
    "    ham_probs = classifier.feature_log_prob['ham']\n",
    "    \n",
    "    # è¨ˆç®—å·®ç•° (log odds ratio)\n",
    "    word_scores = {}\n",
    "    for word in classifier.vocabulary:\n",
    "        spam_prob = np.exp(spam_probs[word])\n",
    "        ham_prob = np.exp(ham_probs[word])\n",
    "        # è¶Šæ­£è¡¨ç¤ºè¶Šåƒ Spam,è¶Šè² è¡¨ç¤ºè¶Šåƒ Ham\n",
    "        score = np.log(spam_prob / ham_prob)\n",
    "        word_scores[word] = score\n",
    "    \n",
    "    # æ’åº\n",
    "    sorted_words = sorted(word_scores.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "    \n",
    "    # å–å‰ top_n\n",
    "    words = [w for w, s in sorted_words[:top_n]]\n",
    "    scores = [s for w, s in sorted_words[:top_n]]\n",
    "    \n",
    "    # ç¹ªåœ–\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    colors = ['#ff6b6b' if s > 0 else '#4ecdc4' for s in scores]\n",
    "    plt.barh(words, scores, color=colors)\n",
    "    plt.xlabel('Log Odds Ratio (Spam vs Ham)', fontsize=12)\n",
    "    plt.title('Most Discriminative Words', fontsize=14)\n",
    "    plt.axvline(x=0, color='black', linestyle='--', linewidth=0.8)\n",
    "    \n",
    "    # æ·»åŠ åœ–ä¾‹\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [\n",
    "        Patch(facecolor='#ff6b6b', label='Spam'),\n",
    "        Patch(facecolor='#4ecdc4', label='Ham')\n",
    "    ]\n",
    "    plt.legend(handles=legend_elements, loc='best')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_feature_importance(nb_classifier, top_n=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. é€²éš: è™•ç†çœŸå¯¦æ•¸æ“šé›†\n",
    "\n",
    "### 7.1 ä½¿ç”¨ SMS Spam Collection æ•¸æ“šé›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¦‚æœæœ‰ SMS Spam æ•¸æ“šé›†\n",
    "try:\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # å˜—è©¦è¼‰å…¥æ•¸æ“š\n",
    "    # df = pd.read_csv('../../datasets/sms_spam/SMSSpamCollection', \n",
    "    #                  sep='\\t', names=['label', 'message'])\n",
    "    \n",
    "    # æš«æ™‚ä½¿ç”¨æ¨¡æ“¬æ•¸æ“š\n",
    "    df = pd.DataFrame({\n",
    "        'label': ['spam', 'ham'] * 50,\n",
    "        'message': ['free money now'] * 50 + ['hello friend'] * 50\n",
    "    })\n",
    "    \n",
    "    # åˆ‡åˆ†è¨“ç·´é›†èˆ‡æ¸¬è©¦é›†\n",
    "    X = df['message'].tolist()\n",
    "    y = df['label'].tolist()\n",
    "    \n",
    "    X_train_real, X_test_real, y_train_real, y_test_real = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # è¨“ç·´\n",
    "    nb_real = NaiveBayesClassifier(alpha=1.0)\n",
    "    nb_real.fit(X_train_real, y_train_real)\n",
    "    \n",
    "    # é æ¸¬\n",
    "    y_pred_real = nb_real.predict(X_test_real)\n",
    "    \n",
    "    # è©•ä¼°\n",
    "    from sklearn.metrics import accuracy_score, f1_score\n",
    "    \n",
    "    accuracy = accuracy_score(y_test_real, y_pred_real)\n",
    "    f1 = f1_score(y_test_real, y_pred_real, pos_label='spam')\n",
    "    \n",
    "    print(\"çœŸå¯¦æ•¸æ“šé›†è©•ä¼°çµæœ:\")\n",
    "    print(f\"æº–ç¢ºç‡: {accuracy:.2%}\")\n",
    "    print(f\"F1 åˆ†æ•¸: {f1:.2%}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"ç„¡æ³•è¼‰å…¥çœŸå¯¦æ•¸æ“šé›†: {e}\")\n",
    "    print(\"è«‹ç¢ºä¿æ•¸æ“šé›†ä½æ–¼æ­£ç¢ºè·¯å¾‘\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. èª²å¾Œç·´ç¿’\n",
    "\n",
    "### ç·´ç¿’ 1: å¯¦ä½œ Bernoulli Naive Bayes\n",
    "\n",
    "æç¤º: èˆ‡ Multinomial ä¸åŒ,Bernoulli åªè€ƒæ…®è©æ˜¯å¦å‡ºç¾ (0/1),è€Œéå‡ºç¾æ¬¡æ•¸ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: å¯¦ä½œ Bernoulli Naive Bayes\n",
    "class BernoulliNaiveBayes:\n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = alpha\n",
    "        # TODO: åˆå§‹åŒ–å…¶ä»–å¿…è¦åƒæ•¸\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # TODO: å¯¦ä½œè¨“ç·´é‚è¼¯\n",
    "        # æç¤º: P(w|C) = (å‡ºç¾è©²è©çš„æ–‡æª”æ•¸ + Î±) / (é¡åˆ¥ C çš„æ–‡æª”æ•¸ + 2Î±)\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # TODO: å¯¦ä½œé æ¸¬é‚è¼¯\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ç·´ç¿’ 2: è™•ç†ä¸­æ–‡æ–‡æœ¬\n",
    "\n",
    "ä½¿ç”¨ jieba åˆ†è©,å°‡ Naive Bayes æ‡‰ç”¨åˆ°ä¸­æ–‡åƒåœ¾ç°¡è¨Šåˆ†é¡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: ä¸­æ–‡æ–‡æœ¬åˆ†é¡\n",
    "import jieba\n",
    "\n",
    "# ä¸­æ–‡æ•¸æ“šé›†ç¯„ä¾‹\n",
    "chinese_spam = [\n",
    "    \"æ­å–œæ‚¨ä¸­çäº†,è«‹é»æ“Šé€£çµé ˜å–\",\n",
    "    \"å…è²»è´ˆé€ç¦®å“,é¦¬ä¸Šé ˜å–\"\n",
    "]\n",
    "\n",
    "chinese_ham = [\n",
    "    \"æ˜å¤©æœƒè­°æ™‚é–“æ”¹åˆ°ä¸‹åˆä¸‰é»\",\n",
    "    \"é€±æœ«ä¸€èµ·å»åƒé£¯å§\"\n",
    "]\n",
    "\n",
    "# TODO: ä½¿ç”¨ jieba åˆ†è© + Naive Bayes åˆ†é¡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. æœ¬ç¯€ç¸½çµ\n",
    "\n",
    "### âœ… é—œéµè¦é»\n",
    "\n",
    "1. **è²è‘‰æ–¯å®šç†**: P(C|D) = P(D|C) * P(C) / P(D)\n",
    "   - å¾Œé©—æ©Ÿç‡ = (ä¼¼ç„¶ Ã— å…ˆé©—) / è­‰æ“š\n",
    "\n",
    "2. **æ¢ä»¶ç¨ç«‹å‡è¨­**: P(w1, w2, ..., wn | C) = âˆ P(wi|C)\n",
    "   - ç°¡åŒ–è¨ˆç®—,ä½†å¯èƒ½æå¤±è©åºä¿¡æ¯\n",
    "\n",
    "3. **Laplace Smoothing**: (count + Î±) / (total + Î±|V|)\n",
    "   - é¿å…é›¶æ©Ÿç‡å•é¡Œ\n",
    "   - Î±=1 ç¨±ç‚ºåŠ ä¸€å¹³æ»‘\n",
    "\n",
    "4. **å°æ•¸æŠ€å·§**: ä½¿ç”¨ log é¿å…æ•¸å€¼ä¸‹æº¢\n",
    "   - log P(C|D) = log P(C) + Î£ log P(wi|C)\n",
    "\n",
    "5. **å¯¦ä½œé©—è­‰**: èˆ‡ sklearn å°æ¯”ç¢ºä¿æ­£ç¢ºæ€§\n",
    "\n",
    "### ğŸ“Š å„ªç¼ºé»åˆ†æ\n",
    "\n",
    "**å„ªé»**:\n",
    "- âœ… å¯¦ä½œç°¡å–®,è¨ˆç®—é«˜æ•ˆ\n",
    "- âœ… å°å°æ•¸æ“šé›†è¡¨ç¾è‰¯å¥½\n",
    "- âœ… å¯è§£é‡‹æ€§å¼· (å¯æŸ¥çœ‹ç‰¹å¾µæ©Ÿç‡)\n",
    "- âœ… å¤šåˆ†é¡ä»»å‹™è¡¨ç¾ç©©å®š\n",
    "\n",
    "**ç¼ºé»**:\n",
    "- âŒ æ¢ä»¶ç¨ç«‹å‡è¨­éå¼· (å¿½ç•¥è©åº)\n",
    "- âŒ å°ç‰¹å¾µç›¸é—œæ€§æ•æ„Ÿ\n",
    "- âŒ ç„¡æ³•æ•æ‰è¤‡é›œæ¨¡å¼\n",
    "\n",
    "### ğŸ“š å»¶ä¼¸é–±è®€\n",
    "\n",
    "- [Naive Bayes æ•¸å­¸æ¨å°](https://en.wikipedia.org/wiki/Naive_Bayes_classifier)\n",
    "- [sklearn MultinomialNB æ–‡æª”](https://scikit-learn.org/stable/modules/naive_bayes.html)\n",
    "- [åƒåœ¾éƒµä»¶åˆ†é¡æ¡ˆä¾‹](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection)\n",
    "\n",
    "### ğŸš€ ä¸‹ä¸€ç¯€é å‘Š\n",
    "\n",
    "**åº•å±¤å¯¦ä½œ02: å¾é›¶æ‰“é€  MLP (å¤šå±¤æ„ŸçŸ¥å™¨)**\n",
    "- å‰å‘å‚³æ’­å¯¦ä½œ\n",
    "- åå‘å‚³æ’­æ¨å°\n",
    "- æ¿€æ´»å‡½æ•¸èˆ‡æ¬Šé‡åˆå§‹åŒ–\n",
    "\n",
    "---\n",
    "\n",
    "**èª²ç¨‹**: iSpan Python NLP Cookbooks v2\n",
    "**è¬›å¸«**: Claude AI\n",
    "**æœ€å¾Œæ›´æ–°**: 2025-10-17"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
