{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 底層實作01: 從零打造 Naive Bayes 分類器\n",
    "\n",
    "**課程**: iSpan Python NLP Cookbooks v2\n",
    "**章節**: 底層實作系列\n",
    "**版本**: v1.0\n",
    "**更新日期**: 2025-10-17\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 本節學習目標\n",
    "\n",
    "1. 從數學原理推導 Naive Bayes 算法\n",
    "2. 理解貝葉斯定理與條件獨立假設\n",
    "3. 從零實作 Multinomial Naive Bayes\n",
    "4. 掌握 Laplace Smoothing 技巧\n",
    "5. 與 scikit-learn 版本對比驗證\n",
    "\n",
    "---\n",
    "\n",
    "## 1. 貝葉斯定理數學推導\n",
    "\n",
    "### 1.1 貝葉斯定理 (Bayes' Theorem)\n",
    "\n",
    "**定義**: 描述在已知某些條件下,事件發生的機率\n",
    "\n",
    "```\n",
    "P(A|B) = P(B|A) * P(A) / P(B)\n",
    "\n",
    "其中:\n",
    "- P(A|B): 後驗機率 (Posterior) - 在 B 發生條件下 A 的機率\n",
    "- P(B|A): 似然 (Likelihood) - 在 A 發生條件下 B 的機率\n",
    "- P(A):   先驗機率 (Prior) - A 發生的機率\n",
    "- P(B):   證據 (Evidence) - B 發生的機率\n",
    "```\n",
    "\n",
    "### 1.2 應用到文本分類\n",
    "\n",
    "**目標**: 給定文本 D,預測類別 C\n",
    "\n",
    "```\n",
    "P(C|D) = P(D|C) * P(C) / P(D)\n",
    "\n",
    "簡化 (因為 P(D) 對所有類別相同):\n",
    "C_pred = argmax_C [ P(D|C) * P(C) ]\n",
    "\n",
    "展開文本 D = {w1, w2, ..., wn} (詞序列):\n",
    "C_pred = argmax_C [ P(w1, w2, ..., wn | C) * P(C) ]\n",
    "```\n",
    "\n",
    "### 1.3 條件獨立假設 (Naive Assumption)\n",
    "\n",
    "**假設**: 每個詞彙在類別 C 下**相互獨立**\n",
    "\n",
    "```\n",
    "P(w1, w2, ..., wn | C) = P(w1|C) * P(w2|C) * ... * P(wn|C)\n",
    "\n",
    "簡化為:\n",
    "P(w1, w2, ..., wn | C) = ∏ P(wi|C)  (i=1 to n)\n",
    "\n",
    "最終分類公式:\n",
    "C_pred = argmax_C [ P(C) * ∏ P(wi|C) ]\n",
    "\n",
    "取對數避免下溢 (underflow):\n",
    "C_pred = argmax_C [ log P(C) + Σ log P(wi|C) ]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# 視覺化貝葉斯定理\n",
    "def visualize_bayes_theorem():\n",
    "    # 模擬數據: 垃圾郵件 vs 正常郵件\n",
    "    categories = ['Spam', 'Ham']\n",
    "    priors = [0.3, 0.7]  # P(Spam)=0.3, P(Ham)=0.7\n",
    "    \n",
    "    # 似然: P(包含\"free\"|類別)\n",
    "    likelihoods = [0.8, 0.2]  # Spam中80%有\"free\", Ham中20%有\"free\"\n",
    "    \n",
    "    # 計算後驗機率 P(類別|\"free\")\n",
    "    evidence = sum(p * l for p, l in zip(priors, likelihoods))\n",
    "    posteriors = [(l * p) / evidence for p, l in zip(priors, likelihoods)]\n",
    "    \n",
    "    # 繪圖\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    axes[0].bar(categories, priors, color=['#ff6b6b', '#4ecdc4'])\n",
    "    axes[0].set_title('Prior P(C)', fontsize=14)\n",
    "    axes[0].set_ylabel('Probability')\n",
    "    \n",
    "    axes[1].bar(categories, likelihoods, color=['#ff6b6b', '#4ecdc4'])\n",
    "    axes[1].set_title('Likelihood P(\"free\"|C)', fontsize=14)\n",
    "    axes[1].set_ylabel('Probability')\n",
    "    \n",
    "    axes[2].bar(categories, posteriors, color=['#ff6b6b', '#4ecdc4'])\n",
    "    axes[2].set_title('Posterior P(C|\"free\")', fontsize=14)\n",
    "    axes[2].set_ylabel('Probability')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"貝葉斯更新過程:\")\n",
    "    print(f\"Prior:     Spam={priors[0]:.2f}, Ham={priors[1]:.2f}\")\n",
    "    print(f\"Posterior: Spam={posteriors[0]:.2f}, Ham={posteriors[1]:.2f}\")\n",
    "    print(f\"\\n結論: 看到'free'後,Spam機率從 {priors[0]:.0%} 上升到 {posteriors[0]:.0%}\")\n",
    "\n",
    "visualize_bayes_theorem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Multinomial Naive Bayes 實作\n",
    "\n",
    "### 2.1 算法步驟\n",
    "\n",
    "**訓練階段**:\n",
    "1. 計算每個類別的先驗機率 P(C)\n",
    "2. 計算每個詞在每個類別中的條件機率 P(w|C)\n",
    "3. 應用 Laplace Smoothing 避免零機率\n",
    "\n",
    "**預測階段**:\n",
    "1. 對新文本計算 log P(C) + Σ log P(wi|C)\n",
    "2. 選擇分數最高的類別\n",
    "\n",
    "### 2.2 從零實作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier:\n",
    "    def __init__(self, alpha=1.0):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        alpha : float\n",
    "            Laplace smoothing 參數 (加法平滑)\n",
    "        \"\"\"\n",
    "        self.alpha = alpha\n",
    "        self.class_log_prior = {}     # log P(C)\n",
    "        self.feature_log_prob = {}    # log P(w|C)\n",
    "        self.classes = None\n",
    "        self.vocabulary = set()\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        訓練模型\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : list of str\n",
    "            訓練文本列表\n",
    "        y : list of str/int\n",
    "            訓練標籤列表\n",
    "        \"\"\"\n",
    "        n_samples = len(X)\n",
    "        self.classes = np.unique(y)\n",
    "        \n",
    "        # 建立詞彙表\n",
    "        for text in X:\n",
    "            self.vocabulary.update(text.lower().split())\n",
    "        \n",
    "        vocab_size = len(self.vocabulary)\n",
    "        \n",
    "        # 對每個類別計算統計量\n",
    "        for c in self.classes:\n",
    "            # 獲取該類別的所有文本\n",
    "            c_texts = [X[i] for i in range(n_samples) if y[i] == c]\n",
    "            n_c = len(c_texts)\n",
    "            \n",
    "            # 計算先驗機率 P(C)\n",
    "            self.class_log_prior[c] = np.log(n_c / n_samples)\n",
    "            \n",
    "            # 統計詞頻\n",
    "            word_counts = defaultdict(int)\n",
    "            total_words = 0\n",
    "            \n",
    "            for text in c_texts:\n",
    "                words = text.lower().split()\n",
    "                for word in words:\n",
    "                    word_counts[word] += 1\n",
    "                    total_words += 1\n",
    "            \n",
    "            # 計算條件機率 P(w|C) with Laplace Smoothing\n",
    "            self.feature_log_prob[c] = {}\n",
    "            for word in self.vocabulary:\n",
    "                count = word_counts[word]\n",
    "                # Laplace Smoothing: (count + alpha) / (total + alpha * vocab_size)\n",
    "                prob = (count + self.alpha) / (total_words + self.alpha * vocab_size)\n",
    "                self.feature_log_prob[c][word] = np.log(prob)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        預測新文本\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : list of str\n",
    "            待預測文本列表\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        predictions : list\n",
    "            預測的類別列表\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        \n",
    "        for text in X:\n",
    "            words = text.lower().split()\n",
    "            class_scores = {}\n",
    "            \n",
    "            for c in self.classes:\n",
    "                # 初始化為 log P(C)\n",
    "                score = self.class_log_prior[c]\n",
    "                \n",
    "                # 累加 Σ log P(wi|C)\n",
    "                for word in words:\n",
    "                    if word in self.vocabulary:\n",
    "                        score += self.feature_log_prob[c][word]\n",
    "                \n",
    "                class_scores[c] = score\n",
    "            \n",
    "            # 選擇分數最高的類別\n",
    "            predicted_class = max(class_scores, key=class_scores.get)\n",
    "            predictions.append(predicted_class)\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        預測機率\n",
    "        \"\"\"\n",
    "        probabilities = []\n",
    "        \n",
    "        for text in X:\n",
    "            words = text.lower().split()\n",
    "            class_scores = {}\n",
    "            \n",
    "            for c in self.classes:\n",
    "                score = self.class_log_prior[c]\n",
    "                for word in words:\n",
    "                    if word in self.vocabulary:\n",
    "                        score += self.feature_log_prob[c][word]\n",
    "                class_scores[c] = score\n",
    "            \n",
    "            # 轉換為機率 (softmax)\n",
    "            scores = np.array(list(class_scores.values()))\n",
    "            exp_scores = np.exp(scores - np.max(scores))  # 數值穩定\n",
    "            probs = exp_scores / exp_scores.sum()\n",
    "            \n",
    "            probabilities.append(dict(zip(class_scores.keys(), probs)))\n",
    "        \n",
    "        return probabilities\n",
    "\n",
    "# 測試實作\n",
    "print(\"✅ Naive Bayes 分類器實作完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Laplace Smoothing 深入理解\n",
    "\n",
    "### 3.1 零機率問題\n",
    "\n",
    "**問題**: 如果訓練集中某個詞從未在某類別出現,則 P(w|C) = 0\n",
    "\n",
    "```\n",
    "訓練集:\n",
    "Spam: \"buy free now\"\n",
    "Ham:  \"hello friend\"\n",
    "\n",
    "測試: \"free hello\"\n",
    "P(Spam|\"free hello\") = P(Spam) * P(free|Spam) * P(hello|Spam)\n",
    "                     = 0.5 * 0.33 * 0  ← hello 從未在 Spam 出現\n",
    "                     = 0  ← 整個機率變 0!\n",
    "```\n",
    "\n",
    "### 3.2 Laplace Smoothing 解決方案\n",
    "\n",
    "**公式**:\n",
    "```\n",
    "P(wi|C) = (count(wi, C) + α) / (count(C) + α * |V|)\n",
    "\n",
    "其中:\n",
    "- α: smoothing 參數 (通常為 1)\n",
    "- |V|: 詞彙表大小\n",
    "- count(wi, C): 詞 wi 在類別 C 中出現次數\n",
    "- count(C): 類別 C 的總詞數\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演示 Laplace Smoothing 效果\n",
    "def compare_smoothing():\n",
    "    # 模擬數據\n",
    "    word_counts = {'buy': 2, 'free': 3, 'now': 1}  # Spam 詞頻\n",
    "    total_words = sum(word_counts.values())  # 6\n",
    "    vocab_size = 10  # 假設詞彙表有 10 個詞\n",
    "    \n",
    "    test_word = 'hello'  # 訓練集中未出現的詞\n",
    "    \n",
    "    # 不使用 smoothing\n",
    "    prob_no_smooth = word_counts.get(test_word, 0) / total_words\n",
    "    \n",
    "    # 使用 Laplace smoothing (α=1)\n",
    "    prob_smooth = (word_counts.get(test_word, 0) + 1) / (total_words + vocab_size)\n",
    "    \n",
    "    print(\"Laplace Smoothing 效果對比:\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"詞彙: '{test_word}' (訓練集中未出現)\")\n",
    "    print(f\"\\n不使用 Smoothing:\")\n",
    "    print(f\"  P('{test_word}'|Spam) = {word_counts.get(test_word, 0)}/{total_words} = {prob_no_smooth}\")\n",
    "    print(f\"\\n使用 Laplace Smoothing (α=1):\")\n",
    "    print(f\"  P('{test_word}'|Spam) = ({word_counts.get(test_word, 0)}+1)/({total_words}+{vocab_size}) = {prob_smooth:.4f}\")\n",
    "    print(f\"\\n結果: Smoothing 避免了零機率問題!\")\n",
    "\n",
    "compare_smoothing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. 實戰案例: 垃圾郵件分類\n",
    "\n",
    "### 4.1 準備數據集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 垃圾郵件分類數據集\n",
    "spam_emails = [\n",
    "    \"free money now\",\n",
    "    \"win prizes click here\",\n",
    "    \"buy cheap products\",\n",
    "    \"get rich quick\",\n",
    "    \"earn money fast\",\n",
    "    \"free gift click now\",\n",
    "    \"win big prizes today\",\n",
    "    \"make money online\"\n",
    "]\n",
    "\n",
    "ham_emails = [\n",
    "    \"hello friend how are you\",\n",
    "    \"meeting tomorrow at noon\",\n",
    "    \"please review the document\",\n",
    "    \"thanks for your help\",\n",
    "    \"see you next week\",\n",
    "    \"happy birthday dear friend\",\n",
    "    \"project deadline is friday\",\n",
    "    \"let me know your thoughts\"\n",
    "]\n",
    "\n",
    "# 合併數據\n",
    "X_train = spam_emails + ham_emails\n",
    "y_train = ['spam'] * len(spam_emails) + ['ham'] * len(ham_emails)\n",
    "\n",
    "print(f\"訓練集大小: {len(X_train)}\")\n",
    "print(f\"Spam 樣本: {y_train.count('spam')}\")\n",
    "print(f\"Ham 樣本:  {y_train.count('ham')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 訓練與預測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練模型\n",
    "nb_classifier = NaiveBayesClassifier(alpha=1.0)\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# 測試數據\n",
    "X_test = [\n",
    "    \"free money click here\",      # 應該是 spam\n",
    "    \"meeting next week\",           # 應該是 ham\n",
    "    \"win big prizes now\",          # 應該是 spam\n",
    "    \"thanks for the document\",     # 應該是 ham\n",
    "    \"buy free products\",           # 應該是 spam\n",
    "]\n",
    "\n",
    "# 預測\n",
    "predictions = nb_classifier.predict(X_test)\n",
    "probabilities = nb_classifier.predict_proba(X_test)\n",
    "\n",
    "# 顯示結果\n",
    "print(\"\\n預測結果:\")\n",
    "print(\"=\"*70)\n",
    "for i, (text, pred, prob) in enumerate(zip(X_test, predictions, probabilities), 1):\n",
    "    print(f\"{i}. 文本: '{text}'\")\n",
    "    print(f\"   預測: {pred.upper()}\")\n",
    "    print(f\"   機率: Spam={prob['spam']:.2%}, Ham={prob['ham']:.2%}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 查看學習到的特徵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 找出每個類別最重要的詞\n",
    "def get_top_features(classifier, top_n=5):\n",
    "    for class_name in classifier.classes:\n",
    "        # 獲取該類別的詞機率\n",
    "        word_probs = classifier.feature_log_prob[class_name]\n",
    "        \n",
    "        # 排序 (log 機率越大越重要)\n",
    "        sorted_words = sorted(word_probs.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        print(f\"\\n{class_name.upper()} 最重要的 {top_n} 個詞:\")\n",
    "        for word, log_prob in sorted_words[:top_n]:\n",
    "            prob = np.exp(log_prob)\n",
    "            print(f\"  {word:15s} → P(w|{class_name}) = {prob:.4f}\")\n",
    "\n",
    "get_top_features(nb_classifier, top_n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. 與 scikit-learn 對比驗證\n",
    "\n",
    "### 5.1 使用相同數據訓練 sklearn 版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# 建立 sklearn Pipeline\n",
    "sklearn_pipeline = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('classifier', MultinomialNB(alpha=1.0))\n",
    "])\n",
    "\n",
    "# 訓練\n",
    "sklearn_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# 預測\n",
    "sklearn_predictions = sklearn_pipeline.predict(X_test)\n",
    "sklearn_proba = sklearn_pipeline.predict_proba(X_test)\n",
    "\n",
    "print(\"sklearn 預測結果:\")\n",
    "print(\"=\"*70)\n",
    "classes = sklearn_pipeline.classes_\n",
    "for i, (text, pred, proba) in enumerate(zip(X_test, sklearn_predictions, sklearn_proba), 1):\n",
    "    print(f\"{i}. 文本: '{text}'\")\n",
    "    print(f\"   預測: {pred.upper()}\")\n",
    "    prob_dict = dict(zip(classes, proba))\n",
    "    print(f\"   機率: Spam={prob_dict.get('spam', 0):.2%}, Ham={prob_dict.get('ham', 0):.2%}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 結果對比分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 對比預測結果\n",
    "comparison_df = pd.DataFrame({\n",
    "    '文本': X_test,\n",
    "    '自製預測': predictions,\n",
    "    'sklearn預測': sklearn_predictions,\n",
    "    '結果一致': [p1 == p2 for p1, p2 in zip(predictions, sklearn_predictions)]\n",
    "})\n",
    "\n",
    "print(\"\\n預測結果對比:\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# 計算一致率\n",
    "accuracy = comparison_df['結果一致'].mean()\n",
    "print(f\"\\n一致率: {accuracy:.0%}\")\n",
    "\n",
    "if accuracy == 1.0:\n",
    "    print(\"✅ 完美! 自製版本與 sklearn 預測完全一致\")\n",
    "else:\n",
    "    print(\"⚠️  存在差異,需檢查實作細節\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. 效能分析與可視化\n",
    "\n",
    "### 6.1 混淆矩陣"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "# 在訓練集上評估\n",
    "train_predictions = nb_classifier.predict(X_train)\n",
    "\n",
    "# 計算混淆矩陣\n",
    "cm = confusion_matrix(y_train, train_predictions, labels=['spam', 'ham'])\n",
    "\n",
    "# 繪製混淆矩陣\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Spam', 'Ham'],\n",
    "            yticklabels=['Spam', 'Ham'])\n",
    "plt.title('Confusion Matrix (Training Set)', fontsize=14)\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "# 分類報告\n",
    "print(\"\\n分類報告 (訓練集):\")\n",
    "print(classification_report(y_train, train_predictions, target_names=['spam', 'ham']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 特徵重要性可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 比較 Spam vs Ham 的詞機率\n",
    "def visualize_feature_importance(classifier, top_n=10):\n",
    "    # 獲取共同詞彙\n",
    "    spam_probs = classifier.feature_log_prob['spam']\n",
    "    ham_probs = classifier.feature_log_prob['ham']\n",
    "    \n",
    "    # 計算差異 (log odds ratio)\n",
    "    word_scores = {}\n",
    "    for word in classifier.vocabulary:\n",
    "        spam_prob = np.exp(spam_probs[word])\n",
    "        ham_prob = np.exp(ham_probs[word])\n",
    "        # 越正表示越像 Spam,越負表示越像 Ham\n",
    "        score = np.log(spam_prob / ham_prob)\n",
    "        word_scores[word] = score\n",
    "    \n",
    "    # 排序\n",
    "    sorted_words = sorted(word_scores.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "    \n",
    "    # 取前 top_n\n",
    "    words = [w for w, s in sorted_words[:top_n]]\n",
    "    scores = [s for w, s in sorted_words[:top_n]]\n",
    "    \n",
    "    # 繪圖\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    colors = ['#ff6b6b' if s > 0 else '#4ecdc4' for s in scores]\n",
    "    plt.barh(words, scores, color=colors)\n",
    "    plt.xlabel('Log Odds Ratio (Spam vs Ham)', fontsize=12)\n",
    "    plt.title('Most Discriminative Words', fontsize=14)\n",
    "    plt.axvline(x=0, color='black', linestyle='--', linewidth=0.8)\n",
    "    \n",
    "    # 添加圖例\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [\n",
    "        Patch(facecolor='#ff6b6b', label='Spam'),\n",
    "        Patch(facecolor='#4ecdc4', label='Ham')\n",
    "    ]\n",
    "    plt.legend(handles=legend_elements, loc='best')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_feature_importance(nb_classifier, top_n=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. 進階: 處理真實數據集\n",
    "\n",
    "### 7.1 使用 SMS Spam Collection 數據集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果有 SMS Spam 數據集\n",
    "try:\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # 嘗試載入數據\n",
    "    # df = pd.read_csv('../../datasets/sms_spam/SMSSpamCollection', \n",
    "    #                  sep='\\t', names=['label', 'message'])\n",
    "    \n",
    "    # 暫時使用模擬數據\n",
    "    df = pd.DataFrame({\n",
    "        'label': ['spam', 'ham'] * 50,\n",
    "        'message': ['free money now'] * 50 + ['hello friend'] * 50\n",
    "    })\n",
    "    \n",
    "    # 切分訓練集與測試集\n",
    "    X = df['message'].tolist()\n",
    "    y = df['label'].tolist()\n",
    "    \n",
    "    X_train_real, X_test_real, y_train_real, y_test_real = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # 訓練\n",
    "    nb_real = NaiveBayesClassifier(alpha=1.0)\n",
    "    nb_real.fit(X_train_real, y_train_real)\n",
    "    \n",
    "    # 預測\n",
    "    y_pred_real = nb_real.predict(X_test_real)\n",
    "    \n",
    "    # 評估\n",
    "    from sklearn.metrics import accuracy_score, f1_score\n",
    "    \n",
    "    accuracy = accuracy_score(y_test_real, y_pred_real)\n",
    "    f1 = f1_score(y_test_real, y_pred_real, pos_label='spam')\n",
    "    \n",
    "    print(\"真實數據集評估結果:\")\n",
    "    print(f\"準確率: {accuracy:.2%}\")\n",
    "    print(f\"F1 分數: {f1:.2%}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"無法載入真實數據集: {e}\")\n",
    "    print(\"請確保數據集位於正確路徑\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. 課後練習\n",
    "\n",
    "### 練習 1: 實作 Bernoulli Naive Bayes\n",
    "\n",
    "提示: 與 Multinomial 不同,Bernoulli 只考慮詞是否出現 (0/1),而非出現次數。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 實作 Bernoulli Naive Bayes\n",
    "class BernoulliNaiveBayes:\n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = alpha\n",
    "        # TODO: 初始化其他必要參數\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # TODO: 實作訓練邏輯\n",
    "        # 提示: P(w|C) = (出現該詞的文檔數 + α) / (類別 C 的文檔數 + 2α)\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # TODO: 實作預測邏輯\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習 2: 處理中文文本\n",
    "\n",
    "使用 jieba 分詞,將 Naive Bayes 應用到中文垃圾簡訊分類。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 中文文本分類\n",
    "import jieba\n",
    "\n",
    "# 中文數據集範例\n",
    "chinese_spam = [\n",
    "    \"恭喜您中獎了,請點擊連結領取\",\n",
    "    \"免費贈送禮品,馬上領取\"\n",
    "]\n",
    "\n",
    "chinese_ham = [\n",
    "    \"明天會議時間改到下午三點\",\n",
    "    \"週末一起去吃飯吧\"\n",
    "]\n",
    "\n",
    "# TODO: 使用 jieba 分詞 + Naive Bayes 分類"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. 本節總結\n",
    "\n",
    "### ✅ 關鍵要點\n",
    "\n",
    "1. **貝葉斯定理**: P(C|D) = P(D|C) * P(C) / P(D)\n",
    "   - 後驗機率 = (似然 × 先驗) / 證據\n",
    "\n",
    "2. **條件獨立假設**: P(w1, w2, ..., wn | C) = ∏ P(wi|C)\n",
    "   - 簡化計算,但可能損失詞序信息\n",
    "\n",
    "3. **Laplace Smoothing**: (count + α) / (total + α|V|)\n",
    "   - 避免零機率問題\n",
    "   - α=1 稱為加一平滑\n",
    "\n",
    "4. **對數技巧**: 使用 log 避免數值下溢\n",
    "   - log P(C|D) = log P(C) + Σ log P(wi|C)\n",
    "\n",
    "5. **實作驗證**: 與 sklearn 對比確保正確性\n",
    "\n",
    "### 📊 優缺點分析\n",
    "\n",
    "**優點**:\n",
    "- ✅ 實作簡單,計算高效\n",
    "- ✅ 對小數據集表現良好\n",
    "- ✅ 可解釋性強 (可查看特徵機率)\n",
    "- ✅ 多分類任務表現穩定\n",
    "\n",
    "**缺點**:\n",
    "- ❌ 條件獨立假設過強 (忽略詞序)\n",
    "- ❌ 對特徵相關性敏感\n",
    "- ❌ 無法捕捉複雜模式\n",
    "\n",
    "### 📚 延伸閱讀\n",
    "\n",
    "- [Naive Bayes 數學推導](https://en.wikipedia.org/wiki/Naive_Bayes_classifier)\n",
    "- [sklearn MultinomialNB 文檔](https://scikit-learn.org/stable/modules/naive_bayes.html)\n",
    "- [垃圾郵件分類案例](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection)\n",
    "\n",
    "### 🚀 下一節預告\n",
    "\n",
    "**底層實作02: 從零打造 MLP (多層感知器)**\n",
    "- 前向傳播實作\n",
    "- 反向傳播推導\n",
    "- 激活函數與權重初始化\n",
    "\n",
    "---\n",
    "\n",
    "**課程**: iSpan Python NLP Cookbooks v2\n",
    "**講師**: Claude AI\n",
    "**最後更新**: 2025-10-17"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
