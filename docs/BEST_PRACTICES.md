# iSpan Python NLP Cookbooks - æœ€ä½³å¯¦è¸æŒ‡å—

**ç‰ˆæœ¬**: v1.0
**æœ€å¾Œæ›´æ–°**: 2025-10-17
**ç›®æ¨™è®€è€…**: å­¸ç”Ÿã€é–‹ç™¼è€…ã€è¬›å¸«

---

## ğŸ“‹ ç›®éŒ„

1. [ä»£ç¢¼å“è³ªæœ€ä½³å¯¦è¸](#1-ä»£ç¢¼å“è³ªæœ€ä½³å¯¦è¸)
2. [Notebook æ’°å¯«è¦ç¯„](#2-notebook-æ’°å¯«è¦ç¯„)
3. [NLP å°ˆæ¡ˆé–‹ç™¼æµç¨‹](#3-nlp-å°ˆæ¡ˆé–‹ç™¼æµç¨‹)
4. [æ€§èƒ½å„ªåŒ–æŠ€å·§](#4-æ€§èƒ½å„ªåŒ–æŠ€å·§)
5. [æ¨¡å‹è¨“ç·´æœ€ä½³å¯¦è¸](#5-æ¨¡å‹è¨“ç·´æœ€ä½³å¯¦è¸)
6. [æ•¸æ“šè™•ç†è¦ç¯„](#6-æ•¸æ“šè™•ç†è¦ç¯„)
7. [éƒ¨ç½²èˆ‡ç¶­è­·](#7-éƒ¨ç½²èˆ‡ç¶­è­·)
8. [å®‰å…¨æ€§è€ƒé‡](#8-å®‰å…¨æ€§è€ƒé‡)

---

## 1. ä»£ç¢¼å“è³ªæœ€ä½³å¯¦è¸

### 1.1 Python ç·¨ç¢¼è¦ç¯„

#### **PEP 8 æ ¸å¿ƒåŸå‰‡**

```python
# âœ… å¥½çš„ä»£ç¢¼: æ¸…æ™°ã€å¯è®€ã€ç¬¦åˆè¦ç¯„
def calculate_tf_idf(documents, vocabulary):
    """
    Calculate TF-IDF scores for documents.

    Args:
        documents (list): List of tokenized documents
        vocabulary (set): Vocabulary set

    Returns:
        np.ndarray: TF-IDF matrix
    """
    n_docs = len(documents)
    n_vocab = len(vocabulary)

    # Initialize TF-IDF matrix
    tfidf_matrix = np.zeros((n_docs, n_vocab))

    # Calculate TF-IDF
    for doc_idx, doc in enumerate(documents):
        for word_idx, word in enumerate(vocabulary):
            tf = doc.count(word) / len(doc)
            idf = np.log(n_docs / (1 + sum(word in d for d in documents)))
            tfidf_matrix[doc_idx, word_idx] = tf * idf

    return tfidf_matrix


# âŒ å£çš„ä»£ç¢¼: ä¸æ¸…æ™°ã€æ²’è¨»è§£ã€å‘½åå·®
def calc(d,v):
    n=len(d)
    m=len(v)
    r=np.zeros((n,m))
    for i,x in enumerate(d):
        for j,y in enumerate(v):
            r[i,j]=(x.count(y)/len(x))*np.log(n/(1+sum(y in z for z in d)))
    return r
```

#### **å‘½åè¦ç¯„**

| é¡å‹ | è¦ç¯„ | ç¯„ä¾‹ |
|------|------|------|
| è®Šæ•¸ | `snake_case` | `word_count`, `document_list` |
| å‡½æ•¸ | `snake_case` | `tokenize_text()`, `train_model()` |
| é¡åˆ¥ | `PascalCase` | `TextPreprocessor`, `SentimentAnalyzer` |
| å¸¸æ•¸ | `UPPER_SNAKE_CASE` | `MAX_LENGTH`, `DEFAULT_BATCH_SIZE` |
| ç§æœ‰æ–¹æ³• | `_snake_case` | `_validate_input()` |

#### **Type Hints (é¡å‹æç¤º)**

```python
from typing import List, Dict, Optional, Tuple
import numpy as np

# âœ… ä½¿ç”¨ Type Hints
def preprocess_text(
    text: str,
    lowercase: bool = True,
    remove_punct: bool = True
) -> str:
    """
    Preprocess input text.

    Args:
        text: Input text string
        lowercase: Convert to lowercase
        remove_punct: Remove punctuation

    Returns:
        Preprocessed text
    """
    if lowercase:
        text = text.lower()

    if remove_punct:
        text = re.sub(r'[^\w\s]', '', text)

    return text


def train_classifier(
    X_train: np.ndarray,
    y_train: np.ndarray,
    epochs: int = 10,
    batch_size: int = 32
) -> Tuple[object, Dict[str, float]]:
    """Train classification model."""
    # Training code...
    return model, metrics
```

### 1.2 éŒ¯èª¤è™•ç†

#### **é˜²ç¦¦æ€§ç·¨ç¨‹**

```python
# âœ… å®Œå–„çš„éŒ¯èª¤è™•ç†
def load_dataset(file_path: str) -> pd.DataFrame:
    """
    Load dataset with comprehensive error handling.
    """
    # æª¢æŸ¥æª”æ¡ˆå­˜åœ¨
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"Dataset not found: {file_path}")

    # æª¢æŸ¥æª”æ¡ˆæ ¼å¼
    if not file_path.endswith('.csv'):
        raise ValueError(f"Expected CSV file, got: {file_path}")

    try:
        df = pd.read_csv(file_path, encoding='utf-8')
    except UnicodeDecodeError:
        # å˜—è©¦å…¶ä»–ç·¨ç¢¼
        try:
            df = pd.read_csv(file_path, encoding='big5')
        except Exception as e:
            raise RuntimeError(f"Failed to read file: {e}")

    # é©—è­‰å¿…è¦æ¬„ä½
    required_columns = ['text', 'label']
    missing = set(required_columns) - set(df.columns)
    if missing:
        raise ValueError(f"Missing required columns: {missing}")

    return df


# âŒ æ²’æœ‰éŒ¯èª¤è™•ç†
def load_dataset(file_path):
    return pd.read_csv(file_path)  # å¯èƒ½å´©æ½°!
```

#### **ä½¿ç”¨ try-except-finally**

```python
# å®Œæ•´çš„è³‡æºç®¡ç†
def process_large_file(file_path: str):
    file_handle = None
    try:
        file_handle = open(file_path, 'r', encoding='utf-8')

        # è™•ç†æ•¸æ“š
        for line in file_handle:
            process_line(line)

    except FileNotFoundError:
        print(f"Error: File not found - {file_path}")
        raise
    except UnicodeDecodeError:
        print(f"Error: Encoding issue in {file_path}")
        raise
    finally:
        # ç¢ºä¿æª”æ¡ˆé—œé–‰
        if file_handle:
            file_handle.close()


# æ›´å¥½çš„æ–¹å¼: ä½¿ç”¨ context manager
def process_large_file(file_path: str):
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                process_line(line)
    except Exception as e:
        print(f"Error processing {file_path}: {e}")
        raise
```

### 1.3 æ—¥èªŒè¨˜éŒ„

```python
import logging

# é…ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('training.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# åœ¨ä»£ç¢¼ä¸­ä½¿ç”¨
def train_model(data, epochs=10):
    logger.info(f"Starting training with {len(data)} samples")

    for epoch in range(epochs):
        logger.debug(f"Epoch {epoch+1}/{epochs}")

        try:
            loss = training_step(data)
            logger.info(f"Epoch {epoch+1} - Loss: {loss:.4f}")
        except Exception as e:
            logger.error(f"Training failed at epoch {epoch+1}: {e}")
            raise

    logger.info("Training completed successfully")
```

---

## 2. Notebook æ’°å¯«è¦ç¯„

### 2.1 Notebook çµæ§‹æ¨¡æ¿

```markdown
# CH0X-0Y: [ç« ç¯€æ¨™é¡Œ]

**èª²ç¨‹**: iSpan Python NLP Cookbooks v2
**ç« ç¯€**: CH0X [ç« ç¯€åç¨±]
**ç‰ˆæœ¬**: v1.0
**æ›´æ–°æ—¥æœŸ**: YYYY-MM-DD

---

## ğŸ“š æœ¬ç¯€å­¸ç¿’ç›®æ¨™

1. å­¸ç¿’ç›®æ¨™ 1
2. å­¸ç¿’ç›®æ¨™ 2
3. å­¸ç¿’ç›®æ¨™ 3

---

## 1. ç†è«–åŸºç¤

### 1.1 æ ¸å¿ƒæ¦‚å¿µ
[ç†è«–èªªæ˜]

### 1.2 æ•¸å­¸åŸç†
[å…¬å¼æ¨å°]

---

## 2. å¯¦ä½œç¤ºç¯„

### 2.1 ç’°å¢ƒæº–å‚™
[å°å…¥å¥—ä»¶ã€è¨­ç½®åƒæ•¸]

### 2.2 æ•¸æ“šæº–å‚™
[åŠ è¼‰æ•¸æ“šã€EDA]

### 2.3 æ ¸å¿ƒå¯¦ä½œ
[ä¸»è¦ä»£ç¢¼]

---

## 3. çµæœåˆ†æ

### 3.1 çµæœå±•ç¤º
[è¼¸å‡ºçµæœ]

### 3.2 è¦–è¦ºåŒ–
[åœ–è¡¨]

---

## 4. å»¶ä¼¸ç·´ç¿’

### ç·´ç¿’ 1: [ç·´ç¿’åç¨±]
[ç·´ç¿’èªªæ˜]

---

## 5. æœ¬ç¯€ç¸½çµ

- âœ… é‡é» 1
- âœ… é‡é» 2
- âœ… é‡é» 3

---

## 6. å»¶ä¼¸é–±è®€

- [è³‡æº 1]
- [è³‡æº 2]
```

### 2.2 Cell çµ„ç¹”åŸå‰‡

#### **Markdown Cell ä½¿ç”¨**
```markdown
# âœ… æ¸…æ™°çš„ç« ç¯€æ¨™é¡Œ
## 1. æ•¸æ“šè¼‰å…¥èˆ‡æ¢ç´¢
### 1.1 è¼‰å…¥æ•¸æ“š

# âœ… ä½¿ç”¨åˆ—è¡¨èªªæ˜æ­¥é©Ÿ
**æ¥ä¸‹ä¾†æˆ‘å€‘å°‡:**
1. è¼‰å…¥æ•¸æ“šé›†
2. æª¢æŸ¥æ•¸æ“šç¶­åº¦
3. æŸ¥çœ‹å‰ 5 ç­†è³‡æ–™

# âœ… ä½¿ç”¨è¡¨æ ¼æ•´ç†è³‡è¨Š
| åƒæ•¸ | èªªæ˜ | é»˜èªå€¼ |
|------|------|--------|
| max_length | æœ€å¤§åºåˆ—é•·åº¦ | 512 |
| batch_size | æ‰¹æ¬¡å¤§å° | 32 |

# âœ… ä½¿ç”¨è­¦å‘Šæ¡†
> âš ï¸ **æ³¨æ„**: æ­¤æ­¥é©Ÿéœ€è¦ 8GB+ RAM
> ğŸ’¡ **æç¤º**: å¯ä»¥æ¸›å°‘ batch_size ä¾†é™ä½è¨˜æ†¶é«”éœ€æ±‚
```

#### **Code Cell åŸå‰‡**

```python
# âœ… åŸå‰‡ 1: ä¸€å€‹ Cell åšä¸€ä»¶äº‹
# Cell 1: å°å…¥å¥—ä»¶
import numpy as np
import pandas as pd
from transformers import pipeline

# Cell 2: è¼‰å…¥æ•¸æ“š
df = pd.read_csv('data.csv')

# Cell 3: æ•¸æ“šæ¢ç´¢
print(df.shape)
df.head()

# âŒ é¿å…: ä¸€å€‹ Cell åšå¤ªå¤šäº‹
import numpy as np
df = pd.read_csv('data.csv')
df.dropna(inplace=True)
model = train_model(df)
results = evaluate(model)
plot_results(results)  # å¤ªè¤‡é›œ!
```

```python
# âœ… åŸå‰‡ 2: é‡è¦è¼¸å‡ºè¦é¡¯ç¤º
print(f"âœ… æ•¸æ“šè¼‰å…¥å®Œæˆ: {len(df)} ç­†è³‡æ–™")
print(f"âœ… è¨“ç·´å®Œæˆ: Accuracy = {accuracy:.2%}")

# âœ… åŸå‰‡ 3: ä½¿ç”¨é€²åº¦æ¢ (é•·æ™‚é–“æ“ä½œ)
from tqdm.notebook import tqdm

for epoch in tqdm(range(100), desc="Training"):
    train_epoch()

# âœ… åŸå‰‡ 4: çµæœè¦–è¦ºåŒ–
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
plt.plot(epochs, losses)
plt.title('Training Loss', fontsize=14)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.grid(True, alpha=0.3)
plt.show()
```

### 2.3 è¨»è§£èˆ‡æ–‡æª”å­—ä¸²

```python
# âœ… å®Œæ•´çš„å‡½æ•¸æ–‡æª”
def calculate_cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:
    """
    Calculate cosine similarity between two vectors.

    Cosine similarity measures the cosine of the angle between two vectors,
    ranging from -1 (opposite) to 1 (identical).

    Formula:
        similarity = (A Â· B) / (||A|| Ã— ||B||)

    Args:
        vec1 (np.ndarray): First vector (shape: [n,])
        vec2 (np.ndarray): Second vector (shape: [n,])

    Returns:
        float: Cosine similarity score in range [-1, 1]

    Raises:
        ValueError: If vectors have different dimensions

    Examples:
        >>> vec1 = np.array([1, 2, 3])
        >>> vec2 = np.array([4, 5, 6])
        >>> similarity = calculate_cosine_similarity(vec1, vec2)
        >>> print(f"Similarity: {similarity:.4f}")
        Similarity: 0.9746
    """
    if vec1.shape != vec2.shape:
        raise ValueError(f"Vector dimensions mismatch: {vec1.shape} vs {vec2.shape}")

    # è¨ˆç®—é»ç©
    dot_product = np.dot(vec1, vec2)

    # è¨ˆç®—ç¯„æ•¸
    norm1 = np.linalg.norm(vec1)
    norm2 = np.linalg.norm(vec2)

    # é¿å…é™¤ä»¥é›¶
    if norm1 == 0 or norm2 == 0:
        return 0.0

    return dot_product / (norm1 * norm2)
```

---

## 3. NLP å°ˆæ¡ˆé–‹ç™¼æµç¨‹

### 3.1 æ¨™æº–é–‹ç™¼æµç¨‹

```
éšæ®µ 1: å•é¡Œå®šç¾© (1-2 å¤©)
â”œâ”€â”€ æ˜ç¢ºæ¥­å‹™ç›®æ¨™
â”œâ”€â”€ å®šç¾©è©•ä¼°æŒ‡æ¨™
â””â”€â”€ ç¢ºèªæ•¸æ“šå¯å¾—æ€§

éšæ®µ 2: æ•¸æ“šæº–å‚™ (3-5 å¤©)
â”œâ”€â”€ æ•¸æ“šæ”¶é›†
â”œâ”€â”€ æ•¸æ“šæ¸…ç†
â”œâ”€â”€ EDA (æ¢ç´¢æ€§æ•¸æ“šåˆ†æ)
â””â”€â”€ æ•¸æ“šæ¨™è¨» (å¦‚éœ€è¦)

éšæ®µ 3: åŸºæº–æ¨¡å‹ (2-3 å¤©)
â”œâ”€â”€ å»ºç«‹ç°¡å–®åŸºæº– (Baseline)
â”œâ”€â”€ è©•ä¼°åŸºæº–æ€§èƒ½
â””â”€â”€ è¨­å®šç›®æ¨™æ”¹é€²å¹…åº¦

éšæ®µ 4: æ¨¡å‹é–‹ç™¼ (5-10 å¤©)
â”œâ”€â”€ ç‰¹å¾µå·¥ç¨‹
â”œâ”€â”€ æ¨¡å‹é¸æ“‡èˆ‡è¨“ç·´
â”œâ”€â”€ è¶…åƒæ•¸èª¿å„ª
â””â”€â”€ äº¤å‰é©—è­‰

éšæ®µ 5: è©•ä¼°èˆ‡å„ªåŒ– (3-5 å¤©)
â”œâ”€â”€ éŒ¯èª¤åˆ†æ
â”œâ”€â”€ æ¨¡å‹æ”¹é€²
â”œâ”€â”€ A/B æ¸¬è©¦
â””â”€â”€ æœ€çµ‚è©•ä¼°

éšæ®µ 6: éƒ¨ç½²èˆ‡ç›£æ§ (2-4 å¤©)
â”œâ”€â”€ æ¨¡å‹æ‰“åŒ…
â”œâ”€â”€ API é–‹ç™¼
â”œâ”€â”€ éƒ¨ç½²åˆ°ç”Ÿç”¢
â””â”€â”€ å»ºç«‹ç›£æ§ç³»çµ±
```

### 3.2 å¯¦æˆ°æ¡ˆä¾‹: åƒåœ¾éƒµä»¶åˆ†é¡å™¨

#### **éšæ®µ 1: å•é¡Œå®šç¾©**

```markdown
## æ¥­å‹™ç›®æ¨™
éæ¿¾åƒåœ¾éƒµä»¶,é™ä½ç”¨æˆ¶å¹²æ“¾

## æˆåŠŸæŒ‡æ¨™
- Precision > 95% (é¿å…èª¤åˆ¤æ­£å¸¸éƒµä»¶)
- Recall > 90% (æŠ“ä½å¤§éƒ¨åˆ†åƒåœ¾éƒµä»¶)
- æ¨ç†æ™‚é–“ < 100ms

## æ•¸æ“šéœ€æ±‚
- 5000+ æ¨™è¨»éƒµä»¶
- åƒåœ¾/æ­£å¸¸ æ¯”ä¾‹ 1:1
```

#### **éšæ®µ 2: æ•¸æ“šæº–å‚™**

```python
import pandas as pd
from sklearn.model_selection import train_test_split

# 1. è¼‰å…¥æ•¸æ“š
df = pd.read_csv('spam_dataset.csv')

# 2. EDA
print(f"æ•¸æ“šç¸½æ•¸: {len(df)}")
print(f"åƒåœ¾éƒµä»¶: {(df['label'] == 1).sum()}")
print(f"æ­£å¸¸éƒµä»¶: {(df['label'] == 0).sum()}")

# 3. æª¢æŸ¥ç¼ºå¤±å€¼
print(f"\nç¼ºå¤±å€¼:\n{df.isnull().sum()}")

# 4. æ•¸æ“šæ¸…ç†
df = df.dropna()
df = df.drop_duplicates(subset=['text'])

# 5. åŠƒåˆ†æ•¸æ“šé›†
X_train, X_test, y_train, y_test = train_test_split(
    df['text'],
    df['label'],
    test_size=0.2,
    random_state=42,
    stratify=df['label']  # ä¿æŒé¡åˆ¥æ¯”ä¾‹
)

print(f"\nè¨“ç·´é›†: {len(X_train)} | æ¸¬è©¦é›†: {len(X_test)}")
```

#### **éšæ®µ 3: å»ºç«‹åŸºæº–**

```python
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report

# ç°¡å–®åŸºæº–: TF-IDF + Naive Bayes
vectorizer = TfidfVectorizer(max_features=1000)
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

baseline_model = MultinomialNB()
baseline_model.fit(X_train_tfidf, y_train)

# è©•ä¼°
y_pred = baseline_model.predict(X_test_tfidf)
print("Baseline Model Performance:")
print(classification_report(y_test, y_pred, target_names=['Normal', 'Spam']))
```

#### **éšæ®µ 4: æ¨¡å‹é–‹ç™¼**

```python
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer
)

# 1. è¼‰å…¥é è¨“ç·´æ¨¡å‹
model_name = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=2
)

# 2. Tokenization
train_encodings = tokenizer(
    X_train.tolist(),
    truncation=True,
    padding=True,
    max_length=128
)

# 3. è¨“ç·´é…ç½®
training_args = TrainingArguments(
    output_dir='./spam_classifier',
    num_train_epochs=3,
    per_device_train_batch_size=16,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="f1",
    logging_steps=100
)

# 4. è¨“ç·´
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    compute_metrics=compute_metrics
)

trainer.train()
```

#### **éšæ®µ 5: éŒ¯èª¤åˆ†æ**

```python
# æ‰¾å‡ºé æ¸¬éŒ¯èª¤çš„æ¡ˆä¾‹
y_pred = trainer.predict(eval_dataset).predictions.argmax(-1)
errors = X_test[y_pred != y_test]

print(f"éŒ¯èª¤é æ¸¬æ•¸: {len(errors)}")
print("\nå‰ 5 å€‹éŒ¯èª¤æ¡ˆä¾‹:")
for i, (text, true_label, pred_label) in enumerate(
    zip(errors[:5], y_test[y_pred != y_test][:5], y_pred[y_pred != y_test][:5])
):
    print(f"\næ¡ˆä¾‹ {i+1}:")
    print(f"  æ–‡æœ¬: {text[:100]}...")
    print(f"  çœŸå¯¦æ¨™ç±¤: {true_label}")
    print(f"  é æ¸¬æ¨™ç±¤: {pred_label}")
```

---

## 4. æ€§èƒ½å„ªåŒ–æŠ€å·§

### 4.1 æ•¸æ“šè™•ç†å„ªåŒ–

#### **ä½¿ç”¨å‘é‡åŒ–æ“ä½œ**

```python
# âŒ æ…¢: ä½¿ç”¨å¾ªç’°
def count_words_slow(texts):
    word_counts = []
    for text in texts:
        count = len(text.split())
        word_counts.append(count)
    return word_counts

# âœ… å¿«: ä½¿ç”¨ Pandas å‘é‡åŒ–
def count_words_fast(texts):
    return texts.str.split().str.len()

# é€Ÿåº¦å°æ¯”
import time
texts = pd.Series(["sample text"] * 10000)

start = time.time()
result1 = count_words_slow(texts)
print(f"å¾ªç’°æ–¹å¼: {time.time() - start:.3f}s")

start = time.time()
result2 = count_words_fast(texts)
print(f"å‘é‡åŒ–: {time.time() - start:.3f}s")
# å‘é‡åŒ–å¿« 10-100 å€!
```

#### **æ‰¹æ¬¡è™•ç†å¤§å‹æ•¸æ“š**

```python
# âœ… ä½¿ç”¨ chunk è™•ç†å¤§æª”æ¡ˆ
def process_large_csv(file_path, chunk_size=10000):
    """é€å¡Šè™•ç†å¤§å‹ CSV,é¿å…è¨˜æ†¶é«”æº¢å‡º"""
    results = []

    for chunk in pd.read_csv(file_path, chunksize=chunk_size):
        # è™•ç†æ¯å€‹ chunk
        processed_chunk = preprocess_chunk(chunk)
        results.append(processed_chunk)

    # åˆä½µçµæœ
    return pd.concat(results, ignore_index=True)
```

### 4.2 æ¨¡å‹æ¨ç†å„ªåŒ–

#### **ä½¿ç”¨ @torch.no_grad() ç¯€çœè¨˜æ†¶é«”**

```python
import torch

# âœ… æ¨ç†æ™‚é—œé–‰æ¢¯åº¦è¨ˆç®—
@torch.no_grad()
def predict(model, text):
    inputs = tokenizer(text, return_tensors="pt")
    outputs = model(**inputs)
    return outputs.logits.argmax(-1)

# æˆ–ä½¿ç”¨ context manager
def predict_batch(model, texts):
    with torch.no_grad():
        # ä¸æœƒè¨ˆç®—æ¢¯åº¦,ç¯€çœè¨˜æ†¶é«”
        predictions = []
        for text in texts:
            pred = model(text)
            predictions.append(pred)
    return predictions
```

#### **æ¨¡å‹é‡åŒ–**

```python
# å‹•æ…‹é‡åŒ– (INT8)
import torch

model_fp32 = load_model()  # FP32 æ¨¡å‹
model_int8 = torch.quantization.quantize_dynamic(
    model_fp32,
    {torch.nn.Linear, torch.nn.LSTM},
    dtype=torch.qint8
)

# æ¯”è¼ƒå¤§å°
print(f"FP32 size: {get_model_size(model_fp32):.2f} MB")
print(f"INT8 size: {get_model_size(model_int8):.2f} MB")
# é€šå¸¸æ¸›å°‘ 75%
```

### 4.3 GPU ä½¿ç”¨å„ªåŒ–

```python
# æª¢æŸ¥ä¸¦ä½¿ç”¨ GPU
import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# å°‡æ¨¡å‹ç§»è‡³ GPU
model = model.to(device)

# æ¨ç†æ™‚ä¹Ÿè¦ç§»åˆ° GPU
inputs = tokenizer(text, return_tensors="pt")
inputs = {k: v.to(device) for k, v in inputs.items()}

outputs = model(**inputs)

# æ¸…ç† GPU è¨˜æ†¶é«”
torch.cuda.empty_cache()
```

---

## 5. æ¨¡å‹è¨“ç·´æœ€ä½³å¯¦è¸

### 5.1 è¨“ç·´å‰æª¢æŸ¥æ¸…å–®

- [ ] **æ•¸æ“šé©—è­‰**
  - [ ] ç„¡ç¼ºå¤±å€¼æˆ–å·²è™•ç†
  - [ ] é¡åˆ¥å¹³è¡¡æˆ–å·²åŠ æ¬Š
  - [ ] è¨“ç·´/é©—è­‰/æ¸¬è©¦é›†åŠƒåˆ†æ­£ç¢º

- [ ] **æ¨¡å‹é…ç½®**
  - [ ] è¶…åƒæ•¸è¨­ç½®åˆç†
  - [ ] å­¸ç¿’ç‡é©ç•¶ (2e-5 ~ 5e-5 for BERT)
  - [ ] Batch size é©é…è¨˜æ†¶é«”

- [ ] **è©•ä¼°æŒ‡æ¨™**
  - [ ] é¸æ“‡åˆé©çš„æŒ‡æ¨™ (Accuracy, F1, AUC...)
  - [ ] å®šç¾©æˆåŠŸæ¨™æº–

- [ ] **å¯¦é©—è¿½è¹¤**
  - [ ] ä½¿ç”¨ TensorBoard æˆ– W&B
  - [ ] è¨˜éŒ„è¶…åƒæ•¸
  - [ ] å¯é‡ç¾ (è¨­ç½® random seed)

### 5.2 é˜²æ­¢éæ“¬åˆ

```python
from transformers import TrainingArguments, EarlyStoppingCallback

# 1. Early Stopping
training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,  # è¼‰å…¥æœ€ä½³æ¨¡å‹
    metric_for_best_model="f1"
)

trainer = Trainer(
    model=model,
    args=training_args,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]
)

# 2. Dropout (æ¨¡å‹å±¤é¢)
from transformers import AutoConfig

config = AutoConfig.from_pretrained("bert-base-uncased")
config.hidden_dropout_prob = 0.2  # å¢åŠ  dropout
config.attention_probs_dropout_prob = 0.2

model = AutoModelForSequenceClassification.from_pretrained(
    "bert-base-uncased",
    config=config
)

# 3. Weight Decay (L2 æ­£å‰‡åŒ–)
training_args = TrainingArguments(
    weight_decay=0.01  # L2 æ­£å‰‡åŒ–
)

# 4. æ•¸æ“šå¢å¼·
def augment_text(text):
    """ç°¡å–®æ•¸æ“šå¢å¼·: åŒç¾©è©æ›¿æ›"""
    # ä½¿ç”¨ nlpaug æˆ–æ‰‹å‹•å¯¦ä½œ
    return augmented_text
```

### 5.3 è¶…åƒæ•¸èª¿å„ª

```python
# ä½¿ç”¨ Optuna è‡ªå‹•æœç´¢è¶…åƒæ•¸
def model_init():
    return AutoModelForSequenceClassification.from_pretrained(
        "distilbert-base-uncased",
        num_labels=2
    )

def hp_space(trial):
    return {
        "learning_rate": trial.suggest_float("learning_rate", 1e-5, 5e-5, log=True),
        "num_train_epochs": trial.suggest_int("num_train_epochs", 2, 5),
        "per_device_train_batch_size": trial.suggest_categorical(
            "per_device_train_batch_size", [8, 16, 32]
        ),
        "weight_decay": trial.suggest_float("weight_decay", 0.0, 0.1)
    }

trainer = Trainer(
    model_init=model_init,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    compute_metrics=compute_metrics
)

# åŸ·è¡Œæœç´¢
best_run = trainer.hyperparameter_search(
    direction="maximize",
    backend="optuna",
    hp_space=hp_space,
    n_trials=20
)

print(f"Best hyperparameters: {best_run.hyperparameters}")
```

---

## 6. æ•¸æ“šè™•ç†è¦ç¯„

### 6.1 æ–‡æœ¬æ¸…ç†æ¨™æº–æµç¨‹

```python
import re
import string

class TextCleaner:
    """æ¨™æº–æ–‡æœ¬æ¸…ç†é¡"""

    def __init__(self):
        self.punct = set(string.punctuation)

    def clean(self, text: str) -> str:
        """
        å®Œæ•´æ¸…ç†æµç¨‹
        """
        # 1. è½‰å°å¯«
        text = text.lower()

        # 2. ç§»é™¤ URL
        text = re.sub(r'http\S+|www\S+', '', text)

        # 3. ç§»é™¤ email
        text = re.sub(r'\S+@\S+', '', text)

        # 4. ç§»é™¤æ•¸å­— (è¦–éœ€æ±‚)
        text = re.sub(r'\d+', '', text)

        # 5. ç§»é™¤å¤šé¤˜ç©ºç™½
        text = re.sub(r'\s+', ' ', text).strip()

        # 6. ç§»é™¤æ¨™é» (è¦–éœ€æ±‚)
        text = ''.join(char for char in text if char not in self.punct)

        return text

# ä½¿ç”¨
cleaner = TextCleaner()
clean_text = cleaner.clean("Check out http://example.com! Email: test@test.com")
print(clean_text)
# "check out email"
```

### 6.2 ä¸­æ–‡æ–‡æœ¬è™•ç†

```python
import jieba
import re
from opencc import OpenCC

class ChineseTextProcessor:
    """ä¸­æ–‡æ–‡æœ¬è™•ç†å™¨"""

    def __init__(self, stopwords_path='shared_resources/stopwords/stopwords_zh_tw.txt'):
        # è¼‰å…¥åœç”¨è©
        with open(stopwords_path, 'r', encoding='utf-8') as f:
            self.stopwords = set(f.read().splitlines())

        # ç°¡ç¹è½‰æ›å™¨
        self.cc = OpenCC('s2tw')  # ç°¡é«”è½‰ç¹é«”

    def clean(self, text: str) -> str:
        """æ¸…ç†ä¸­æ–‡æ–‡æœ¬"""
        # ç§»é™¤è‹±æ–‡å’Œæ•¸å­—
        text = re.sub(r'[a-zA-Z0-9]', '', text)

        # ç§»é™¤æ¨™é»ç¬¦è™Ÿ
        text = re.sub(r'[^\w\s]', '', text)

        # ç§»é™¤ç©ºç™½
        text = re.sub(r'\s+', '', text)

        return text

    def segment(self, text: str, remove_stopwords: bool = True) -> list:
        """ä¸­æ–‡æ–·è©"""
        # æ–·è©
        words = jieba.lcut(text)

        # ç§»é™¤åœç”¨è©
        if remove_stopwords:
            words = [w for w in words if w not in self.stopwords]

        # ç§»é™¤å–®å­—
        words = [w for w in words if len(w) > 1]

        return words

    def convert_to_traditional(self, text: str) -> str:
        """ç°¡é«”è½‰ç¹é«”"""
        return self.cc.convert(text)

# ä½¿ç”¨ç¯„ä¾‹
processor = ChineseTextProcessor()

text = "æˆ‘çˆ±è‡ªç„¶è¯­è¨€å¤„ç†!"
text_tw = processor.convert_to_traditional(text)  # ç¹é«”
words = processor.segment(text_tw)                 # æ–·è©

print(f"åŸæ–‡: {text}")
print(f"ç¹é«”: {text_tw}")
print(f"æ–·è©: {words}")
```

### 6.3 æ•¸æ“šå¢å¼·æŠ€è¡“

```python
# 1. å›è­¯ (Back Translation)
from transformers import pipeline

translator_en2de = pipeline("translation_en_to_de", model="Helsinki-NLP/opus-mt-en-de")
translator_de2en = pipeline("translation_de_to_en", model="Helsinki-NLP/opus-mt-de-en")

def back_translate(text):
    """è‹±æ–‡å›è­¯å¢å¼·"""
    # è‹± â†’ å¾·
    de_text = translator_en2de(text)[0]['translation_text']
    # å¾· â†’ è‹±
    augmented = translator_de2en(de_text)[0]['translation_text']
    return augmented

# 2. åŒç¾©è©æ›¿æ›
import nltk
from nltk.corpus import wordnet

def synonym_replacement(text, n=2):
    """éš¨æ©Ÿæ›¿æ› n å€‹è©ç‚ºåŒç¾©è©"""
    words = text.split()

    for _ in range(n):
        idx = random.randint(0, len(words)-1)
        word = words[idx]

        synonyms = wordnet.synsets(word)
        if synonyms:
            synonym = random.choice(synonyms).lemmas()[0].name()
            words[idx] = synonym

    return ' '.join(words)

# 3. éš¨æ©Ÿæ’å…¥/åˆªé™¤/äº¤æ›
def random_insertion(text):
    """éš¨æ©Ÿæ’å…¥è©"""
    # å¯¦ä½œé‚è¼¯...
    pass

def random_deletion(text, p=0.1):
    """éš¨æ©Ÿåˆªé™¤è© (æ©Ÿç‡ p)"""
    words = text.split()
    if len(words) == 1:
        return text

    new_words = [w for w in words if random.random() > p]
    return ' '.join(new_words) if new_words else random.choice(words)
```

---

## 7. éƒ¨ç½²èˆ‡ç¶­è­·

### 7.1 æ¨¡å‹ç‰ˆæœ¬ç®¡ç†

```python
# ä½¿ç”¨ MLflow è¿½è¹¤æ¨¡å‹
import mlflow
import mlflow.pytorch

# é–‹å§‹å¯¦é©—
mlflow.start_run()

# è¨˜éŒ„åƒæ•¸
mlflow.log_param("learning_rate", 2e-5)
mlflow.log_param("batch_size", 16)
mlflow.log_param("epochs", 3)

# è¨“ç·´æ¨¡å‹
trainer.train()

# è¨˜éŒ„æŒ‡æ¨™
mlflow.log_metric("accuracy", accuracy)
mlflow.log_metric("f1_score", f1)

# ä¿å­˜æ¨¡å‹
mlflow.pytorch.log_model(model, "model")

# çµæŸå¯¦é©—
mlflow.end_run()
```

### 7.2 A/B æ¸¬è©¦

```python
# ç·šä¸Š A/B æ¸¬è©¦æ¡†æ¶
class ABTestRouter:
    def __init__(self, model_a, model_b, split_ratio=0.5):
        self.model_a = model_a  # åŸºæº–æ¨¡å‹
        self.model_b = model_b  # æ–°æ¨¡å‹
        self.split_ratio = split_ratio
        self.metrics_a = []
        self.metrics_b = []

    def predict(self, text, user_id):
        """æ ¹æ“š user_id è·¯ç”±åˆ°ä¸åŒæ¨¡å‹"""
        # ä½¿ç”¨ user_id hash æ±ºå®šä½¿ç”¨å“ªå€‹æ¨¡å‹
        use_model_b = hash(user_id) % 100 < (self.split_ratio * 100)

        if use_model_b:
            result = self.model_b(text)
            self.metrics_b.append(result)
            return result, "model_b"
        else:
            result = self.model_a(text)
            self.metrics_a.append(result)
            return result, "model_a"

    def get_stats(self):
        """ç²å– A/B æ¸¬è©¦çµ±è¨ˆ"""
        return {
            "model_a_requests": len(self.metrics_a),
            "model_b_requests": len(self.metrics_b),
            # æ·»åŠ æ›´å¤šæŒ‡æ¨™...
        }
```

---

## 8. å®‰å…¨æ€§è€ƒé‡

### 8.1 è¼¸å…¥é©—è­‰

```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field, validator

class TextInput(BaseModel):
    text: str = Field(..., min_length=1, max_length=5000)

    @validator('text')
    def validate_text(cls, v):
        # æª¢æŸ¥æ˜¯å¦ç‚ºç©ºç™½
        if not v.strip():
            raise ValueError("Text cannot be empty or whitespace only")

        # æª¢æŸ¥æƒ¡æ„è¼¸å…¥ (ç°¡å–®ç¯„ä¾‹)
        if '<script>' in v.lower():
            raise ValueError("Potential XSS attack detected")

        return v

@app.post("/predict")
def predict(input_data: TextInput):
    # è¼¸å…¥å·²ç¶“éé©—è­‰
    result = classifier(input_data.text)
    return result
```

### 8.2 API é€Ÿç‡é™åˆ¶

```python
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address

limiter = Limiter(key_func=get_remote_address)
app.state.limiter = limiter

# é™åˆ¶: æ¯åˆ†é˜ 60 æ¬¡è«‹æ±‚
@app.post("/predict")
@limiter.limit("60/minute")
def predict(request: Request, input_data: TextInput):
    result = classifier(input_data.text)
    return result
```

### 8.3 æ¨¡å‹å®‰å…¨

```python
# é¿å…æ¨¡å‹è¢«ç›œç”¨: æ·»åŠ æ°´å°
def add_watermark(model, watermark_text="iSpan NLP Project"):
    """åœ¨æ¨¡å‹ä¸­åµŒå…¥æ°´å°"""
    # é€²éšæŠ€å·§: åœ¨ç‰¹å®šå±¤æ·»åŠ éš±è—ç‰¹å¾µ
    pass

# é¿å…æ¨¡å‹æŠ•æ¯’æ”»æ“Š: é©—è­‰è¨“ç·´æ•¸æ“š
def validate_training_data(dataset):
    """æª¢æŸ¥è¨“ç·´æ•¸æ“šç•°å¸¸"""
    # æª¢æŸ¥æ¨™ç±¤åˆ†å¸ƒ
    label_dist = dataset['label'].value_counts()
    if label_dist.min() / label_dist.max() < 0.1:
        print("âš ï¸ Warning: Severe class imbalance detected")

    # æª¢æŸ¥ç•°å¸¸æ–‡æœ¬
    for text in dataset['text']:
        if len(text) > 10000:  # ç•°å¸¸é•·æ–‡æœ¬
            print(f"âš ï¸ Warning: Abnormally long text found")
```

---

## 9. èª²å¾Œç¸½çµ

### âœ… æ ¸å¿ƒæœ€ä½³å¯¦è¸

1. **ä»£ç¢¼å“è³ª**
   - éµå¾ª PEP 8
   - ä½¿ç”¨ Type Hints
   - å®Œæ•´éŒ¯èª¤è™•ç†
   - è©³ç´°è¨»è§£

2. **Notebook è¦ç¯„**
   - æ¸…æ™°çµæ§‹
   - Markdown èªªæ˜
   - è¦–è¦ºåŒ–çµæœ
   - ç·´ç¿’èˆ‡ç¸½çµ

3. **æ€§èƒ½å„ªåŒ–**
   - å‘é‡åŒ–æ“ä½œ
   - æ‰¹æ¬¡è™•ç†
   - GPU åŠ é€Ÿ
   - æ¨¡å‹é‡åŒ–

4. **æ¨¡å‹è¨“ç·´**
   - åŸºæº–å…ˆè¡Œ
   - é˜²æ­¢éæ“¬åˆ
   - å¯¦é©—è¿½è¹¤
   - éŒ¯èª¤åˆ†æ

5. **éƒ¨ç½²ç¶­è­·**
   - ç‰ˆæœ¬ç®¡ç†
   - A/B æ¸¬è©¦
   - ç›£æ§å‘Šè­¦
   - å®‰å…¨é˜²è­·

### ğŸ¯ æŒçºŒæ”¹é€²

- å®šæœŸ Code Review
- é—œæ³¨æœ€æ–°æŠ€è¡“
- åƒèˆ‡é–‹æºç¤¾ç¾¤
- å»ºç«‹å€‹äººçŸ¥è­˜åº«

---

**æ–‡ä»¶ç‰ˆæœ¬**: v1.0
**ç¶­è­·è€…**: iSpan NLP Team
**æˆæ¬Š**: CC BY-NC-SA 4.0
**æœ€å¾Œæ›´æ–°**: 2025-10-17
